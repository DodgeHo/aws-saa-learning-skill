---
source: 17 - Decoupling applications SQS, SNS, Kinesis, Active MQ\015 Kinesis Data Firehose Hands On_zh.srt
---

教师：好的, 让我们练习使用Kinesis

Data Firehose, 使用交付流｡

所以我点击了传送流｡ 

在这里,

我可以创建一个传输流｡

我们有一个Kinesis Data Firehose工作原理的详细图表｡

所以我们从生产者那里摄取数据, 这些生产者可以是Kinesis数据流｡

这就是我们的用例, 或者它们可以是通过Kinesis

Data Agents､ Kinesis Agents､ 其他一些AWS服务（如CloudWatch､

IoT Core､ EventBridge等）完成的直接PUT, 也可以是您自己的应用程序,

这些应用程序使用SDK可以将数据直接发送到Kinesis Data Firehose｡

因此, 一旦我们摄取数据,

我们就可以使用Lambda函数直接转换它｡

这也可以用来做很多事情, 比如转换记录格式等等｡

然后, 我们将数据加载到目标存储中｡ 

因此, 我们有Amazon S3, Amazon OpenSearch

Service, 它被重命名为ElasticSearch,

Amazon Redshift以及各种HTTP端点目的地｡

因此, 在本例中,

我们的源将是Kinesis数据流,

目标将是Amazon S3｡

但重要的是你要注意到我们有开放搜索服务｡

所以ElasticSearch, RedShift,

S3, 你需要记住这三个｡

然后我们有一些,

很多第三方服务, 所以你不需要记住他们全部或根本｡

但请记住, 它们是第三方服务｡ 

或者您也可以选择的任何自定义HTTP端点｡ 

所以我们会选择Amazon S3｡ 

现在对于源代码,

我们需要浏览并选择我们的流｡

我们在这里输入了ARN Demostream｡ 

所以这很好｡ 

然后, 交付流名称自动生成, 所以这是完美的｡

现在, 我们进入转换和转换记录部分｡ 

所以这是可选的｡ 

但是我们想使用Lambda转换源记录｡ 

所以在这里我们可以转换, 过滤,

解压缩, 转换, 处理源记录｡

因此, 这些Lambda函数只是您可以在AWS中运行的代码片段,

它们可以在Kinesis Data

Firehose交付之前对这些数据执行任何您想要的操作｡

所以这会很方便｡ 

如果您启用它,

则需要选择Lambda函数｡

好吧

接下来, 转换记录格式选项｡ 

因此, 根据您将数据发送到的位置,

根据一些高级选项将这些记录转换为Parquet或ORC可能会很有用｡

因此, 这不在范围内, 但请记住,

您可以使用Kinesis Data Firehose转换记录格式｡

现在, 这更详细地介绍了AWS的数据和分析认证｡

现在, 只需要知道在一个高层次上,

你可以转换记录格式｡

接下来, 我们需要选择目的地｡ 

因此, 我们可以选择之前创建的S3存储桶或创建一个｡

所以我已经创建了一个S3

bucket｡

所以我用这个｡ 

所以演示消防水龙带斯蒂芬V3｡ 

我会选择这一个,

但请随意创建一个桶或选择一个现有的｡

你想要动态分区吗？

所以现在, 我们要说不｡ 

S3 bucket prefix,

那么我们要给数据加上前缀吗？

现在我们不需要

还有, 如果我们需要的话, 一个存储桶错误输出前缀,

如果你想的话, 你有,

例如, 错误｡

但我们现在不想谈这个｡ 

我们会保持它非常, 非常简单｡ 

现在, 更重要的是,

围绕缓冲提示,

压缩和加密｡

因此, 缓冲区是Kinesis

Data Firehose在将记录传递到目标之前积累记录的一种方式｡

因此, 默认情况下,

Kinesis将在将数据传输到目标之前将5 MB的数据写入缓冲区,

因此Amazon S3｡

现在, 您可以使缓冲区更有效, 例如, 如果您想获得更大的缓冲区大小和更高的效率,

则可以使缓冲区超过128 MB, 或者如果您想尽快交付数据, 则可以使缓冲区大小更小｡

所以我们把它设为1.

然后是缓冲区间隔, 这是如果缓冲区大小没有填满时的速度｡

它应该以多快的速度冲入目标？

因此, 如果您选择300秒,

则需要等待5分钟来填充缓冲区大小｡

但是如果缓冲区大小在五分钟后没有被填满, 那么它仍然会被刷新｡

因此, 如果我们设置一个较低的缓冲区间隔,

例如60秒, 我们可以保证最多每60秒将缓冲区刷新到Amazon

S3中｡

如果我们设置了一个很长的缓冲区大小,

比如900秒, 那么我们需要等待, 我想, 在缓冲区被刷新到Amazon S3之前需要等待15分钟, 所以要长得多｡

所以为了这个演示的目的,

我们不想有效率｡

我们要快｡ 

所以我们选择60秒,

这是最小值｡

接下来, 我们要启用压缩和加密,

这样我们就可以压缩目标中的记录, 例如使用GZIP､

Snappy､ Zip或Hadoop兼容的Snappy｡

我们的想法是, 您将节省一些空间, 因为我们在存储到Amazon

S3之前压缩了数据｡

节省一些成本｡ 

还有, 你想加密你的记录吗？

是或不是

有一些高级设置｡ 

但你们要看到的最重要的是这里的许可｡

这将自动创建一个名为this的IAM角色｡

这个IAM角色将拥有写入Amazon

S3所需的所有权限｡

这就是Kinesis Data Firehose能够写入目标存储桶以及从Kinesis

Data Stream读取的方式｡

让我们创建这个交付流｡ 

它是活跃的｡ 

所以我们可以看看一些指标｡ 

因此, 通过Kinesis Data Firehose的数据越多,

这些指标就越多, 这对生产非常有帮助｡

你可以看看配置,

但我们已经做过了｡

然后, 我们可以查看错误日志的目的地｡ 

现在, 这是CloudWatch｡

好的, 我们这里有一个源Kinesis数据流的Kinesis

Data Firehose, 我们需要测试一些流经它的数据｡

所以你可以使用这里的测试数据来测试它进入Amazon

S3, 但我们不想使用这个｡

实际上, 因为我们有Amazon Kinesis

Data Stream, 让我们也使用它｡

这里的Kinesis数据流名为DemoStream,

我们将向它发送更多数据｡

因为即使我们已经创建了Firehose

Delivery Stream,

即使过去向Kinesis Data Stream发送了一些数据,

您实际上也需要在设置Firehose后发送新数据以使Firehose处于活动状态｡

所以让我们使用CloudShell,

我们将使用这里的命令｡

所以我们必须修改它, 并确保你有正确的流名称｡

所以DemoStream是我今天拥有的一个, 用户注册｡

这很好

然后, 粘贴此命令｡ 

我们按回车键｡ 

数据已发送｡ 

我们有用户注册｡ 

然后, 我们有用户登录｡ 

然后, 我们将让用户注销｡ 

好的, 已经发送了三张唱片｡ 

我现在能做的就是进入Amazon S3,

看看它们是否已经出现在Amazon S3中｡

让我们进入S3控制台｡ 

我要去打消防水龙带｡ 

我找到我的桶了｡ 

正如你所看到的,

目前, 我的bucket中没有对象｡

这是因为Kinesis Data Firehose有60秒的缓冲时间｡

因此, 我们需要等待60秒,

直到数据进入Amazon S3｡

所以我们就等着吧｡ 

我暂停视频｡ 

好吧, 已经超过60秒了｡ 

所以我要刷新一下｡ 

正如你所看到的,

更新已经出现在我的亚马逊S3桶中｡

所以我可以点击,

它说按日期分区等等｡

在这里, 我有记录, 所以我可以点击它｡

点击打开, 然后用我的文本编辑器打开它｡

正如你所看到的,

不是很吸引人,

但是我们在一个文本文件中有用户注册,

用户登录和用户注销｡

因此, Kinesis Data Firehose正在工作,

并且工作得很好｡

这节课就到这里｡ 

希望你喜欢｡ 

为了清理它,

请确保首先删除该交付流｡

所以你需要输入名字,

你就有了｡

然后最重要的是,

删除DemoStream本身｡

因为如果你让它运行,

那么它会花费你的钱每小时, 好吗？

这节课就到这里｡ 

希望你喜欢｡ 

我们下节课再见｡
