---
source: 22 - Data & Analytics\007 Glue_zh.srt
---

解说员：现在我们来谈谈AWS Glue｡ 

因此Glue是一个托管的提取､

转换和加载服务, 通常也称为ETL服务｡

它对于准备和转换数据以进行分析非常有用｡

所以这是一个完全无服务器的服务, 你只要提交你想要的任何东西,

它就会实现｡

例如, 假设您在S3存储桶或Amazon

RDS数据库中有数据,

并且您希望将其加载到Redshift数据仓库中｡

所以你可以用胶水把它取出来｡ 

然后, 您可以转换它,

如果需要的话, 可能会过滤一些数据或添加一些列等,

无论您想要什么｡

然后将最终的输出数据加载到Redshift数据仓库中｡

所有这些都是在Glue ETL服务中发生的｡ 

您只需要编写一些代码,

启动ETL作业, 然后就可以开始了｡

这是一个例子｡ 

另一个例子是如何将数据转换成Parquet格式.

那你为什么要这么做？

因为, 嗯, Parquet格式是一种列式数据格式, 因此,

它在使用时要好得多, 例如, 与Athena这样的服务一起使用｡

例如, 假设您正在向S3存储桶中执行插入操作,

这些文件是CSV格式的｡

然后, 您将使用Glue ETL服务导入CSV, 并在Glue服务中将其转换为Parquet格式｡

然后将其发送到输出S3桶中｡ 

而当在镶木地板格式,

然后亚马逊雅典娜是要分析这个文件在一个更好的方式｡

因此, 您可以自动执行整个过程的另一件事是,

无论何时将文件插入到S3桶中, 您都可以向Lambda函数发送事件通知, 该函数随后将触发Glue

ETL作业｡

但是您也可以用事件桥来替换Lambda函数｡

这可以作为一种替代方案｡ 

好吧, 我会的

Glue还有另一个特性,

称为Glue数据目录, 用于对数据集进行分类｡

因此, Glue Data Catalog将运行Glue

Data Crawler, 它们将连接到各种数据源,

例如Amazon S3､ Amazon RDS､ Amazon

DynamoDB或您在内部拥有的兼容JDC数据库｡

因此, Glue Data Crawler将抓取这些数据库,

并将表､ 列､ 数据类型等的所有元数据写入Glue

Data Catalog｡

因此, 它将拥有所有的数据库､ 表和元数据,

Glue作业将利用这些数据库､

表和元数据来执行ETF｡

现在, 当您在后台使用Amazon Athena执行数据发现和SQL发现时,

Amazon Athena将利用AWS

Glue数据目录｡

亚马逊红移光谱也会如此｡ 

亚马逊电子病历也会如此｡ 

因此, 正如您所看到的, Glue Data Catalog服务是许多其他AWS服务的核心｡

因此, 其他功能, 可以出现在考试上的胶水,

你应该知道在一个高水平, 第一个是胶水工作书签｡

这是为了防止在运行新的ETL作业时重新处理所有数据｡

所以这是非常重要的,

它可以在考试中出现｡

然后, 您就有了粘附弹性视图｡ 

这就是使用SQL跨多个数据存储区组合和复制数据｡

例如, 您可以创建一个跨RDS数据库､

Aura数据库和Amazon

S3的视图｡

因此没有自定义代码｡ 

Glue将监视源数据中的更改, 并且它将是无服务器的｡

因此, 通过这种方式, 您将创建一个虚拟表,

它是一个跨多个数据存储区的不成熟视图｡

您有Glue DataBrew, 它用于使用预构建的转换来清理和规范化数据｡

您有Glue Studio, 它是一个GUI, 用于在Glue中创建､

运行和监视ETL作业｡

然后是Glue Streaming ETL,

它实际上构建在Apache

Spark Structured Streaming之上,

您可以将其作为流作业运行, 而不是运行ETL作业（如您所知的批处理作业）｡

因此, 您可以使用Glue Streaming

ETL从Kinesis数据流或Kafka或MSK读取数据, 我们将看到哪一个是AWS上管理的Kafka｡

好吧, 我会的

这节课就讲到这里｡ 

希望你喜欢｡ 

我们下节课再见｡
