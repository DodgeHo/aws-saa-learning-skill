SAA-C03     1019Q

1.A company collects data for temperature, humidity, and atmospheric pressure in cities across multiple continents. The average volume of data that the company collects from each site daily is 500 GB. Each site has a high-speed Internet connection. The company wants to aggregate the data from all these global sites as quickly as possible in a single Amazon S3 bucket. The solution must minimize operational complexity. Which solution meets these requirements?

A ��Turn on S3 Transfer Acceleration on the destination S3  bucket.  Use  multipart  uploads to directly  upload site data to the destination S3 bucket.

B ��Upload the data from each site to an S3 bucket in the closest Region. Use S3 Cross-Region Replication to copy objects to the destination S3 bucket. Then remove the data from the origin S3 bucket.

C ��Schedule AWS Snowball Edge Storage Optimized device jobs daily to transfer data from each site to the closest Region. Use S3 Cross-Region Replication to copy objects to the destination S3 bucket.

D ��Upload the data from each site to an Amazon EC2 instance in the closest Region. Store the data in an Amazon Elastic Block Store (Amazon EBS) volume. At regular intervals, take an EBS snapshot and copy it to the Region that contains the destination S3 bucket. Restore the EBS volume in that Region.

�𰸣�A

���� �� Correct Answer A �� Turn on S3 Transfer Acceleration on the destination S3 bucket. Use multipart uploads to directly upload site data to the destination S3 bucket. Detailed Explanation The company needs to aggregate 500 GB daily per site from global locations into a single S3 bucket as quickly as possible while minimizing operational complexity. - Option A uses S3 Transfer Acceleration, which optimizes long - distance uploads by routing data through AWS ��s Cloud front edge locations. This reduces latency and speeds  up transfers,  especially for geographically distant sites.  Multipart  uploads  further  improve  reliability  and speed for large files (like 500 GB daily data). This solution requires no intermediate steps (e.g., replication, manual jobs) and is fully managed by AWS, minimizing operational overhead. - Option B (cross - region replication) adds delays because data must first  upload to  a  regional  bucket,  then  replicate  to the  destination.  It  also  requires  managing  replication  rules  and  cleanup, increasing complexity. - Option C  (Snowball  Edge)  is  unnecessary  here  because the  sites  already  have  high -  speed  internet. Snowball is designed for slow/no connectivity or massive data volumes (petabytes), not daily 500 GB transfers. - Option D (EC2 + EBS snapshots) introduces extra steps (snapshots, copying, restoring) and higher costs, making it slow and complex. Why A is best: Transfer Acceleration + multipart uploads provide the fastest and simplest way to upload large data directly to a central S3 bucket  globally.  Reference   Links  -   [S3  Transfer  Acceleration](https://aws.amazon.com/s3/transfer-acceleration/)  -   [Multipart Uploads](https://docs.aws.amazon.com/AmazonS3/latest/user guide/mpu overview.html)

2.A company needs the ability to analyze the log files of its proprietary application. The logs are stored in JSON format in an Amazon S3  bucket. Queries will  be simple and will  run on-demand. A solutions architect  needs to  perform the analysis with minimal changes to the existing architecture. What should the solutions architect do to meet these requirements with the LEAST amount of operational overhead?

A ��Use Amazon Redshift to load all the content into one place and run the SQL queries as needed.

B ��Use Amazon Cloud watch Logs to store the logs. Run SQL queries as needed from the Amazon Cloud watch console. C ��Use Amazon Athena directly with Amazon S3 to run the queries as needed.

D ��Use AWS Glue to catalog the logs. Use a transient Apache Spark cluster on Amazon EMR to run the SQL queries as needed.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The best choice is Amazon Athena because it allows direct querying of JSON data stored in Amazon S3 without moving or transforming the data. Here ��s why: - Minimal Changes: Since the logs are already in S3, Athena works directly on the existing data (no data  migration  required). - On-Demand   �� Serverless:  Athena  uses  SQL and charges per query, eliminating the need to manage servers or clusters (unlike EMR/Redshift). - JSON Support: Athena natively queries  JSON  files,   reducing  setup  complexity.  -   Low  Operational  Overhead:   No   infrastructure   provisioning,  scaling,  or maintenance is needed. Other options add unnecessary steps: - A (Redshift): Requires loading data into a data warehouse, which is overkill for simple queries. - B (Cloud watch Logs): Requires moving data out of S3 and offers limited query flexibility. - D (Glue + EMR): Adds complexity with data cataloging and cluster management, even if transient. Reference Links: - [Amazon Athena Overview](https://aws.amazon.com/athena/)                              -                              [Querying                              JSON                              in
Athena](https://docs.aws.amazon.com/athena/latest/ug/parsing-JSON.html)

3.A company uses AWS Organizations to manage multiple AWS accounts for different departments. The management account has an Amazon S3 bucket that contains project reports. The company wants to limit access to this S3 bucket to only users of accounts within the organization in AWS Organizations. Which solution  meets these  requirements with the  LEAST amount of operational overhead?

A ��Add the aws Principal orgid global condition key with a reference to the organization ID to the S3 bucket policy.

B ��Create an organizational unit (OU) for each department. Add the aws:Principal org paths global condition key to the S3 bucket policy.

C    �� Use    AWS     Cloud trail    to     monitor     the     Create account,    Invite account to organization,     Leave organization,     and Remove account from organization events. Update the S3 bucket policy accordingly.

D ��Tag each user that needs access to the S3 bucket. Add the aws:Principal tag global condition key to the S3 bucket policy.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution is to use the `aws:Principal orgid` global condition key in the S3 bucket policy. This key checks if the requesting account is part of the specified AWS Organizations organization. By referencing the organization  ID, the policy automatically grants access to all current and future accounts within the organization without manual  updates.  This  approach  requires  minimal  operational  overhead  because  it  leverages  AWS  Organizations'   built  -  in integration,  eliminating the  need  for  tracking  account  changes  (Option  C),  managing  OU  paths  (Option  B),  or  tagging  users (Option                         D).                          Reference                          Link:                         [AWS                          Global                         Condition
Keys](https://docs.aws.amazon.com/IAM/latest/User guide/reference_policies_condition-keys.html)

4.An application runs on an Amazon  EC2  instance in a VPC. The application processes  logs that are stored  in an Amazon S3 bucket. The EC2 instance needs to access the S3 bucket without connectivity to the internet. Which solution will provide private network connectivity to Amazon S3?

A ��Create a gateway VPC endpoint to the S3 bucket.

B ��Stream the logs to Amazon Cloud watch Logs. Export the logs to the S3 bucket.

C ��Create an instance profile on Amazon EC2 to allow S3 access.

D ��Create an Amazon API Gateway API with a private link to access the S3 endpoint.

�𰸣�A

���� �� Correct Answer: A Detailed Explanation: To allow an EC2 instance in a VPC to privately access an S3 bucket without internet connectivity, a Gateway VPC Endpoint is the correct solution. Here's why: - Gateway VPC Endpoints are AWS-managed virtual devices that enable private connectivity between your VPC and AWS services like S3. Traffic stays entirely within the AWS network, avoiding the public internet. - It works by creating a route in your VPC route table that directs S3 traffic through the endpoint (e.g., vpce -  123abc),  bypassing the  need for an  Internet Gateway or  NAT device. - This  is simpler and  more cost - effective than alternatives like NAT Gateways or VPNs. - Other options: - (B) Cloud watch Logs requires additional permissions and still needs S3 access. - (C) Instance profiles handle permissions (IAM roles), not network connectivity. - (D) API Gateway with Private link    is     unnecessary     complexity,     as    S3     natively     supports    VPC     endpoints.     Reference     Links:     [AWS     VPC Endpoints](https://docs.aws.amazon.com/vpc/latest/private link/vpce            -            gateway.html)            [AWS            S3            VPC
Endpoints](https://docs.aws.amazon.com/AmazonS3/latest/user guide/private link - interface - endpoints.html)

5.A company is hosting a web application on AWS using a single Amazon EC2 instance that stores user-uploaded documents in an Amazon EBS volume. For better scalability and availability, the company duplicated the architecture and created a second EC2 instance and EBS volume in another Availability Zone, placing both behind an Application Load Balancer. After completing this change, users reported that, each time they refreshed the website, they could see one subset of their documents or the other, but never all of the documents at the same time. What should a solutions architect propose to ensure users see all of their documents at once?

A ��Copy the data so both EBS volumes contain all the documents

B ��Configure the Application Load Balancer to direct a user to the server with the documents

C ��Copy the data from both EBS volumes to Amazon EFS. Modify the application to save new documents to Amazon EFS

D ��Configure the Application Load Balancer to send the request to both servers. Return each document from the correct server�𰸣�C
���� �� Correct Answer: C Detailed Explanation: The issue arises because the two EC2 instances use separate EBS volumes, so user-uploaded  documents  are  stored  locally  on  each  instance's  EBS.  When  the  load  balancer  routes  requests  to  different instances, users see documents only from the EBS volume of the instance they hit. - Option C solves this by using Amazon EFS, a shared network file system. Both EC2 instances can mount the same EFS volume, ensuring all documents are stored in a central location. This way, no matter which instance serves the request, the application accesses the same set of files. Migrating existing data to EFS and updating the app to save new documents there ensures consistency. - Why other options fail: - A: Copying data between EBS volumes manually is error-prone and doesn ��t scale. New uploads would still be stored locally, causing the same issue. - B: Sticky sessions (routing a user to the same instance) reduce availability. If one instance fails, the user loses access to their  documents. -  D:  ALB  cannot send  requests  to  multiple  servers  simultaneously. This  would  cause  duplication or  errors.
Reference Link: [Amazon EFS Overview](https://aws.amazon.com/efs/)

6.A company uses NFS to store large video files in on-premises network attached storage. Each video file ranges in size from 1 MB to 500 GB. The total storage is 70 TB and is no longer growing. The company decides to migrate the video files to Amazon S3. The company must migrate the video files as soon as possible while using the least possible network bandwidth. Which solution will meet these requirements?

A��Create an S3 bucket. Create an IAM role that has permissions to write to the S3 bucket. Use the AWS CLI to copy all files locally

to the S3 bucket.

B ��Create an AWS Snowball Edge job. Receive a Snowball Edge device on premises. Use the Snowball Edge client to transfer data to the device. Return the device so that AWS can import the data into Amazon S3.

C �� Deploy an S3 File Gateway on premises. Create a public service endpoint to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the new file share to the S3 bucket. Transfer the data from the existing NFS file share to the S3 File Gateway.

D �� Set  up  an AWS  Direct  Connect  connection  between  the  on-premises  network  and  AWS.  Deploy  an  S3  File  Gateway  on premises. Create a public virtual interface (V IF) to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the new file share to the S3 bucket. Transfer the data from the existing NFS file share to the S3 File Gateway.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The company needs to migrate 70 TB of large video files to Amazon S3 quickly while  minimizing  network  usage.  -  Option  A  (AWS  CLI)  would  use  the  internet  to  upload  70  TB  directly,  which  is  slow  and consumes massive bandwidth. - Option B (Snowball Edge) uses a physical device to transfer data offline. Snowball is designed for large-scale data migration, avoids internet bottlenecks, and is faster for 70 TB. - Option C (S3 File Gateway) requires transferring all data over the internet, which is bandwidth-heavy and slower. - Option D (Direct Connect + S3 File Gateway) reduces latency but still transfers 70 TB over the network, and setting up Direct Connect takes weeks, delaying migration. Why B is best: Snowball Edge bypasses internet limitations entirely, ensuring the fastest transfer with zero network bandwidth usage. AWS handles the physical        import         to        S3         after        you         return        the         device.         Reference         Links:        [AWS         Snowball Edge](https://aws.amazon.com/snowball/edge/)                          [Large                           Data                           Migration                          to
AWS](https://aws.amazon.com/blogs/aws/migrate-petabytes-of-data-to-aws-with-aws-snowball/)

7.A company has an application that ingests incoming messages.  Dozens of other applications and micro services then quickly consume these messages. The number of messages varies drastically and sometimes increases suddenly to 100,000 each second. The company wants to decouple the solution and increase scalability. Which solution meets these requirements?

A �� Persist  the  messages  to  Amazon  Kinesis  Data  Analytics.  Configure  the  consumer  applications  to  read  and  process  the messages.
B �� Deploy the ingestion application on Amazon EC2 instances in an Auto Scaling group to scale the number of EC2 instances based on CPU metrics.

C �� Write the  messages  to  Amazon  Kinesis  Data  Streams  with  a  single  shard.  Use  an  AWS  Lambda  function  to  preprocess messages and store them in Amazon Dynamo db. Configure the consumer applications to read from Dynamo db to process the
messages.

D �� Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with multiple Amazon Simple Queue Service (Amazon SOS) subscriptions. Configure the consumer applications to process the messages from the queues.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The best solution here is D because it uses Amazon SNS and SQS to decouple the system and handle high scalability. Here's why: 1. Decoupling   �� Fan-Out: - Amazon SNS acts as a pub/sub (publisher/subscriber) service. When messages are published to an SNS topic, they can be automatically delivered to multiple subscribed SQS queues. -

Each consumer application (or micro service) gets its own SQS queue, allowing them to process messages independently at their own  pace. This  ensures  no  single  consumer  blocks  others,  achieving  true  decoupling.  2. Scalability: -  SNS  and SQS  are fully managed and serverless, meaning they automatically scale to handle sudden spikes (like 100,000 messages/sec) without manual intervention. - SQS queues buffer messages, so even if consumers are temporarily slow, messages won ��t be lost. 3. Why Other Options Fail: - A: Kinesis Data Analytics is for real-time analytics, not decoupled message distribution. - B: Scaling EC2 instances based on CPU won ��t decouple producers and consumers. It ��s also harder to manage sudden spikes. - C: A single-shard Kinesis stream can ��t handle 100,000 messages/sec (shards have throughput limits). Dynamo db isn ��t designed for real-time message streaming         to          dozens          of          consumers.          Reference           Links:          -          [Amazon          SNS          Fanout          to SQS](https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html)                                         -                                           [SQS
Scalability](https://aws.amazon.com/sqs/features/#Scalability_and_Elasticity)

8.A  company  is  migrating  a  distributed  application  to  AWS.  The  application  serves  variable  workloads.  The  legacy  platform consists  of  a  primary  server  that  coordinates  jobs  across  multiple  compute  nodes.  The  company  wants  to  modernize  the application with a solution that maximizes resiliency and scalability. How should a solutions architect design the architecture to meet these requirements?

A��Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure EC2 Auto Scaling to use scheduled scaling.

B��Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon  EC2 instances that are  managed in an Auto Scaling group. Configure EC2 Auto Scaling based on the size of the
queue.

C��Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure AWS Cloud trail as a destination for the jobs. Configure EC2 Auto Scaling based on the load on the primary server.

D��Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure Amazon Event bridge (Amazon Cloud watch Events) as a destination for the jobs. Configure EC2 Auto Scaling based on the load on the compute nodes.

�𰸣�B

������ Correct Answer B Detailed Explanation The correct answer is B because it uses Amazon SQS (a managed message queue service) to decouple the primary server from compute nodes, ensuring resiliency and scalability. Here ��s why: - Amazon SQS acts as a buffer for jobs, allowing the primary server to send jobs to the queue without needing to track compute  node availability. This decoupling ensures resiliency (no single point of failure) and handles variable workloads. - EC2 Auto Scaling based on the queue size dynamically adjusts the number of compute nodes. If the queue grows (more jobs), Auto Scaling adds EC2 instances. If the queue shrinks (fewer jobs), it removes instances. This matches capacity to demand, optimizing scalability and cost. Other options are less optimal: - A uses scheduled scaling, which assumes predictable workloads (not "variable"). - C incorrectly uses Cloud trail (an auditing tool) as a job destination. - D relies on Event bridge (event routing) instead of a proper queue and scales based    on     compute     node     load,    which      might    lag     behind     actual     job    demand.     Reference     Links     -     [Amazon SQS](https://aws.amazon.com/sqs/) - [EC2 Auto Scaling](https://aws.amazon.com/ec2/auto scaling/)

9.A company is running an SMB file server in its data center. The file server stores large files that are accessed frequently for the first few days after the files are created. After 7 days the files are rarely accessed. The total data size is increasing and is close to the company's total storage capacity. A solutions architect must increase the company's available storage space without losing low-latency access to the most recently accessed files. The solutions architect must also provide file lifecycle management to avoid future storage issues. Which solution will meet these requirements?

A ��Use AWS Data sync to copy data that is older than 7 days from the SMB file server to AWS.

B ��Create an Amazon S3 File Gateway to extend the company's storage space. Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7 days.

C ��Create an Amazon FSx for Windows File Server file system to extend the company's storage space.

D ��Install a utility on each user's computer to access Amazon S3. Create an S3 Lifecycle policy to transition the data to S3 Glacier Flexible Retrieval after 7 days.

�𰸣�B

������ Correct Answer B Detailed Explanation The best solution is Amazon S3 File Gateway combined with an S3 Lifecycle policy. Here ��s why:  1.  Extend On - Premises Storage: - S3  File Gateway acts as a virtual file server, seamlessly integrating with the existing SMB file server. It stores frequently accessed ("hot") data locally (on - premises) for low - latency access, while older data is automatically tiered to Amazon S3 in the cloud. This frees up on - premises storage capacity without disrupting user workflows.
2. Lifecycle Management: - A Lifecycle policy can transition files older than 7 days from Amazon S3 Standard to S3 Glacier Deep Archive, the cheapest storage class for rarely accessed data. This ensures cost savings and prevents future storage overflows. 3. Low - Latency Access: - Recent files (accessed within 7 days) remain cached locally on the File Gateway, ensuring fast access. Older files  are  still accessible via  S3  but  with  slightly  higher  latency,  which  aligns  with the  requirement that files  are  rarely accessed after 7 days. Why Other Options Fail: - A (Data sync): Copies data to AWS but doesn ��t free up on - premises storage or manage  lifecycle  policies  automatically. -  C  (FSx for  Windows):  Replaces  the  on -  premises file  server  entirely, which  isn ��t required here. It also lacks automated tiering to Glacier. - D (User Utilities + S3): Forces users to change how they access files (via S3  instead  of  SMB)  and  doesn  �� t  guarantee  low  -  latency  access  for  recent  files.   Reference   Links  -  [Amazon  S3   File Gateway](https://aws.amazon.com/storage gateway/file/s3/)                      -                      [Amazon                      S3                      Lifecycle Policies](https://docs.aws.amazon.com/AmazonS3/latest/user guide/object - lifecycle - mgmt.html)

10.A company is building an ecommerce web application on AWS. The application sends information about new orders to an Amazon API Gateway REST API to process. The company wants to ensure that orders are processed in the order that they are received. Which solution will meet these requirements?

A ��Use an API Gateway integration to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic when the application receives an order. Subscribe an AWS Lambda function to the topic to perform processing.

B ��Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FI FO queue when the application receives an order. Configure the SQS FI FO queue to invoke an AWS Lambda function for processing.

C ��Use an API Gateway authorizer to block any requests while the application processes an order.

D ��Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) standard queue when the application receives an order. Configure the SQS standard queue to invoke an AWS Lambda function for processing.

�𰸣�B

������Correct Answer: B Detailed Explanation: The correct solution is to use Amazon SQS FI FO queue because it guarantees that messages are processed in the exact order they are received. Here ��s why: - Option A (SNS): Amazon SNS does not guarantee message ordering. Subscribers might process messages out of order, making it unsuitable for sequential processing. - Option B (SQS F IFO): SQS F IFO queues ensure strict first - in - first - out processing. When integrated with Lambda via an event source mapping,  Lambda  processes  messages  in  the  order  they  arrive,  meeting  the  requirement.  -  Option  C  (Authorizer):  Blocking

requests during processing doesn ��t address message ordering and would harm scalability. - Option D (SQS Standard): Standard queues offer "best - effort" ordering, which can lead to out - of - order processing. For a beginner: Think of SQS FI FO queues like a line at a coffee shop �� each customer (message) is served in the exact order they joined the line. This ensures fairness and predictability, which is critical for tasks like processing e - commerce orders sequentially. Reference Link: [Using AWS Lambda with          Amazon           SQS](https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html)            [Amazon           SQS           FIFO Queues](https://aws.amazon.com/sqs/features/fifo/)

11.A company has an application that runs on Amazon EC2 instances and uses an Amazon Aurora database. The EC2 instances connect to the database by using user names and passwords that are stored locally in a file. The company wants to minimize the operational overhead of credential management. What should a solutions architect do to accomplish this goal?

A ��Use AWS Secrets Manager. Turn on automatic rotation.

B ��Use AWS Systems Manager Parameter Store. Turn on automatic rotation.

C��Create an Amazon S3 bucket to store objects that are encrypted with an AWS Key Management Service (AWS KMS) encryption key. Migrate the credential file to the S3 bucket. Point the application to the S3 bucket.

D ��Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume for each EC2 instance. Attach the new EBS volume to each EC2 instance. Migrate the credential file to the new EBS volume. Point the application to the new EBS volume.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution is to use AWS Secrets Manager with automatic rotation. Here's why: 1. Problem: Storing database credentials in local files creates security risks and management overhead (manual updates, rotation, etc.) 2. Secrets Manager Benefits: - Stores credentials securely (encrypted at rest) - Automatically rotates credentials on a schedule (no manual work) - EC2 instances can retrieve credentials programmatically via API - Integrates well with RDS/Aurora databases 3. Other Options Explained: - B: Parameter Store can store secrets, but doesn't offer built - in automatic rotation for database credentials like Secrets Manager does - C/D: Moving credentials to S3/EBS just changes storage location without solving rotation/management challenges For a beginner: Imagine Secrets Manager as a secure vault that not only stores your passwords safely,  but  automatically  changes  them  regularly  like  a  robot  assistant.  Your  EC2  instances  just  ask  the  vault  for  the  latest password            when             needed,             eliminating            manual             password             updates.             Reference            Link: https://aws.amazon.com/secrets-manager/features/#Secret_rotation

12.A global company hosts its web application on Amazon EC2 instances behind an Application Load Balancer (ALB). The web application has static data and dynamic data. The company stores its static data in an Amazon S3 bucket. The company wants to improve  performance and  reduce  latency for the static data and dynamic data. The company is  using  its own domain  name registered with Amazon Route 53. What should a solutions architect do to meet these requirements?

A��Create an Amazon Cloud front distribution that has the S3 bucket and the ALB as origins. Configure Route 53 to route traffic to the Cloud front distribution.

B �� Create  an  Amazon  Cloud front  distribution  that  has  the  ALB  as  an  origin.  Create  an  AWS  Global  Accelerator  standard accelerator that has the S3 bucket as an endpoint Configure Route 53 to route traffic to the Cloud front distribution.

C ��Create an Amazon Cloud front distribution that has the S3 bucket as an origin. Create an AWS Global Accelerator standard accelerator that has t he ALB and the Cloud front distribution as endpoints. Create a custom domain  name that points to the accelerator DNS name. Use the custom domain name as an endpoint for the web application.

D �� Create  an  Amazon  Cloud front  distribution  that  has  the  ALB  as  an  origin.  Create  an  AWS  Global  Accelerator  standard accelerator that has the S3 bucket as an endpoint. Create two domain names. Point one domain name to the Cloud front DNS name for dynamic content. Point the other domain name to the accelerator DNS name for static content. Use the domain names as endpoints for the web application.

�𰸣�A

������Correct Answer: A Detailed Explanation: To improve performance and reduce latency for both static and dynamic content,  the  best approach  is to  use Amazon Cloud front. Cloud front is a Content  Delivery  Network  (CDN)  that caches static content  (stored in S3) at edge locations globally, ensuring faster delivery. For dynamic content (served by the ALB/EC2), Cloud front can  route requests to the ALB with optimized network paths, reducing latency. By configuring Cloud front with both the S3 bucket  (for static data) and ALB (for dynamic data) as origins, all traffic is routed through Cloud front. Route 53 then directs the domain �� s traffic to the Cloud front distribution, simplifying DNS management and ensuring users connect to the nearest edge location. This setup avoids unnecessary complexity (like Global Accelerator) and ensures a unified, efficient solution. Reference Links: -  [Amazon                                                                                                                                                                                                              Cloud front
Origins](https://docs.aws.amazon.com/Amazon cloud front/latest/Developer guide/Download dists3And custom origins.html)      - [Using                                                                                                  Cloud front                                                                                                   with
ALB](https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-cloud front-integration-with-application-load-bal ancer/)

13.A  company  performs  monthly  maintenance  on  its  AWS  infrastructure.  During  these  maintenance  activities,  the  company needs to rotate the credentials for its Amazon RDS for MySQL databases across multiple AWS Regions. Which solution will meet these requirements with the LEAST operational overhead?

A �� Store the  credentials  as  secrets  in  AWS  Secrets  Manager.  Use  multi-Region  secret  replication  for  the  required  Regions. Configure Secrets Manager to rotate the secrets on a schedule.

B �� Store the credentials as secrets in AWS Systems  Manager by creating a secure string parameter. Use  multi-Region secret replication for the required Regions. Configure Systems Manager to rotate the secrets on a schedule.

C��Store the credentials in an Amazon S3 bucket that has server-side encryption (SSE) enabled. Use Amazon Event bridge (Amazon Cloud watch Events) to invoke an AWS Lambda function to rotate the credentials.

D ��Encrypt the credentials as secrets by using AWS Key Management Service (AWS KMS) multi-Region customer managed keys. Store the secrets in an Amazon Dynamo db global table. Use an AWS Lambda function to retrieve the secrets from Dynamo db. Use t he RDS API to rotate the secrets.

�𰸣�A

������ Correct Answer: A Detailed Explanation: AWS Secrets Manager is specifically designed to manage secrets like database credentials securely. It offers built-in  rotation functionality that can automatically update credentials on a schedule, reducing manual effort. Multi-Region replication allows the same secret to be replicated across multiple AWS Regions, ensuring all RDS instances use the latest credentials without manual updates. Other options (B, C, D) either lack native rotation support, require custom  code,  or   involve   more  complex  setups,   leading  to   higher   operational  overhead.   Reference  Links:  -   [AWS  Secrets Manager](https://aws.amazon.com/secrets-manager/)                                                          -                                                           [Rotating
Secrets](https://docs.aws.amazon.com/secrets manager/latest/user guide/rotating-secrets.html)                 -                 [Multi-Region
Secrets](https://docs.aws.amazon.com/secrets manager/latest/user guide/create-manage-multi-region-secrets.html)

14.A company runs an ecommerce application on Amazon EC2 instances behind an Application Load Balancer. The instances run

in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales based on CPU utilization metrics. The ecommerce application stores the transaction data in a MySQL 8.0 database that is hosted on a large EC2 instance. The database's performance degrades quickly as application load increases. The application handles more read requests than write  transactions.  The  company  wants  a  solution  that  will  automatically  scale  the  database  to  meet  the  demand  of unpredictable read workloads while maintaining high availability. Which solution will meet these requirements?

A ��Use Amazon Redshift with a single node for leader and compute functionality.

B ��Use Amazon RDS with a Single-AZ deployment Configure Amazon RDS to add reader instances in a different Availability Zone. C ��Use Amazon Aurora with a Multi-AZ deployment. Configure Aurora Auto Scaling with Aurora Replicas.

D ��Use Amazon Elastic ache for Memcached with EC2 Spot Instances.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The best solution is Amazon Aurora with Multi-AZ deployment and Aurora Auto Scaling because: 1. High Availability: Aurora Multi-AZ ensures automatic failover to a standby replica if the primary database fails, maintaining uptime. 2. Read Scaling: Aurora Replicas handle read traffic, reducing load on the primary database. The application has more reads than writes, so distributing read requests across replicas improves performance. 3. Automatic Scaling: Aurora Auto Scaling dynamically adjusts the number of replicas based on workload, which is critical for unpredictable read traffic. 4. Compatibility: Aurora is MySQL-compatible, so migrating from MySQL 8.0 is straightforward. Why other options are incorrect: - A (Redshift):  Redshift  is for analytics,  not transactional workloads. Single-node setups  lack  high availability. -  B (RDS Single-AZ): Single-AZ deployments risk downtime during failures. Adding read replicas  manually doesn ��t  provide automatic scaling. -  D (Elastic ache): Memcached is a caching layer, not a database. It doesn ��t scale reads or ensure high availability for the database itself.                                             Reference                                             Link:                                             [Aurora                                             Auto
Scaling](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/Aurora.Integrating.Auto scaling.html)

15.A company recently migrated to AWS and wants to implement a solution to protect the traffic that flows in and out of t he production VPC. The company had an inspection server in its on-premises data center. The inspection server performed specific operations such as traffic flow inspection and traffic filtering. The company wants to have the same functionalities in the AWS Cloud. Which solution will meet these requirements?

A ��Use Amazon Guard duty for traffic inspection and traffic filtering in the production VPC.

B ��Use Traffic Mirroring to mirror traffic from the production VPC for traffic inspection and filtering.

C ��Use AWS Network Firewall to create the required rules for traffic inspection and traffic filtering for the production VPC.

D ��Use AWS Firewall Manager to create the required rules for traffic inspection and traffic filtering for the production VPC.

�𰸣�C

������Correct Answer C ��Use AWS Network Firewall to create the required rules for traffic inspection and traffic filtering for the production VPC. Detailed Explanation AWS Network Firewall is a managed firewall service designed to inspect and filter traffic at the VPC level. It allows you to define custom rules (like stateful inspection, intrusion detection/prevention, and web filtering) to monitor and control traffic entering or leaving the VPC. This directly replicates the functionality of the on - premises inspection server, which  performed  traffic  inspection  and filtering. Why  not A? Amazon  Guard duty  is  a threat  detection  service  (using machine  learning to analyze logs), but it does not actively filter or block traffic. Why not B? Traffic  Mirroring copies network

traffic for analysis  (e.g., to a third -  party tool)  but  does  not filter  or  block traffic. Why  not  D?  Firewall  Manager  centralizes firewall rule  management across  multiple AWS accounts/VPCs  but  relies  on other services  (like Network  Firewall or WAF) to perform the actual inspection/filtering.  For a single VPC  needing active traffic inspection and filtering, AWS  Network  Firewall (Option C) is the simplest and most direct solution. Reference Links - [AWS Network Firewall](https://aws.amazon.com/network - firewall/)         -         [AWS         Network         Firewall         vs.         Other         Services](https://docs.aws.amazon.com/network          - firewall/latest/developer guide/how - it - works.html)

16.A company hosts a data lake on AWS. The data lake consists of data in Amazon S3 and Amazon  RDS for  Postgresql. The company needs a reporting solution that provides data visualization and includes all the data sources within the data lake. Only the company's  management team should have full access to all the visualizations. The rest of t he company should have only limited access. Which solution will meet these requirements?

A �� Create  an  analysis  in Amazon  Quick sight. Connect  all the  data sources and  create  new  datasets.  Publish  dashboards to visualize the data. Share the dashboards with the appropriate IAM roles.

B �� Create  an  analysis  in  Amazon Quick sight. Connect  all the  data sources  and create  new  datasets.  Publish  dashboards  to visualize the data. Share the dashboards with the appropriate users and groups.

C��Create an AWS Glue table and crawler for the data in Amazon S3. Create an AWS Glue extract, transform, and load (ETL) job to produce reports. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.

D ��Create an AWS Glue table and crawler for the data in Amazon S3. Use Amazon Athena Federated Query to access data within Amazon  RDS  for  Postgresql.  Generate  reports  by  using  Amazon  Athena.  Publish  the  reports  to  Amazon  S3.  Use  S3  bucket policies to limit access to the reports.

�𰸣�B

������ Correct Answer B Detailed Explanation Amazon Quick sight is AWS ��s business intelligence (BI) service designed for data visualization and reporting. Here ��s why option B is correct:  1. Connect All Data Sources: Quick sight natively integrates with Amazon S3 and Amazon RDS for Postgresql (and other AWS data sources). You can create datasets directly from these sources. 2. Access Control: - The management team can be granted full access to dashboards (e.g., view, edit, share) by sharing dashboards with their user accounts or SSO groups. - Other employees can be given limited access (e.g., view-only) by sharing dashboards with specific users/groups and restricting permissions. 3. IAM Roles vs. Users/Groups: - Option A suggests sharing via IAM roles, which is less common for user-level access in Quick sight. IAM roles are typically used for AWS services (e.g.,  EC2, Lambda). - Option B uses users and groups, which aligns with Quick sight ��s built-in sharing model. You can map AWS IAM Identity Center (SSO) groups or individual users to control permissions directly. Options C and D use AWS Glue and Athena for generating reports but  lack  interactive  dashboards  and  granular  user-level  access  control.  S3  bucket  policies  only  manage  file-level  access,  not dashboard               permissions.                 Reference                Links               -                 [Amazon               Quick sight                Dashboard Sharing](https://docs.aws.amazon.com/quick sight/latest/user/sharing-dashboards.html)              -               [Quick sight              Data Sources](https://docs.aws.amazon.com/quick sight/latest/user/supported-data-sources.html)

17.A company  is  implementing a  new  business  application. The application  runs on two Amazon  EC2  instances  and  uses  an Amazon S3 bucket for document storage. A solutions architect needs to ensure that the EC2 instances can access the S3 bucket. What should the solutions architect do to meet this requirement?

A ��Create an IAM role that grants access to the S3 bucket. Attach the role to the EC2 instances.

B ��Create an IAM policy that grants access to the S3 bucket. Attach the policy to the EC2 instances.

C ��Create an IAM group that grants access to the S3 bucket. Attach the group to the EC2 instances.

D ��Create an IAM user that grants access to the S3 bucket. Attach the user account to the EC2 instances.

�𰸣�A

������ Correct Answer: A Detailed Explanation: To securely allow EC2 instances to access an S3 bucket, you should use an IAM role. Here's why: 1. IAM roles are designed to grant temporary permissions to AWS services (like EC2). When you attach a role to an EC2 instance, it automatically provides temporary credentials to access other AWS resources like S3. 2. Option A is correct because it uses the proper AWS security pattern: create a role with an S3 - access policy, then attach it to EC2 instances. 3. Other options are  incorrect: -  B:  Policies  can't  be  directly  attached to  EC2  instances  (must  be  attached  to  roles/users/groups).  - C: Groups manage IAM users, not EC2 instances. - D: IAM user credentials (long - term keys) shouldn't be stored on EC2 instances due          to          security           risks.           Reference           Link           [          [AWS           Documentation:           IAM           Roles          for EC2](https://docs.aws.amazon.com/AWSEC2/latest/User guide/iam - roles - for - amazon - ec2.html)

18.A company has a three-tier web application that is deployed on AWS. The web servers are deployed in a public subnet in a VPC. The application servers and database servers are deployed in private subnets in the same VPC. The company has deployed a third-party virtual firewall appliance from AWS Marketplace in an inspection VPC. The appliance is configured with an IP interface that can accept IP packets. A solutions architect needs to integrate the web application with the appliance to inspect all traffic to the  application   before  the  traffic   reaches  the  web  server.  Which  solution  will   meet  these   requirements  with  the   LEAST operational overhead?

A ��Create a Network Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.

B ��Create an Application Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.

C �� Deploy  a  transit  gateway  in  the  inspection Vp configure  route  tables  to  route the  incoming  packets  through  the transit gateway.
D ��Deploy a Gateway Load Balancer in the inspection VPC. Create a Gateway Load Balancer endpoint to receive the incoming packets and forward the packets to the appliance.

�𰸣�D

������ Correct Answer D Detailed Explanation The best solution is D because using a Gateway Load Balancer (GWLB) simplifies traffic inspection with minimal operational overhead. Here's why: - Gateway Load Balancer (GWLB) is designed specifically for deploying third - party security appliances (like firewalls). It uses the GENEVE protocol to handle traffic forwarding and scales automatically. - GWLB Endpoint acts as a single entry/exit point in the application ��s VPC. Incoming traffic is routed to the GWLB in the inspection VPC, where the firewall inspects it. After inspection, traffic is sent back to the application ��s web servers. - This setup  requires  minimal  manual  routing  changes.  AWS  handles  scaling,  health  checks,  and  traffic  distribution  across  firewall instances. Other options are less efficient: - A/B (NLB/ALB) require complex routing rules and don ��t  natively support traffic inspection workflows. - C (Transit Gateway) adds unnecessary complexity for a single inspection VPC setup.  Reference Links - [AWS Gateway Load Balancer](https://aws.amazon.com/elastic load balancing/gateway-load-balancer/) - [What is a Gateway Load Balancer?](https://docs.aws.amazon.com/elastic load balancing/latest/gateway/introduction.html)

19.A company wants to improve its ability to clone large amounts of production data into a test environment in the same AWS Region. The data is stored in Amazon EC2 instances on Amazon Elastic Block Store (Amazon EBS) volumes. Modifications to the

cloned data must not affect the  production environment. The software that accesses this data requires consistently high  I/O performance.  A  solutions  architect  needs  to  minimize  the  time  that  is  required  to  clone  the  production  data  into  the  test environment. Which solution will meet these requirements?

A �� Take  EBS  snapshots  of the  production  EBS  volumes.  Restore the  snapshots  onto  EC2  instance  store volumes  in the test environment.

B��Configure the production EBS volumes to use t he EBS Multi-Attach feature. Take EBS snapshots of the production EBS volumes. Attach the production EBS volumes to the EC2 instances in the test environment.

C��Take EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment before restoring the volumes from the production EBS snapshots.

D ��Take EBS snapshots of the production EBS volumes. Turn on t he EBS fast snapshot restore feature on t he EBS snapshots. Restore the snapshots into new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment.

�𰸣�D

������Correct Answer: D Detailed Explanation: The best solution is D because using EBS Fast Snapshot Restore (FSR) significantly reduces the time  required to  restore  snapshots  into  new  EBS  volumes.  Here �� s  why:  1.  EBS  Snapshots:  These  capture  the production data state. 2. Fast Snapshot Restore (FSR): This feature pre-initializes the snapshot data, eliminating the "lazy loading" delay that normally occurs when restoring a snapshot. Without FSR, the restored volume initially retrieves data from S3 on first access, causing slower  I/O.  FSR  ensures the  restored volume  is  immediately  ready for  high-performance  access.  3.  Isolation: Restoring snapshots into new EBS volumes ensures the test environment ��s modifications don��t affect production. Other options fail because: - A: Instance store volumes are ephemeral (data lost on instance stop/termination) and unsuitable for persistent test environments. - B: Multi-Attach allows sharing a single volume across instances, but modifying the same volume in both environments would impact production. - C: Creating and initializing new volumes before restoring adds unnecessary steps and delays. FSR ensures both speed and consistent I/O performance, making D the correct choice. Reference Link: [Amazon EBS Fast Snapshot Restore](https://docs.aws.amazon.com/AWSEC2/latest/User guide/ebs-fast-snapshot-restore.html)

20.An ecommerce company wants to launch a one-deal-a-day website on AWS. Each day will feature exactly one product on sale for a period of 24 hours. The company wants to be able to handle millions of requests each hour with millisecond latency during peak hours. Which solution will meet these requirements with the LEAST operational overhead?

A ��Use Amazon S3 to host the full website in different S3 buckets. Add Amazon Cloud front distributions. Set the S3 buckets as origins for the distributions. Store the order data in Amazon S3.

B ��Deploy the full website on Amazon EC2 instances that run in Auto Scaling groups across multiple Availability Zones. Add an Application  Load  Balancer  (ALB)  to  distribute  the  website  traffic.  Add  another  ALB  for  the  backend  APIs.  Store  the  data  in Amazon RDS for MySQL.

C ��Migrate the full application to run in containers. Host the containers on Amazon Elastic Ku bernet es Service (Amazon EKS). Use the Ku bernet es Cluster Auto scaler to increase and decrease the number of pods to process bursts in traffic. Store the data in Amazon RDS for MySQL.

D��Use an Amazon S3 bucket to host the website's static content. Deploy an Amazon Cloud front distribution. Set the S3 bucket as the origin. Use Amazon API Gateway and AWS Lambda functions for the backend APIs. Store the data in Amazon Dynamo db.

�𰸣�D

�� �� �� Correct Answer:  D  Detailed  Explanation:  The correct answer  is  D  because  it  uses  fully  managed  AWS services that minimize operational overhead while  scaling to  handle  millions  of  requests  with  low  latency.  Here's  why:  - S3 +  Cloud front efficiently serves static content  (e.g.,  product  images,  HTML/CSS)  globally  with  high  speed  and  low  latency  via a  CDN. - API Gateway + Lambda automatically scales backend APIs without server management, handling traffic spikes during peak hours. - Dynamo db provides millisecond latency for database queries at any scale, ideal for high-traffic applications. Other options have drawbacks: - A incorrectly uses S3 for dynamic order data (S3 isn ��t a database). - B and C rely on EC2/EKS, requiring manual scaling, patching, and infrastructure management (high operational effort). By combining serverless (Lambda) and fully managed services (S3, Cloud front, Dynamo db), D achieves scalability, performance, and minimal maintenance.  Reference Links: - [AWS Serverless                      Overview](https://aws.amazon.com/serverless/)                       -                       [Amazon                       Dynamo db
Performance](https://aws.amazon.com/dynamo db/performance/)                 -                 [Amazon                 Cloud front                 with
S3](https://docs.aws.amazon.com/Amazon cloud front/latest/Developer guide/ Serving static content.html)

21.A solutions architect is using Amazon S3 to design the storage architecture of a new digital media application. The media files must be resilient to the loss of an Availability Zone. Some files are accessed frequently while other files are rarely accessed in an unpredictable pattern. The solutions architect must minimize the costs of storing and retrieving the media files. Which storage option meets these requirements?

A ��S3 Standard

B ��S3 Intelligent-Tiering

C ��S3 Standard-Infrequent Access (S3 Standard-IA)

D ��S3 One Zone-Infrequent Access (S3 One Zone-IA)

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: Amazon S3 Intelligent-Tiering is the best choice here because it automatically moves objects between access tiers (Frequent, Infrequent, Archive) based on changing access patterns. This minimizes costs for unpredictable access since it avoids manual tier management and retrieval fees. It also maintains multi-AZ resilience (unlike S3 One Zone-IA) while being cheaper than keeping all data in S3 Standard. The other options either lack automatic cost optimization (A/C), or sacrifice AZ-level resilience (D). Reference Link: https://aws.amazon.com/s3/storage-classes/intelligent-tiering/

22.A  company  is  storing  backup  files  by  using  Amazon  S3  Standard  storage.  The  files  are  accessed  frequently for  1  month. However, the files are not accessed after 1 month. The company must keep the files indefinitely. Which storage solution will meet these requirements MOST cost-effectively?

A ��Configure S3 Intelligent-Tiering to automatically migrate objects.

B ��Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep Archive after 1 month.

C ��Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA)
after 1 month.

D��Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 month.

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: The best choice is B because S3 Glacier Deep Archive is the cheapest storage class for data that is rarely accessed and needs to be stored indefinitely. Here's why: 1. For the first month, files are accessed frequently, so S3 Standard is appropriate. 2. After 1 month, the files are never accessed again but must be kept forever. Glacier Deep Archive  has the  lowest storage  costs  (as  low  as $0.00099  per  GB/month) for  long - term  archival.  3. Options C and  D (Infrequent Access tiers) are  more expensive than Glacier  Deep Archive and are designed for data that still needs occasional access. 4. Option A  (Intelligent - Tiering) adds  monitoring costs and  isn't  needed  since  access  patterns are  predictable  here. Reference Link: [Amazon S3 Storage Classes](https://aws.amazon.com/s3/storage - classes/)

23.A company observes an increase in Amazon EC2 costs in its most recent bill. The billing team notices unwanted vertical scaling of instance types for a couple of EC2 instances. A solutions architect needs to create a graph comparing the last 2 months of EC2 costs and  perform an  in-depth analysis to  identify the  root cause of the vertical scaling.  How should the solutions  architect generate the information with the LEAST operational overhead?

A ��Use AWS Budgets to create a budget report and compare EC2 costs based on instance types.

B ��Use Cost Explorer's granular filtering feature to perform an in-depth analysis of EC2 costs based on instance types.

C��Use graphs from the AWS Billing and Cost Management dashboard to compare EC2 costs based on instance types for the last 2 months.

D��Use AWS Cost and Usage Reports to create a report and send it to an Amazon S3 bucket. Use Amazon Quick sight with Amazon S3 as a source to generate an interactive graph based on instance types.

�𰸣�B

������ Correct Answer: B Detailed Explanation: Cost Explorer is designed for in-depth cost analysis with minimal setup. It allows filtering by instance types and comparing costs over custom time ranges (like the last 2 months) directly in the AWS console. This avoids the operational overhead of configuring  reports (A/CUR  in  D) or  relying on  limited pre-built dashboards (C). Option A (AWS Budgets) focuses on budget alerts, not detailed historical analysis. Option C (Billing Dashboard) lacks granular filtering by instance types and custom date comparisons. Option D (CUR + Quick sight) requires  manual report generation, S3 setup, and dashboard creation, which adds operational complexity. Cost Explorer provides instant, interactive cost breakdowns by instance types       with        no       extra        steps,        making       it        the        simplest       solution.        Reference        Link:        [AWS       Cost Explorer](https://aws.amazon.com/aws-cost-management/aws-cost-explorer/)

24.A company is designing an application. The application uses an AWS Lambda function to receive information through Amazon API Gateway and to store the information in an Amazon Aurora Postgresql database.  During the proof-of-concept stage, the company has to increase the Lambda quotas significantly to handle the high volumes of data that the company needs to load into the database. A solutions architect must recommend a new design to improve scalability and minimize the configuration effort. Which solution will meet these requirements?

A �� Refactor the Lambda function code to Apache Tomcat code that runs on Amazon EC2 instances. Connect the database by using native Java Database Connectivity (JDBC) drivers.

B ��Change the platform from Aurora to Amazon Dynamo d provision a Dynamo db Accelerator (DAX) cluster. Use the DAX client SDK to point the existing Dynamo db API calls at the DAX cluster.

C ��Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using Amazon Simple Notification Service (Amazon SNS).

D ��Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using an Amazon Simple Queue Service (Amazon SQS) queue.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The original setup uses a single Lambda function to handle both receiving data via  API  Gateway  and  writing  to  Aurora  Postgresql.  During  high  traffic,  this  causes  Lambda  concurrency  limits,  forcing  the company to increase quotas. Why SQS (Option D) is better than SNS (Option C): - Decoupling   �� Buffering: Amazon SQS acts as a queue, letting the first Lambda dump incoming data into the queue and the second Lambda process it asynchronously. This prevents overwhelming the database writer Lambda during traffic spikes. SNS (Option C) is a pub/sub service that immediately triggers the second Lambda for every message, risking concurrency limits again. - Batching   �� Efficiency: SQS allows the second Lambda to process messages in batches (e.g., 10 at a time), reducing the number of Lambda invocations and database calls. SNS cannot do this. - Retry Safety:  If the database writer  Lambda fails, SQS automatically retries the message. SNS would require additional setup (e.g., a dead - letter queue) for retries. Why not other options: - Option A (EC2/Tomcat): Managing servers (EC2) adds operational overhead, defeating the "minimal configuration effort" requirement. - Option B (Dynamo db/DAX): Switching databases and adding a cache (DAX) optimizes database performance but doesn ��t fix the Lambda scaling bottleneck. Simple analogy: Imagine a busy coffee shop. SQS is like a ticket queue��customers (data) line up, and baristas (Lambda) process orders one  by  one.  SNS  is  like  shouting  orders  to  all  baristas  at  once,  causing  chaos.  Option  D  keeps  things  orderly  and  scalable. Reference  Links:  -  [AWS  Lambda  Scaling](https://docs.aws.amazon.com/lambda/latest/dg/invocation  -  scaling.html)  -  [SQS  vs SNS](https://aws.amazon.com/sqs/faqs/#SQS_vs_SNS)

25.A  company  needs to  review  its  AWS  Cloud  deployment to ensure that  its Amazon S3  buckets  do  not  have  unauthorized configuration changes. What should a solutions architect do to accomplish this goal?

A ��Turn on AWS Config with the appropriate rules.

B ��Turn on AWS Trusted Advisor with the appropriate checks.

C ��Turn on Amazon Inspector with the appropriate assessment template.

D ��Turn on Amazon S3 server access logging. Configure Amazon Event bridge (Amazon Cloud Watch Events).

�𰸣�A

������ Correct Answer: A Detailed Explanation: To monitor unauthorized configuration changes in Amazon S3 buckets, the best approach is to use AWS Config. Here's why: 1. AWS Config continuously tracks and evaluates resource configurations. It records changes (e.g., S3 bucket policies, ACLs) and checks compliance against predefined rules (e.g., "S3 buckets must not be publicly accessible"). This directly addresses the requirement to detect unauthorized configuration changes. 2. Option B (Trusted Advisor) focuses  on cost optimization, security  best  practices,  and  performance  checks,  but  it  doesn ��t  specifically track  real  - time configuration changes or provide historical auditing. 3. Option C (Amazon Inspector) scans for vulnerabilities in EC2 instances or applications, not configuration changes in S3 buckets. 4. Option D (S3 server access logging + Event bridge) monitors access to S3 objects (e.g., who downloaded a file), not configuration changes (e.g., modifying bucket policies). By enabling AWS Config with rules  like  `s3  -   bucket  -  public  -   read  -  prohibited`  or  custom  rules,  the  company  can  automatically  detect  and  alert  on unauthorized S3 configuration changes. Reference Link: [AWS Config Overview](https://aws.amazon.com/config/)

26.A company is launching a new application and will display application metrics on an Amazon Cloud watch dashboard. The company's product manager needs to access this dashboard periodically. The product manager does not have an AWS account. A solutions architect must provide access to the product manager by following the principle of least privilege. Which solution will

meet these requirements?

A ��Share the dashboard from the Cloud watch console. Enter the product manager's email address, and complete the sharing steps. Provide a shareable link for the dashboard to the product manager.

B ��Create an IAM user specifically for the product manager. Attach the Cloud watch read only access AWS managed policy to the user. Share the  new  login  credentials  with the  product  manager.  Share  the  browser  URL  of  the  correct  dashboard with the product manager.

C��Create an IAM user for the company's employees. Attach the View only access AWS managed policy to the IAM user. Share the new login credentials with the product manager. Ask the product manager to navigate to the Cloud watch console and locate the dashboard by name in the Dashboards section.

D ��Deploy a bastion server in a public subnet. When the product manager requires access to the dashboard, start the server and share the RDP credentials. On the bastion server, ensure that the browser is configured to open the dashboard URL with cached AWS credentials that have appropriate permissions to view the dashboard.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The correct solution is to use Cloud watch's built-in dashboard sharing feature. This allows sharing a specific dashboard with someone who doesn't have an AWS account by providing a shareable link. Here's why it's the best choice: 1. No AWS Account Needed: The product manager can access the dashboard through a simple web link without logging into AWS. 2. Least Privilege: This method only exposes the specific dashboard, not the entire AWS account or other Cloud watch data. 3. Security: The shared  link  uses temporary credentials and  HTTPS  encryption, avoiding the  risks  of creating permanent IAM users or exposing servers. Other options are less ideal because: - B/C require creating IAM users (needs AWS login) and grant broader permissions - D introduces unnecessary infrastructure and security risks Reference Link: [Amazon Cloud watch                                                                                                                                                                                                        Dashboard
Sharing](https://docs.aws.amazon.com/Amazon cloud watch/latest/monitoring/cloud watch-dashboard-sharing.html)

27.A company is migrating applications to AWS. The applications are deployed in different accounts. The company manages the accounts centrally by using AWS Organizations. The company's security team needs a single sign-on (SSO) solution across all the company's accounts. The company must continue  managing the users and groups in  its on-premises self-managed  Microsoft Active Directory. Which solution will meet these requirements?

A ��Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a one-way forest trust or a one-way domain trust to connect the company's self-managed  Microsoft Active  Directory with AWS SSO  by using AWS  Directory Service for  Microsoft Active Directory.

B ��Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a two-way forest trust to connect the company's self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.

C ��Use AWS Directory Service. Create a two-way trust relationship with the company's self-managed Microsoft Active Directory.

D ��Deploy an identity provider (IdP) on premises. Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The company needs to maintain its on - premises Active Directory (AD) as the source of truth for users/groups while enabling AWS SSO for centralized access across multiple AWS accounts. Here's why option

B  is  correct:  1.  AWS  SSO  Integration:  Enabling  AWS  SSO  provides  a  unified  SSO  experience  across  all  AWS  accounts  in  the Organizations  structure.  2. Two  -  Way  Forest Trust:  -  A two  -  way trust  between  the  on  -  premises  AD  and  AWS  Managed Microsoft AD  (via  AWS  Directory  Service)  allows  bidirectional  authentication.  -  This  means  AWS  SSO  can  authenticate  on  - premises AD users, while the on - premises AD can also authenticate AWS Managed Microsoft AD users if needed (though t he scenario  doesn't  require  this).   3.  Why  Not  Other  Options:  -  Option  A   (one  -  way  trust):   Only   allows  one  -   directional authentication (e.g., AWS trusting on - premises AD but not vice versa), which isn't necessary here. - Option C (AWS Directory Service alone): This would create a standalone AWS Managed AD without integrating with AWS SSO. - Option D (on - prem IdP): Adds unnecessary complexity since the company already has AD; AWS SSO can directly integrate with AD. Key Analogy: Think of the two - way trust as a mutual agreement between your on - premises AD and AWS. Like two neighboring countries agreeing to recognize each other's passports (user credentials), allowing seamless travel (SSO access) between them. Reference Link: [AWS SSO with On - Premises AD](https://docs.aws.amazon.com/single sign on/latest/user guide/azure - ad.html)

28.A company provides a Voice over Internet Protocol (VoIP) service that uses UDP connections. The service consists of Amazon EC2 instances that  run in an Auto Scaling group. The company has deployments across  multiple AWS  Regions. The company needs to route users to the Region with the lowest latency. The company also needs automated failover between Regions. Which solution will meet these requirements?

A��Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with the Auto Scaling group. Use the NLB as an AWS Global Accelerator endpoint in each Region.

B ��Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group with the Auto Scaling group. Use the ALB as an AWS Global Accelerator endpoint in each Region.

C��Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with the Auto Scaling group. Create an Amazon Route 53 latency record that points to aliases for each NLB. Create an Amazon Cloud front distribution that uses the latency record as an origin.

D ��Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group with the Auto Scaling group.  Create  an  Amazon   Route  53  weighted   record  that   points  to  aliases  for  each  ALB.   Deploy  an  Amazon  Cloud front distribution that uses the weighted record as an origin.

�𰸣�A

������ Correct Answer A Detailed Explanation For a VoIP service using UDP, you need a Network Load Balancer (NLB) because ALB (Application Load Balancer) only supports HTTP/HTTPS (not UDP). Why A is correct: 1. Lowest latency routing: AWS Global Accelerator  uses Anycast  IP  addresses to automatically  route traffic to the  closest  healthy AWS  Region  based  on  real  -  time network conditions. 2. Automated failover: Global Accelerator continuously monitors endpoint health. If a Region fails, traffic shifts to the next - best Region without manual intervention. 3. UDP support: NLB works with Global Accelerator to handle UDP traffic, while ALB (Options B/D) cannot. Why other options are wrong: - B/D: ALB doesn ��t support UDP. - C: Route 53 latency routing relies on DNS, which is slower to update than Global Accelerator. Cloud front is for HTTP/HTTPS content delivery, not UDP.
-    D:   Weighted    records   in    Route   53   don   �� t    automatically   adjust   for    latency.    Reference    Links    -    [AWS    Global Accelerator](https://aws.amazon.com/global-accelerator/)                                 -                                  [Network                                   Load Balancer](https://aws.amazon.com/elastic load balancing/network-load-balancer/)

29.A development team runs monthly resource-intensive tests on its general purpose Amazon RDS for MySQL DB instance with Performance Insights enabled. The testing lasts for 48 hours once a month and is the only process that uses the database. The team wants to reduce the cost of running the tests without reducing the compute and memory attributes of the DB instance. Which solution meets these requirements MOST cost-effectively?

A ��Stop the DB instance when tests are completed. Restart the DB instance when required.

B ��Use an Auto Scaling policy with the DB instance to automatically scale when tests are completed.

C ��Create a snapshot when tests are completed. Terminate the DB instance and restore the snapshot when required.

D ��Modify the DB instance to a low-capacity instance when tests are completed. Modify the DB instance again when required.

�𰸣�C

�� �� ��  Correct  Answer:  C  Detailed  Explanation:  The  most  cost-effective  solution  is  to  create  a  snapshot  when  tests  are completed, terminate the DB instance, and restore the snapshot when needed. Here's why: - Option C saves costs by stopping all charges for the DB instance (compute + storage) when it ��s not in use. Only the snapshot storage (cheaper than running the instance) incurs costs during idle periods. Restoring from a snapshot recreates the DB instance with the same compute/memory configuration, meeting the requirement to avoid downgrading resources. - Other options are less optimal: - A: Stopping the DB instance still charges for storage. - B: RDS doesn ��t support Auto Scaling for instance size. - D: Modifying the instance size reduces compute/memory, violating the requirement. Reference Link: [Amazon RDS Pricing](https://aws.amazon.com/rds/pricing/)

30.A company that hosts its web application on AWS wants to ensure all Amazon EC2 instances. Amazon RDS DB instances. and Amazon Redshift clusters are configured with tags. The company wants to minimize the effort of configuring and operating this check. What should a solutions architect do to accomplish this?

A ��Use AWS Config rules to define and detect resources that are not properly tagged.

B ��Use Cost Explorer to display resources that are not properly tagged. Tag those resources manually.

C ��Write API calls to check all resources for proper tag allocation. Periodically run the code on an EC2 instance.

D �� Write  API  calls  to  check  all  resources  for  proper  tag  allocation.  Schedule  an  AWS  Lambda  function  through  Amazon Cloud watch to periodically run the code.

�𰸣�A

���� �� Correct Answer: A  Detailed  Explanation: AWS Config is a service that continuously  monitors and  records your AWS resource configurations,  allowing you to automatically  check  if  resources  comply with  predefined  rules.  By  creating  an  AWS Config rule (e.g., the built - in "required - tags" rule or a custom rule), you can automatically detect untagged EC2 instances, RDS databases, and Redshift clusters. This requires minimal setup and operates continuously, eliminating manual checks or custom code maintenance. Options B, C, and D involve manual work, recurring code execution, or extra operational overhead, making AWS         Config         the         simplest          and         most         efficient          solution.         Reference          Link:         [AWS         Config Rules](https://docs.aws.amazon.com/config/latest/developer guide/evaluate-config.html)

31.A development team needs to host a website that will be accessed by other teams. The website contents consist of HTML, CSS, client-side Javascript, and images. Which method is t he MOST cost-effective for hosting the website?

A ��Container ize the website and host it in AWS Fargate.

B ��Create an Amazon S3 bucket and host the website there.

C ��Deploy a web server on an Amazon EC2 instance to host the website.

D ��Configure an Application Load Balancer with an AWS Lambda target that uses the Express.js framework.

�𰸣�B

���� �� Correct Answer: B. Create an Amazon S3 bucket and host the website there. Detailed Explanation: Amazon S3 (Simple Storage Service) is the most cost-effective solution for hosting static websites (HTML, CSS, Javascript, images) because:  1.  No Server Costs: S3 charges only for storage and data transfer, unlike EC2/Fargate which charge for compute  resources 24/7. 2. Serverless: No need to manage servers or scaling. 3. Built-in HTTP Hosting: S3 has a built-in static website hosting feature. 4. High Availability: Automatically stored redundantly across AWS Availability Zones. Other options are less optimal: - A (Fargate)   �� C (EC2): Require paying for always-running compute resources. - D (Lambda + ALB): Unnecessary complexity for static content, and Lambda  has  per-request  pricing.  S3  is  purpose-built  for  static  content,  making  it  both  the  simplest  and  cheapest  option. Reference                                 Link:                                 [Amazon                                 S3                                  Static                                 Website
Hosting](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Website hosting.html)

32.A  company  runs an  online  marketplace web  application on AWS. The  application serves  hundreds  of thousands of  users during peak hours. The company needs a scalable, near-real-time solution to share the details of millions of financial transactions with several other internal applications. Transactions also need to be processed to remove sensitive data before being stored in a document database for low-latency retrieval. What should a solutions architect recommend to meet these requirements?

A �� Store the transactions  data  into  Amazon  Dynamo db.  Set  up  a  rule  in  Dynamo db  to  remove  sensitive  data  from  every transaction upon write. Use Dynamo db Streams to share the transactions data with other applications.

B��Stream the transactions data into Amazon Kinesis Data Firehose to store data in Amazon Dynamo db and Amazon S3. Use AWS Lambda  integration with  Kinesis  Data  Firehose  to  remove sensitive  data. Other applications can consume the data stored  in Amazon S3.

C ��Stream the transactions data into Amazon Kinesis Data Streams. Use AWS Lambda integration to remove sensitive data from every transaction and then store the transactions data in Amazon Dynamo db. Other applications can consume the transactions data off the Kinesis data stream.

D ��Store the batched transactions data in Amazon S3 as files. Use AWS Lambda to process every file and remove sensitive data before updating the files in Amazon S3. The Lambda function then stores the data in Amazon Dynamo db. Other applications can consume transaction files stored in Amazon S3.

�𰸣�C

���� �� Correct Answer: C Detailed Explanation: The best solution is to use Amazon Kinesis  Data Streams for real-time data ingestion  and  processing.  Here  �� s  why:  1.   Near-Real-Time  Processing:  Kinesis   Data  Streams  can  handle  high-throughput, low-latency data streaming, which is critical for serving hundreds of thousands of users during peak hours. 2. Data Cleaning with AWS Lambda: By integrating Lambda with Kinesis, sensitive data can be removed before storing transactions in Dynamo db. This ensures the database only contains sanitized data. 3. Scalability: Kinesis scales automatically to handle millions of transactions, and multiple internal applications can consume the same data stream simultaneously without delays. 4. Low-Latency Retrieval: Dynamo db, a document database, provides fast access to processed data, meeting the low-latency requirement. Other options fail because: - A: Dynamo db Streams process data after it ��s written, so sensitive data would still be stored temporarily. - B: Kinesis Firehose introduces delays (batch processing) and doesn ��t support  real-time sharing between applications. - D: S3 + Lambda   batch   processing   isn   �� t    �� near-real-time  ��   and   adds   latency.   Reference   Links:   -   [Amazon   Kinesis   Data Streams](https://aws.amazon.com/kinesis/data-streams/)                        -                        [AWS                        Lambda                        with
Kinesis](https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html)

33.A company hosts its multi-tier applications on AWS. For compliance, governance, auditing, and security, the company must track configuration changes on  its AWS  resources and  record a  history of API calls  made to these  resources.  What should a solutions architect do to meet these requirements?

A ��Use AWS Cloud trail to track configuration changes and AWS Config to record API calls.

B ��Use AWS Config to track configuration changes and AWS Cloud trail to record API calls.

C ��Use AWS Config to track configuration changes and Amazon Cloud watch to record API calls.

D ��Use AWS Cloud trail to track configuration changes and Amazon Cloud watch to record API calls.

�𰸣�B

������ Correct Answer B Explanation To meet the requirements: - AWS Config tracks configuration changes of AWS resources (e.g., security group rules, EC2 instance settings). It provides a detailed history of what changed, when, and how a resource was configured  over time.  - AWS  Cloud trail  records  API  calls  (e.g.,  who  started/deleted  an  EC2  instance).  It  logs  who  made  the request, when, and what action was taken, which is critical for auditing and governance. Why not other options? - Options A, C, D mix up the roles of Cloud trail and Config. Cloud trail focuses on actions (API calls), while Config focuses on resource states. - Cloud watch  (options  C/D)  is  for  monitoring/ metrics,  not  auditing  configuration  changes  or  API  calls.  Reference  Links  [AWS Config](https://aws.amazon.com/config/) [AWS Cloud trail](https://aws.amazon.com/cloud trail/)

34.A company is preparing to launch a public-facing web application in the AWS Cloud. The architecture consists of Amazon EC2 instances within a VPC behind an Elastic Load Balancer (ELB). A third-party service is used for the DNS. The company's solutions architect  must  recommend  a  solution  to  detect  and  protect  against  large-scale  DDoS  attacks.  Which  solution  meets  these requirements?

A ��Enable Amazon Guard duty on the account.

B ��Enable Amazon Inspector on the EC2 instances.

C ��Enable AWS Shield and assign Amazon Route 53 to it.

D ��Enable AWS Shield Advanced and assign the ELB to it.

�𰸣�D

������ Correct Answer: D. Enable AWS Shield Advanced and assign the ELB to it. Detailed Explanation: AWS Shield Advanced is specifically designed to protect against large - scale DDoS attacks. While AWS Shield Standard (automatically included for free) provides basic DDoS protection, Shield Advanced offers enhanced protections like advanced attack visibility, 24/7 DDoS response team access, and coverage for higher costs due to scaling during attacks. Since the application uses an Elastic Load Balancer (ELB) in AWS, enabling Shield Advanced and assigning it to the ELB ensures the ELB (a critical entry point for traffic) is protected. The third - party DNS service doesn ��t affect this setup because Shield Advanced focuses on protecting AWS resources like ELB, not the DNS itself. Other options: - A (Guard duty): Detects threats like unauthorized access but doesn ��t mitigate DDoS attacks. - B (Inspector): Scans EC2 instances for vulnerabilities but doesn ��t protect against DDoS. - C (Shield + Route 53): Irrelevant here because the company uses a third - party DNS (not Route 53). Shield Standard alone isn ��t sufficient for large - scale attacks. Reference       Links:        -        [AWS       Shield        Advanced](https://aws.amazon.com/shield/)       -        [DDoS        Protection       for ELB](https://docs.aws.amazon.com/waf/latest/developer guide/ddos-aws-services.html)

35.A company is building an application in the AWS Cloud. The application will store data in Amazon S3  buckets in two AWS Regions. The company must use an AWS Key Management Service (AWS KMS) customer managed key to encrypt all data that is stored in the S3 buckets. The data in both S3 buckets must be encrypted and decrypted with the same KMS key. The data and the key  must  be  stored  in  each  of  the  two  Regions.  Which  solution  will  meet  these  requirements  with  the  LEAST  operational overhead?

A �� Create an  S3  bucket  in  each  Region.  Configure  the  S3  buckets  to  use  server-side  encryption  with  Amazon  S3  managed encryption keys (SSE-S3). Configure replication between the S3 buckets.

B ��Create a customer managed multi-Region KMS key. Create an S3 bucket in each Region. Configure replication between the S3 buckets. Configure the application to use the KMS key with client-side encryption.

C ��Create a customer managed KMS key and an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Configure replication between the S3 buckets.

D��Create a customer managed KMS key and an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with AWS KMS keys (SSE-KMS). Configure replication between the S3 buckets.

�𰸣�B

�� �� ��  Correct  Answer   B   Detailed  Explanation  The  question   requires   using  the  same   customer-managed   KMS  key  to encrypt/decrypt data in S3 buckets across two AWS Regions, with both data and keys stored in each Region. Here's why Option B is correct:  1. Multi-Region KMS  Key: - A multi-Region KMS key allows the same key material to exist in multiple Regions. This ensures data encrypted  in one  Region can  be  decrypted  in the other  using the same  logical  key. - The  key  is stored  in  both Regions,  meeting  the  requirement for  "data  and  key  stored  in  each  Region."  2.  Client-Side  Encryption:  -  By  encrypting  data client-side (before uploading to S3), the application ensures the data is encrypted with the multi-Region key upfront. - When replicated, the encrypted data retains its encryption, and both Regions can decrypt it using their local replica of the multi-Region key. Why Other Options Fail: - A   �� C:  Use SSE-S3 (Amazon-managed keys), violating the requirement for customer-managed keys. - D: Uses separate KMS keys in each Region (not multi-Region). These are distinct keys, so data encrypted in one Region cannot be decrypted in the other. Operational Overhead: Option B avoids complex cross-Region key policies or re-encryption during  replication,  as  the  multi-Region  key  inherently  works  across  Regions.  Reference  Links  -  [AWS  KMS  Multi-Region Keys](https://docs.aws.amazon.com/kms/latest/developer guide/multi-region-keys-overview.html) - [Client-Side Encryption with AWS KMS](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Using client side encryption.html)

36.A company recently launched a variety of new workloads on Amazon EC2 instances in its AWS account. The company needs to create a strategy to access and administer the instances remotely and securely. The company needs to implement a repeatable process that works with native AWS services and follows the AWS Well-Architected Framework. Which solution will meet these requirements with the LEAST operational overhead?

A ��Use the EC2 serial console to directly access the terminal interface of each instance for administration.

B ��Attach the appropriate IAM role to each existing instance and new instance. Use AWS Systems Manager Session Manager to establish a remote SSH session.

C ��Create an administrative SSH key pair. Load the public key into each EC2 instance. Deploy a bastion host in a public subnet to provide a tunnel for administration of each instance.

D �� Establish an AWS Site-to-Site VPN connection. Instruct administrators to use their local on-premises machines to connect

directly to the instances by using SSH keys across the VPN tunnel.

�𰸣�B

���� �� Correct Answer B Detailed Explanation The best solution is Option B because it uses AWS Systems Manager Session Manager, a native AWS service that securely manages EC2 instances without requiring open inbound ports (like SSH port 22) or maintaining SSH keys. Here ��s why: 1. Least Operational Overhead: - By attaching an IAM role to instances, you enable automatic authentication for Session  Manager.  New  instances  can  inherit the  role  during  launch  (via  launch templates  or  automation), making the  process  repeatable.  -  No  need  to  manage  SSH  keys,  bastion  hosts,  or  VPN  configurations.  2.  Security:  -  Session Manager encrypts all traffic and logs sessions to AWS Cloud trail/S3 for auditing, aligning with AWS security best practices. - No public exposure of instances (no need for a bastion host in a public subnet). 3. AWS Well-Architected: - Reduces complexity (no bastion host setup or VPN maintenance). - Uses IAM for granular access control instead of shared SSH keys. Option C (bastion host) adds maintenance and security risks. Option D (VPN) requires managing VPNs and SSH keys. Option A (serial console) is for emergency       access,       not       daily        administration.       Reference       Links        -       [AWS       Systems        Manager       Session Manager](https://docs.aws.amazon.com/systems-manager/latest/user guide/session-manager.html)    -    [AWS    Well-Architected Framework: Security Pillar](https://aws.amazon.com/architecture/well-architected/security/)

37.A company is hosting a static website on Amazon S3 and is using Amazon  Route 53 for  DNS. The website is experiencing increased demand from around the world. The company must decrease latency for users who access the website. Which solution meets these requirements MOST cost-effectively?

A ��Replicate the S3 bucket that contains the website to all AWS Regions. Add Route 53 geolocation routing entries.

B ��Provision accelerators in AWS Global Accelerator. Associate the supplied IP addresses with the S3 bucket. Edit the Route 53 entries to point to the IP addresses of the accelerators.

C �� Add  an Amazon  Cloud front  distribution  in front of the S3  bucket.  Edit  the  Route  53  entries to  point to the  Cloud front distribution.

D ��Enable S3 Transfer Acceleration on the bucket. Edit the Route 53 entries to point to the new endpoint.

�𰸣�C

������ Correct Answer C ��Add an Amazon Cloud front distribution in front of the S3 bucket. Edit the Route 53 entries to point to the Cloud front distribution. Detailed Explanation To reduce latency for a global audience accessing a static website hosted on Amazon S3, Amazon Cloud front is the most cost-effective solution. Here ��s why: 1. Cloud front is a Content Delivery Network (CDN): It caches copies of the static website (like HTML, images, etc.) at edge locations worldwide. When users request the site, they get it from the nearest edge location, drastically reducing latency. 2. Seamless Integration with S3: Cloud front can directly pull content from the S3 bucket, requiring minimal setup. No need to manually replicate the bucket across regions (unlike Option A). 3. Cost-Effective: Cloud front ��s pricing is pay-as-you-go, with lower costs for data transfer compared to alternatives like S3 Transfer Acceleration (Option D) or AWS Global Accelerator (Option B). 4. Route 53 Configuration: Simply point the Route 53 DNS record to the Cloud front distribution ��s domain name. This ensures users are automatically routed to the optimal edge location. Why  Other   Options  Are   Less  Suitable:  -  Option  A   (Replicate  S3   Buckets):   Manually  copying  the   bucket  to  all   regions  is time-consuming,  expensive,  and  unnecessary  when  Cloud front  handles  global  distribution  automatically.  -  Option  B  (Global Accelerator):  Designed  for  improving  availability  and  performance  of  dynamic  applications  (e.g.,  APIs),  not  static  websites. Overkill for this use case and more expensive. - Option  D (S3 Transfer Acceleration): Optimizes uploads to S3 (e.g.,  large file transfers),  not  downloads for  end-users.  Cloud front  is  better  suited  for  website  access.  Reference  Link  [Amazon  Cloud front Overview](https://aws.amazon.com/cloud front/)

38.A  company  maintains  a  searchable  repository  of  items  on  its  website.  The  data  is  stored  in  an  Amazon  RDS  for  MySQL database table that contains  more than  10  million  rows.  The  database  has  2 TB  of General  Purpose  SSD  storage. There are millions  of  updates  against this  data  every  day through the  company's website. The  company  has  noticed  that  some  insert operations are taking 10 seconds or longer. The company has determined that the database storage performance is the problem. Which solution addresses this performance issue?

A ��Change the storage type to Provisioned IOPS SSD.

B ��Change the DB instance to a memory optimized instance class.

C ��Change the DB instance to a burstable performance instance class.

D ��Enable Multi-AZ RDS read replicas with MySQL native asynchronous replication.

�𰸣�A

�� �� ��  Correct  Answer:  A.  Change  the  storage  type  to  Provisioned  IOPS  SSD.  Explanation:  The  problem  here  is  storage performance due to high daily write operations (millions of updates/inserts). Amazon RDS for MySQL uses General Purpose SSD (gp2) by default, which provides a baseline of 3 IOPS per gigabyte of storage. For a 2 TB volume, this means a baseline of 6,000 IOPS (2,000 GB * 3 IOPS/GB).  However, gp2 has a burst capability that temporarily allows higher  IOPS, but if the workload is consistently heavy (like millions of daily updates), the burst credits run out, causing latency spikes. Provisioned IOPS SSD (io1/io2) lets you specify the exact IOPS you need (independent of storage size) and guarantees consistent performance. This is ideal for write-heavy workloads like frequent inserts/updates, as it avoids the IOPS limitations of gp2. Why the other options are wrong: - B (Memory-optimized instance): Memory optimization helps with CPU-bound or in-memory processing (e.g., complex queries), not  storage  I/O   bottlenecks.  -  C  (Burstable  instance):   Burstable  instances  (e.g.,  T3/T4g)  are  cost-effective  for  intermittent workloads. They  would  worsen  performance  under  sustained  heavy  writes.  -  D  (Multi-AZ  read  replicas):  Multi-AZ  improves availability,  not  write  performance.  Read  replicas  offload  read  traffic  but  don �� t  solve  storage  I/O  bottlenecks  for  writes. Reference Link: [Amazon RDS Storage Types](https://docs.aws.amazon.com/Amazon rds/latest/User guide/CHAP_Storage.html)

39.A company has thousands of edge devices that collectively generate 1 TB of status alerts each day. Each alert is approximately 2 KB in size. A solutions architect needs to implement a solution to ingest and store the alerts for future analysis. The company wants a  highly  available solution.  However, the  company  needs  to  minimize  costs and  does  not want to  manage  additional infrastructure. Additionally, the company wants to keep 14 days of data available for immediate analysis and archive any data older than 14 days. What is the MOST operationally efficient solution that meets these requirements?

A ��Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.

B �� Launch Amazon EC2 instances across two Availability Zones and place them behind an Elastic Load Balancer to ingest the alerts. Create a script on the EC2 instances that will store the alerts in an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.

C ��Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon  Open search Service  (Amazon  Elastic search  Service)  cluster. Set  up the Amazon  Open search Service (Amazon Elastic search Service) cluster to take manual snapshots every day and delete data from the cluster that is older than 14 days.

D ��Create an Amazon Simple Queue Service (Amazon SQS) standard queue to ingest the alerts, and set the message retention

period to 14 days. Configure consumers to poll the SQS queue, check the age of the message, and analyze the message data as needed. If the message is 14 days old, the consumer should copy the message to an Amazon S3 bucket and delete the message from the SQS queue.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution is to use Amazon Kinesis Data Firehose to ingest the alerts and deliver them directly to Amazon S3. Kinesis Data Firehose is a fully managed service that automatically scales to handle high data volumes (like 1 TB/day) without requiring infrastructure management. It writes data directly to S3, which is highly available and durable. For cost optimization, an S3 Lifecycle policy can automatically transition data older than 14 days to the cheaper storage class Amazon S3 Glacier, while  keeping recent data in S3 for immediate analysis. This approach minimizes operational effort, ensures high availability, and reduces costs  by leveraging serverless services and automated lifecycle  rules. Other options fail because: -  B (EC2-based):  Requires  managing  EC2  instances  and scripts, which adds operational overhead. - C  (Open search): Storing 14 TB in Open search is expensive, and manual snapshot management is inefficient. - D (SQS): SQS is not designed for large-scale data storage; copying messages to S3 via consumers is complex and error-prone. Reference Links: - [Amazon Kinesis Data                Firehose](https://aws.amazon.com/kinesis/data-firehose/)                 -                 [Amazon                 S3                 Lifecycle Policies](https://docs.aws.amazon.com/AmazonS3/latest/user guide/object-lifecycle-mgmt.html)

40.A company's application integrates with multiple software-as-a-service (SaaS) sources for data collection. The company runs Amazon EC2 instances to receive the data and to upload the data to an Amazon S3 bucket for analysis. The same EC2 instance that receives and uploads the data also sends a notification to the user when an upload is complete. The company has noticed slow  application  performance  and  wants to  improve the  performance  as  much  as  possible.  Which  solution  will  meet  these requirements with the LEAST operational overhead?

A ��Create an Auto Scaling group so that EC2 instances can scale out. Configure an S3 event notification to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.

B �� Create  an Amazon AppFlow flow  to transfer data  between  each  SaaS source and the S3  bucket.  Configure  an S3 event notification to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.

C ��Create an Amazon Event bridge (Amazon Cloud watch Events) rule for each SaaS source to send output data. Configure the S3 bucket as the rule's target. Create a second Event bridge (Cloud Watch Events) rule to send events when the upload to the S3 bucket is complete. Configure an Amazon Simple Notification Service (Amazon SNS) topic as the second rule's target.

D ��Create a Docker container to use instead of an EC2 instance. Host the containerized application on Amazon Elastic Container Service  (Amazon  ECS).  Configure  Amazon  Cloud watch  Container  Insights  to  send  events  to  an  Amazon  Simple  Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The current setup uses EC2 instances for three tasks: receiving data from SaaS sources,  uploading to S3, and sending  notifications. This creates a  bottleneck, as the  EC2 instance is  handling everything. To reduce operational overhead and improve performance, the goal is to eliminate the EC2 instance entirely and use managed AWS services. Why Option B Works Best: 1. Amazon AppFlow: This fully managed service directly integrates with SaaS sources (like Sales force, Slack, etc.) and automatically transfers data to S3. This removes the need for EC2 instances to handle data collection and uploads. 2. S3 Event Notifications + SNS: When AppFlow uploads data to S3, an S3 event notification triggers an SNS topic to send the completion alert. This replaces the EC2 ��s notification task. Why Other Options Are Less Optimal: - Option A still relies on EC2 instances (with Auto Scaling), requiring ongoing management. - Option C uses Event bridge rules, but Event bridge isn ��t

designed for bulk data transfers from SaaS to S3, and configuring  rules for each SaaS source is complex. - Option D shifts to containers but doesn ��t address the core issue (EC2 overhead) and adds container management complexity. Simplified Take away: AppFlow automates data transfers (no EC2), and S3+SNS automates notifications. This serverless approach minimizes operational work   and    scales    effortlessly.    Reference    Links:    -    [Amazon   AppFlow](https://aws.amazon.com/appflow/)    -    [S3    Event Notifications](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Notification how to.html)

41.A company runs a highly available image-processing application on Amazon EC2 instances in a single VPC. The EC2 instances run inside several subnets across multiple Availability Zones. The EC2 instances do not communicate with each other. However, the  EC2  instances  download  images  from  Amazon  S3  and  upload  images  to  Amazon  S3 through  a  single  NAT  gateway. The company is concerned about data transfer charges. What is the MOST cost-effective way for the company to avoid Regional data transfer charges?

A ��Launch the NAT gateway in each Availability Zone.

B ��Replace the NAT gateway with a NAT instance.

C ��Deploy a gateway VPC endpoint for Amazon S3.

D ��Provision an EC2 Dedicated Host to run the EC2 instances.

�𰸣�C

���� �� Correct Answer: C Detailed Explanation: The company's EC2 instances are using a NAT gateway to communicate with Amazon S3, which incurs data transfer costs. By deploying a gateway VPC endpoint for Amazon S3 (option C), the EC2 instances can  access  S3  directly  through  AWS's  private  network  without  routing  traffic  through  the  internet  or  a  NAT  gateway.  This eliminates data transfer charges  between the VPC and S3 within the same AWS  Region. - Option A  (multiple  NAT gateways) increases costs due to per-GB NAT data processing fees and redundant NAT deployment. - Option B (NAT instance) still incurs data transfer costs and adds management complexity. - Option D (Dedicated Host) addresses licensing/hardware isolation, not data transfer costs. A VPC endpoint is free (no hourly charges) and only costs for data transfer if crossing Regions, which isn ��t the case      here.       This       is       the       most      cost-effective       solution.       Reference       Link:       [AWS       VPC       Endpoints      for S3](https://docs.aws.amazon.com/vpc/latest/private link/vpc-endpoints-s3.html)

42.A company has an on-premises application that generates a large amount of time-sensitive data that is backed up to Amazon S3. The application has grown and there are user complaints about internet bandwidth limitations. A solutions architect needs to design a long-term solution that allows for both timely backups to Amazon S3 and with minimal impact on internet connectivity for internal users. Which solution meets these requirements?

A ��Establish AWS VPN connections and proxy all traffic through a VPC gateway endpoint.

B ��Establish a new AWS Direct Connect connection and direct backup traffic through this new connection.

C ��Order daily AWS Snowball devices. Load the data onto the Snowball devices and return the devices to AWS each day.

D ��Submit a support ticket through the AWS Management Console. Request the removal of S3 service limits from the account.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution is to establish an AWS Direct Connect connection. Here's why:
1. Bandwidth Issue: Direct Connect provides a dedicated network connection between on-premises systems and AWS, bypassing

the public internet. This solves the bandwidth bottleneck caused by regular internet connections. 2. Time-Sensitive  Data: Unlike daily Snowball deliveries (Option C), Direct Connect allows continuous real-time backups instead of batch transfers once per day.
3. Network Separation: Backup traffic uses the dedicated Direct Connect link, while regular user internet traffic uses the normal connection. This prevents backup activities from competing with user traffic. 4. Long-Term Solution: Direct Connect is designed for ongoing high-volume data transfers, unlike temporary solutions like VPN (Option A) which still rely on internet bandwidth, or Snowball which requires daily manual operations. 5. Cost Efficiency: For large daily data volumes, Direct Connect becomes more cost-effective   than   repeated    Snowball   device    rentals   over   time.    Reference    Links:   -   AWS    Direct   Connect    overview: https://aws.amazon.com/directconnect/                    -                    Comparing                    data                    transfer                     methods: https://aws.amazon.com/answers/networking/aws-data-transfer/

43.A company has an application that provides marketing services to stores. The services are based on previous purchases by store customers. The stores upload transaction data to the company through SFTP, and the data is processed and analyzed to generate new marketing offers. Some of the files can exceed 200 GB in size. Recently, the company discovered that some of the stores have uploaded files that contain personally identifiable information (P II) that should not have been included. The company wants administrators to  be alerted  if  PII  is  shared again. The  company also wants to automate  remediation. What should  a solutions architect do to meet these requirements with the LEAST development effort?

A��Use an Amazon S3 bucket as a secure transfer point. Use Amazon Inspector to scan the objects in the bucket. If objects contain P II, trigger an S3 Lifecycle policy to remove the objects that contain P II.

B��Use an Amazon S3 bucket as a secure transfer point. Use Amazon Macie to scan the objects in the bucket. If objects contain P II, use Amazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain P II.

C ��Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain P II, use Amazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain P II.

D ��Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain P II, use Amazon Simple Email Service (Amazon SES) to trigger a notification to the administrators and trigger an S3 Lifecycle policy to remove the meats that contain P II.

�𰸣�B

���� �� Correct Answer:  B  Detailed  Explanation: The question requires detecting  P II in large files uploaded to an S3 bucket, alerting administrators, and automating  remediation with  minimal development effort.  Here ��s why  B  is the  best choice:  1. Amazon Macie  is a managed service specifically designed to detect sensitive data like P II  using  machine learning.  It scans S3 objects  automatically,  eliminating  the  need  for  custom  code  (unlike  options  C/D,  which   require  building  and  maintaining Lambda-based scanners). 2. Amazon SNS triggers real-time alerts to administrators when Macie finds P II. While remediation in B is   manual  (admins  delete  the  files),  the  question  emphasizes   least  development  effort.  Automating   remediation  (e.g., auto-deleting files) would require additional Lambda functions or workflows, increasing complexity. 3. Why not other options: - A: Amazon Inspector scans for vulnerabilities, not P II. Lifecycle policies are time-based (e.g., delete after 1 day), not event-driven, so they can ��t immediately remove files. - C/D: Custom Lambda code for P II detection requires significant development effort, which contradicts the "least effort" requirement. Lifecycle policies in D also don ��t work for instant remediation. In summary, B uses purpose-built services (Macie + SNS) for detection and alerts with minimal coding, aligning with the requirement to prioritize simplicity. Full automation would require more effort, but the question doesn ��t mandate it as a strict condition. Reference Links:
- [Amazon Macie](https://aws.amazon.com/macie/) - [Amazon SNS](https://aws.amazon.com/sns/)

44.A  company  needs  guaranteed  Amazon  EC2  capacity  in  three  specific  Availability  Zones  in  a  specific  AWS  Region  for  an

upcoming event that will last 1 week. What should the company do to guarantee the EC2 capacity?

A ��Purchase Reserved Instances that specify the Region needed.

B ��Create an On-Demand Capacity Reservation that specifies the Region needed.

C ��Purchase Reserved Instances that specify the Region and three Availability Zones needed.

D ��Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed.

�𰸣�D

������ Correct Answer D �� Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed. Detailed Explanation To guarantee EC2 capacity in three specific Availability Zones (AZs) for a short-term event (1 week), the company should use On-Demand Capacity  Reservations. Here ��s why:  1. Capacity  Reservations vs.  Reserved  Instances: - Reserved Instances (RIs) are for long-term commitments (1-3 years) and primarily provide cost savings, not guaranteed capacity in  specific  AZs.  Even  "Zonal  RIs"  (which  reserve  capacity  in  a  specific  AZ)  require  a  long-term  commitment,  making  them unsuitable for a 1-week event. - On-Demand Capacity Reservations allow you to reserve capacity in specific AZs for any duration (even hours). You pay for the reserved capacity but can release it when no longer needed. 2. Why Option D is Correct: - Capacity Reservations are created per AZ. To guarantee capacity in three AZs, the company must create three separate reservations (one in each AZ). - Option D explicitly mentions specifying both the Region and three AZs, ensuring capacity is reserved exactly where needed. 3. Why Other Options Are Incorrect: - A/C (Reserved Instances): RIs are for long-term use, not a 1-week event. Regional RIs  don ��t guarantee AZ  capacity, and Zonal  RIs  require  a  1-3  year  term.  -  B  (Capacity  Reservation  without  AZs):  Capacity Reservations  must  specify  an  AZ.  A  Region-only  reservation  isn  �� t  possible.  In  short:  For  short-term,  AZ-specific  capacity guarantees,     On-Demand      Capacity     Reservations      are     the      only     solution.      Reference     Links      -      [AWS     Capacity Reservations](https://docs.aws.amazon.com/AWSEC2/latest/User guide/capacity-reservations.html)   -   [Reserved   Instances   vs. Capacity Reservations](https://aws.amazon.com/ec2/pricing/reserved-instances/)

45.A company's website uses an Amazon EC2 instance store for its catalog of items. The company wants to make sure that the catalog is highly available and that the catalog is stored in a durable location. What should a solutions architect do to meet these requirements?

A ��Move the catalog to Amazon Elastic ache for Redis.

B ��Deploy a larger EC2 instance with a larger instance store.

C ��Move the catalog from the instance store to Amazon S3 Glacier Deep Archive.

D ��Move the catalog to an Amazon Elastic File System (Amazon EFS) file system.

�𰸣�D

������ Correct Answer D �� Move the catalog to an Amazon Elastic File System (Amazon EFS) file system. Detailed Explanation The EC2 instance store is ephemeral storage, meaning data is lost if the instance stops, fails, or is terminated. To ensure high availability (HA) and durability, the catalog  must be stored in a persistent, shared, and redundant storage service. - Option A (Elastic ache for Redis) is a caching service, not durable storage. - Option B (larger instance store) does not solve the ephemeral nature of instance storage. - Option C (S3 Glacier  Deep Archive) is for long - term archival, not real - time access. - Option D (Amazon EFS) is a managed network - attached file system that provides scalable, shared, and durable storage. EFS automatically replicates data across multiple Availability Zones (AZs), ensuring HA and durability. Key Terms - Durability: Data survives failures

(e.g.,  hardware crashes). -  High Availability:  Data remains accessible even during outages. - EFS: A shared file system for  EC2 instances, designed for HA and durability. Reference Link [Amazon EFS Overview](https://aws.amazon.com/efs/)

46.A company stores call transcript files on a monthly basis. Users access the files randomly within 1 year of the call, but users access the files infrequently after 1 year. The company wants to optimize its solution by giving users the ability to query and retrieve files that are less than 1-year-old as quickly as possible. A delay in retrieving older files is acceptable. Which solution will meet these requirements MOST cost-effectively?

A ��Store individual files with tags in Amazon S3 Glacier Instant Retrieval. Query the tags to retrieve the files from S3 Glacier Instant Retrieval.

B��Store individual files in Amazon S3 Intelligent-Tiering. Use S3 Lifecycle policies to move the files to S3 Glacier Flexible Retrieval after 1 year. Query and retrieve the files that are in Amazon S3 by using Amazon Athena. Query and retrieve the files that are in S3 Glacier by using S3 Glacier Select.

C ��Store individual files with tags in Amazon S3 Standard storage. Store search metadata for each archive in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3 Glacier Instant Retrieval after 1 year. Query and retrieve the files by searching for metadata from Amazon S3.

D ��Store individual files in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3 Glacier Deep Archive after 1 year. Store search metadata in Amazon RDS. Query the files from Amazon RDS. Retrieve the files from S3 Glacier Deep Archive.

�𰸣�B

������ Correct Answer B Detailed Explanation The best solution is B because it balances cost efficiency and performance based on the access patterns: 1. Amazon S3 Intelligent-Tiering automatically moves files between "Frequent Access" and "Infrequent Access" tiers based on usage. For the first year, when files are accessed randomly (but still relatively often), this ensures quick retrieval without manual management. 2. After 1 year, S3 Lifecycle policies move files to S3 Glacier Flexible Retrieval, a cheaper storage tier for rarely accessed data.  Retrieval from Glacier  Flexible Retrieval takes minutes to hours, which  is acceptable for infrequent access. 3. Amazon Athena allows direct SQL queries on files stored in S3 (for  newer files), while S3 Glacier Select enables querying within Glacier (for older files). This avoids the cost/complexity of maintaining a separate metadata database (like RDS in Option D). Why not other options? - A: Glacier Instant Retrieval is designed for rare but immediate access, making it unnecessarily expensive for frequently accessed files (�� 1 year old). - C: Glacier Instant Retrieval is costlier than Glacier Flexible Retrieval for long-term storage. Separately managing metadata in S3 adds complexity. - D: Glacier Deep Archive has very slow retrieval (up to 48 hours) and requires managing an RDS database for metadata, increasing cost and effort. Key Take away Use S3 Intelligent-Tiering + Lifecycle policies to automate cost optimization based on access patterns. Pair with Athena/Glacier Select for querying,           avoiding            extra            infrastructure.            Reference            Links            -            [Amazon            S3            Storage Classes](https://aws.amazon.com/s3/storage-classes/)                                                                -                                                                 [S3
Intelligent-Tiering](https://aws.amazon.com/s3/storage-classes/intelligent-tiering/)                                 -                                 [Amazon
Athena](https://aws.amazon.com/athena/)                                             -                                              [S3                                              Glacier
Select](https://docs.aws.amazon.com/AmazonS3/latest/user guide/querying-glacier-data.html)

47.A  company  has  a  production  workload  that  runs  on   1,000  Amazon   EC2  Linux  instances.  The  workload  is  powered  by third-party  software.  The  company  needs  to  patch  the  third-party  software  on  all  EC2  instances  as  quickly  as  possible  to remediate a critical security vulnerability. What should a solutions architect do to meet these requirements?

A ��Create an AWS Lambda function to apply the patch to all EC2 instances.

B ��Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.

C ��Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2 instances.

D ��Use AWS Systems Manager Run Command to run a custom command that applies the patch to all EC2 instances.

�𰸣�D

������ Correct Answer D �� Use AWS Systems Manager Run Command to run a custom command that applies the patch to all EC2  instances.  Detailed  Explanation  The  best  approach  here  is  D  because:  -  AWS  Systems  Manager  Run  Command  allows executing a custom script or command across many EC2 instances simultaneously. Since the patch is for third - party software (not  managed  by AWS -  native tools),  a  custom command/script  is  likely  needed.  - Speed:  Run  Command  applies the  patch immediately without delays (unlike Maintenance Windows, which require scheduling). - Scalability: It handles 1,000 instances efficiently. - Patch Manager (Option B) is ideal for OS/Amazon - approved patches but may not support third - party software. Lambda (Option A) would require complex orchestration. Maintenance Window (Option C) adds unnecessary delay for a critical fix.                           Reference                           Link                          [AWS                           Systems                           Manager                           Run
Command](https://docs.aws.amazon.com/systems-manager/latest/user guide/execute-remote-commands.html)

48.A company wants to migrate its on-premises application to AWS. The application produces output files that vary in size from tens of gigabytes to hundreds of terabytes. The application data must be stored in a standard file system structure. The company wants a solution that scales automatically. is highly available, and requires minimum operational overhead. Which solution will meet these requirements?

A ��Migrate the application to run as containers on Amazon Elastic Container Service (Amazon ECS). Use Amazon S3 for storage.

B ��Migrate the application to run as containers on Amazon Elastic Ku bernet es Service (Amazon EKS). Use Amazon Elastic Block Store (Amazon EBS) for storage.

C ��Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) for storage.

D ��Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic Block Store (Amazon EBS) for storage.

�𰸣�C

�� �� �� Correct Answer:  C  Detailed  Explanation:  The  best  solution  is  C  because  Amazon  Elastic  File  System  (EFS)  is  a  fully managed, scalable, and highly available file storage service that automatically grows and shrinks as files are added or removed. It provides a standard file system structure (like NFS), which matches the application's requirement. By using EC2 instances in a Multi-AZ Auto Scaling group, the solution ensures high availability and automatic scaling of compute resources. EFS is designed for workloads with varying file sizes (from GBs to TBs) and requires minimal operational overhead compared to managing block storage (EBS) or object storage (S3). - Why not A/B: Amazon S3 (in A) is object storage, not a standard file system, and would require application changes. Amazon  EBS (in  B/D)  has volume size  limits  (up to  16 TB  per volume) and  lacks native  multi-AZ sharing, making it unsuitable for scaling to hundreds of TBs with minimal effort. - Why not D: EBS volumes are tied to individual EC2  instances  and  lack  the  automatic  scaling  and  shared  file  system  capabilities  of   EFS.   Reference  Links:  -   [Amazon  EFS Overview](https://aws.amazon.com/efs/) - [Amazon EBS vs. EFS](https://aws.amazon.com/ebs/features/)

49.A company needs to store its accounting records in Amazon S3. The records must be immediately accessible for 1 year and then must be archived for an additional 9 years. No one at the company, including administrative users and root users, can be

able to delete the records during the entire 10-year period. The records must be stored with maximum resiliency. Which solution will meet these requirements?

A ��Store the records in S3 Glacier for the entire 10-year period. Use an access control policy to deny deletion of the records for a period of 10 years.

B ��Store the records by using S3 Intelligent-Tiering. Use an IAM policy to deny deletion of the records. After 10 years, change the IAM policy to allow deletion.

C ��Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. Use S3 Object Lock in compliance mode for a period of 10 years.

D ��Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 year. Use S3 Object Lock in governance mode for a period of 10 years.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The solution uses S3 Object Lock in compliance mode, which prevents anyone (including root users) from deleting or altering the records for the 10-year period. The S3 Lifecycle policy transitions the data to S3 Glacier Deep Archive after 1 year, which is ideal for long-term archiving. S3 Glacier Deep Archive provides maximum resiliency by storing data across multiple Availability Zones. Key points for a beginner: - S3 Object Lock (compliance mode): Ensures no one can delete/modify the data for the specified retention period, even administrators. - Lifecycle Policy: Automatically moves data from S3 Standard (for immediate access) to Glacier Deep Archive (cheaper for archiving) after 1 year. - Resiliency: S3 Standard and Glacier  Deep Archive  both  store  data  redundantly  across  multiple AZs,  meeting  the  "maximum  resiliency"  requirement. Reference Link: [Amazon S3 Object Lock](https://docs.aws.amazon.com/AmazonS3/latest/user guide/object-lock.html)

50.A company runs multiple Windows workloads on AWS. The company's employees use Windows file shares that are hosted on two Amazon EC2 instances. The file shares synchronize data between themselves and maintain duplicate copies. The company wants a highly available and durable storage solution that preserves how users currently access the files. What should a solutions architect do to meet these requirements?

A ��Migrate all the data to Amazon S3. Set up IAM authentication for users to access files.

B ��Set up an Amazon S3 File Gateway. Mount the S3 File Gateway on the existing EC2 instances.

C ��Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration. Migrate all the data to FSx for Windows File Server.

D ��Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ configuration. Migrate all the data to Amazon EFS.

�𰸣�C

���� �� Correct Answer: C Detailed Explanation: The company needs a highly available and durable Windows-compatible file storage  solution  that   maintains   user   access   methods.  Amazon   FSx  for  Windows   File  Server   is  designed  for  Windows environments and supports the SMB  protocol,  ensuring  seamless  integration  with  existing Windows workflows.  Its  Multi-AZ configuration  automatically  replicates  data  across  Availability  Zones  (AZs),  providing  high  availability  and  durability  without requiring  manual synchronization. - Why  not A/B/D? - A (Amazon S3): S3  is object storage,  not a direct  replacement for file shares.  Users  would  need  to  switch  to  a  different  access  method  (e.g.,  APIs),  breaking  their  current  workflow.  -  B  (S3  File

Gateway): While  it  provides an SMB  interface,  it  relies on the existing  EC2 instances, which could still fail.  FSx offers  built-in Multi-AZ redundancy, which is more robust. - D (Amazon EFS): EFS is for Linux-based systems and doesn ��t support Windows file protocols like SMB. Reference Links: - [Amazon FSx for Windows File Server](https://aws.amazon.com/fsx/windows/) - [Multi-AZ for Amazon FSx](https://docs.aws.amazon.com/fsx/latest/Windows guide/high-availability-multiAZ.html)

51.A solutions architect is developing a VPC architecture that includes multiple subnets. The architecture will host applications that use Amazon EC2 instances and Amazon RDS DB instances. The architecture consists of six subnets in two Availability Zones. Each Availability Zone includes a public subnet, a private subnet, and a dedicated subnet for databases. Only EC2 instances that run in the private subnets can have access to t he RDS databases. Which solution will meet these requirements?

A �� Create a  new  route table that excludes the  route to the  public subnets'  CIDR  blocks. Associate the  route table with the database subnets.

B ��Create a security group that denies inbound traffic from the security group that is assigned to instances in the public subnets. Attach the security group to the DB instances.

C��Create a security group that allows inbound traffic from the security group that is assigned to instances in the private subnets. Attach the security group to the DB instances.

D ��Create a new peering connection between the public subnets and the private subnets. Create a different peering connection between the private subnets and the database subnets.

�𰸣�C

���� �� Correct Answer: C Detailed Explanation: To restrict access to t he RDS databases to only EC2 instances in the private subnets, you must configure security groups (not route tables or peering connections). Security groups act as virtual firewalls for AWS resources. Here's how it works: 1. The RDS database has a security group attached to it. 2. This security group should allow inbound traffic (e.g., MySQL, Postgresql ports) only from the security group assigned to the private subnet EC2 instances. 3. By doing this, the RDS database explicitly permits connections from the private subnet EC2 instances while blocking all other traffic by default (including public subnet instances). Why other options are wrong: - A: Route tables control traffic routing between subnets but cannot enforce granular access control (e.g., blocking specific EC2 instances). - B: Security groups default to deny all inbound traffic, so explicitly denying public subnet traffic is unnecessary and redundant. - D: VPC peering connects separate VPCs, but   all   subnets   here   are    in   the   same   VPC.    Peering   isn't    required   or   useful    here.   Reference    Links:   [AWS   Security Groups](https://docs.aws.amazon.com/vpc/latest/user guide/VPC_Security groups.html)                         [Amazon                          RDS
Security](https://aws.amazon.com/rds/features/security/)

52.A  company   has  registered  its  domain   name  with  Amazon   Route  53.  The  company   uses  Amazon  API  Gateway  in  the ca-central-1 Region as a public interface for its backend micro service APIs. Third-party services consume the APIs securely. The company wants to  design  its API  Gateway  URL  with the  company's  domain  name  and  corresponding  certificate so that the third-party services can use HTTPS. Which solution will meet these requirements?

A �� Create  stage variables  in API Gateway with  Name=`Endpoint-URL`  and Value=`Company  Domain  Name` to overwrite the default URL. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM).

B ��Create Route 53 DNS records with the company's domain name. Point the alias record to the Regional API Gateway stage endpoint. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region.

C ��Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the

public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the same Region. Attach the certificate to the API Gateway endpoint. Configure Route 53 to route traffic to the API Gateway endpoint.

D ��Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain  name  into AWS Certificate  Manager  (ACM)  in the  us-east-1  Region. Attach the certificate to the API Gateway APIs. Create Route 53 DNS records with the company's domain name. Point an A record to the company's domain name.

�𰸣�C

������ Correct Answer C Detailed Explanation To use a custom domain name with Amazon API Gateway and enforce HTTPS, you need to: 1. Create a Regional API Gateway endpoint (since the question specifies the `ca-central-1` Region). 2. Import the SSL/TLS certificate for the company ��s domain into AWS Certificate Manager (ACM) in the same Region as the API Gateway (in this case, `ca-central-1`). Certificates for Regional API Gateway endpoints must be in the same Region. 3. Attach the certificate to the API Gateway custom domain name configuration. 4. Configure Route 53 to route traffic to the API Gateway endpoint using an alias record. Why Other Options Are Incorrect - Option A: Stage variables are used to manage environment-specific configurations (like  backend  URLs),  not  for  custom  domain  names  or  certificates.  -  Option  B:  ACM  certificates  for  Regional  API  Gateway endpoints  must  be  in  the  same  Region  as  the  API  Gateway  (`ca-central-1`  here),  not  `us-east-1`  (which  is  required  only  for edge-optimized endpoints). - Option D: Certificates for Regional API Gateway endpoints cannot be in `us-east-1`; they must be in the   same    Region   as   the   API    Gateway.   Reference    Link    [Setting   Up    Custom   Domain    Names   for    REST   APIs    in   API Gateway](https://docs.aws.amazon.com/api gateway/latest/developer guide/how-to-custom-domains.html)

53.A company is running a popular social media website. The website gives users the ability to upload images to share with other users. The company wants to make sure that the images do not contain inappropriate content. The company needs a solution that minimizes development effort. What should a solutions architect do to meet these requirements?

A ��Use Amazon Comprehend to detect inappropriate content. Use human review for low-confidence predictions.

B ��Use Amazon Re k ognition to detect inappropriate content. Use human review for low-confidence predictions.   C ��Use Amazon Sage maker to detect inappropriate content. Use ground truth to label low-confidence predictions.

D �� Use AWS Fargate to deploy a custom machine learning model to detect inappropriate content. Use ground truth to label low-confidence predictions.

�𰸣�B

�� �� �� Correct Answer:  B  Detailed  Explanation:  Amazon  Re k ognition  is  specifically  designed  for  image  and video  analysis, including detecting inappropriate content like explicit or suggestive imagery. Since the requirement is to minimize development effort,  using  a  fully  managed   pre-trained  service  like  Re k ognition  (which   requires  no   ML  model  training)  is  ideal.  For low-confidence predictions, human review can be added as a fall back. Other options are less optimal: - A (Comprehend): Text analysis  only,  not  for  images.  -  C  (Sage maker  +  Ground  Truth):  Requires  building/training  a  custom  model,  which  increases development work.  -  D  (Fargate  +  custom  model):  Even  more  development-heavy  due  to  container  deployment  and  model maintenance.                                   Reference:                                   [Amazon                                    Re k ognition                                   Content
Moderation](https://docs.aws.amazon.com/re k ognition/latest/dg/moderation.html)

54.A  company  wants  to  run  its  critical  applications  in  containers  to  meet  requirements  for  scalability  and  availability.  The company  prefers  to  focus  on  maintenance  of  the  critical  applications.  The  company  does  not  want  to  be  responsible  for provisioning and managing the underlying infrastructure that runs the containerized workload. What should a solutions architect

do to meet these requirements?

A ��Use Amazon EC2 instances, and install Docker on the instances.

B ��Use Amazon Elastic Container Service (Amazon ECS) on Amazon EC2 worker nodes.

C ��Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate.

D ��Use Amazon EC2 instances from an Amazon Elastic Container Service (Amazon ECS)-optimized Amazon Machine Image (AMI).

�𰸣�C

���� �� Correct Answer C �� Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Detailed Explanation The company wants to run containerized applications without managing the underlying infrastructure. AWS Fargate is a serverless compute engine for containers that removes the need to provision and manage EC2 instances. With ECS on Fargate, the company can focus solely on deploying and maintaining their critical applications, while AWS automatically handles server provisioning, scaling, and availability. - Option A (EC2 + Docker): Requires manual setup and management of EC2 instances, which violates the requirement of avoiding infrastructure management. - Option B (ECS on EC2): Still requires managing EC2 worker nodes (e.g., patching, scaling), which the company wants to avoid. - Option D (ECS-optimized AMI): Uses pre-configured EC2 instances but still requires managing the servers. - Option C (ECS on Fargate):  Fully serverless �� no EC2 management needed. AWS handles infrastructure, scaling, and availability. Reference Link [AWS Fargate](https://aws.amazon.com/fargate/)

55.A company hosts more than 300 global websites and applications. The company requires a platform to analyze more than 30 TB of clickstream data each day. What should a solutions architect do to transmit and process the clickstream data?

A ��Design an AWS Data Pipeline to archive the data to an Amazon S3 bucket and run an Amazon EMR cluster with the data to generate analytics.

B ��Create an Auto Scaling group of Amazon EC2 instances to process the data and send it to an Amazon S3 data lake for Amazon Redshift to use for analysis.

C ��Cache the data to Amazon Cloud front. Store the data in an Amazon S3 bucket. When an object is added to the S3 bucket. run an AWS Lambda function to process the data for analysis.

D ��Collect the data from Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to transmit the data to an Amazon S3 data lake. Load the data in Amazon Redshift for analysis.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The best approach is to use Amazon Kinesis Data Streams to collect real-time clickstream  data  and Amazon  Kinesis  Data  Firehose  to  transmit  it.  Here �� s  why:  1.  Amazon  Kinesis  Data  Streams:  Handles high-volume, real-time data ingestion (like 30 TB/day of click streams) with low latency and automatic scaling. 2. Amazon Kinesis Data Firehose: Automatically batches and loads streaming data into Amazon S3, creating a "data lake." It also integrates with Amazon Redshift to load data directly for analytics. 3. Amazon Redshift: A fast, scalable data warehouse for analyzing massive datasets using SQL queries. Other options are less optimal: - Option A (Data Pipeline + EMR) is batch-oriented, not real-time. - Option B (EC2 Auto Scaling) requires manual infrastructure management and lacks real-time capabilities. - Option C (Cloud front + Lambda) is unsuitable because Cloud front is a CDN (not for data collection), and Lambda has runtime limits for large datasets. Reference  Links:  -  [Amazon  Kinesis  Data  Streams](https://aws.amazon.com/kinesis/data-streams/)  -  [Amazon  Kinesis  Data Firehose](https://aws.amazon.com/kinesis/data-firehose/) - [Amazon Redshift](https://aws.amazon.com/redshift/)

56.A company has a website hosted on AWS. The website is  behind an Application Load  Balancer (ALB) that is configured to handle  HTTP and  HTTPS separately. The company wants to forward all  requests to the website so that the  requests will  use HTTPS. What should a solutions architect do to meet this requirement?

A��Update the ALB's network ACL to accept only HTTPS traffic.

B��Create a rule that replaces the HTTP in the URL with HTTPS.

C��Create a listener rule on the ALB to redirect HTTP traffic to HTTPS.

D ��Replace the ALB with a Network Load Balancer configured to use Server Name Indication (SNI).

�𰸣�C

������ Correct Answer C�� Create a listener rule on the ALB to redirect HTTP traffic to HTTPS. Detailed Explanation To ensure all website traffic uses HTTPS, the best approach is to configure the Application Load Balancer (ALB) to automatically redirect HTTP (port  80)  requests  to  HTTPS  (port  443).  Here �� s  why:  Option  C  is  correct  because  ALBs  natively  support  listener  rules  for redirection. You can set  up  an  HTTP  listener  (port  80)  with  a  rule  that  redirects  all traffic to  HTTPS. This  returns  a  301/302 redirect response to clients, forcing browsers to use HTTPS without blocking HTTP requests (unlike Option A). Why not the others?
- Option A  (block  HTTP via  Network ACLs) would  reject  HTTP  requests outright, causing errors instead of  redirecting  users. - Option B (rewrite URLs) isn ��t a standard ALB feature; ALBs can ��t modify URLs in - flight without additional tools like Lambda. - Option  D  (using  a  Network  Load  Balancer)  isn ��t  suitable  because  NLBs  operate  at  the  transport  layer  (Layer  4)  and  lack HTTP/HTTPS redirection capabilities (which require Layer 7 features). This method is simple, cost - effective, and leverages ALB��s built        -         in        functionality         to        enforce         HTTPS.         Reference         Links        -         [ALB         HTTPS         Redirection Guide](https://docs.aws.amazon.com/elastic load balancing/latest/application/listener-update-rules.html#redirect-actions) - [ALB vs NLB Differences](https://aws.amazon.com/elastic load balancing/features/)

57.A company is developing a two-tier web application on AWS. The company's developers have deployed the application on an Amazon  EC2 instance that connects directly to a backend Amazon  RDS database. The company must  not hardcode database credentials in the application. The company must also implement a solution to automatically rotate the database credentials on a regular basis. Which solution will meet these requirements with the LEAST operational overhead?

A��Store the database credentials in the instance metadata. Use Amazon Event bridge (Amazon Cloud watch Events) rules to run a scheduled AWS Lambda function that updates t he RDS credentials and instance metadata at the same time.

B ��Store the database credentials in a configuration file in an encrypted Amazon S3 bucket. Use Amazon Event bridge (Amazon Cloud watch Events) rules to run a scheduled AWS Lambda function that updates t he RDS credentials and the credentials in the configuration file at the same time. Use S3 Versioning to ensure the ability to fall back to previous values.

C ��Store the database credentials as a secret in AWS Secrets Manager. Turn on automatic rotation for the secret. Attach the required permission to the EC2 role to grant access to the secret.

D �� Store the  database  credentials  as  encrypted  parameters  in  AWS  Systems  Manager  Parameter  Store.  Turn  on  automatic rotation  for  the  encrypted  parameters.  Attach  the  required  permission  to  the  EC2   role  to  grant  access  to  the  encrypted parameters.

���� �� Correct Answer C Explanation The best solution is C because AWS Secrets Manager is specifically designed to handle secrets  like  database  credentials  securely.  Here's  why:  1.  No  Hardcoded  Credentials:  The  EC2  instance  retrieves  credentials directly  from  Secrets   Manager,  avoiding  hardcoding.  2.  Automatic   Rotation:  Secrets   Manager  natively  supports  automatic rotation for RDS credentials. You just enable it, and AWS handles the rotation process (e.g., updating the password in  RDS and Secrets Manager). 3.  Least Operational Overhead:  No need to write custom  Lambda functions or manage S3 buckets. Secrets Manager  automates  rotation,  reducing  manual  work.  Options  A  and  B  require  custom  Lambda  code  and  manual  credential updates, which adds complexity. Option D uses Parameter Store, which can store secrets but lacks native automatic rotation for RDS         credentials         (you         ��   d          need         custom          scripts).         Reference         Links          -         [AWS         Secrets Manager](https://aws.amazon.com/secrets-manager/)                -                [Rotating                AWS                Secrets                Manager secrets](https://docs.aws.amazon.com/secrets manager/latest/user guide/rotating-secrets.html)

58.A company is deploying a new public web application to AWS. The application will run behind an Application Load Balancer (ALB). The application  needs to be encrypted at the edge with an SSL/TLS certificate that is  issued  by an external certificate authority (CA). The certificate must be rotated each year before the certificate expires. What should a solutions architect do to meet these requirements?

A��Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate. Apply the certificate to the ALB. Use the managed renewal feature to automatically rotate the certificate.

B ��Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate. Import the key material from the certificate. Apply the certificate to t he ALUse the managed renewal feature to automatically rotate the certificate.

C ��Use AWS Certificate Manager (ACM) Private Certificate Authority to issue an SSL/TLS certificate from the root CA. Apply the certificate to the ALB. Use the managed renewal feature to automatically rotate the certificate.

D �� Use  AWS  Certificate  Manager  (ACM)  to  import  an  SSL/TLS  certificate.  Apply  the  certificate  to  the  ALB.   Use  Amazon Event bridge (Amazon Cloud watch Events) to send a notification when the certificate is nearing expiration. Rotate the certificate manually.

�𰸣�D

���� �� Correct Answer:  D  Detailed Explanation: The company needs an SSL/TLS certificate issued by an external CA, which
means they cannot use ACM ��s auto-renewal feature (only available for ACM-issued certificates). Here ��s why Option D is correct:
1. Import the certificate into ACM: Since the certificate is issued externally, it must be imported into ACM to attach it to the ALB.
2.  Use  Amazon  Event bridge  for  expiration  alerts:  ACM  tracks  the  expiration  date  of  imported  certificates.  Event bridge (Cloud watch Events) can trigger notifications (e.g., via Amazon SNS) when the certificate nears expiration. 3. Manual rotation: The team must manually renew the certificate with the external CA and re-import it into ACM before expiration. Other options fail because: - A/B/C assume ACM auto-renewal works for external certificates, which it doesn ��t. - C uses ACM Private CA, which is       for       internal        certificates,       not        external       ones.        Reference        Links:       -        [AWS       Certificate        Manager FAQs](https://aws.amazon.com/certificate-manager/faqs/)                    -                    [Importing                    Certificates                    into ACM](https://docs.aws.amazon.com/acm/latest/user guide/import-certificate.html)

59.A  company  runs  its  infrastructure  on  AWS  and  has  a  registered  base  of  700,000  users  for  its  document  management application. The company intends to create a product that converts large .pdf files to .jpg image files. The .pdf files average 5 MB in size. The company needs to store the original files and the converted files. A solutions architect must design a scalable solution to accommodate demand that will grow rapidly over time. Which solution meets these requirements MOST cost-effectively?

A ��Save the .pdf files to Amazon S3. Configure an S3 PUT event to invoke an AWS Lambda function to convert the files to .jpg format and store them back in Amazon S3.

B ��Save the .pdf files to Amazon Dynamo d use the Dynamo db Streams feature to invoke an AWS Lambda function to convert the files to .jpg format and store them back in Dynamo db.

C ��Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic Block Store (Amazon EBS) storage, and an Auto Scaling group. Use a program in the EC2 instances to convert the files to .jpg format. Save the .pdf files and the .jpg files in t he EBS store.

D ��Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic File System (Amazon EFS) storage, and an Auto Scaling group. Use a program in the EC2 instances to convert the file to .jpg format. Save the .pdf files and the .jpg files in t he EBS store.

�𰸣�A

������ Correct Answer A Detailed Explanation The best solution is A because: 1. Serverless and Scalable: - Amazon S3 is ideal for storing  large  files  (like  5MB  PDFs)  cost-effectively.  -  AWS  Lambda  automatically  scales  to  handle  demand  without  manual infrastructure  management.  When  a  user  uploads  a  PDF  to  S3,  the  "PUT  event"  triggers  Lambda  instantly,  ensuring  rapid processing. 2. Cost Optimization: - Lambda charges only for execution time (per millisecond), so costs align directly with usage. This avoids the fixed cost of running EC2 instances (Options C/D) or Dynamo db (Option B). - S3 storage costs are much lower than Dynamo db (Option B) or EBS/EFS (Options C/D). 3. Avoid Poor Choices: - Option B is invalid because Dynamo db is a NoSQL database, not designed for large file storage. It has item size limits (400KB per item) and would be expensive for 5MB files. - Options  C/D  use  EC2  instances  with  Elastic  Beanstalk,  which  requires  managing  servers,  scaling  policies,  and  storage.  EC2 instances run continuously (costly during low demand), and EBS/EFS adds complexity for file sharing across instances. Why A Wins:  -  Fully  automated,  pay-as-you-go,  and  no  infrastructure  management.  -  S3  +  Lambda  is  a  standard  pattern  for  file processing           workflows             on           AWS.             Reference            Links            -            [AWS            Lambda            with             S3 Triggers](https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html)                           -                           [Amazon                           S3

Pricing](https://aws.amazon.com/s3/pricing/)



-


[Dynamo db

Limits](https://docs.aws.amazon.com/amazon dynamo db/latest/developer guide/Limits.html)

60.A company has more than 5 TB of file data on Windows file servers that run on premises. Users and applications interact with the data each day. The company is moving its Windows workloads to AWS. As the company continues this process, the company requires  access to AWS  and  on-premises  file  storage  with  minimum  latency.  The  company  needs  a  solution  that  minimizes operational  overhead  and  requires  no  significant  changes  to  the  existing  file  access  patterns.  The  company  uses  an  AWS Site-to-Site VPN connection for connectivity to AWS. What should a solutions architect do to meet these requirements?

A ��Deploy and configure Amazon FSx for Windows File Server on AWS. Move the on-premises file data to FSx for Windows File Server. Reconfigure the workloads to use FSx for Windows File Server on AWS.

B �� Deploy  and  configure an Amazon S3  File  Gateway on  premises.  Move  the on-premises file  data to the S3  File  Gateway. Reconfigure the on-premises workloads and the cloud workloads to use the S3 File Gateway.

C��Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to Amazon S3. Reconfigure the workloads to use either Amazon S3 directly or the S3 File Gateway. depending on each workload's location.

D �� Deploy and configure Amazon FSx for Windows File Server on AWS. Deploy and configure an Amazon FSx File Gateway on premises. Move the on-premises file data to the FSx File Gateway. Configure the cloud workloads to use FSx for Windows File Server on AWS. Configure the on-premises workloads to use the FSx File Gateway.

������ Correct Answer: D Detailed Explanation: The company needs to access file storage both on AWS and on-premises with minimal latency during migration. Here's why Option D works best: 1. Amazon FSx for Windows File Server on AWS provides fully managed Windows-native file shares (SMB protocol), matching existing on-premises access patterns. This avoids major changes for applications/workloads. 2. FSx File Gateway (part of AWS Storage Gateway) deployed on-premises acts as a local cache. It keeps frequently accessed data locally, minimizing latency for on-premises users/apps while syncing with the central FSx in AWS.
3. No access  pattern changes: On-premises apps use the same SMB file paths (pointing to FSx  File Gateway), and cloud apps access FSx directly. Both environments see a consistent Windows file system. 4. Operational simplicity: FSx and Storage Gateway are  fully  managed  services,  reducing  maintenance  overhead.  Other  options  fail  because:  -  A  forces  all  workloads  to  AWS immediately, causing high latency for on-premises users. - B/C use S3 (object storage) requiring app changes to use S3 APIs or adapt to S3 File Gateway's limited SMB features, breaking existing workflows. - D uniquely maintains native Windows file access in          both           locations          with           caching          for           low           latency.          Reference:           [Amazon           FSx          File Gateway](https://docs.aws.amazon.com/file gateway/latest/files3/ʲô�� File gateway.html#file-gateway-fsx)

61.A hospital recently deployed a RESTful API with Amazon API Gateway and AWS Lambda. The hospital uses API Gateway and Lambda to upload reports that are in PDF format and JPEG format. The hospital needs to modify the Lambda code to identify protected  health  information  (PHI)  in  the  reports.  Which  solution  will  meet  these  requirements  with  the  LEAST  operational overhead?

A ��Use existing Python libraries to extract the text from the reports and to identify the PHI from the extracted text.

B ��Use Amazon T extract to extract the text from the reports. Use Amazon Sage maker to identify the PHI from the extracted text.

C �� Use Amazon T extract to extract the text from the reports. Use Amazon Comprehend Medical to identify the PHI from the extracted text.

D ��Use Amazon Re k ognition to extract the text from the reports. Use Amazon Comprehend Medical to identify the PHI from the extracted text.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The correct solution uses Amazon T extract and Amazon Comprehend Medical. Here's why this is the best option with minimal operational overhead: 1. Amazon T extract is purpose-built for extracting text and data from  scanned  documents  (JPEG)  and  PDFs.  It  handles  complex  layouts  and  tables  better  than  generic  Python  libraries (Option A) or Amazon Re k ognition (Option D, which focuses on images/videos, not documents). 2. Amazon Comprehend Medical is a pre-trained service specifically designed to detect  Protected  Health  Information  (PHI)  like patient  names, diagnoses, and medications. It requires no machine  learning expertise or model training, unlike Amazon Sage maker (Option B), which would require coding and maintaining a custom model. 3. Both services are fully managed by AWS, meaning the hospital only needs to call their APIs in the Lambda function. This eliminates the need to maintain libraries (Option A) or ML infrastructure (Option B). Option D fails because Re k ognition is not optimized for document text extraction. Option A requires custom code for parsing PDFs/JPEGs, which is error-prone. Option C combines the right tools for the job with minimal coding. Reference Links: - Amazon T extract: https://aws.amazon.com/t extract/ - Amazon Comprehend Medical: https://aws.amazon.com/comprehend/medical/

62.A company has an application that generates a large number of files, each approximately 5 MB in size. The files are stored in Amazon S3. Company policy requires the files to be stored for 4 years  before they can be deleted.  Immediate accessibility is always required as the files contain critical business data that is not easy to reproduce. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first 30 days. Which storage solution is MOST cost-effective?

A ��Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Glacier 30 days from object creation. Delete the files

4 years after object creation.

B��Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) 30 days from object creation. Delete the files 4 years after object creation.

C ��Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Delete the files 4 years after object creation.

D ��Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Move the files to S3 Glacier 4 years after object creation.

�𰸣�C

������ Correct Answer: C. Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Delete the files 4 years after object creation. Detailed Explanation: 1. Immediate Accessibility Requirement: The files are critical and must always be accessible. S3 Glacier (options A and D) introduces retrieval delays (minutes to hours), violating the immediate access requirement. S3 Standard-IA and One Zone-IA maintain millisecond access. 2. Durability vs. Cost: S3 One Zone-IA (option B) stores data in only one Availability Zone, risking data loss if that zone fails. Since the files are "critical business data," S3 Standard-IA (99.999999999% durability across multiple AZs) is safer despite slightly higher costs than One Zone-IA. 3. Cost Optimization: - First 30 days: S3 Standard is ideal for frequent access. - After 30 days: S3 Standard-IA reduces storage costs by ~40% compared to S3 Standard while maintaining instant access for rare retrievals. - No Glacier/Deep Archive: Glacier  is cheaper  but  unsuitable  due to  retrieval delays.  Deleting  after 4 years  (instead  of  moving to Glacier) avoids unnecessary complexity. 4. Lifecycle Policy Alignment: Moving to Standard-IA at 30 days and deleting after 4 years meets    all     requirements:    cost-efficiency,     accessibility,    and     compliance.    Reference     Links:     -    [Amazon     S3    Storage Classes](https://aws.amazon.com/s3/storage-classes/)                                      -                                      [S3                                      Lifecycle
Policies](https://docs.aws.amazon.com/AmazonS3/latest/user guide/object-lifecycle-mgmt.html)

63.A company hosts an application on multiple Amazon EC2 instances. The application processes messages from an Amazon SQS queue, writes to an Amazon RDS table, and deletes the message from the queue. Occasional duplicate records are found in the RDS table. The SQS queue does not contain any duplicate messages. What should a solutions architect do to ensure messages are being processed once only?

A ��Use the Create queue API call to create a new queue.

B ��Use the Add permission API call to add appropriate permissions.

C ��Use the Receive message API call to set an appropriate wait time.

D ��Use the Change message visibility API call to increase the visibility timeout.

�𰸣�D

���� �� Correct Answer D �� Use the Change message visibility API call to increase the visibility timeout. Detailed Explanation When an EC2 instance receives a message from an SQS queue, the message becomes invisible to other consumers temporarily (default: 30 seconds). This is called the visibility timeout. The problem occurs when: 1. An EC2 instance takes too long to process the  message  (write  to  RDS  +  delete  message)  2.  The  visibility  timeout  expires  before  processing  finishes  3.  SQS  makes  the message visible again  �� another EC2 instance processes it   �� duplicate record Solution: By using `Change message visibility`, you can extend the visibility timeout while the message is being processed. This gives the EC2 instance enough time to complete all tasks (write to RDS + delete message)  before the timeout expires, preventing other instances from reprocessing the same

message. Other options don't solve the core issue: - A (new queue): Doesn't address processing logic - B (permissions): Unrelated to message duplication - C (wait time): Affects how long SQS waits for messages during polling, not visibility duration Reference

Links



-


[SQS


Visibility

Timeout](https://docs.aws.amazon.com/Aws simple queue service/latest/Sqs developer guide/sqs-visibility-timeout.html)            - [Change message visibility
API](https://docs.aws.amazon.com/Aws simple queue service/latest/Api reference/API_Change message visibility.html)

64.A solutions architect is designing a new hybrid architecture to extend a company's on-premises infrastructure to AWS. The company requires a highly available connection with consistent low latency to an AWS Region. The company needs to minimize costs and is willing to accept slower traffic if the primary connection fails. What should the solutions architect do to meet these requirements?

A �� Provision  an AWS  Direct  Connect connection to a  Region.  Provision  a VPN  connection as a  backup if the  primary  Direct Connect connection fails.

B ��Provision a VPN tunnel connection to a Region for private connectivity. Provision a second VPN tunnel for private connectivity and as a backup if the primary VPN connection fails.

C��Provision an AWS Direct Connect connection to a Region. Provision a second Direct Connect connection to the same Region as a backup if the primary Direct Connect connection fails.

D �� Provision an AWS Direct Connect connection to a  Region.  Use the  Direct Connect failover attribute from the AWS CLI to automatically create a backup connection if the primary Direct Connect connection fails.

�𰸣�A

������ Correct Answer: A Explanation: The best solution is to use AWS Direct Connect as the primary connection for consistent low latency and high availability. To minimize costs, a VPN connection (which is cheaper but slower) is set up as a backup. This meets the requirement of accepting slower traffic if the primary connection fails. Direct Connect provides dedicated, low-latency network connectivity between on-premises and AWS. VPN acts as a cost-effective backup but has higher latency (since it uses the public internet). Options C and D involve redundant Direct Connect connections, which are more expensive and don ��t align with the goal of cost minimization. Option B relies only on VPN, which doesn ��t guarantee low latency for the primary connection. Reference          Links:           [AWS           Direct           Connect](https://aws.amazon.com/directconnect/)           [AWS          Site-to-Site VPN](https://aws.amazon.com/vpn/)

65.A company is running a business-critical web application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances are in an Auto Scaling group. The application uses an Amazon Aurora Postgresql database that is deployed in a single Availability Zone. The company wants the application to be highly available with minimum downtime and minimum loss of data. Which solution will meet these requirements with the LEAST operational effort?

A �� Place  the  EC2  instances  in  different  AWS  Regions.  Use  Amazon  Route  53  health  checks  to  redirect  traffic.  Use  Aurora Postgresql Cross-Region Replication.

B ��Configure the Auto Scaling group to use multiple Availability Zones. Configure the database as Multi-AZ. Configure an Amazon RDS Proxy instance for the database.

C �� Configure the Auto Scaling  group to  use  one Availability Zone. Generate  hourly snapshots  of the database.  Recover the database from the snapshots in the event of a failure.

D ��Configure the Auto Scaling group to use multiple AWS Regions. Write the data from the application to Amazon S3. Use S3 Event Notifications to launch an AWS Lambda function to write the data to the database.

�𰸣�B

������ Correct Answer B Detailed Explanation The question requires a solution for high availability (HA) with minimal downtime, data loss, and operational effort. Here ��s why Option B is the best choice: 1. Auto Scaling Group in Multiple Availability Zones (AZs): - Spreading EC2 instances across multiple AZs ensures redundancy. If one AZ fails, the Auto Scaling group automatically redistributes  traffic  to   healthy  instances  in  other  AZs.  This   minimizes  downtime  without   manual   intervention.  2.  Aurora Postgresql Multi-AZ Deployment: - Aurora Multi-AZ automatically creates a standby replica in a different AZ. During an AZ failure, AWS promotes the standby to the primary instance within seconds, ensuring database availability. This reduces data loss and downtime  while  requiring  no  manual  effort  (managed  by  AWS).  3.  RDS  Proxy:  -  RDS  Proxy  manages  database  connections efficiently during failover s. It routes traffic to the healthy database instance automatically, preventing application errors during database  failover. This  further  reduces  operational  complexity.  Why  Other  Options  Are  Incorrect:  -  Option  A:  Cross-Region replication  adds  complexity  and  cost.  Multi-Region  deployments  are  overkill  unless  the  application  requires  global  disaster recovery (not specified here). - Option C: Using a single AZ for Auto Scaling and hourly snapshots risks downtime and data loss (up to an hour). Manual recovery from snapshots is time-consuming and error-prone. - Option D: Multi-Region deployments for EC2 and S3-based data synchronization introduce significant operational overhead. Lambda-based solutions require custom code and  monitoring,  increasing  effort.  Key  AWS  Services:  -  Multi-AZ  Aurora:  Automates  database  failover.  -  Auto  Scaling  Group: Distributes  EC2  instances  across  AZs.  -  RDS  Proxy:  Simplifies  database  connection  management.  Reference  Links:  -  [Amazon Aurora    Multi-AZ](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/Concepts.Aurora high availability.html)    - [Auto  Scaling  Groups  Across  AZs](https://docs.aws.amazon.com/auto scaling/ec2/user guide/auto-scaling-benefits.html)  -  [RDS Proxy](https://aws.amazon.com/rds/proxy/)

66.A  company's  HTTP  application  is  behind  a  Network  Load  Balancer  (NLB).  The  NLB's  target  group  is  configured  to  use  an Amazon EC2 Auto Scaling group with multiple EC2 instances that run the web service. The company notices that the NLB is not detecting HTTP errors for the application. These errors require a manual restart of the EC2 instances that run the web service. The company needs to improve the application's availability without writing custom scripts or code. What should a solutions architect do to meet these requirements?

A��Enable HTTP health checks on the NLB, supplying the URL of the company's application.

B��Add a cron job to the EC2 instances to check the local application's logs once each minute. If HTTP errors are detected. the application will restart.

C �� Replace  the  NLB  with  an Application  Load  Balancer.  Enable  HTTP  health  checks  by  supplying  the  URL  of  the  company's application. Configure an Auto Scaling action to replace unhealthy instances.

D ��Create an Amazon Cloud Watch alarm that monitors the Unhealthy hostcount metric for the NLB. Configure an Auto Scaling action to replace unhealthy instances when the alarm is in the ALARM state.

�𰸣�C

�� �� �� Correct Answer C  Detailed  Explanation  The  key  issue  is that the  Network  Load  Balancer  (NLB)  operates  at  Layer  4
(TCP/UDP) and cannot perform HTTP health checks, meaning it only checks if the EC2 instance is reachable via TCP, not if the application is returning  HTTP errors. - Option C replaces the NLB with an Application  Load Balancer (ALB), which operates at Layer 7 (HTTP/HTTPS) and supports HTTP health checks. By configuring the ALB to check the application ��s URL, it can detect HTTP errors (e.g., 5xx errors) and mark instances as unhealthy. The Auto Scaling group will then automatically replace unhealthy instances, improving availability without custom scripts. - Other options fail  because: - A:  NLB does  not support  HTTP  health

checks. - B: Requires custom scripts (cron job), which violates the requirement. - D: The NLB ��s `Unhealthy hostcount` metric only reflects          TCP-layer          failures,           not           HTTP           errors.          Reference           Link           [AWS           Load          Balancer Types](https://docs.aws.amazon.com/elastic load balancing/latest/user guide/load-balancer-types.html)

67.A  company  runs  a  shopping  application  that   uses  Amazon   Dynamo db  to  store  customer  information.   In  case  of  data corruption, a solutions architect needs to design a solution that meets a recovery point objective (RPO) of 15 minutes and a recovery time objective (RTO) of 1 hour. What should the solutions architect recommend to meet these requirements?

A ��Configure Dynamo db global tables. For RPO recovery, point the application to a different AWS Region.

B ��Configure Dynamo db point-in-time recovery. For RPO recovery, restore to the desired point in time.

C �� Export the Dynamo db data to Amazon S3 Glacier on a daily basis.  For RPO recovery, import the data from S3 Glacier to Dynamo db.

D ��Schedule Amazon Elastic Block Store (Amazon EBS) snapshots for the Dynamo db table every 15 minutes. For RPO recovery, restore the Dynamo db table by using the EBS snapshot.

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: The correct answer is B because Dynamo db Point-in-Time Recovery (PITR) is specifically designed to meet recovery objectives like the ones described. Here ��s why: - Recovery Point Objective (RPO) of 15 minutes: Dynamo db PITR provides continuous backups, allowing you to restore the table to any second within the last 35 days. This means you can recover data with a granularity of 1 second, easily meeting the 15 - minute RPO requirement. - Recovery Time Objective (RTO) of 1 hour: Restoring a table using  PITR creates a new table with the recovered data. While the time to restore depends on the table size, AWS optimizes this process for Dynamo db, and it typically completes within the 1 - hour RTO window. This avoids lengthy manual processes like exporting/importing data (as in Option C) or rebuilding from snapshots (which isn ��t applicable for Dynamo db, as in Option D). Why the other options are incorrect: - A. Dynamo db Global Tables: Global Tables replicate data across AWS  Regions for disaster  recovery, not for  recovering from data corruption.  If data  is corrupted  in one Region, the corruption will  replicate to other  Regions,  making  this  unsuitable  for  RPO/RTO  compliance.  - C.  Exporting to  S3 Glacier daily: Daily exports to S3 Glacier result in an RPO of 24 hours, far exceeding the 15 - minute requirement. Additionally, restoring from Glacier is slow and manual, violating the 1 - hour RTO. - D. EBS Snapshots: Dynamo db is a managed service that does  not  use  EBS  volumes. You cannot take  EBS snapshots of  Dynamo db tables,  so this  option  is  invalid.  Reference  Links:  - [Dynamo db                                                                                                                                                                                                   Point-in-Time
Recovery](https://docs.aws.amazon.com/amazon dynamo db/latest/developer guide/Point in time recovery.html)  -  [Global  Tables vs. PITR](https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-dynamo db/)

68.A company runs a  photo  processing application that  needs to frequently  upload and download  pictures from Amazon S3 buckets that are located in the same AWS Region. A solutions architect has noticed an increased cost in data transfer fees and needs to implement a solution to reduce these costs. How can the solutions architect meet this requirement?

A ��Deploy Amazon API Gateway into a public subnet and adjust the route table to route S3 calls through it.

B ��Deploy a NAT gateway into a public subnet and attach an endpoint policy that allows access to the S3 buckets.

C ��Deploy the application into a public subnet and allow it to route through an internet gateway to access the S3 buckets.

D ��Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy that allows access to the S3 buckets.

�𰸣�D

������ Correct Answer D �� Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy that allows access to the S3 buckets. Detailed Explanation When an application in a VPC (Virtual Private Cloud) accesses an Amazon S3 bucket in the same AWS Region, data transfer costs can increase if the traffic goes over the public internet (e.g., through an internet gateway or NAT gateway). To reduce these costs, a VPC gateway endpoint for S3 creates a private, direct connection between the VPC and S3. This keeps traffic within the AWS network, avoiding public internet fees. - Why not the other options? - A. API Gateway is for managing APIs, not S3 access, and routing S3 calls through it adds unnecessary complexity. - B. A NAT gateway allows private subnets to access the internet, but it still routes S3 traffic over the public internet, incurring costs. - C.  Deploying in a public subnet with an internet gateway exposes S3 traffic to the public internet, increasing costs. Key Benefit: A VPC gateway endpoint for S3 is free to use and avoids data transfer charges for traffic within the same Region. Reference Links - [AWS VPC Endpoints for S3](https://docs.aws.amazon.com/AmazonS3/latest/user guide/private link-interface-endpoints.html)  -   [Reducing  Data  Transfer Costs](https://aws.amazon.com/blogs/aws/aws-private link-update-vpc-endpoint-for-amazon-s3/)

69.A  company  wants  to  move  a  multi-tiered  application  from  on  premises  to  the  AWS  Cloud  to  improve  the  application's performance.  The  application  consists  of  application  tiers  that  communicate  with  each  other  by  way  of  RESTful  services. Transactions are dropped when one tier becomes overloaded. A solutions architect must design a solution that resolves these issues and modernizes the application. Which solution meets these requirements and is the MOST operationally efficient?

A ��Use Amazon API Gateway and direct transactions to the AWS Lambda functions as the application layer. Use Amazon Simple Queue Service (Amazon SQS) as the communication layer between application services.

B ��Use Amazon Cloud watch metrics to analyze the application performance history to determine the servers' peak utilization during  the   performance  failures.  Increase  the  size   of  the   application  server's  Amazon   EC2   instances  to   meet  the   peak requirements.

C �� Use Amazon Simple  Notification Service  (Amazon SNS) to  handle the  messaging  between application servers  running on Amazon EC2 in an Auto Scaling group. Use Amazon Cloud watch to monitor the SNS queue length and scale up and down as required.

D ��Use Amazon Simple Queue Service (Amazon SQS) to handle the messaging between application servers running on Amazon EC2 in an Auto Scaling group. Use Amazon Cloud watch to monitor the SQS queue length and scale up when communication failures are detected.

�𰸣�A

���� �� Correct Answer: A  Detailed  Explanation: The  best solution  is A  because it  uses serverless and  managed services to  modernize the application while ensuring  reliability  and scalability.  Here's why: - Amazon API  Gateway  manages  RESTful API    traffic, handles load balancing, and integrates seamlessly with AWS Lambda (serverless compute). Lambda automatically scales    to handle traffic spikes, preventing overloads in the application layer. - Amazon SQS acts as a buffer between application tiers. If   one tier is overloaded, messages wait in the queue instead of being dropped. This decouples the tiers, improves fault tolerance,    and ensures no transactions are lost. - Operational Efficiency: API Gateway, Lambda, and SQS are fully managed services. They    eliminate the need to provision or manage servers (unlike EC2-based solutions in other options), reducing operational overhead.   Other options fall short: - B relies on manual scaling (inefficient and reactive). - C uses SNS, which is a pub/sub service and doesn �� t buffer messages like SQS. - D uses EC2 and SQS but still requires managing servers, making it less operationally efficient than a    serverless     approach     (A).     Reference      Links:     -     [AWS     Lambda](https://aws.amazon.com/lambda/)     -      [Amazon     API   Gateway](https://aws.amazon.com/api-gateway/) - [Amazon SQS](https://aws.amazon.com/sqs/)

70.A  company  receives  10 TB  of  instrumentation  data each  day from several  machines  located  at  a single factory. The  data

consists of JSON files stored on a storage area  network (SAN)  in an on-premises data center  located within the factory. The company wants to send this data to Amazon S3 where it can  be accessed  by several additional systems that  provide critical near-real-time analytics. A secure transfer is important because the data is considered sensitive. Which solution offers the MOST reliable data transfer?

A ��AWS Data sync over public internet

B ��AWS Data sync over AWS Direct Connect

C ��AWS Database Migration Service (AWS DMS) over public internet

D ��AWS Database Migration Service (AWS DMS) over AWS Direct Connect

�𰸣�B

������Correct Answer B ��AWS Data sync over AWS Direct Connect Detailed Explanation The company needs to securely transfer 10 TB of JSON files daily from an on - premises SAN to Amazon S3. Here ��s why AWS Data sync over AWS Direct Connect is the best choice: 1. Why AWS Data sync? - Data sync is designed for large - scale, high - speed data transfers (like 10 TB/day) between on - premises storage and AWS services like S3. - It automatically handles encryption (data is encrypted in transit and at rest) and integrity validation to ensure secure, error - free transfers. - Unlike AWS DMS (which is meant for database migrations), Data sync works directly with files and storage systems like SANs, making it ideal for JSON files. 2. Why Direct Connect? - Direct Connect provides a private, dedicated network connection between the factory ��s on - premises data center and AWS. - This avoids the unpredictability and security risks of transferring sensitive data over the public internet (e.g., latency, outages, or exposure to threats). - Direct Connect ensures reliability and consistent performance for transferring massive datasets daily. 3. Why Not DMS or Public Internet? - AWS DMS is optimized for databases (e.g., migrating from Oracle to Amazon RDS), not file - based systems like SANs. - Using the public internet (Options A and C) is less secure and less reliable for sensitive, high - volume data. Reference Links - [AWS Data sync](https://aws.amazon.com/data sync/) - [AWS Direct Connect](https://aws.amazon.com/directconnect/)

71.A company needs to configure a real-time data ingestion architecture for its application. The company needs an API, a process that transforms data as the data is streamed, and a storage solution for the data. Which solution will meet these requirements with the LEAST operational overhead?

A ��Deploy an Amazon EC2 instance to host an API that sends data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data to Amazon S3.

B �� Deploy an Amazon EC2 instance to host an API that sends data to AWS Glue. Stop source/destination checking on the EC2 instance. Use AWS Glue to transform the data and to send the data to Amazon S3.

C �� Configure  an Amazon API Gateway API to send data to an Amazon  Kinesis  data stream. Create an Amazon  Kinesis  Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data to Amazon S3.

D ��Configure an Amazon API Gateway API to send data to AWS Glue. Use AWS Lambda functions to transform the data. Use AWS Glue to send the data to Amazon S3.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The best solution is to use Amazon API Gateway, Kinesis Data Streams, Lambda,

and Kinesis Data Firehose. Here's why: 1. API Gateway (Option C) is fully managed and requires no server maintenance, unlike EC2 (Options A/B) which needs manual scaling/updates. 2. Kinesis Data Streams handle real - time data ingestion, while AWS Glue (Options B/D) is designed for batch ETL jobs, not real - time streaming. 3. Lambda automatically processes streaming data without managing servers, aligning with "transform as data is streamed." AWS Glue (Options B/D) would introduce delays as it's not real - time. 4. Firehose automatically batches and loads data into S3, completing the serverless pipeline. This architecture uses 100% managed AWS services (no EC2 servers to maintain), making it the lowest operational overhead solution. Options A/B involve   EC2   management,   while    B/D   misuse   AWS   Glue   for   real   -   time   workloads.    Reference   Links:   -    [Amazon   API Gateway](https://aws.amazon.com/api  -  gateway/)  -   [Amazon  Kinesis  Data  Streams](https://aws.amazon.com/kinesis/data  - streams/)            -             [AWS             Lambda](https://aws.amazon.com/lambda/)             -            [Amazon             Kinesis             Data Firehose](https://aws.amazon.com/kinesis/data - firehose/)

72.A company needs to keep user transaction data in an Amazon Dynamo db table. The company must retain the data for 7 years. What is the MOST operationally efficient solution that meets these requirements?

A ��Use Dynamo db point-in-time recovery to back up the table continuously.

B ��Use AWS Backup to create backup schedules and retention policies for the table.

C ��Create an on-demand backup of the table by using the Dynamo db console. Store the backup in an Amazon S3 bucket. Set an S3 Lifecycle configuration for the S3 bucket.

D ��Create an Amazon Event bridge (Amazon Cloud watch Events) rule to invoke an AWS Lambda function. Configure the Lambda function to back up the table and to store the backup in an Amazon S3 bucket. Set an S3 Lifecycle configuration for the S3 bucket.

�𰸣�B

���� �� Correct Answer  B  Detailed  Explanation The  most operationally efficient solution is  B because AWS  Backup is a fully managed  service  designed to centralize and  automate  backups  across AWS services,  including  Dynamo db.  Here ��s  why:  1. Automated  Backups: AWS  Backup allows you to create backup schedules (e.g., daily, weekly) without writing custom code or configuring Lambda functions. 2. Retention Policies: You can directly define a retention policy of 7 years in AWS Backup, ensuring backups are automatically deleted after the required period. 3. No Manual Intervention: Unlike options C and D (which require manual backups or custom Lambda code), AWS Backup handles backup lifecycle management, reducing operational overhead. 4. Compliance and Reliability: AWS Backup integrates with Dynamo db natively, ensuring backups are consistent and compliant with retention  requirements. Options A  (PITR) only  retains  backups for 35  days, which doesn ��t  meet the 7 - year  requirement. Options C and D involve manual steps or custom code, making them less efficient and error - prone compared to a managed service    like     AWS    Backup.     Reference     Links     -    [AWS     Backup     for    Dynamo db](https://docs.aws.amazon.com/aws     - backup/latest/dev guide/supported                 -                 services.html)                 -                 [Dynamo db                 Backup                 and
Restore](https://aws.amazon.com/dynamo db/backup - restore/)

73.A  company  is  planning  to  use  an  Amazon   Dynamo db  table  for   data  storage.  The   company  is  concerned  about  cost optimization. The table will not be used on most mornings. In the evenings, the read and write traffic will often be unpredictable. When traffic spikes occur, they will happen very quickly. What should a solutions architect recommend?

A ��Create a Dynamo db table in on-demand capacity mode.

B ��Create a Dynamo db table with a global secondary index.

C ��Create a Dynamo db table with provisioned capacity and auto scaling.

D ��Create a Dynamo db table in provisioned capacity mode, and configure it as a global table.

�𰸣�A

������ Correct Answer: A. Create a Dynamo db table in on-demand capacity mode. Detailed Explanation: Dynamo db offers two capacity modes: provisioned (manual capacity planning) and on-demand (automatic scaling). Here's why on-demand mode (A) is the  best  choice  here:  1.  Unpredictable  Evening  Traffic:  On-demand  mode  automatically  scales  to  handle  sudden  spikes  in read/write  activity  without  manual  intervention.  Provisioned  mode  with  auto  scaling  (C)  introduces  delays  because  scaling actions take time to trigger, risking throttling during rapid spikes. 2. Cost Optimization: During quiet mornings, on-demand mode charges only for actual usage. Provisioned mode (C/D) forces you to pay for baseline capacity even when idle, wasting money. 3. No Operational Overhead: On-demand requires no capacity planning. Provisioned mode (C/D) demands ongoing adjustments to balance performance and cost. Why other options are less optimal: - B (Global Secondary Index): Improves query flexibility but doesn't address cost/spike concerns. - C (Provisioned + Auto Scaling): Risk of throttling during sudden spikes due to scaling delays.
-  D  (Global  Table  +  Provisioned):  Global  tables  are  for  multi-region  replication  (unmentioned  requirement),  and  provisioned mode  is  inefficient  for   idle  periods.  Reference:   [Amazon  Dynamo db  Pricing](https://aws.amazon.com/dynamo db/pricing/) [On-Demand                                                                                                                                                                                                           Capacity Mode](https://docs.aws.amazon.com/amazon dynamo db/latest/developer guide/How it works.Readwrite capacity mode.html#Ho w it works.On demand)

74.A company recently signed a contract with an AWS  Managed Service  Provider  (MSP)  Partner for  help with an application migration initiative. A solutions architect needs ta share an Amazon Machine Image (AMI) from an existing AWS account with the MSP Partner's AWS account. The AMI is backed by Amazon Elastic Block Store (Amazon EBS) and uses an AWS Key Management Service (AWS KMS) customer managed key to encrypt EBS volume snapshots. What is the MOST secure way for the solutions architect to share the AMI with the MSP Partner's AWS account?

A ��Make the encrypted AMI and snapshots publicly available. Modify the key policy to allow the MSP Partner's AWS account to use the key.

B ��Modify the launch permission property of the AMI. Share the AMI with the MSP Partner's AWS account only. Modify the key policy to allow the MSP Partner's AWS account to use the key.

C ��Modify the launch permission property of the AMI. Share the AMI with the MSP Partner's AWS account only. Modify the key policy to trust a new KMS key that is owned by the MSP Partner for encryption.

D ��Export the AMI from the source account to an Amazon S3 bucket in the MSP Partner's AWS account, Encrypt the S3 bucket with a new KMS key that is owned by the MSP Partner. Copy and launch the AMI in the MSP Partner's AWS account.

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: To securely share an encrypted AMI with another AWS account (like an MSP Partner's account), two critical steps are required: 1. Share the AMI: Modify the AMI's `launch permission` to grant access only to the MSP Partner's AWS account. This ensures only they can launch instances from the AMI. 2. Share the KMS Key: Update the key policy of the customer-managed KMS key used to encrypt the AMI's EBS snapshots. This allows the MSP Partner's account to decrypt  the  snapshots  when  launching  instances  from  the  shared  AMI.  Why  other  options  are  incorrect:  -  A:  Making  the AMI/snapshots public exposes them to all AWS users, which is highly insecure. - C: Using a new KMS key owned by the MSP Partner would require re-encrypting the snapshots, which isn't necessary here. - D: Exporting to S3 adds unnecessary complexity; direct     AMI      sharing     is      simpler      and     more      secure.     Reference      Link:      [AWS      Docs:     Sharing      an      Encrypted AMI](https://docs.aws.amazon.com/AWSEC2/latest/User guide/AMI-shared-encrypted.html)

75.A solutions architect is designing the cloud architecture for a new application being deployed on AWS. The process should run in parallel while adding and removing application nodes as needed based on the number of jobs to be processed. The processor application is stateless. The solutions architect must ensure that the application is loosely coupled and the job items are durably stored. Which design should the solutions architect use?

A��Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch configuration that uses the AMI. Create an Auto Scaling group using the launch configuration. Set the scaling policy for the Auto Scaling group to add and remove nodes based on CPU usage.

B �� Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch configuration that uses the AMI. Create an Auto Scaling group using the launch configuration. Set the scaling policy for the Auto Scaling group to add and remove nodes based on network usage.

C �� Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon  Machine Image (AMI) that consists of the processor application. Create a launch template that uses the AMI. Create an Auto Scaling group using the launch template. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of items in the SQS queue.

D��Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of  the  processor  application.  Create  a  launch  template  that  uses  the  AMI.  Create  an  Auto  Scaling  group  using  the  launch template.  Set  the  scaling  policy  for  the  Auto  Scaling  group  to  add  and  remove  nodes  based  on  the  number  of  messages published to the SNS topic.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The correct answer is C because it uses Amazon SQS (Simple Queue Service) to durably store job items and Auto Scaling based on the queue size, which meets all requirements. Here's why: 1. Loose Coupling &  Durability: SQS  provides  a  reliable, decoupled queue for job storage.  Producers (job creators) and consumers (processor nodes) don't need to know about each other, making the system loosely coupled. 2. Auto Scaling Trigger: Scaling based on the number  of  messages  in  SQS  ("Approximate number of messages visible"  metric)  directly  aligns  with  the  requirement  to  scale based on workload (jobs to process). This is more efficient than scaling based on CPU/network usage (Options A/B) which are indirect indicators. 3. Modern Approach: Using a launch template (instead of older launch configurations in B) is AWS's current recommendation for Auto Scaling configuration. 4. Stateless Processing: Since the application is stateless, multiple EC2 instances can  process  messages  from the queue  in  parallel  without  coordination. Other  options fail  because: - A/D  use  SNS  (pub/sub service) which doesn't store messages durably -  B  uses  network usage  metrics which don't reflect actual workload -  D's SNS message count doesn't represent unprocessed jobs Reference Links: - Amazon SQS: https://aws.amazon.com/sqs/ - Auto Scaling with      SQS:       https://docs.aws.amazon.com/auto scaling/ec2/user guide/as-using-sqs-queue.html       -       Launch       Templates: https://docs.aws.amazon.com/AWSEC2/latest/User guide/ec2-launch-templates.html

76.A company's dynamic website is hosted using on-premises servers in the United States. The company is launching its product in Europe, and it wants to optimize site loading times for new European users. The site's backend must remain in the United States. The product is being launched in a few days, and an immediate solution is needed. What should the solutions architect recommend?

A ��Launch an Amazon EC2 instance in us-east-1 and migrate the site to it.

B ��Move the website to Amazon S3. Use Cross-Region Replication between Regions.

C ��Use Amazon Cloud front with a custom origin pointing to the on-premises servers.

D ��Use an Amazon Route 53 geo proximity routing policy pointing to on-premises servers.

�𰸣�C

�� �� �� Correct Answer:  C.  Use  Amazon  Cloud front  with  a  custom  origin  pointing  to  the  on  -  premises  servers.  Detailed Explanation: Amazon Cloud front is a Content Delivery Network (CDN) that caches static content (like images, CSS, Javascript) at edge locations worldwide. For dynamic content (e.g., personalized pages), Cloud front still optimizes performance by using faster network routes and TCP optimizations between the edge location and the origin (on - premises servers in the U.S.). This reduces latency for European users without moving the backend. Other options fail because: - A: Hosting in `us - east - 1` doesn ��t solve latency for Europe. - B: S3 is for static sites, not dynamic backends. - D: Geo proximity routing alone can ��t reduce  latency if servers are still in the U.S. Reference Link: [Amazon Cloud front Overview](https://aws.amazon.com/cloud front/)

77.A company wants to reduce the cost of its existing three-tier web architecture. The web, application, and database servers are running on Amazon EC2 instances for the development, test, and production environments. The EC2 instances average 30% CPU utilization during peak hours and 10% CPU utilization during non-peak hours. The production EC2 instances run 24 hours a day. The development and test EC2 instances run for at least 8 hours each day. The company plans to implement automation to stop the  development  and  test  EC2  instances  when  they  are  not  in  use.  Which  EC2  instance  purchasing  solution  will  meet  the company's requirements MOST cost-effectively?

A ��Use Spot Instances for the production EC2 instances. Use Reserved Instances for the development and test EC2 instances.

B �� Use  Reserved  Instances  for  the  production  EC2  instances.  Use  On-Demand  Instances  for  the  development  and  test  EC2 instances.

C ��Use Spot blocks for the production EC2 instances. Use Reserved Instances for the development and test EC2 instances.

D ��Use On-Demand Instances for the production EC2 instances. Use Spot blocks for the development and test EC2 instances.

�𰸣�B

������ Correct Answer B Detailed Explanation The production environment runs 24/7, making Reserved Instances (RIs) the best choice. RIs offer significant cost savings (up to 72% vs. On-Demand) for long-term, predictable workloads. For development/test environments  (running  8  hours/day),  On-Demand  Instances  are  more  cost-effective.  Since  these  instances  are  automatically stopped when unused, On-Demand billing (per-second usage) aligns better with their irregular runtime than RIs (which require upfront payment and are ideal for steady usage). Why other options are less optimal: - Spot Instances (A, C, D) risk interruptions, which  is  unacceptable  for  production.  -  Spot  Blocks  (C,  D)  guarantee  uptime  but  only  for  1  - 6  hours,  insufficient  for  24/7 production. - Using RIs for dev/test (A, C) wastes money if instances aren ��t running full-time. Reference Links [AWS EC2 Pricing Models](https://aws.amazon.com/ec2/pricing/)  [Reserved  Instances](https://aws.amazon.com/ec2/pricing/reserved-instances/) [On-Demand Instances](https://aws.amazon.com/ec2/pricing/on-demand/)

78.A company has a production web application in which users upload documents through a web interface or a mobile app. According to a new regulatory requirement. new documents cannot be modified or deleted after they are stored. What should a solutions architect do to meet this requirement?

A ��Store the uploaded documents in an Amazon S3 bucket with S3 Versioning and S3 Object Lock enabled.

B��Store the uploaded documents in an Amazon S3 bucket. Configure an S3 Lifecycle policy to archive the documents periodically.

C ��Store the uploaded documents in an Amazon S3 bucket with S3 Versioning enabled. Configure an ACL to restrict all access to read-only.

D ��Store the uploaded documents on an Amazon Elastic File System (Amazon EFS) volume. Access the data by mounting the volume in read-only mode.

�𰸣�A

������ Correct Answer A Detailed Explanation The key requirement is that new documents cannot be modified or deleted after storage. Here's why Option A is correct: 1. S3 Versioning: Keeps multiple versions of an object, preventing accidental overwrites or deletions.  Even  if a  user tries to  delete/modify an object, the original version  is  preserved. 2. S3  Object  Lock:  Enforces  a "write-once-read-many" (WORM) model. In "Compliance Mode," objects become immutable (cannot be modified/deleted) for a specified retention period, even by root AWS accounts. This directly meets the regulatory requirement. Why other options fail: - Option B: Lifecycle policies only manage storage tiers (e.g., moving to Glacier), not immutability. - Option C: ACLs (Access Control Lists) are  not foolproof.  Permissions  can  be  misconfigured,  and versioning alone doesn ��t  block  deletions  (it just  keeps old versions). - Option D: EFS read-only mounts are temporary and depend on client-side configurations. Malicious actors with write access             could             still             modify/delete             files.             Reference             Link             [Amazon             S3              Object Lock](https://docs.aws.amazon.com/AmazonS3/latest/user guide/object-lock.html)

79.A company has several web servers that need to frequently access a common Amazon RDS MySQL Multi-AZ DB instance. The company wants a secure method for the web servers to connect to the database while meeting a security requirement to rotate user credentials frequently. Which solution meets these requirements?

A��Store the database user credentials in AWS Secrets Manager. Grant the necessary IAM permissions to allow the web servers to access AWS Secrets Manager.

B ��Store the database user credentials in AWS Systems Manager Ops center. Grant the necessary IAM permissions to allow the web servers to access Ops center.

C ��Store the database user credentials in a secure Amazon S3 bucket. Grant the necessary IAM permissions to allow the web servers to retrieve credentials and access the database.

D ��Store the database user credentials in files encrypted with AWS Key Management Service (AWS KMS) on the web server file system. The web server should be able to decrypt the files and access the database.

�𰸣�A

�� �� �� Correct Answer: A  Detailed  Explanation: The  best solution  is to  use AWS Secrets  Manager  (Option A)  because  it  is specifically  designed  to  securely  store  and  automatically  rotate  credentials  like  database  passwords.  Here's  why:  1.  Secure Storage: Secrets  Manager  encrypts  credentials  at  rest  and  allows  granular  access  control  via  IAM  policies.  Web  servers  get temporary  credentials  through  IAM  roles,  avoiding  hard-coded  passwords.  2.  Automatic  Rotation:  Secrets  Manager  can automatically rotate RDS MySQL credentials on a schedule (e.g., every 30 days). This meets the security requirement for frequent credential rotation without manual intervention. 3. Multi-AZ Compatibility: Since t he RDS instance is Multi-AZ, Secrets Manager ensures credentials  remain synchronized across AZs during rotation. Other options are  less  ideal: -  B (Ops center): Ops center focuses on operational issue tracking,  not credential management. - C (S3):  Manual updates  required for rotation, increasing security  risks. -  D  (KMS-encrypted files):  No  built-in  rotation  mechanism;  credentials  remain  static  unless  manually  updated. Reference   Links:   -    AWS   Secrets    Manager:    https://aws.amazon.com/secrets-manager/   -    Automatic    Rotation   for    RDS: https://docs.aws.amazon.com/secrets manager/latest/user guide/rotating-secrets-strategies.html

80.A company hosts an application on AWS  Lambda functions that are invoked by an Amazon API Gateway API. The  Lambda functions  save  customer  data  to  an  Amazon  Aurora  MySQL  database.  Whenever  the  company  upgrades  the  database,  the Lambda functions fail to establish database connections until the upgrade is complete. The result is that customer data is not recorded for some of the event. A solutions architect needs to design a solution that stores customer data that is created during database upgrades. Which solution will meet these requirements?

A ��Provision an Amazon RDS proxy to sit between the Lambda functions and the database. Configure the Lambda functions to connect to t he RDS proxy.

B �� Increase  the  run time of the  Lambda functions to the  maximum.  Create a  retry  mechanism  in the  code that  stores the customer data in the database.

C �� Persist the customer data to Lambda local storage. Configure new Lambda functions to scan the local storage to save the customer data to the database.

D ��Store the customer data in an Amazon Simple Queue Service (Amazon SQS) FI FO queue. Create a new Lambda function that polls the queue and stores the customer data in the database.

�𰸣�D

�� �� ��  Correct  Answer:  D  Detailed  Explanation:  When  the  Aurora  MySQL  database  is  undergoing  upgrades,  it  becomes temporarily unavailable, causing  Lambda functions to fail in connecting and saving customer data. The  key requirement is to ensure data isn't lost during this downtime. Here's why option D works best: 1. Amazon SQS FI FO Queue as a Buffer: - Instead of directly writing to the database, the  Lambda functions first store the  customer  data  in  an SQS  F IFO  queue  during  database downtime. - SQS queues durably store messages for up to 14 days, acting as a temporary buffer. - A separate Lambda function can  poll this queue and  retry writing the  data to the  database once  it ��s  back  online.  2.  Decoupling  and  Resilience:  - This decouples the data generation  (API  Gateway +  Lambda) from the  data  persistence  (database), ensuring temporary database unavailability doesn ��t block the application. - Even if the database upgrade takes hours, the data remains safely stored in the queue. 3. Why Other Options Fail: - A (RDS Proxy): RDS Proxy manages database connections but doesn ��t store data.  If the database is offline, the proxy can ��t help. - B (Lambda Retries): Lambda ��s max runtime is 15 minutes. If the database upgrade takes longer, retries will fail, and data will be lost. - C (Lambda Local Storage): Lambda ��s local storage (/tmp) is ephemeral and instance    -    specific.    Data    isn    �� t     durable    or    centralized,    risking    loss.     Reference    Links:    -     [Amazon    SQS    FI FO Queues](https://docs.aws.amazon.com/Aws simple queue service/latest/Sqs developer guide/FIFO    -     queues.html)     -     [AWS Lambda Retry Behavior](https://docs.aws.amazon.com/lambda/latest/dg/invocation - retries.html)

81.A company uses Amazon S3 to store its confidential audit documents. The S3 bucket uses bucket policies to restrict access to audit team IAM user credentials according to the principle of least privilege. Company managers are worried about accidental deletion of documents in the S3 bucket and want a more secure solution. What should a solutions architect do to secure the audit documents?

A ��Enable the versioning and MFA Delete features on the S3 bucket.

B ��Enable multi-factor authentication (MFA) on the IAM user credentials for each audit team IAM user account.

C ��Add an S3 Lifecycle policy to the audit team's IAM user accounts to deny the s3:Deleteobject action during audit dates.

D �� Use AWS Key Management Service (AWS KMS) to encrypt the S3 bucket and restrict audit team IAM user accounts from accessing the KMS key.

�𰸣�A

���� �� Correct Answer A �� Enable the versioning and MFA Delete features on the S3 bucket. Explanation The managers are concerned about accidental deletion of audit documents.  Enabling S3 Versioning preserves  multiple versions of an object, so even if a file is deleted, it can be restored from a previous version. However, versioning alone doesn ��t fully prevent deletions. MFA Delete adds an extra security layer: to permanently delete an object or disable versioning, the requester must provide a valid Multi-Factor Authentication (MFA) code. This ensures that even if an attacker gains access to an audit team member ��s IAM credentials (with delete permissions), they cannot delete objects without the physical MFA device (e.g., a smartphone app or hardware token). Why other options are less suitable: - B (MFA on IAM users): This secures login access but doesn ��t prevent deletions  if  credentials  are  compromised.  -  C  (Lifecycle  policy):  Lifecycle  policies  manage  object  transitions  (e.g.,  moving  to Glacier), not permissions. Denying `s3:Deleteobject` via IAM/bucket policies would be better, but it doesn ��t address accidental deletions by authorized users. - D (KMS encryption): Encryption protects data at rest but doesn ��t prevent deletions. Blocking KMS access would stop the audit team from accessing documents entirely, which isn ��t the goal. Reference Links - [Amazon S3 Versioning](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Versioning.html)           -            [MFA            Delete            for S3](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Multifactor authentication delete.html)

82.A company is using a SQL database to store movie data that is  publicly accessible. The database  runs on an Amazon  RDS Single-AZ DB instance. A script runs queries at random intervals each day to record the number of new movies that have been added to the database. The script must report a final total during business hours. The company's development team notices that the  database  performance  is  inadequate  for   development  tasks   when  the  script  is  running.  A   solutions  architect  must recommend a solution to resolve this issue. Which solution will meet this requirement with the LEAST operational overhead?

A ��Modify the DB instance to be a Multi-AZ deployment.

B ��Create a read replica of the database. Configure the script to query only the read replica.

C ��Instruct the development team to manually export the entries in the database at the end of each day.

D ��Use Amazon Elastic ache to cache the common queries that the script runs against the database.

�𰸣�B

���� �� Correct Answer: B. Create a read replica of the database. Configure the script to query only the read replica. Detailed Explanation: The performance issue occurs because the script's queries compete with development tasks for resources on the primary database. A read replica is a copy of the primary database that handles read-only traffic, offloading the main database. This  requires  minimal setup  (just creating the  replica  in AWS  RDS)  and  no code changes except  redirecting the script to the replica. Other options either add complexity (Elastic ache), don��t reduce load (Multi-AZ), or are manual/inefficient (daily exports). Reference                                   Links:                                    -                                    [Amazon                                   RDS                                    Read
Replicas](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_Read repl.html)  -  [Offload  Read  Traffic  with  Read
Replicas](https://aws.amazon.com/rds/features/read-replicas/)

83.A company has applications that run on Amazon EC2 instances in a VPC. One of the applications needs to call the Amazon S3 API to store and read objects. According to the company's security  regulations, no traffic from the applications is allowed to travel across the internet. Which solution will meet these requirements?

A ��Configure an S3 gateway endpoint.

B ��Create an S3 bucket in a private subnet.

C ��Create an S3 bucket in the same AWS Region as the EC2 instances.

D ��Configure a NAT gateway in the same subnet as the EC2 instances.

�𰸣�A

������ Correct Answer: A. Configure an S3 gateway endpoint. Detailed Explanation: To allow EC2 instances in a VPC to securely access Amazon S3 without internet traffic, an S3 gateway endpoint is the correct solution.  Here's why:  1. Gateway  Endpoint Basics: - A VPC endpoint acts like a private tunnel between your VPC and AWS services (like S3). - It uses AWS's internal network, not the  public  internet,  meeting  the  "no  internet  traffic"  requirement.  2.  Why  Other  Options  Fail:  -  B  (S3  bucket  in  private subnet):  Impossible  because  S3  is  a  regional  service,  not  deployable  in  subnets.  -  C  (Same  Region  bucket):  While  good  for performance, it doesn't prevent internet routing by default. - D (NAT Gateway): Allows private subnet internet access, but still uses the public network, violating the security rule. 3. How Gateway Endpoint Works: - It adds a route in your VPC route table pointing S3 traffic directly to the endpoint. - No public IPs, internet gateways, or NAT devices are involved. - Security: Works with VPC    security     groups     and     bucket     policies    for     access     control.     Reference     Link:     [Amazon     VPC     Endpoints    for S3](https://docs.aws.amazon.com/vpc/latest/private link/vpc-endpoints-s3.html)

84.A company runs an on-premises application that is powered by a MySQL database. The company is migrating the application to  AWS  to  increase  the  application's  elasticity  and  availability.  The  current  architecture  shows  heavy  read  activity  on  the database during times of normal operation. Every 4 hours, the company's development team pulls a full export of the production database to  populate  a  database  in the  staging  environment.  During  this  period,  users  experience  unacceptable  application latency. The development team is unable to use the staging environment until the procedure completes. A solutions architect must recommend replacement architecture that alleviates the application latency issue. The replacement architecture also must give the development team the ability to continue using the staging environment without delay. Which solution meets these requirements?

A ��Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Populate the staging database by implementing a backup and restore process that uses the mysqldump utility.

B��Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Use database cloning to create the staging database on-demand.

C ��Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Use the standby instance for the staging database.

D ��Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Populate the staging database by implementing a backup and restore process that uses the mysqldump utility.

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: The best solution is Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production and database cloning for the staging environment.  Here's why:  1.  Problem  1:  Heavy  read  activity  causes  latency during  exports  Aurora   Replicas  handle  read  traffic,  offloading  the   primary  database.  This   reduces  latency  during   normal operations and when exporting data. 2.  Problem 2: Staging environment downtime during exports Aurora's database cloning creates an instant, point-in-time copy without impacting production performance. Developers get immediate access to staging data  without  waiting  for  traditional  backups  (like  `mysqldump`),  which  block  resources.  3.  Why  other  options  fail:  -  A/D: `mysqldump` locks tables during backups, worsening latency. - C: RDS Multi-AZ's standby is for failover, not read traffic. Using it for staging would risk availability and performance. Key Benefits of Aurora Cloning: - Near-zero storage cost (clones share data with the source) -  No  performance  hit  to  production  -  Instant  availability  for  developers  Reference  Links:  -  [Amazon  Aurora

Cloning](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/Aurora.Managing.Clone.html)     -     [Aurora     Read Scaling](https://aws.amazon.com/rds/aurora/features/#Read_scaling)

85.A company is designing an application where users upload small files into Amazon S3. After a user  uploads a file, the file requires one-time simple processing to transform the data and save the data in JSON format for later analysis. Each file must be processed as quickly as possible after it is uploaded. Demand will vary. On some days, users will upload a high number of files. On other days, users will upload a few files or no files. Which solution meets these requirements with the LEAST operational overhead?

A ��Configure Amazon EMR to read text files from Amazon S3. Run processing scripts to transform the data. Store the resulting JSON file in an Amazon Aurora DB cluster.

B ��Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon SQS) queue. Use Amazon EC2 instances to read from the queue and process the data. Store the resulting JSON file in Amazon Dynamo db.

C ��Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function to read from the queue and process the data. Store the resulting JSON file in Amazon Dynamo db.

D ��Configure Amazon Event bridge (Amazon Cloud watch Events) to send an event to Amazon Kinesis Data Streams when a new file is uploaded. Use an AWS Lambda function to consume the event from the stream and process the data. Store the resulting JSON file in an Amazon Aurora DB cluster.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The best solution is option C because it uses serverless services (AWS Lambda and Amazon SQS) to minimize operational overhead. Here ��s why: 1. S3 Event Notifications: When a file is uploaded to S3, it triggers  an  event  notification  to  an  SQS  queue.  This  ensures  immediate  awareness  of  new  files.  2.  AWS  Lambda:  Lambda automatically processes messages from the SQS queue. It scales seamlessly �� spinning up more instances during high demand and reducing them when idle ��without manual intervention. 3. Dynamo db: Storing JSON in Dynamo db is efficient for flexible, schema-less data, and it ��s fully managed, reducing database maintenance. Other options have drawbacks: - A (EMR + Aurora): EMR is overkill for small, simple processing and requires cluster management. Aurora adds relational database complexity. - B (EC2): Managing EC2 instances and scaling adds operational work. - D (Kinesis + Aurora): Kinesis requires configuring shards, and Aurora is less optimal for JSON storage. Option C eliminates server management, auto-scales, and uses purpose-built services, making     it     the     simplest     and     most     cost-effective.      Reference     Links:     -      [AWS     Lambda     with     SQS     as     Event Source](https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html)                            -                             [S3                             Event
Notifications](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Notification how to.html)   -   [Amazon    Dynamo db   for Serverless Apps](https://aws.amazon.com/dynamo db/)

86.An application allows users at a company's headquarters to access product data. The product data is stored in an Amazon RDS MySQL DB instance. The operations team has isolated an application performance slow down and wants to separate read traffic from write traffic. A  solutions  architect  needs  to  optimize the  application's  performance  quickly. What  should the  solutions architect recommend?

A ��Change the existing database to a Multi-AZ deployment. Serve the read requests from the primary Availability Zone.

B ��Change the existing database to a Multi-AZ deployment. Serve the read requests from the secondary Availability Zone.

C ��Create read replicas for the database. Configure the read replicas with half of the compute and storage resources as the source database.

D��Create read replicas for the database. Configure the read replicas with the same compute and storage resources as the source database.

�𰸣�D

������ Correct Answer D Detailed Explanation The goal is to quickly optimize an application ��s performance by separating read traffic from write traffic in an Amazon RDS MySQL database. Here ��s why Option D is the correct choice: 1. Read Replicas vs. Multi-AZ: -  Multi-AZ deployments (Options A and  B) are designed for  high availability,  not  read scaling. The secondary AZ in Multi-AZ acts as a passive standby for failover and does not serve read traffic. - Read Replicas (Options C and D) are designed to offload read traffic from the primary database. This directly addresses the problem of separating read/write traffic. 2. Resource Allocation for Read Replicas: - If read replicas are under-provisioned (Option C: half the resources), they may become bottlenecks, defeating the purpose of offloading read traffic. - Using the same compute/storage resources as the source (Option D) ensures the replicas can handle read traffic efficiently, matching the primary ��s write-handling capacity. Why Not Other Options? - A/B: Multi-AZ does not scale read traffic. - C: Under-resourced replicas risk performance issues. Reference Link [Amazon RDS  Read Replicas](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_Read repl.html)

87.An Amazon EC2 administrator created the following policy associated with an IAM group containing several users:     What is the effect of this policy?

A ��Users can terminate an EC2 instance in any AWS Region except us-east-1 .

B ��Users can terminate an EC2 instance with the IP address 10.100.100.1 in the us-east-1 Region.

C ��Users can terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.100.100.254.

D ��Users cannot terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.100.100.254.

�𰸣�C

�� �� ��  Correct Answer  C ��  Users  can  terminate  an  EC2  instance  in  the  us-east-1  Region  when  the  user's  source  IP  is 10.100.100.254. Detailed Explanation This IAM policy allows users to terminate EC2 instances only if both conditions are true: 1. Region Restriction: The EC2 instance must be in the us-east-1 region (specified by "ec2:Region": "us-east-1"). 2. IP Restriction: The   request  to  terminate  the   instance   must  come  from  the   IP   address   10.100.100.254   (specified   by   "aws:Source ip": "10.100.100.254/32"). The policy uses AND logic for conditions. This means both the region and the source IP must match for the termination action to be permitted. If either condition fails (e.g., the instance is in another region, or the user ��s IP is different), the action  is  blocked. Why Other Options Are  Incorrect: - A: The  policy  explicitly  restricts termination to  us-east-1, so  other regions are not allowed. - B: The IP 10.100.100.1 does not match the policy ��s required IP 10.100.100.254. - D: The policy allows termination when the source IP is 10.100.100.254, so this is the opposite of what the policy does. Reference Links - [IAM Policy Condition        Keys](https://docs.aws.amazon.com/IAM/latest/User guide/reference_policies_condition-keys.html)         -        [EC2 Resource-Level
Permissions](https://docs.aws.amazon.com/AWSEC2/latest/User guide/ec2-supported-iam-actions-resources.html)

88.A company has a large Microsoft Sharepoint deployment running on-premises that requires Microsoft Windows shared file storage. The company wants to migrate this workload to the AWS Cloud and is considering various storage options. The storage solution  must  be  highly  available  and  integrated  with  Active  Directory  for  access  control.  Which  solution  will  satisfy  these requirements?

A ��Configure Amazon EFS storage and set the Active Directory domain for authentication.

B ��Create an SMB file share on an AWS Storage Gateway file gateway in two Availability Zones.

C ��Create an Amazon S3 bucket and configure Microsoft Windows Server to mount it as a volume.

D ��Create an Amazon FSx for Windows File Server file system on AWS and set the Active Directory domain for authentication.

�𰸣�D

�� �� �� Correct Answer:  D  Detailed  Explanation:  The  correct  answer  is  D  because  Amazon  FSx  for  Windows  File  Server  is specifically  designed  for  Windows-based  file  storage  needs  in  AWS.  It  supports  the  Server  Message  Block  (SMB)  protocol, integrates natively with Active Directory for access control, and provides built-in high availability through Multi-AZ deployments. This makes it ideal for migrating Microsoft Sharepoint workloads requiring Windows-compatible shared file storage. - Option A (Amazon  EFS)  is  incorrect  because  EFS  is  optimized for  Linux-based  NFS file  storage,  not Windows SMB. -  Option  B  (Storage Gateway)  is  designed for  hybrid  cloud  setups,  not  fully  cloud-native deployments,  and  lacks  native  Multi-AZ  HA.  -  Option  C (Amazon S3) is object storage and cannot be mounted as a Windows file system volume natively. Reference Link: [Amazon FSx for Windows File Server](https://aws.amazon.com/fsx/windows/)

89.An image-processing company has a web application that users use to upload images. The application uploads the images into an Amazon S3 bucket. The company has set up S3 event notifications to publish the object creation events to an Amazon Simple Queue Service (Amazon SQS) standard queue. The SQS queue serves as the event source for an AWS Lambda function that  processes the  images and sends the  results to  users through email.  Users  report that they are  receiving  multiple  email messages for every uploaded image. A solutions architect determines that SQS messages are invoking the Lambda function more than once, resulting in multiple email  messages. What should the solutions architect do to resolve this issue with the  LEAST operational overhead?

A ��Set up long polling in the SQS queue by increasing the Receive message wait time to 30 seconds.

B ��Change the SQS standard queue to an SQS FI FO queue. Use the message de duplication ID to discard duplicate messages.

C ��Increase the visibility timeout in the SQS queue to a value that is greater than the total of the function timeout and the batch window timeout.

D �� Modify the Lambda function to delete each message from the SQS queue immediately after the  message is read before processing.

�𰸣�C

������ Correct Answer: C Detailed Explanation: When a Lambda function processes messages from an SQS standard queue, it temporarily  hides  messages  via  a  visibility  timeout  to  prevent  other  consumers  from  processing  the  same  message.  If  the Lambda function takes  longer to  process the  message than the visibility timeout, the  message  becomes visible  again  in the queue and might be  reprocessed, causing duplicates.  By increasing the visibility timeout to a value greater than the  Lambda function ��s timeout (plus any batch processing time), the message stays hidden until the function completes. This ensures the message isn ��t prematurely returned to the queue and reprocessed, resolving the duplicate email issue. Other options: - A (Long polling) reduces empty responses but doesn ��t fix duplicates. - B (FI FO queue) guarantees exactly-once processing but requires redesigning the system (e.g., adding de duplication IDs), increasing operational effort. - D (Delete messages immediately) risks losing        messages       if        processing        fails        after        deletion.        Reference        Links:        -        [AWS        Lambda        with SQS](https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html)                           -                            [SQS                            Visibility
Timeout](https://docs.aws.amazon.com/Aws simple queue service/latest/Sqs developer guide/sqs-visibility-timeout.html)

90.A company is implementing a shared storage solution for a gaming application that is hosted in an on-premises data center. The company needs the ability to use Lustre clients to access data. The solution must be fully managed. Which solution meets these requirements?

A �� Create  an  AWS  Storage  Gateway  file  gateway.  Create  a  file  share  that  uses  the  required  client  protocol.  Connect  the application server to the file share.

B �� Create  an Amazon  EC2 Windows  instance.  Install  and  configure a Windows file  share  role on the  instance.  Connect the application server to the file share.

C��Create an Amazon Elastic File System (Amazon EFS) file system, and configure it to support Lustre. Attach the file system to the origin server. Connect the application server to the file system.

D ��Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system.

�𰸣�D

������ Correct Answer D Explanation The question requires a fully managed storage solution that supports Lustre clients for an on - premises gaming application. Here ��s why Amazon FSx for Lustre (Option D) is correct: 1. Fully Managed: Amazon FSx is a fully managed service, so AWS handles maintenance, updates, and scaling. 2. Lustre Compatibility: FSx for Lustre is specifically designed to support the Lustre file system, which is optimized for high - performance workloads like gaming. 3. On - Premises Access: The FSx file system can be securely accessed from on - premises servers using AWS Direct Connect or VPN, enabling the gaming application (hosted locally) to connect to it. Other options fail because: - A (Storage Gateway): File Gateway supports SMB/ NFS, not Lustre. - B (EC2 Windows File Share): Self - managed (not fully managed) and uses SMB, not Lustre. - C (EFS): EFS uses NFS, not Lustre. While EFS can integrate with Lustre in specific scenarios, it ��s not natively  Lustre - compatible like FSx. Reference     Links      -     [Amazon      FSx     for      Lustre](https://aws.amazon.com/fsx/lustre/)      -     [Comparing      AWS     Storage Services](https://aws.amazon.com/storage/services/)

91.A  company's  containerized  application   runs  on  an  Amazon   EC2  instance.  The  application   needs  to  download  security certificates before it can communicate with other business applications. The company wants a highly secure solution to encrypt and decrypt the certificates in near real time. The solution also needs to store data in highly available storage after the data is encrypted. Which solution will meet these requirements with the LEAST operational overhead?

A ��Create AWS Secrets Manager secrets for encrypted certificates. Manually update the certificates as needed. Control access to the data by using fine-grained IAM access.

B ��Create an AWS Lambda function that uses the Python cryptography library to receive and perform encryption operations. Store the function in an Amazon S3 bucket.

C �� Create an AWS  Key  Management Service  (AWS KMS) customer  managed key. Allow the  EC2  role to use the KMS  key for encryption operations. Store the encrypted data on Amazon S3.

D �� Create an AWS  Key Management Service (AWS KMS) customer  managed key. Allow the  EC2  role to use the KMS  key for encryption operations. Store the encrypted data on Amazon Elastic Block Store (Amazon EBS) volumes.

������ Correct Answer: C Detailed Explanation: The best solution is to use AWS KMS for encryption and Amazon S3 for storage. Here's  why:  1.  AWS  KMS  is  a  managed  service  that  handles  encryption/decryption  in  real  time  with  minimal  operational overhead.  By  granting  the   EC2  instance's  IAM  role  access  to  the   KMS   key,  the  application  can  securely  encrypt/decrypt certificates without  managing  keys  manually. 2. Amazon S3  is  highly available and durable, automatically storing data across multiple Availability Zones. This meets the requirement for highly available storage after encryption. 3. Option D uses Amazon EBS, which is less ideal because EBS volumes are tied to a single Availability Zone, making them less available than S3. Options A and  B  involve  manual  updates  (A)  or  custom  code  (B),  which  increase  operational  complexity.  Reference  links:  -  [AWS KMS](https://aws.amazon.com/kms/) - [Amazon S3 Durability](https://aws.amazon.com/s3/features/)

92.A solutions architect is designing a VPC with public and private subnets. The VPC and subnets use IPv4 CIDR blocks. There is one public subnet and one private subnet in each of three Availability Zones (AZs) for high availability. An internet gateway is used to provide internet access for the public subnets. The private subnets require access to the internet to allow Amazon EC2 instances  to  download  software  updates.  What  should  the  solutions  architect  do  to  enable  Internet  access  for  the  private subnets?

A ��Create three NAT gateways, one for each public subnet in each AZ. Create a private route table for each AZ that forwards non-VPC traffic to the NAT gateway in its AZ.

B ��Create three NAT instances, one for each private subnet in each AZ. Create a private route table for each AZ that forwards non-VPC traffic to the NAT instance in its AZ.

C ��Create a second internet gateway on one of the private subnets. Update the route table for the private subnets that forward non-VPC traffic to the private internet gateway.

D ��Create an egress-only internet gateway on one of the public subnets. Update the route table for the private subnets that forward non-VPC traffic to the egress-only Internet gateway.

�𰸣�A

������ Correct Answer: A Detailed Explanation: To enable internet access for private subnets in a VPC, you need a NAT (Network Address Translation) gateway. Here ��s why A is correct: 1. NAT Gateway vs. Internet Gateway: - Private subnets cannot directly use an internet gateway (used for public subnets). Instead, they use a NAT gateway to access the internet outbound (e.g., for software updates) while remaining private (inbound traffic is blocked). - A NAT gateway must be placed in a public subnet (since it needs internet access via the internet gateway). 2.  High Availability: - The question specifies three Availability Zones (AZs). Creating one  NAT gateway  per AZ  (in each  public subnet)  ensures  redundancy.  If one AZ fails, the other  NAT gateways  keep working. - Each private subnet ��s route table points to the NAT gateway in its own AZ. This avoids cross - AZ traffic delays and ensures availability. 3. Why Other Options Are Wrong: - B: NAT instances (not gateways) require manual management and scaling. They ��re less reliable than AWS - managed NAT gateways. - C: Internet gateways can ��t be attached to private subnets. Private subnets can ��t have public internet routes. - D: Egress - only internet gateways work for IPv6, not IPv4 (the question uses IPv4). Reference Links: - [AWS NAT Gateway](https://docs.aws.amazon.com/vpc/latest/user guide/vpc - nat - gateway.html) - [Public vs. Private Subnets](https://docs.aws.amazon.com/vpc/latest/user guide/VPC_Scenario2.html)

93.A company has an AWS Glue extract, transform, and load (ETL) job that runs every day at the same time. The job processes XML data that is in an Amazon S3 bucket. New data is added to the S3 bucket every day. A solutions architect notices that AWS Glue is processing all the data during each run. What should the solutions architect do to prevent AWS Glue from reprocessing old data?

A ��Edit the job to use job bookmarks.

B ��Edit the job to delete data after the data is processed.

C ��Edit the job by setting the Number of workers field to 1.

D ��Use a Find matches machine learning (ML) transform.

�𰸣�A

������ Correct Answer A �� Edit the job to use job bookmarks. Detailed Explanation AWS Glue Job Bookmarks track processed data to avoid reprocessing. Here's why this works: 1. How it helps Job Bookmarks act like a checkpoint - they remember which files/data partitions were already processed. On the next job run, only new/unprocessed data gets handled. 2. Simple analogy Think of it like reading a book - the  bookmark shows where you stopped last time.  Next time, you start from the bookmark instead of re-reading the whole book. 3. Implementation Enabling this feature requires just a few clicks in AWS Glue job settings. No code changes needed for basic use cases. 4. Why other options fail - B (Delete data): Destructive and loses historical data - C (Workers=1): Affects  performance,  not  data  selection  -  D  (Find matches):  For  data  de duplication,  not  incremental  processing Reference                                 Links                                 [AWS                                  Glue                                 Job                                 Bookmarks
Documentation](https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html)              [AWS              Glue              Best Practices](https://docs.aws.amazon.com/glue/latest/dg/best-practices.html)

94.A company is preparing to deploy a new serverless workload. A solutions architect must use the principle of least privilege to configure permissions that will be used to run an AWS Lambda function. An Amazon Event bridge (Amazon Cloud watch Events) rule will invoke the function. Which solution meets these requirements?

A ��Add an execution role to the function with lambda:Invoke function as the action and * as the principal.

B ��Add an execution role to the function with lambda:Invoke function as the action and Service: lambda.amazon aws.com as the principal.

C��Add a resource-based policy to the function with lambda:* as the action and Service: events.amazon aws.com as the principal.

D��Add a resource-based policy to the function with lambda:Invoke function as the action and Service: events.amazon aws.com as the principal.

�𰸣�D

������ Correct Answer: D Detailed Explanation: When an AWS service like Amazon Event bridge (Cloud watch Events) needs to invoke a Lambda function, you must grant explicit permission to that service. Here's why Option D is correct: 1. Resource-Based Policy: Lambda uses resource-based policies (attached directly to the function) to control which services or accounts can invoke it. This is required for Event bridge to trigger the function. 2. Principle of Least Privilege: - Action: `lambda:Invoke function` grants only the permission to invoke the function (no unnecessary permissions). - Principal: `Service: events.amazon aws.com` restricts access only to the  Event bridge service. Why Other Options Are Wrong: - A/B  use an execution  role, which defines what the Lambda function can do (e.g., access S3),  not who can invoke it. - C uses `lambda:*`, which grants overly broad permissions (violates                              least                               privilege).                               Reference:                              [AWS                               Lambda
Permissions](https://docs.aws.amazon.com/lambda/latest/dg/access-control-resource-based.html)

95.A company is preparing to store confidential data in Amazon S3. For compliance reasons, the data must be encrypted at rest. Encryption  key  usage  must  be  logged  for  auditing  purposes.  Keys  must  be  rotated  every  year.  Which  solution  meets  these requirements and is the MOST operationally efficient?

A ��Server-side encryption with customer-provided keys (SSE-C)

B ��Server-side encryption with Amazon S3 managed keys (SSE-S3)

C ��Server-side encryption with AWS KMS keys (SSE-KMS) with manual rotation

D ��Server-side encryption with AWS KMS keys (SSE-KMS) with automatic rotation

�𰸣�D

������ Correct Answer D �� Server-side encryption with AWS KMS keys (SSE-KMS) with automatic rotation Explanation The best solution is SSE-KMS with automatic rotation because: 1. Encryption at rest: SSE-KMS uses AWS Key Management Service (KMS) to manage encryption keys, ensuring data is encrypted in S3. 2. Audit logging: AWS KMS automatically logs key usage in AWS
Cloud trail, meeting compliance requirements for auditing. 3. Key rotation: Enabling automatic annual rotation in KMS ensures keys are  rotated yearly without  manual effort,  making  it operationally efficient. Other options fall short: - A  (SSE-C)  requires manual key management and lacks built-in logging. - B (SSE-S3) uses S3-managed keys but doesn ��t provide key usage logs or customizable rotation. - C (SSE-KMS manual rotation) forces you to manually create/update keys yearly, which is less efficient. Reference   Links    -   [AWS    KMS    Key   Rotation](https://docs.aws.amazon.com/kms/latest/developer guide/rotate-keys.html)    - [Amazon S3 Encryption Types](https://docs.aws.amazon.com/AmazonS3/latest/user guide/serv-side-encryption.html)

96.A company has an automobile sales website that stores its listings in a database on Amazon RDS. When an automobile is sold, the listing needs to be removed from the website and the data must be sent to multiple target systems. Which design should a solutions architect recommend?

A �� Create an AWS  Lambda function triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) queue for the targets to consume.

B �� Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) FI FO queue for the targets to consume.

C��Subscribe to an RDS event notification and send an Amazon Simple Queue Service (Amazon SQS) queue fanned out to multiple Amazon Simple Notification Service (Amazon SNS) topics. Use AWS Lambda functions to update the targets.

D ��Subscribe to an RDS event notification and send an Amazon Simple Notification Service (Amazon SNS) topic fanned out to multiple Amazon Simple Queue Service (Amazon SQS) queues. Use AWS Lambda functions to update the targets.

�𰸣�A

������ Correct Answer D Detailed Explanation When a car is sold, the database update needs to trigger actions to remove the listing and send data to multiple target systems. Here's why Option D is correct: 1. RDS Event Notifications: Amazon RDS cannot directly trigger AWS Lambda on data changes. Instead, you use RDS Event Notifications to detect database updates (e.g., a record deletion). These notifications are sent to an Amazon Simple Notification Service (Amazon SNS) topic. 2. Fan-out with SNS: An SNS topic can "fan out" (broadcast) the event to multiple Amazon Simple Queue Service (SQS) queues, each representing a target system. This ensures all target systems receive the data simultaneously. 3. Lambda Processing: Each SQS queue can trigger an AWS Lambda function to process the data for its specific target system. This design decouples the systems, improves reliability, and scales independently. Why other options are incorrect: - A/B: RDS cannot trigger Lambda directly. SQS queues alone cannot broadcast to multiple systems. - C: SQS cannot "fan out" to SNS topics; SNS is designed for fan-out. - B ��s  F IFO queue: FI FO guarantees order, but it ��s unnecessary here and doesn ��t solve  multi-target delivery. Reference  Links - [Amazon RDS Event Notifications](https://docs.aws.amazon.com/Amazon rds/latest/User guide/US_Events.html)     -      [Fan-out     with      SNS      and

SQS](https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html)

97.A company  needs to store data in Amazon S3 and  must  prevent the data from  being  changed. The company wants  new objects that are uploaded to Amazon S3 to remain unchangeable for a nonspecific amount of time until the company decides to modify the objects. Only specific users in the company's AWS account can have the ability 10 delete the objects. What should a solutions architect do to meet these requirements?

A ��Create an S3 Glacier vault. Apply a write-once, read-many (WORM) vault lock policy to the objects.

B ��Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Set a retention period of 100 years. Use governance mode as the S3 bucket ��s default retention mode for new objects.

C ��Create an S3 bucket. Use AWS Cloud trail to track any S3 API events that modify the objects. Upon notification, restore the modified objects from any backup versions that the company has.

D  �� Create  an  S3  bucket  with  S3  Object   Lock  enabled.  Enable  versioning.  Add  a  legal  hold  to  the  objects.  Add  the s3:Put object legal hold permission to the IAM policies of users who need to delete the objects.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The company needs to enforce immutability (prevent changes) for S3 objects until they explicitly decide to modify them. Here ��s why Option D works best: 1. S3 Object Lock + Versioning: Enabling S3 Object Lock (with versioning) allows objects to be stored in a "write-once-read-many" (WORM) state, preventing deletion or overwriting.
2. Legal Hold: Unlike fixed retention periods (e.g., 100 years in Option B), a legal hold keeps objects immutable indefinitely until the  hold  is explicitly  removed. This  matches the  requirement of  a  "non-specific amount of time."  3. Controlled  Permissions: Adding the `s3:Put object legal hold` permission to specific IAM users ensures only authorized users can remove the legal hold (and then delete/modify objects). Why Not Other Options? - A: S3 Glacier vaults are for archival storage, not S3 objects. - B: A 100-year retention period is too rigid and doesn ��t allow flexibility for earlier changes. - C: Cloud trail only monitors changes; it doesn         ��    t           proactively          prevent           modifications.          Reference           Link:           [Amazon          S3          Object Lock](https://docs.aws.amazon.com/AmazonS3/latest/user guide/object-lock.html)

98.A company recently migrated a message processing system to AWS. The system receives messages into an Activemq queue running on an Amazon EC2 instance. Messages are processed by a consumer application running on Amazon EC2. The consumer application processes the messages and writes results to a MySQL database running on Amazon EC2. The company wants this application to be highly available with low operational complexity. Which architecture offers the HIGHEST availability?

A ��Add a second Activemq server to another Availability Zone. Add an additional consumer EC2 instance in another Availability Zone. Replicate the MySQL database to another Availability Zone.

B �� Use Amazon  MQ with  active/standby  brokers  configured across two Availability Zones. Add an additional consumer  EC2 instance in another Availability Zone. Replicate the MySQL database to another Availability Zone.

C �� Use Amazon  MQ with  active/standby  brokers  configured across two Availability Zones. Add an additional consumer  EC2 instance in another Availability Zone. Use Amazon RDS for MySQL with Multi-AZ enabled.

D �� Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an Auto Scaling group for the consumer EC2 instances across two Availability Zones. Use Amazon RDS for MySQL with Multi-AZ enabled.

�𰸣�D

���� �� Correct Answer:  D  Detailed  Explanation: The  best approach is to use fully managed AWS services with  built-in high availability (HA) features while minimizing operational effort. 1. Amazon MQ with active/standby brokers: Replaces self-managed Activemq  on  EC2.  It  automatically  handles  broker  failover  across  Availability  Zones  (AZs),  reducing  operational  complexity compared  to  manually  managing  Activemq  servers.  2.  Auto  Scaling  group  for  consumers:  Instead  of  manually  adding  EC2 instances, Auto Scaling automatically maintains the desired number of healthy instances across AZs. If an instance fails, it gets replaced automatically, ensuring continuous message processing. 3. RDS Multi-AZ: Provides automatic failover for the database. Unlike manual MySQL replication on EC2 (which requires admin intervention), RDS handles failover seamlessly with a standby replica in another AZ. Other options are inferior because: - A: Uses self-managed components requiring manual HA setup - B/C: Lack Auto Scaling for consumers (manual instance management) - B: Uses manual MySQL replication instead of managed RDS Reference    links:    -    [Amazon    MQ    HA](https://aws.amazon.com/amazon-mq/features/#High_availability)    -    [Auto    Scaling Benefits](https://aws.amazon.com/auto scaling/) - [RDS Multi-AZ](https://aws.amazon.com/rds/details/multi-az/)

99.A  company  hosts a  containerized web  application on  a fleet  of on-premises  servers that  process  incoming  requests.  The number of requests is growing quickly. The on-premises servers cannot handle the increased number of requests. The company wants to move the application to AWS with minimum code changes and minimum development effort. Which solution will meet these requirements with the LEAST operational overhead?

A ��Use AWS Fargate on Amazon Elastic Container Service (Amazon ECS) to run the containerized web application with Service Auto Scaling. Use an Application Load Balancer to distribute the incoming requests.

B ��Use two Amazon EC2 instances to host the containerized web application. Use an Application Load Balancer to distribute the incoming requests.

C��Use AWS Lambda with a new code that uses one of the supported languages. Create multiple Lambda functions to support the load. Use Amazon API Gateway as an entry point to the Lambda functions.

D��Use a high performance computing (HPC) solution such as AWS Parallel cluster to establish an HPC cluster that can process the incoming requests at the appropriate scale.

�𰸣�A

�� �� �� Correct Answer: A  Detailed  Explanation: The  correct  answer  is  A  because AWS  Fargate  on  Amazon  ECS  provides  a serverless compute engine for containers, eliminating the need to manage EC2 instances. This minimizes operational overhead since AWS  handles  server  provisioning,  scaling,  and  maintenance.  Service  Auto  Scaling  automatically  adjusts the  number  of containers based on demand, ensuring the application scales seamlessly with incoming traffic. The Application Load  Balancer (ALB) efficiently distributes requests across containers, improving availability and fault tolerance. This solution requires minimal code changes, as the existing containerized application can be deployed directly to ECS/Fargate. Other options fall short: - B (EC2 instances)  requires  manual  scaling  and  server  management,  increasing  operational  effort.  -  C  (Lambda)  would  force  code rewrites (if the app isn ��t  in a supported language), conflicting with the "minimum code changes" requirement. - D (HPC) is designed    for     specialized     compute-heavy     workloads,     not    general     web     applications.     Reference     Links:     -     [AWS Fargate](https://aws.amazon.com/fargate/)                      -                      [Amazon                       ECS                      Service                      Auto
Scaling](https://docs.aws.amazon.com/Amazon ecs/latest/developer guide/service-auto-scaling.html)      -       [Application      Load Balancer](https://aws.amazon.com/elastic load balancing/application-load-balancer/)

100.A company uses 50 TB of data for reporting. The company wants to move this data from on premises to AWS. A custom application in the company ��s data center runs a weekly data transformation job. The company plans to pause the application until the data transfer is complete and needs to begin the transfer process as soon as possible. The data center does not have any available network bandwidth for additional workloads. A solutions architect must transfer the data and must configure the

transformation  job  to  continue  to   run  in  the  AWS  Cloud.  Which  solution  will   meet  these   requirements  with  the   LEAST operational overhead?

A ��Use AWS Data sync to move the data. Create a custom transformation job by using AWS Glue.

B ��Order an AWS Snow cone device to move the data. Deploy the transformation application to the device.

C ��Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. Create a custom transformation job by using AWS Glue.

D ��Order an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute. Copy the data to the device. Create a new EC2 instance on AWS to run the transformation application.

�𰸣�C

�� �� �� Correct Answer:  C  Detailed  Explanation:  The  company  needs  to  transfer  50  TB  of  data with  no  available  network bandwidth,  so  offline  methods  like  AWS  Snowball  Edge  Storage  Optimized  (supports  up  to  80  TB)  are  ideal.  Option  C  uses Snowball  Edge to  physically  ship the data,  bypassing  network  limitations.  After  migration, AWS  Glue  (serverless  ETL  service) automates the data transformation job with minimal operational overhead, eliminating manual EC2 management. Other options are less optimal: - A (Data sync) requires network bandwidth, which isn ��t available. - B (Snow cone) has insufficient capacity (max 14 TB) for 50 TB. - D (Snowball + EC2) adds complexity by requiring manual setup of EC2 instances for the transformation job, whereas  Glue  is  fully   managed.  Reference  Links:  -   [AWS  Snowball   Edge](https://aws.amazon.com/snowball/edge/)  -   [AWS Glue](https://aws.amazon.com/glue/)

101.A company  has created  an  image  analysis application  in which  users  can  upload  photos  and add  photo frames to their images. The users upload images and metadata to indicate which photo frames they want to add to their images. The application uses a single Amazon EC2 instance and Amazon Dynamo db to store the metadata. The application is becoming more popular, and the number of users is increasing. The company expects the number of concurrent users to vary significantly depending on the time of day and day of week. The company must ensure that the application can scale to meet the needs of the growing user base. Which solution meats these requirements?

A ��Use AWS Lambda to process the photos. Store the photos and metadata in Dynamo db.

B ��Use Amazon Kinesis Data Firehose to process the photos and to store the photos and metadata.

C ��Use AWS Lambda to process the photos. Store the photos in Amazon S3. Retain Dynamo db to store the metadata.

D �� Increase the number of EC2 instances to three. Use Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volumes to store the photos and metadata.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The best solution is to use AWS Lambda for processing photos, store the photos in Amazon S3, and keep Dynamo db for metadata. Here's why: 1. AWS Lambda: Automatically scales to handle varying workloads (like sudden user spikes) without manual intervention. This matches the requirement for unpredictable concurrency. 2. Amazon S3:  Designed for storing  large files  (like  images) cost - effectively and scales  infinitely.  Dynamo db  isn't  ideal for storing  large binary data like photos. 3. Dynamo db: Retained for metadata (e.g., photo frame preferences) as it handles structured data and scales  automatically  with  access  patterns.  Other  options  fail  because:  -  A:  Storing  photos  in  Dynamo db  is  inefficient  and expensive. - B: Kinesis Data Firehose isn't optimized for image processing workflows. - D: EC2 instances require manual scaling,

and  EBS  isn't  cost  -  effective  for  large  -  scale  storage.  Reference:  [AWS  Lambda  Scaling](https://aws.amazon.com/lambda/), [Amazon S3 Use Cases](https://aws.amazon.com/s3/), [Dynamo db Basics](https://aws.amazon.com/dynamo db/)

102.A medical records company is hosting an application on Amazon EC2 instances. The application processes customer data files that are stored on Amazon S3. The EC2 instances are hosted in public subnets. The EC2 instances access Amazon S3 over the internet,  but  they  do  not  require  any  other  network  access.  A  new  requirement  mandates  that  the  network  traffic  for  file transfers take a private route and not be sent over the internet. Which change to the network architecture should a solutions architect recommend to meet this requirement?

A ��Create a NAT gateway. Configure the route table for the public subnets to send traffic to Amazon S3 through the NAT gateway.

B �� Configure the  security group for the  EC2  instances to  restrict outbound traffic so that only traffic to the S3  prefix  list  is permitted.

C ��Move the EC2 instances to private subnets. Create a VPC endpoint for Amazon S3, and link the endpoint to the route table for the private subnets.

D ��Remove the internet gateway from the VPC. Set up an AWS Direct Connect connection, and route traffic to Amazon S3 over the Direct Connect connection.

�𰸣�C

���� �� Correct Answer: C Detailed Explanation: The EC2 instances are currently in public subnets, which have a route to the internet via an internet gateway. This  means their S3 traffic uses the public internet. To enforce a private route, the solution involves: 1. Moving EC2 instances to private subnets: Private subnets have no direct internet access, eliminating public internet routing. 2. Creating a VPC Gateway Endpoint for S3: This creates a private connection between the VPC and S3. The endpoint is added to the private subnet ��s route table, directing S3 traffic through AWS ��s internal network instead of the public internet. Why other options fail: - A: NAT gateways allow private subnet instances to access the internet, but S3 traffic would still traverse the public internet. - B: Security groups control traffic permissions, not routing paths. Traffic would still use the internet. - D: Direct Connect connects on - premises networks to AWS, but the EC2 instances are already in AWS. A VPC endpoint is simpler and cheaper. Key Concept: VPC Gateway Endpoints provide private, secure connectivity to AWS services (like S3) without internet exposure.    Private    subnets    ensure    no    unintended    internet    routing.    Reference     Links:    -    [AWS    VPC    Endpoints    for S3](https://docs.aws.amazon.com/AmazonS3/latest/user guide/private link  -  interface  -  endpoints.html)  -   [Public  vs.   Private Subnets](https://docs.aws.amazon.com/vpc/latest/user guide/VPC_Subnets.html)

103.A company stores its application logs in an Amazon Cloud watch Logs log group. A new policy requires the company to store all application logs in Amazon Open search Service (Amazon Elastic search Service) in near-real time. Which solution will meet this requirement with the LEAST operational overhead?

A ��Configure a Cloud watch Logs subscription to stream the logs to Amazon Open search Service (Amazon Elastic search Service).

B ��Create an AWS Lambda function. Use the log group to invoke the function to write the logs to Amazon Open search Service (Amazon Elastic search Service).

C ��Create an Amazon Kinesis Data Firehose delivery stream. Configure the log group as the delivery streams sources. Configure Amazon Open search Service (Amazon Elastic search Service) as the delivery stream's destination.

D �� Install and configure Amazon Kinesis Agent on each application server to deliver the logs to Amazon Kinesis Data Streams. Configure Kinesis Data Streams to deliver the logs to Amazon Open search Service (Amazon Elastic search Service).

�𰸣�A

������ Correct Answer: C Detailed Explanation: To meet the requirement of streaming logs from Cloud watch Logs to Amazon Open search Service in near-real time with minimal operational overhead, the best solution is Option C. Here ��s why: 1. Amazon Kinesis Data Firehose is a fully managed service that automatically handles data delivery to destinations like Open search. 2. You configure a subscription filter  in  Cloud watch  Logs  to  send  logs  to  Kinesis  Data  Firehose.  Firehose  then  streams  the  data  to Open search without requiring custom code. 3. This approach eliminates the need to write Lambda functions (Option B) or install agents on servers (Option D), reducing operational complexity. 4. Option A is incorrect because Cloud watch Logs subscriptions cannot directly stream to Open search. Subscriptions must route logs to Kinesis Data Firehose, Lambda, or Kinesis Data Streams first.   Firehose   is   the   most   seamless   way  to  forward   logs   to   Open search.   Reference   Link:   [Amazon   Cloud watch   Logs Subscriptions](https://docs.aws.amazon.com/Amazon cloud watch/latest/logs/Subscriptions.html)   Reference    Link:    [Streaming Cloud watch                                Logs                                to                                Amazon                                Open search                                via
Firehose](https://docs.aws.amazon.com/firehose/latest/dev/cloud watch-log-subscription.html)

104.A company is building a web-based application running on Amazon EC2 instances in multiple Availability Zones. The web application will provide access to a repository of text documents totaling about 900 TB in size. The company anticipates that the web application will experience periods of high demand. A solutions architect must ensure that the storage component for the text documents can scale to meet the demand of the application at all times. The company is concerned about the overall cost of the solution. Which storage solution meets these requirements MOST cost-effectively?

A ��Amazon Elastic Block Store (Amazon EBS)

B ��Amazon Elastic File System (Amazon EFS)

C ��Amazon Open search Service (Amazon Elastic search Service)

D ��Amazon S3

�𰸣�D

���� �� Correct Answer:  D. Amazon S3  Detailed  Explanation: Amazon S3 is the most cost-effective solution for this scenario because: 1. Scalability: S3 automatically scales storage and handles unlimited requests, perfect for unpredictable high-demand periods.  There's  no  need  to  pre-allocate  storage  for  900TB  upfront.  2.  Cost  Efficiency:  S3  charges  only  for  what  you  use (~$$23/TB/ month for Standard tier).  For  900TB, this  is  significantly cheaper than  EBS/EFS.  No  upfront  fees or  minimums.  3. Durability   �� Availability:  S3 offers 99.999999999% durability (11 nines) and multi-AZ redundancy by default, protecting the 900TB  data.  4.  Performance:  S3  supports  thousands  of  requests  per  second,  ideal  for  web  app  users  accessing  documents simultaneously. Why not other options: - A.  EBS:  Requires  manual scaling,  locked to single  EC2  instances  (not  multi-instance accessible), and expensive for 900TB. - B. EFS: While scalable and multi-instance accessible, it costs ~3x more t han S3 ($$0.08/GB vs S3's ~$0.023/GB). - C. Open search: A search/analytics engine, not designed for bulk storage. Reference Links: - [Amazon S3 Pricing](https://aws.amazon.com/s3/pricing/) - [AWS Storage Services Overview](https://aws.amazon.com/products/storage/)

105.A global company is using Amazon API Gateway to design REST APIs for its loyalty club users in the us-east-1 Region and the ap-southeast-2  Region. A solutions architect must design a solution to protect these API Gateway managed  REST APIs across multiple accounts from SQL  injection  and cross-site scripting attacks. Which solution will  meet these  requirements  with the LEAST amount of administrative effort?

A ��Set up AWS WAF in both Regions. Associate Regional web ACLs with an API stage.

B ��Set up AWS Firewall Manager in both Regions. Centrally configure AWS WAF rules.

C ��Set up AWS Shield in bath Regions. Associate Regional web ACLs with an API stage.

D ��Set up AWS Shield in one of the Regions. Associate Regional web ACLs with an API stage.

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: The best solution is to use AWS Firewall Manager because it allows centrally managing AWS WAF  rules across  multiple AWS  accounts and  regions within an organization. Since the  requirement  involves protecting  APIs  in  multiple   regions  and  accounts,   Firewall  Manager  significantly  reduces  administrative  effort  by  enabling consistent security policies (like SQL injection and XSS protection) without manual configuration in each account/region. - AWS WAF (Option A) would require manual setup in each region/account, increasing overhead. - AWS Shield (Options C/D) focuses on DDoS protection, not SQL/XSS attacks. - Firewall Manager (Option B) automates WAF rule deployment, aligning with the "least administrative effort" requirement. Reference Link: [AWS Firewall Manager](https://aws.amazon.com/firewall-manager/) [AWS WAF](https://aws.amazon.com/waf/)

106.A company has implemented a self-managed DNS solution on three Amazon EC2 instances behind a Network Load Balancer (NLB) in the us-west-2 Region. Most of the company's users are located in the United States and Europe. The company wants to improve the  performance  and  availability  of the solution. The  company  launches  and  configures three  EC2  instances  in  the eu-west-1 Region and adds the EC2 instances as targets for a new NLB. Which solution can the company use to route traffic to all the EC2 instances?

A �� Create  an  Amazon  Route  53  geolocation  routing  policy  to  route  requests  to  one  of  t he  two  NLBs.  Create  an  Amazon Cloud front distribution. Use the Route 53 record as the distribution ��s origin.

B ��Create a standard accelerator in AWS Global Accelerator. Create endpoint groups in us-west-2 and eu-west-1. Add the two NLBs as endpoints for the endpoint groups.

C��Attach Elastic IP addresses to the six EC2 instances. Create an Amazon Route 53 geolocation routing policy to route requests to one of the six EC2 instances. Create an Amazon Cloud front distribution. Use the Route 53 record as the distribution's origin.

D ��Replace the two NLBs with two Application Load Balancers (ALBs). Create an Amazon Route 53 latency routing policy to route requests to one of the two ALBs. Create an Amazon Cloud front distribution. Use the Route 53 record as the distribution ��s origin.

�𰸣�B

������ Correct Answer B Detailed Explanation The best solution is AWS Global Accelerator. Here ��s why: 1. Global Accelerator uses Anycast IP addresses to provide static entry points for your applications. This means users connect to the nearest AWS edge location, which then routes traffic over AWS ��s optimized network to the closest healthy endpoint (NLB in this case). 2. Endpoint groups in us - west - 2 (US) and eu - west - 1 (Europe) ensure users in the US and Europe automatically connect to the nearest region,  reducing  latency.  3.  Global  Accelerator  performs  continuous  health  checks  on  the  NLBs.  If  one  region  fails,  traffic  is rerouted to the healthy region, improving availability. 4. Using NLBs (instead of ALBs in Option D) is correct here because NLBs are layer - 4 (TCP/UDP) balancers, which are typically used for DNS solutions (not HTTP - based ALBs). Why other options are less ideal: - Option A/C rely on Route 53 geolocation + Cloud front. While Cloud front caches content at edge locations, it ��s designed for static/dynamic web content, not low - latency TCP/UDP traffic like DNS. - Option C bypasses the NLBs (using Elastic IPs directly on EC2 instances), losing load balancing and health checks. - Option D replaces NLBs with ALBs unnecessarily (DNS typically uses TCP/UDP,        not         HTTP)        and        adds         complexity        with         Cloud front.        Reference         Link         [AWS        Global Accelerator](https://aws.amazon.com/global - accelerator/)

107.A  company  is  running  an  online  transaction  processing  (OLTP)  workload  on  AWS.  This  workload  uses  an  unencrypted Amazon  RDS  DB  instance  in  a  Multi-AZ  deployment.  Daily  database  snapshots  are  taken  from  this  instance.  What  should  a solutions architect do to ensure the database and snapshots are always encrypted moving forward?

A ��Encrypt a copy of the latest DB snapshot. Replace existing DB instance by restoring the encrypted snapshot.

B ��Create a new encrypted Amazon Elastic Block Store (Amazon EBS) volume and copy the snapshots to it. Enable encryption on the DB instance.

C ��Copy the snapshots and enable encryption using AWS Key Management Service (AWS KMS) Restore encrypted snapshot to an existing DB instance.

D �� Copy the snapshots to an Amazon S3  bucket that  is encrypted  using server-side encryption with AWS  Key  Management Service (AWS KMS) managed keys (SSE-KMS).

�𰸣�A

������ Correct Answer: A Detailed Explanation: To encrypt an existing unencrypted RDS database and its future snapshots, you cannot directly enable encryption on the running instance. Instead, you must: 1. Take a snapshot of the unencrypted database. 2. Copy the snapshot and enable encryption during the copy process using AWS KMS. 3. Restore the encrypted snapshot to create a new  RDS  instance  (this  new  instance  will  be  encrypted).  4.  Replace  the  old  unencrypted  instance  with  the  newly  created encrypted instance. After this, all future snapshots of the encrypted database will automatically be encrypted. Options B, C, and D are incorrect because they involve unsupported methods (e.g., EBS volume manipulation, restoring to an existing instance, or using             S3             for              RDS              snapshots).             Reference              Link:              [Encrypting             Amazon              RDS Resources](https://docs.aws.amazon.com/Amazon rds/latest/User guide/Overview.Encryption.html)

108.A company wants to build a scalable key management infrastructure to support developers who need to encrypt data in their applications. What should a solutions architect do to reduce the operational burden?

A ��Use multi-factor authentication (MFA) to protect the encryption keys.

B ��Use AWS Key Management Service (AWS KMS) to protect the encryption keys.

C ��Use AWS Certificate Manager (ACM) to create, store, and assign the encryption keys.

D ��Use an IAM policy to limit the scope of users who have access permissions to protect the encryption keys.

�𰸣�B

�� �� �� Correct Answer:  B  Detailed  Explanation:  AWS  Key  Management  Service  (KMS)  is  a  fully  managed  service  designed specifically for creating and controlling encryption keys. It eliminates the operational complexity of managing cryptographic keys manually, such as secure storage, rotation, or scalability. Developers can easily integrate KMS with AWS services (e.g., S3, EBS) or custom applications via APIs.  Options A  and  D focus on  access  control  rather than  key  management,  while ACM  (Option  C) handles SSL/TLS certificates,  not  encryption  keys for application data.  KMS  automatically  handles  security  best  practices  like hardware      security      modules      (HSMs)      and      key      rotation,      reducing       operational      overhead.      Reference      Link: https://aws.amazon.com/kms/

109.A company has a dynamic web application hosted on two Amazon EC2 instances. The company has its own SSL certificate,

which is on each instance to perform SSL termination. There has been an increase in traffic recently, and the operations team determined that SSL encryption and decryption is causing the compute capacity of the web servers to reach their maximum limit. What should a solutions architect do to increase the application's performance?

A ��Create a new SSL certificate using AWS Certificate Manager (ACM). Install the ACM certificate on each instance.

B ��Create an Amazon S3 bucket Migrate the SSL certificate to the S3 bucket. Configure the EC2 instances to reference the bucket for SSL termination.

C �� Create another  EC2  instance as a  proxy server. Migrate the SSL certificate to the  new instance and configure it to direct connections to the existing EC2 instances.

D��Import the SSL certificate into AWS Certificate Manager (ACM). Create an Application Load Balancer with an HTTPS listener that uses the SSL certificate from ACM.

�𰸣�D

���� �� Correct Answer: D Detailed Explanation: The main issue here is that the EC2 instances are overwhelmed by handling SSL/TLS  encryption/decryption  (SSL  termination).  To  solve  this,  we  should  offload  SSL  termination  to  a  dedicated  service.  - Option D uses an Application Load Balancer (ALB) with an HTTPS listener. The ALB handles SSL termination, meaning it decrypts incoming HTTPS traffic and forwards unencrypted HTTP requests to the EC2 instances. This removes the SSL processing burden from the EC2 instances, freeing up their compute capacity for the actual web application. Importing the existing SSL certificate into AWS Certificate Manager (ACM) allows the ALB to use it securely without manual management on the EC2 instances. Other options fail because: - A: Installing ACM certificates on EC2 instances doesn ��t offload SSL termination; instances still  handle encryption/decryption. - B: S3 buckets can ��t terminate SSL; they ��re for storage, not traffic processing. - C: Adding a proxy EC2 instance shifts the SSL workload to another server but doesn ��t solve scaling or reduce management complexity. Reference Links:
-    [AWS    Application    Load     Balancer](https://aws.amazon.com/elastic load balancing/application-load-balancer/)    -    [SSL/TLS Offloading    with     ALB](https://docs.aws.amazon.com/elastic load balancing/latest/application/ssl-offload.html)     -     [Importing Certificates into ACM](https://docs.aws.amazon.com/acm/latest/user guide/import-certificate.html)

110.A company  has a  highly dynamic  batch  processing job that  uses  many  Amazon  EC2  instances to  complete  it. The job  is stateless in nature, can be started and stopped at any given time with no negative impact, and typically takes upwards of 60 minutes total to complete. The company has asked a solutions architect to design a scalable and cost-effective solution that meets the requirements of the job. What should the solutions architect recommend?

A ��Implement EC2 Spot Instances.

B ��Purchase EC2 Reserved Instances.

C ��Implement EC2 On-Demand Instances.

D ��Implement the processing on AWS Lambda.

�𰸣�A

������ Correct Answer A �� Implement EC2 Spot Instances. Detailed Explanation The job is stateless, interruptible, and runs for ~60  minutes.  Spot  Instances  are  ideal  here  because:  1.  Cost-Effective:  Spot  Instances  cost  up  to  90%  less  than  On-Demand Instances. 2. Fault Tolerance: Since the job can restart without issues, Spot interruptions (which are rare for short jobs) won ��t impact completion. 3. Scalability: Spot Fleets or Auto Scaling can dynamically adjust capacity based on availability and workload.

Other options: - Reserved Instances (B): Require long-term commitments and are better for steady workloads, not dynamic jobs.
- On-Demand (C): More expensive and unnecessary since the job tolerates interruptions. - Lambda (D): Lambda has a 15-minute runtime       limit,        making        it       unsuitable        for       60-minute       jobs.        Reference        Link       [Amazon        EC2        Spot Instances](https://aws.amazon.com/ec2/spot/)

111.A solutions architect needs to implement a solution to reduce a company's storage costs. All the company's data is in the Amazon S3 Standard storage class. The company must keep all data for at least 25 years. Data from the most recent 2 years must be highly available and immediately retrievable. Which solution will meet these requirements?

A ��Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive immediately.

B ��Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 2 years.

C ��Use S3 Intelligent-Tiering. Activate the archiving option to ensure that data is archived in S3 Glacier Deep Archive.

D ��Set up an S3 Lifecycle policy to transition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately and to S3 Glacier Deep Archive after 2 years.

�𰸣�B

������ Correct Answer B Detailed Explanation The company needs to retain data for 25 years but requires data from the most recent 2 years to be highly available and immediately retrievable. Here ��s why Option B is the best choice: 1. Amazon S3 Standard is ideal for the first 2 years because it provides high availability and immediate retrieval, meeting the requirement for recent data.
2. After 2 years, the data no longer needs immediate access. Transitioning it to S3 Glacier Deep Archive via a Lifecycle policy ensures: -  Lower  storage  costs: Glacier  Deep  Archive  is the  cheapest storage class for  long  - term  archival  (e.g.,  decades). - Compliance: The data remains stored securely for the required 25 years. - Retrieval limitations: Glacier Deep Archive has longer retrieval times (12+ hours), which is acceptable for older data. Why Other Options Fail - Option A: Transitioning to Glacier Deep Archive immediately would make recent data inaccessible for immediate retrieval. - Option C: S3 Intelligent - Tiering ��s archiving option might move data to Glacier before 2 years if access patterns change, violating the availability requirement. - Option D: Moving to S3 One Zone - IA immediately reduces availability (data stored in only one Availability Zone) and doesn ��t align with the "highly available" requirement. Reference Links - [Amazon S3 Storage Classes](https://aws.amazon.com/s3/storage - classes/)
- [S3 Lifecycle Policies](https://docs.aws.amazon.com/AmazonS3/latest/user guide/object - lifecycle - mgmt.html)

112.A media company is evaluating the possibility of moving its systems to the AWS Cloud. The company needs at least 10 TB of storage with the  maximum  possible  I/O  performance for video  processing, 300 TB  of very durable storage for storing  media content, and 900 TB of storage to meet requirements for archival media that is not in use anymore. Which set of services should a solutions architect recommend to meet these requirements?

A ��Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage

B ��Amazon EBS for maximum performance, Amazon EFS for durable data storage, and Amazon S3 Glacier for archival storage

C ��Amazon EC2 instance store for maximum performance, Amazon EFS for durable data storage, and Amazon S3 for archival storage
D �� Amazon  EC2  instance store for  maximum  performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage

�𰸣�D

���� �� Correct Answer:  D  Detailed Explanation:  1.  Maximum  I/O  Performance (10TB): The  best choice  here is Amazon EC2 Instance Store because it provides the highest possible I/O performance. Instance stores use physically attached storage to the EC2 instance, offering low latency and high throughput��ideal for temporary video processing workloads. Unlike Amazon EBS, instance storage is ephemeral (data is lost if the instance stops), but it ��s perfect for short - term, high - performance needs. 2. Highly  Durable  Storage  (300TB): Amazon S3  is  designed  for 99.999999999%  (11  nines)  durability,  making  it  ideal  for  storing critical media content. It ��s a managed object storage service that scales effortlessly and handles large files like videos efficiently. Amazon EFS (a file storage service) is less cost - effective for large - scale object storage compared to S3. 3. Archival Storage (900TB): Amazon S3 Glacier offers the lowest cost for long - term archival storage. It ��s optimized for data that is rarely accessed, such as inactive  media archives, while still maintaining high durability. Using standard S3 for archival would be unnecessarily expensive. Why Not Other Options? - A/B: Amazon EBS (block storage) is durable but doesn ��t match the raw I/O performance of instance storage. - C: Amazon EFS is a file system better suited for shared access across multiple EC2 instances, not bulk object storage.      Using      S3      for      archival      is      costlier      than      Glacier.      Reference       Links:      -      [Amazon      EC2       Instance Store](https://aws.amazon.com/ec2/instance-types/) - [Amazon S3 Durability](https://aws.amazon.com/s3/faqs/) - [Amazon S3 Glacier](https://aws.amazon.com/s3/storage-classes/glacier/)

113.A company wants to  run  applications  in  containers  in the AWS  Cloud. These  applications are stateless  and  can tolerate disruptions within the underlying infrastructure. The company needs a solution that minimizes cost and operational overhead. What should a solutions architect do to meet these requirements?

A ��Use Spot Instances in an Amazon EC2 Auto Scaling group to run the application containers.

B ��Use Spot Instances in an Amazon Elastic Ku bernet es Service (Amazon EKS) managed node group. C ��Use On-Demand Instances in an Amazon EC2 Auto Scaling group to run the application containers.

D ��Use On-Demand Instances in an Amazon Elastic Ku bernet es Service (Amazon EKS) managed node group.

�𰸣�B

������ Correct Answer B �� Use Spot Instances in an Amazon Elastic Ku bernet es Service (Amazon EKS) managed node group. Explanation The company  needs  a  cost-effective,  low-maintenance  solution for stateless  containerized  apps that  can  handle disruptions. Here ��s why B is the best choice: 1. Spot Instances are much cheaper than On-Demand (up to 90% discount), aligning with the goal to minimize cost. 2. EKS Managed Node Groups automate node setup, scaling, and updates, reducing operational overhead. 3. Stateless apps (tolerant of disruptions) are a perfect fit for Spot Instances, which can be interrupted by AWS with little warning. 4. Using EKS (a managed Ku bernet es service) simplifies container orchestration compared to manually managing containers on EC2 (Option A). Options C/D use expensive On-Demand Instances, and Option A lacks the automation benefits of EKS.                      Reference                      Links                     -                       [Amazon                      EKS                      Managed                      Node
Groups](https://docs.aws.amazon.com/eks/latest/user guide/managed-node-groups.html)                 -                 [AWS                 Spot
Instances](https://aws.amazon.com/ec2/spot/)

114.An application runs on Amazon EC2 instances across multiple Availability Zonas. The instances run in an Amazon EC2 Auto Scaling group behind an Application Load Balancer. The application performs best when the CPU utilization of the EC2 instances is at or near 40%. What should a solutions architect do to maintain the desired performance across all instances in the group?

A ��Use a simple scaling policy to dynamically scale the Auto Scaling group.

B ��Use a target tracking policy to dynamically scale the Auto Scaling group.

C ��Use an AWS Lambda function ta update the desired Auto Scaling group capacity.

D ��Use scheduled scaling actions to scale up and scale down the Auto Scaling group.

�𰸣�B

������ Correct Answer B Detailed Explanation The best solution is B. Use a target tracking policy to dynamically scale the Auto Scaling group. Here ��s why: - Target Tracking Scaling Policy is designed to maintain a specific metric (in this case, CPU utilization) at or near a target value (40%). AWS Auto Scaling automatically adjusts the number of EC2 instances in the Auto Scaling group to achieve this target. It handles scaling actions (adding/removing instances) and creates necessary Amazon Cloud watch alarms behind the scenes. - Why not other options: - A. Simple Scaling Policy: Requires manual configuration of Cloud watch alarms and scaling adjustments. It ��s less automated and less efficient for maintaining a precise metric like CPU at 40%. - C. Lambda Function: Over complicates the solution. AWS already provides built - in scaling policies like target tracking, so custom code is unnecessary.
- D. Scheduled Scaling: Works only for predictable traffic patterns (e.g., scaling up at 9 AM daily). It can ��t dynamically respond to real         -          time         CPU          utilization          changes.          Reference          Link          [AWS         Target          Tracking         Scaling Policies](https://docs.aws.amazon.com/auto scaling/ec2/user guide/as-scaling-target-tracking.html)

115.A company is developing a file-sharing application that will use an Amazon S3 bucket for storage. The company wants to serve all the files through an Amazon Cloud front distribution. The company does not want the files to be accessible through direct navigation to the S3 URL. What should a solutions architect do to meet these requirements?

A ��Write individual policies for each S3 bucket to grant read permission for only Cloud front access.

B ��Create an IAM user. Grant the user read permission to objects in the S3 bucket. Assign the user to Cloud front.

C ��Write an S3 bucket policy that assigns the Cloud front distribution ID as the Principal and assigns the target S3 bucket as the Amazon Resource Name (ARN).

D ��Create an origin access identity (OAI). Assign the OAI to the Cloud front distribution. Configure the S3 bucket permissions so that only the OAI has read permission.

�𰸣�D

������ Correct Answer D Detailed Explanation To securely serve S3 files through Cloud front while blocking direct S3 access: 1. Origin Access Identity (OAI) acts as a special Cloud front "user" to access S3. 2. Cloud front Distribution is assigned this OAI. 3. S3 Bucket Policy is updated to grant read access ONLY to the OAI, blocking public access. This ensures: - Users can only access files via Cloud front  URLs. -  Direct S3  URL  access  is  denied. Other options are  incorrect  because: - A/C - S3  policies  can't directly reference Cloud front distributions as principals. - B - IAM users require credentials (insecure for public content) instead of OAI's role                                    -                                    based                                     access.                                    Reference                                     Link
https://docs.aws.amazon.com/Amazon cloud front/latest/Developer guide/private-content-restricting-access-to-s3.html

116.A company ��s website provides users with downloadable historical performance reports. The website needs a solution that will scale to meet the company ��s website demands globally. The solution should  be cost-effective, limit the provisioning of infrastructure  resources,  and   provide  the  fastest  possible  response  time.  Which  combination  should  a  solutions  architect recommend to meet these requirements?

A ��Amazon Cloud front and Amazon S3

B ��AWS Lambda and Amazon Dynamo db

C ��Application Load Balancer with Amazon EC2 Auto Scaling D ��Amazon Route 53 with internal Application Load Balancers
�𰸣�A

������ Correct Answer: A. Amazon Cloud front and Amazon S3 Detailed Explanation: The best solution here is to use Amazon S3 to store the downloadable reports and Amazon Cloud front to distribute them globally. Here ��s why this works: 1. Scalability: S3 automatically scales to handle any amount of data or requests, so the company won ��t  need to  manually provision storage resources. 2. Cost-effectiveness: You only pay for the storage (S3) and data transfer (Cloud front) you use, avoiding upfront costs or over-provisioning. 3. Fast response times: Cloud front is a Content Delivery Network (CDN) that caches copies of the reports at "edge locations" (data centers worldwide). When users download a report, they get it from the nearest edge location, reducing latency. 4. Low maintenance: Both services are fully managed by AWS, so the company doesn ��t need to manage servers or infrastructure. Other options fail because: - Option B (Lambda + Dynamo db): Dynamo db is a database, not ideal for storing large files like reports. Lambda is for running code, not file storage. - Option C (EC2 + Load Balancer): Requires managing servers (EC2), which contradicts "limit provisioning of infrastructure." - Option D (Route 53 + internal ALB): Route 53 is a DNS service, not a storage   or   CDN   solution.   Internal   load   balancers   can  �� t   serve   public   content   globally.   Reference   Links:   -   [Amazon S3](https://aws.amazon.com/s3/) - [Amazon Cloud front](https://aws.amazon.com/cloud front/)

117.A company runs an Oracle database on  premises. As  part of the company ��s  migration to AWS, the company wants to upgrade the database to the most recent available version. The company also wants to set  up disaster recovery (DR) for the database. The company needs to minimize the operational overhead for normal operations and DR setup. The company also needs to maintain access to the database's underlying operating system. Which solution will meet these requirements?

A ��Migrate the Oracle database to an Amazon EC2 instance. Set up database replication to a different AWS Region.

B ��Migrate the Oracle database to Amazon RDS for Oracle. Activate Cross-Region automated backups to replicate the snapshots to another AWS Region.

C��Migrate the Oracle database to Amazon RDS Custom for Oracle. Create a read replica for the database in another AWS Region.

D ��Migrate the Oracle database to Amazon RDS for Oracle. Create a standby database in another Availability Zone.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The company needs to migrate its Oracle database to AWS while upgrading to the latest version, setting up disaster recovery (DR), minimizing operational overhead, and retaining access to the underlying OS. Here ��s why C is the best choice: 1. Amazon RDS Custom for Oracle: - It is a managed database service designed for applications requiring OS and database customization, which aligns with the need to access the underlying OS. - AWS handles routine tasks like backups, patching, and scaling, reducing operational overhead. - Supports the latest Oracle versions, enabling the upgrade during migration. 2. Disaster Recovery with Cross-Region Read Replica: - RDS Custom allows creating a read replica in another AWS  Region. This  replica  acts  as  a  standby  database for  DR.  -  In  a  disaster, the  read  replica  can  be  promoted  to  a  primary database,  ensuring  minimal  downtime.  -  Automated   replication  simplifies   DR  setup  compared  to   manual  solutions  (like EC2-based  replication).  Other  options  fail  because:  -  A  (EC2):  Requires  manual  replication  and  OS/database  management, increasing operational effort. - B (RDS): Blocks OS access and uses backups (not real-time replication) for DR, leading to potential data  loss.  -  D  (RDS  Multi-AZ):  Only  provides  high  availability  within  a  single  Region,  not  cross-Region  DR.  Reference  Links:  - [Amazon        RDS         Custom         for         Oracle](https://aws.amazon.com/rds/custom/)         -         [RDS         Custom         Disaster Recovery](https://docs.aws.amazon.com/Amazon rds/latest/User guide/custom-disaster-recovery.html)

118.A company wants to move its application to a serverless solution. The serverless solution needs to analyze existing and new data by using SL. The company stores the data in an Amazon S3 bucket. The data requires encryption and must be replicated to a different AWS Region. Which solution will meet these requirements with the LEAST operational overhead?

A ��Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with AWS KMS multi-Region kays (SSE-KMS). Use Amazon Athena to query the data.

B ��Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with AWS KMS multi-Region keys (SSE-KMS). Use Amazon RDS to query the data.

C��Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use Amazon Athena to query the data.

D��Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use Amazon RDS to query the data.

�𰸣�C

�� �� �� Correct Answer: A  Explanation: The correct answer  is A.  Here's  why: - S3 Cross-Region  Replication  (CRR)  replicates encrypted objects to another AWS Region, meeting the replication requirement. - AWS KMS multi-Region keys (SSE-KMS) allow the same encryption key to be used in both  Regions, ensuring encrypted data stays encrypted during replication. This avoids decryption/re-encryption overhead. - Amazon Athena is serverless and can directly query data stored in S3 using SQL, aligning with the "serverless analysis" requirement. Why other options are wrong: - Option C/D use SSE-S3, which encrypts data with S3-managed  keys.  However, SSE-S3  does  not support  replicating  encrypted objects to another  Region  unless the  objects are unencrypted first, violating the encryption requirement. - Option B/D use Amazon RDS, which is a managed relational database service,   not   a    serverless   analytics   tool.    RDS   can   �� t   directly    query   S3   data.    Reference   Links:   -    [S3   Cross-Region Replication](https://docs.aws.amazon.com/AmazonS3/latest/user guide/replication.html)       -        [AWS        KMS        Multi-Region Keys](https://docs.aws.amazon.com/kms/latest/developer guide/multi-region-keys-overview.html)                   -                   [Amazon
Athena](https://aws.amazon.com/athena/)

119.A company runs workloads on AWS. The company needs to connect to a service from an external provider. The service is hosted in the provider's VPC. According to the company ��s security team, the connectivity must be private and must be restricted to  the  target  service.  The  connection  must  be  initiated  only  from  the  company �� s  VPC.  Which  solution  will  mast  these requirements?

A ��Create a VPC peering connection between the company's VPC and the provider's VPC. Update the route table to connect to the target service.

B ��Ask the provider to create a virtual private gateway in its VPC. Use AWS Private link to connect to the target service.

C ��Create a NAT gateway in a public subnet of the company ��s Vp update the route table to connect to the target service. D ��Ask the provider to create a VPC endpoint for the target service. Use AWS Private link to connect to the target service.

�𰸣�D

������ Correct Answer: D Detailed Explanation: AWS Private link is the best solution here. The provider creates a VPC endpoint service for their target service, and the company uses AWS Private link to create an interface VPC endpoint in their own VPC. This setup ensures: 1. Private connectivity: Traffic stays within the AWS network (no public internet exposure). 2. Restricted access: Only the specific endpoint service is accessible, not the provider's entire VPC. 3. Initiator control: The company's VPC initiates the connection to the  provider's  endpoint  service.  Other  options fail  because:  - A  (VPC  peering)  exposes  all  resources  between peered VPCs. - B (virtual private gateway) relates to VPNs, not service - specific access. - C (NAT gateway) uses public internet traffic.  Reference link:  [What is AWS  Private link?](https://docs.aws.amazon.com/white papers/latest/aws - vpc - connectivity - options/aws - private link.html)

120.A company uses AWS Organizations to create dedicated AWS accounts for each business unit to manage each business unit's account independently upon request. The root email recipient missed a notification that was sent to the root user email address of one account. The company wants to ensure that all future notifications are not missed. Future notifications must be limited to account administrators. Which solution will meet these requirements?

A ��Configure the company ��s email server to forward notification email messages that are sent to the AWS account root user email address to all users in the organization.

B ��Configure all AWS account root user email addresses as distribution lists that go to a few administrators who can respond to alerts. Configure AWS account alternate contacts in the AWS Organizations console or programmatically.

C ��Configure all AWS account root user email messages to be sent to one administrator who is responsible for monitoring alerts and forwarding those alerts to the appropriate groups.

D ��Configure all existing AWS accounts and all newly created accounts to use the same root user email address. Configure AWS account alternate contacts in the AWS Organizations console or programmatically.

�𰸣�B

�� �� ��  Correct  Answer:   B.  Configure  all  AWS  account  root  user  email  addresses  as  distribution  lists  that  go  to  a  few administrators  who  can  respond  to  alerts.  Configure  AWS  account  alternate  contacts  in  the  AWS  Organizations  console  or programmatically. Explanation: The root user email address is critical for AWS account recovery and notifications, but it ��s not scalable or secure to rely on a single person to monitor it. Here ��s why Option B works best: 1. Distribution Lists (DLs): By setting the root email as a distribution list (e.g., `aws-admins@company.com`), notifications go to multiple administrators. This ensures no single person is a bottleneck, and alerts aren ��t missed if one person is unavailable. 2. Alternate Contacts: AWS allows you to set alternate contacts (e.g., billing, operations, security) for notifications. Configuring these via AWS Organizations ensures that all accounts (existing and new) automatically use these settings. Alternate contacts can be managed centrally, reducing manual setup. 3. Why Other Options Fail: - Option A/D: Forwarding emails to everyone (A) or using the same root email for all accounts (D) violates AWS rules (root emails must be unique) and security best practices. - Option C: Relying on one administrator creates a              single               point              of               failure.              Reference               Link:               [AWS              Alternate               Contacts Documentation](https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-email-contact.html)

121.A company runs its ecommerce application on AWS. Every new order is published as a massage in a Rabbit mq queue that runs on an Amazon EC2 instance in a single Availability Zone. These messages are processed by a different application that runs on a separate EC2 instance. This application stores the details in a Postgresql database on another EC2 instance. All the EC2 instances are in the same Availability Zone. The company needs to redesign its architecture to provide the highest availability with the least operational overhead. What should a solutions architect do to meet these requirements?

A �� Migrate the queue to a  redundant  pair (active/standby) of  Rabbit mq instances on Amazon  MQ. Create a  Multi-AZ Auto Scaling group for EC2 instances that host the application. Create another Multi-AZ Auto Scaling group for EC2 instances that host the Postgresql database.

B��Migrate the queue to a redundant pair (active/standby) of Rabbit mq instances on Amazon MQ. Create a Multi-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run on a Multi-AZ deployment of Amazon RDS for Postgresql.

C ��Create a Multi-AZ Auto Scaling group for EC2 instances that host the Rabbit mq queue. Create another Multi-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run on a Multi-AZ deployment of Amazon RDS for Postgresql.

D ��Create a Multi-AZ Auto Scaling group for EC2 instances that host the Rabbit mq queue. Create another Multi-AZ Auto Scaling group for  EC2 instances that host the application. Create a third Multi-AZ Auto Scaling group for  EC2 instances that host the Postgresql database

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution is to use managed AWS services that handle high availability automatically. 1. Amazon  MQ (managed  Rabbit mq)  provides active/standby brokers across AZs, eliminating manual setup. 2. Multi-AZ Auto Scaling for the application ensures  it can scale and survive AZ failures. 3. Amazon  RDS  Multi-AZ automatically maintains a standby database replica in another AZ with automatic failover. This approach minimizes operational effort because AWS manages the Rabbit mq (Amazon MQ) and database (RDS) redundancy, while Auto Scaling handles the application layer. Other options  require  manual  database  replication  or  less  reliable  EC2-based  solutions.  Reference  Links:  -  Amazon  MQ  High Availability:           https://aws.amazon.com/amazon-mq/features/#High_availability           -           Amazon           RDS           Multi-AZ: https://aws.amazon.com/rds/features/multi-az/

122.A reporting team receives files each day in an Amazon S3 bucket. The reporting team manually reviews and copies the files from this initial S3 bucket to an analysis S3 bucket each day at the same time to use with Amazon Quick sight. Additional teams are starting to send more files in larger sizes to the initial S3 bucket. The reporting team wants to move the files automatically analysis S3 bucket as the files enter the initial S3 bucket. The reporting team also wants to use AWS Lambda functions to run pattern-matching code on the copied data. In addition, the reporting team wants to send the data files to a pipeline in Amazon Sage maker Pipelines. What should a solutions architect do to meet these requirements with the LEAST operational overhead?

A��Create a Lambda function to copy the files to the analysis S3 bucket. Create an S3 event notification for the analysis S3 bucket. Configure  Lambda  and Sage maker  Pipelines  as  destinations  of t he  event  notification.  Configure s3:Object created:Put  as the event type.

B �� Create  a  Lambda  function  to  copy  the  files  to  the  analysis  S3  bucket.  Configure  the  analysis  S3  bucket  to  send  event notifications to Amazon Event bridge (Amazon Cloud watch Events). Configure an Object created rule in Event bridge (Cloud watch Events). Configure Lambda and Sage maker Pipelines as targets for the rule.

C��Configure S3 replication between the S3 buckets. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and Sage maker Pipelines as destinations of t he event notification. Configure s3:Object created:Put as the event type.

D ��Configure S3 replication between the S3 buckets. Configure the analysis S3 bucket to send event notifications to Amazon Event bridge  (Amazon  Cloud watch  Events).  Configure  an  Object created  rule  in  Event bridge  (Cloud watch  Events).  Configure Lambda and Sage maker Pipelines as targets for the rule.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The best solution is to use S3 Replication for automatic copying between buckets (reducing  Lambda  operational  overhead)  and  Amazon  Event bridge  to   handle  multiple  downstream  targets   (Lambda    & Sage maker). Here's why: 1. S3 Replication: Automatically copies new files from the initial bucket to the analysis bucket without needing custom Lambda code. This is AWS-managed, more reliable, and handles large files better than a DIY Lambda solution. 2. Event bridge Routing: S3 event notifications can only send to one destination directly. By sending events to Event bridge first, you can  create  a  single   rule  that  triggers  both   Lambda  (for  pattern  matching)  and  Sage maker  Pipelines   (for  data  processing) simultaneously. Other options fail because: - A/C: Try to send S3 events directly to multiple destinations (impossible without Event bridge) - B: Uses error-prone Lambda copying instead of S3 Replication - C: Incorrectly assumes S3 notifications can target multiple                services                directly                 Reference                Links:                 -                AWS                S3                 Replication: https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html           -            Amazon           Event bridge            with           S3:
https://docs.aws.amazon.com/event bridge/latest/user guide/eb-s3.html

123.A company runs a web-based portal that provides users with global breaking news, local alerts, and weather updates. The portal delivers each  user a  personalized view  by  using  mixture  of static and dynamic content. Content  is served over  HTTPS through an API server running on an Amazon EC2 instance behind an Application Load Balancer (ALB). The company wants the portal to provide this content to its users across the world as quickly as possible. How should a solutions architect design the application to ensure the LEAST amount of latency for all users?

A �� Deploy the application stack  in a single AWS  Region.  Use Amazon Cloud front to serve all static and dynamic content  by specifying the ALB as an origin.

B��Deploy the application stack in two AWS Regions. Use an Amazon Route 53 latency routing policy to serve all content from the ALB in the closest Region.

C ��Deploy the application stack in a single AWS Region. Use Amazon Cloud front to serve the static content. Serve the dynamic content directly from the ALB.

D �� Deploy the application stack in two AWS Regions. Use an Amazon Route 53 geolocation routing policy to serve all content from the ALB in the closest Region.

�𰸣�A

������ Correct Answer A Explanation The best solution is A because Amazon Cloud front is a Content Delivery Network (CDN) that uses AWS ��s global network of edge locations to cache static content (like images, CSS, etc.) and accelerate dynamic content (like personalized API responses) by routing user requests to the nearest edge location. Even though dynamic content isn ��t cached, Cloud front optimizes the connection between the edge location and the origin (the ALB in this case) using AWS ��s high - speed backbone network, reducing latency for users worldwide. - Why not B or D? Deploying in multiple regions with Route 53 might reduce latency compared to a single region, but ALBs alone don ��t leverage edge locations. Users far from the two regions would still experience higher latency than with Cloud front ��s global edge network. - Why not C? Serving dynamic content directly from the ALB skips Cloud front ��s dynamic acceleration benefits. Users in distant regions would face higher latency when fetching dynamic content. Reference Link [AWS Cloud front Overview](https://aws.amazon.com/cloud front/)

124.A gaming company is designing a highly available architecture. The application runs on a modified Linux kernel and supports only UDP-based traffic. The company needs the front-end tier to provide the best possible user experience. That tier must have low latency, route traffic to the nearest edge location, and provide static IP addresses for entry into the application endpoints. What should a solutions architect do to meet these requirements?

A ��Configure Amazon Route 53 to forward requests to an Application Load Balancer. Use AWS Lambda for the application in AWS Application Auto Scaling.

B ��Configure Amazon Cloud front to forward requests to a Network Load Balancer. Use AWS Lambda for the application in an AWS Application Auto Scaling group.

C �� Configure  AWS  Global  Accelerator to forward  requests  to  a  Network  Load  Balancer.  Use  Amazon  EC2  instances  for  the application in an EC2 Auto Scaling group.

D �� Configure Amazon API Gateway to forward  requests to an Application  Load  Balancer.  Use Amazon  EC2  instances for the application in an EC2 Auto Scaling group.

�𰸣�C

������ Correct Answer C �� Configure AWS Global Accelerator to forward requests to a Network Load Balancer. Use Amazon EC2 instances for the application in an EC2 Auto Scaling group. Detailed Explanation The gaming company ��s requirements include UDP support, low latency, traffic routing to the nearest edge location, and static IP addresses. Here ��s why option C meets all these needs: 1. UDP Support: - The Network Load Balancer (NLB) is a Layer 4 load balancer that supports UDP traffic, unlike the Application Load Balancer (ALB) or API Gateway, which are designed for HTTP(S). 2. Low Latency   �� Nearest  Edge  Routing: - AWS Global Accelerator uses AWS ��s global network infrastructure to route traffic to the nearest edge location, reducing latency. It optimizes the  path for  real - time applications  like gaming. 3. Static  IP Addresses: - Global Accelerator  provides two static anycast IP addresses that act as fixed entry points, ensuring users always connect to the optimal endpoint without relying on DNS changes. 4. Custom  Linux  Kernel: - The application runs on a  modified  Linux  kernel, which  requires full OS  control.  EC2 instances  (managed  via  an  EC2  Auto  Scaling  group)  are  ideal  here,  unlike  serverless  options  like  Lambda,  which  restrict  OS customization. Why Other Options Fail: - A/B: Use ALB or  Lambda, which don ��t support  UDP. -  B/D:  Use Cloud front or API Gateway, which are designed for HTTP(S), not UDP. - A/B/D: Lambda or ALB/API Gateway can��t handle custom kernels or UDP traffic.    Reference     Links    -     [AWS     Global    Accelerator](https://aws.amazon.com/global-accelerator/)     -     [Network    Load Balancer](https://aws.amazon.com/elastic load balancing/network-load-balancer/)

125.A company wants to migrate its existing on-premises monolithic application to AWS. The company wants to keep as much of the  front-end  code  and  the  backend  code  as  possible.  However,  the  company  wants  to  break  the  application  into  smaller applications.  A  different  team  will  manage  each  application.  The  company  needs  a  highly  scalable  solution  that  minimizes operational overhead. Which solution will meet these requirements?

A ��Host the application on AWS Lambda. Integrate the application with Amazon API Gateway.

B ��Host the application with AWS Amplify. Connect the application to an Amazon API Gateway API that is integrated with AWS Lambda.

C �� Host the application on Amazon EC2 instances. Set up an Application Load Balancer with EC2 instances in an Auto Scaling group as targets.
D��Host the application on Amazon Elastic Container Service (Amazon ECS). Set up an Application Load Balancer with Amazon ECS as the target.

�𰸣�D

���� �� Correct Answer:  D  Detailed  Explanation: The  best solution is to use Amazon  Elastic Container Service (ECS) with an
Application Load Balancer (ALB). Here's why this works for the company's needs: 1. Break Monolith into Smaller Applications:

ECS  allows  the  company  to  package  each  part  of  the  monolithic  application  into  separate  Docker  containers.  Each  smaller application (micro service) can be managed independently by different teams, aligning with the requirement for team autonomy. 2.  Minimal Code Changes: Since the company wants to keep most of the existing code, containerization (using  ECS)  requires fewer code modifications compared to rewriting the app for serverless (Lambda) or other architectures. 3. High Scalability   & Low Operational Overhead: ECS automatically handles scaling, deployment, and cluster management. Combined with ALB (which routes traffic to the right containers) and AWS Fargate (serverless compute for containers), the operational burden is significantly reduced.  4.  Why  Other  Options  Are  Less  Ideal:  -  A/B  (Lambda  +  API  Gateway):  Requires  rewriting  the  app  into  serverless functions,  which  may  not  suit  monolithic  code.  -  C  (EC2  +  ALB):  EC2  involves  manual  scaling,  patching,  and  infrastructure management (higher operational overhead). - D (ECS + ALB): Balances scalability, team independence, and minimal code changes effectively. Reference Link: [Amazon ECS Overview](https://aws.amazon.com/ecs/)

126.A company recently started using Amazon Aurora as the data store for its global ecommerce application. When large reports are run, developers report that the ecommerce application is performing poorly. After reviewing metrics in Amazon Cloud watch, a solutions architect finds that the Read iops and Cpu utiliza lion metrics are spiking when monthly reports run. What is the MOST cost-effective solution?

A ��Migrate the monthly reporting to Amazon Redshift.

B ��Migrate the monthly reporting to an Aurora Replica.

C ��Migrate the Aurora database to a larger instance class.

D ��Increase the Provisioned IOPS on the Aurora instance.

�𰸣�B

������ Correct Answer: B Detailed Explanation: When the monthly reports run, they are causing high Read iops and CPU usage on  the   primary  Aurora   instance,  impacting  the   ecommerce  app's  performance.  Aurora   Replicas  are  designed  to   offload read-heavy workloads like reporting from the primary instance. By migrating reporting to an Aurora Replica, the read traffic is redirected to the replica,  reducing load on the  primary  instance. This  is cost-effective  because Aurora  Replicas use the same storage as the primary  (no additional storage cost) and only require compute resources for the  replica instance. Options  like upgrading the instance (C) or increasing IOPS (D) add direct costs, while moving to Redshift (A) introduces complexity and extra services. Using a replica leverages Aurora's built-in scaling for read operations at minimal cost. Reference Links: - [Amazon Aurora Replicas](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/Aurora.Replication.html)       -         [Aurora        Read Scaling](https://aws.amazon.com/rds/aurora/read-scaling/)

127.A company hosts a website analytics application on a single Amazon EC2 On-Demand  Instance. The analytics software is written in PHP and uses a MySQL database. The analytics software, the web server that provides PHP, and the database server are  all  hosted  on  the  EC2  instance.  The  application  is  showing  signs  of  performance  degradation  during  busy  times  and  is presenting  5xx  errors.  The  company   needs  to   make  the  application  scale  seamlessly.  Which  solution  will   meet  these requirements MOST cost-effectively?

A ��Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use an Application Load Balancer to distribute the load to each EC2 instance.

B ��Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use Amazon Route 53 weighted routing to distribute the load across the two EC2 instances.

C ��Migrate the database to an Amazon Aurora MySQL DB instance. Create an AWS Lambda function to stop the EC2 instance and

change the instance type. Create an Amazon Cloud watch alarm to invoke the Lambda function when CPU utilization surpasses 75%.

D ��Migrate the database to an Amazon Aurora MySQL DB instance. Create an AMI of the web application. Apply the AMI to a launch template. Create an Auto Scaling group with the  launch template Configure the  launch template to use a Spot  Fleet. Attach an Application Load Balancer to the Auto Scaling group.

�𰸣�D

���� �� Correct Answer D Detailed Explanation The problem is a single EC2 instance hosting everything (web server, PHP app, MySQL DB) leading to performance issues. The solution must scale seamlessly and cost-effectively. Here ��s why D is best:  1. Aurora MySQL: Migrating to Aurora improves database scalability and cost efficiency compared to standard RDS MySQL (used in A/B). Aurora scales storage automatically and handles high concurrency better. 2. Auto Scaling with Spot Fleet: - Auto Scaling automatically adds/removes EC2 instances based on traffic, ensuring seamless scaling (unlike A/B, which manually add a second instance or use slow DNS routing). - Spot Fleet uses cheaper Spot Instances (up to 90% cost savings vs On - Demand in A/B). While Spot Instances can be interrupted, stateless web servers in an Auto Scaling group can tolerate this. 3. Application Load Balancer (ALB): Distributes traffic instantly and evenly across instances. Route 53 (in B) uses DNS - based routing, which is slower and less dynamic. 4. Vertical scaling (C) is bad: Changing EC2 instance types requires downtime and has hard limits. Lambda - based scaling in C  is  not seamless or cost - effective. Why  not A/B/C? - A/B  use expensive  On -  Demand  instances and  lack automated scaling. - B uses Route 53, which is unsuitable for real - time load distribution. - C relies on vertical scaling and manual intervention,    causing    downtime.     Reference    Links     -    [Amazon     Aurora](https://aws.amazon.com/rds/aurora/)    -     [Spot Fleet](https://aws.amazon.com/ec2/spot/) - [Auto Scaling](https://aws.amazon.com/auto scaling/)

128.A company  runs a stateless  web  application  in  production  on  a group  of Amazon  EC2  On-Demand  Instances  behind  an Application  Load  Balancer.  The  application  experiences  heavy  usage  during  an  8-hour  period  each  business  day.  Application usage is moderate and steady overnight. Application usage is low during weekends. The company wants to minimize its EC2 costs without affecting the availability of the application. Which solution will meet these requirements?

A ��Use Spot Instances for the entire workload.

B �� Use Reserved Instances for the baseline level of usage. Use Spot instances for any additional capacity that the application needs.

C ��Use On-Demand Instances for the baseline level of usage. Use Spot Instances for any additional capacity that the application needs.

D �� Use  Dedicated  Instances  for  the  baseline  level  of  usage.  Use  On-Demand  Instances  for  any  additional capacity that the application needs.

�𰸣�B

������Correct Answer: B Detailed Explanation: The company's workload has a predictable baseline (moderate overnight and low weekends) and variable peaks (8-hour business days). Here's why option B is optimal: 1. Reserved Instances (RIs) are perfect for baseline usage. RIs offer ~70% discount compared to On-Demand for steady, predictable workloads. By reserving capacity for the baseline (overnight/weekends), the company locks in lower costs. 2. Spot Instances handle variable peaks. Spot Instances cost ~90%  less than  On-Demand  but  can  be  interrupted.  Since the application  is stateless  and  uses  an ALB  (which  automatically reroutes traffic if instances are reclaimed), Spot is safe for scaling during daily 8-hour peaks.  Even if some Spot Instances get interrupted,  the   ALB  ensures  availability.  Why   not  other  options?  -  A:   Using   only  Spot  risks  availability  during  sudden price/capacity changes, especially for critical daily peaks. - C: On-Demand for baseline is more expensive than RIs for predictable

usage. - D: Dedicated Instances are for hardware isolation (security/compliance), not cost savings. Reference Links: - [AWS EC2
Pricing                                       Models](https://aws.amazon.com/ec2/pricing/)                                      -                                        [Reserved
Instances](https://aws.amazon.com/ec2/pricing/reserved-instances/) - [Spot Instances](https://aws.amazon.com/ec2/spot/)

129.A company needs to retain application log files for a critical application for 10 years. The application team regularly accesses logs from the past month for troubleshooting, but logs older than 1 month are rarely accessed. The application generates more than 10 TB of logs per month. Which storage option meets these requirements MOST cost-effectively?

A ��Store the logs in Amazon S3. Use AWS Backup to move logs more than 1 month old to S3 Glacier Deep Archive.

B ��Store the logs in Amazon S3. Use S3 Lifecycle policies to move logs more than 1 month old to S3 Glacier Deep Archive.

C ��Store the logs in Amazon Cloud watch Logs. Use AWS Backup to move logs more than 1 month old to S3 Glacier Deep Archive.

D ��Store the logs in Amazon Cloud watch Logs. Use Amazon S3 Lifecycle policies to move logs more than 1 month old to S3 Glacier Deep Archive.

�𰸣�B

���� �� Correct Answer B  Detailed Explanation The  best option is B because it uses Amazon S3 for storage and S3  Lifecycle policies to automatically transition logs older than  1  month to S3 Glacier  Deep Archive, the cheapest storage class for rarely accessed, long - term retention (e.g., 10 years). Here ��s why: 1. Amazon S3 is ideal for storing large volumes (10 TB/ month) and integrates natively with lifecycle policies, which automate cost - saving transitions to Glacier Deep Archive. 2. S3 Lifecycle Policies are simpler and cheaper than AWS Backup (options A/C) because they handle transitions directly within S3 without requiring a separate  backup  service.  3.  Cloud watch   Logs  (options  C/D)   is  designed  for  real  -  time   log   monitoring,  not  bulk  storage. Transferring 10 TB/month to/from Cloud watch would be complex and costly. 4. Glacier  Deep Archive  has the  lowest storage costs for data accessed less than once a year, perfect for logs older than 1 month. Why Not Other Options? - A/C: AWS Backup adds unnecessary complexity and cost for routine lifecycle management. - D: Cloud watch doesn ��t support S3 lifecycle policies directly; you��d need manual exports to S3 first. Reference Link [Amazon S3 Storage Classes](https://aws.amazon.com/s3/storage - classes/)

130.A company has a data ingestion workflow that includes the following components: An Amazon Simple Notification Service (Amazon SNS) topic that receives notifications about new data deliveries An AWS Lambda function that processes and stores the data The ingestion workflow occasionally fails because of network connectivity issues. When failure occurs, the corresponding data  is  not  ingested  unless  the  company  manually  reruns  the  job.  What  should  a  solutions  architect  do  to  ensure  that  all notifications are eventually processed?

A ��Configure the Lambda function for deployment across multiple Availability Zones.

B ��Modify the Lambda function's configuration to increase the CPU and memory allocations for the function.

C ��Configure the SNS topic ��s retry strategy to increase both the number of retries and the wait time between retries.

D ��Configure an Amazon Simple Queue Service (Amazon SQS) queue as the on-failure destination. Modify the Lambda function to process messages in the queue.

�𰸣�D

�� �� �� Correct Answer:  D. Configure an Amazon Simple Queue Service  (Amazon SQS) queue as the on-failure destination.

Modify the Lambda function to process  messages in the queue.  Explanation: The current issue is that network failures cause temporary errors, and the workflow lacks a way to automatically retry failed messages. Here ��s why Option D solves this: 1. Why SQS? - Amazon SQS acts as a durable "buffer" for messages. If the Lambda function fails to process a message (due to network issues), the message stays in the SQS queue. - SQS automatically retries messages until they ��re successfully processed. You can also set a dead-letter queue (DLQ) for messages that fail repeatedly. 2. How It Works: - Instead of SNS triggering Lambda directly, SNS sends messages to an SQS queue. - Lambda pulls messages from the SQS queue. If processing fails, the message goes back to the  queue  (or  a  DLQ)  for  retries.  -  This  ensures  messages  are  never  lost  and  are  retried  automatically  without  manual intervention. Why Other Options Are Wrong: - A (Multi-AZ):  Lambda is already highly available; this doesn ��t fix  retries. -  B (CPU/Memory):  Network issues aren ��t  related to compute  resources. - C  (SNS  Retries): SNS retries are limited and don ��t guarantee persistence. Reference Link: [AWS Lambda with SQS](https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html)

131.A  company  has  a  service that  produces  event  data. The  company wants to  use  AWS to  process  the  event  data  as  it  is received.  The  data  is  written  in  a  specific  order  that  must  be  maintained  throughout  processing.  The  company  wants  to implement a solution that minimizes operational overhead. How should a solutions architect accomplish this?

A ��Create an Amazon Simple Queue Service (Amazon SQS) FI FO queue to hold messages. Set up an AWS Lambda function to process messages from the queue.

B �� Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing payloads to process. Configure an AWS Lambda function as a subscriber.

C ��Create an Amazon Simple Queue Service (Amazon SQS) standard queue to hold messages. Set up an AWS Lambda function to process messages from the queue independently.

D ��Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing payloads to process. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a subscriber.

�𰸣�A

�� �� �� Correct Answer: A  Detailed  Explanation: To  process  event data  in the  exact  order  it ��s  received  while  minimizing operational overhead, the best solution is Amazon SQS FI FO queue + AWS Lambda. Here ��s why: 1. Order Guarantee: - SQS F IFO queues ensure strict first-in-first-out (FI FO) order. Messages are processed exactly once and in the order they ��re sent. This meets the requirement to maintain data order. - Standard SQS (Option C) or SNS (Options  B/D) do not guarantee order. 2.  Minimal Operational Overhead: - AWS Lambda automatically scales to process  messages from the queue without needing servers.  No infrastructure  management  is  required.  -  SNS  (Options  B/D)  would  require  additional  components  (like  queues  or  code)  to ensure ordering,  increasing complexity. Why other options fail: - Option  B  (SNS +  Lambda): SNS  sends  messages to  multiple subscribers in parallel, disrupting order. - Option C (Standard SQS + Lambda): Standard SQS allows out-of-order delivery. - Option D (SNS + SQS): Even with an SQS FI FO subscriber, SNS doesn ��t guarantee message order during delivery to the queue. Reference Links:                                                 -                                                  [Amazon                                                 SQS                                                  FI FO
Queues](https://docs.aws.amazon.com/Aws simple queue service/latest/Sqs developer guide/FIFO-queues.html)        -         [Using Lambda with SQS](https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html)

132.A company is migrating an application from on-premises servers to Amazon EC2 instances. As part of the migration design requirements, a solutions architect must implement infrastructure metric alarms. The company does not need to take action if CPU utilization increases to more than 50% for a short burst of time. However, if the CPU utilization increases to more than 50% and read IOPS on the disk are high at the same time, the company needs to act as soon as possible. The solutions architect also must reduce false alarms. What should the solutions architect do to meet these requirements?

A ��Create Amazon Cloud watch composite alarms where possible.

B ��Create Amazon Cloud watch dashboards to visualize the metrics and react to issues quickly.

C ��Create Amazon Cloud watch Synthetics canaries to monitor the application and raise an alarm.

D ��Create single Amazon Cloud watch metric alarms with multiple metric thresholds where possible.

�𰸣�A

�� �� �� Correct Answer: A. Create Amazon Cloud watch composite alarms where  possible.  Detailed  Explanation: Composite alarms allow you to combine multiple alarms using logical AND/OR conditions. In this scenario, the requirement is to trigger an alarm only when both high CPU utilization (>50%) and high disk read IOPS occur simultaneously. - If you use single metric alarms (Option D), you'd get separate alerts for CPU and IOPS, leading to false alarms when only one metric spikes. - Composite alarms (Option A) solve this by requiring both conditions to be true at the same time, reducing false positives. - Dashboards (Option B) only help visualize metrics  but don't trigger automated alerts. - Synthetics canaries (Option C) are for monitoring application endpoints/APIs,     not      infrastructure     metrics      like     CPU/IOPS.      Reference     Link:      [Amazon     Cloud watch      Composite Alarms](https://docs.aws.amazon.com/Amazon cloud watch/latest/monitoring/Create_Composite_Alarm.html)

133.A company uses a three-tier web application to provide training to new employees. The application is accessed for only 12 hours every day. The company is using an Amazon RDS for MySQL DB instance to store information and wants to minimize costs. What should a solutions architect do to meet these requirements?

A �� Configure an IAM policy for AWS Systems Manager Session Manager. Create an IAM role for the policy. Update the trust relationship of the role. Set up automatic start and stop for the DB instance.

B ��Create an Amazon Elastic ache for Redis cache cluster that gives users the ability to access the data from the cache when the DB instance is stopped. Invalidate the cache after the DB instance is started.

C ��Launch an Amazon EC2 instance. Create an IAM role that grants access to Amazon RDS. Attach the role to the EC2 instance. Configure a cron job to start and stop the EC2 instance on the desired schedule.

D ��Create AWS Lambda functions to start and stop the DB instance. Create Amazon Event bridge (Amazon Cloud watch Events) scheduled rules to invoke the Lambda functions. Configure the Lambda functions as event targets for the rules.

�𰸣�D

������ Correct Answer D Detailed Explanation To minimize costs for an Amazon RDS instance that only needs to run 12 hours daily, the best approach is to automate starting and stopping the database. Here's why: - Option D uses AWS Lambda (serverless, so no extra cost for idle time) and Amazon Event bridge (scheduled rules) to start/stop t he RDS instance on a schedule. This fully automates the  process without  needing to manage servers or cron jobs. - Option A incorrectly focuses on Systems  Manager Session  Manager  (used for  EC2  instance  management,  not  RDS).  - Option  B  introduces  Elastic ache, which  adds  unnecessary complexity  and  cost  for temporary  data  caching.  -  Option  C  requires  running  an  EC2  instance  24/7  just  to  run  a  cron  job, defeating the goal of cost  optimization.  Key  Points:  -  Lambda  +  Event bridge  is  serverless,  cost-effective,  and  aligns with the requirement to run RDS only during active hours. - RDS charges stop when the instance is stopped (not just idle), so scheduling stops                  saves                   money.                   Reference                  Links                   -                   [Stopping                   an                  RDS
Instance](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_Stop instance.html)   -   [Scheduling    Lambda   with Event bridge](https://docs.aws.amazon.com/event bridge/latest/user guide/eb-run-lambda-schedule.html)

134.A company sells ringtones created from clips of popular songs. The files containing the ringtones are stored in Amazon S3

Standard and are at least 128 KB in size. The company has millions of files, but downloads are infrequent for ringtones older than 90 days. The company needs to save  money on storage while  keeping the  most accessed files  readily available for its  users. Which action should the company take to meet these requirements MOST cost-effectively?

A ��Configure S3 Standard-Infrequent Access (S3 Standard-IA) storage for the initial storage tier of the objects.

B ��Move the files to S3 Intelligent-Tiering and configure it to move objects to a less expensive storage tier after 90 days.

C ��Configure S3 inventory to manage objects and move them to S3 Standard-Infrequent Access (S3 Standard-1A) after 90 days.

D ��Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-1A) after 90 days.

�𰸣�D

���� �� Correct Answer: D Detailed Explanation: The best solution is to use an S3 Lifecycle policy to automatically transition objects from S3 Standard to S3 Standard-Infrequent Access  (S3 Standard-IA) after 90 days.  Here ��s why: - S3 Standard-IA  is cheaper for storage than S3 Standard but has a minimum storage duration charge (30 days) and a retrieval fee. Since the files older  than  90  days  are  rarely  accessed,  the  lower  storage  cost  outweighs  the  occasional  retrieval  cost.  -  Lifecycle  policies automate the transition based on a fixed time frame (90 days), which matches the company ��s known access pattern. This avoids manual processes or unnecessary costs (e.g., S3 Intelligent-Tiering ��s monitoring fees). - Minimum object size: S3 Standard-IA requires objects to be at least 128 KB, which the files already meet. Options A, B, and C are less optimal: - A (using S3 Standard-IA from the start) would cost more for frequently accessed newer files. - B (S3 Intelligent-Tiering) automatically adjusts tiers based on access patterns, but it charges a small monitoring fee. Since the company already knows access drops after 90 days, a lifecycle policy (D) is simpler and cheaper. - C (S3 Inventory + manual processes) is inefficient compared to automated lifecycle policies. Reference Link: [Amazon S3 Storage Classes](https://aws.amazon.com/s3/storage-classes/)

135.A company needs to save the  results from a  medical trial to an Amazon S3  repository. The  repository  must allow a few scientists to add new files and must restrict all other users to read-only access. No users can have the ability to modify or delete any files in the repository. The company must keep every file in the repository for a minimum of 1 year after its creation date. Which solution will meet these requirements?

A ��Use S3 Object Lock in governance mode with a legal hold of 1 year.

B ��Use S3 Object Lock in compliance mode with a retention period of 365 days.

C ��Use an IAM role to restrict all users from deleting or changing objects in the S3 bucket. Use an S3 bucket policy to only allow the IAM role.

D ��Configure the S3 bucket to invoke an AWS Lambda function every time an object is added. Configure the function to track the hash of the saved object so that modified objects can be marked accordingly.

�𰸣�B

������ Correct Answer B Detailed Explanation The correct answer is B because it uses S3 Object Lock in compliance mode with a retention period of 365 days. Here ��s why:  1. S3 Object  Lock in compliance mode ensures that no user (including root AWS accounts) can delete or modify objects during the retention period. This enforces the requirement that "no users can modify or delete files."  2.  Retention  period  of  365  days  guarantees  that  objects  are  stored  for  at  least  1  year,  meeting  the  minimum retention requirement. Other options fail because: - A: Governance mode allows privileged users to override retention settings,

violating the "no modifications" rule. Legal hold is time-agnostic and doesn ��t enforce the 1-year requirement. - C: IAM roles and bucket policies alone cannot fully prevent accidental or intentional deletions if misconfigured. S3 Object Lock is more robust for retention. - D: Lambda-based tracking adds unnecessary complexity and doesn ��t enforce immutable retention at the storage
layer.                                    Reference                                    Link                                     [S3                                    Object                                    Lock
Documentation](https://docs.aws.amazon.com/AmazonS3/latest/user guide/object-lock.html)

136.A large media company hosts a web application on AWS. The company wants to start caching confidential media files so that users around the world will have reliable access to the files. The content is stored in Amazon S3 buckets. The company must deliver  the  content  quickly,  regardless  of  where  the  requests  originate  geographically.  Which  solution  will  meet  these requirements?

A ��Use AWS Data sync to connect the S3 buckets to the web application.

B ��Deploy AWS Global Accelerator to connect the S3 buckets to the web application.

C ��Deploy Amazon Cloud front to connect the S3 buckets to Cloud front edge servers.

D ��Use Amazon Simple Queue Service (Amazon SQS) to connect the S3 buckets to the web application.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The best solution is to use Amazon Cloud front because it's a Content Delivery Network (CDN) designed to cache content at global edge  locations.  Here's why this works for the  media company:  1. Global Speed: Cloud front stores copies of files (like media) at edge servers worldwide, so users download from the nearest location, reducing latency. 2. S3 Integration: Cloud front connects directly to S3 buckets, making it easy to distribute stored media files. 3. Caching: It caches content at edge locations, reducing repeated requests to the origin (S3), which improves reliability and speed.
4. Security: Cloud front supports features like signed URLs/Cookies to protect confidential media files. Other options are incorrect:
- A (Data sync): Used for bulk data transfers, not caching or content delivery. - B (Global Accelerator): Optimizes network paths for dynamic traffic (e.g., APIs) but doesn ��t cache static files  like media. - D (SQS): A message queue service for decoupling app components,          unrelated           to           content           delivery.          Reference           Links:           -           [Amazon           Cloud front Overview](https://aws.amazon.com/cloud front/)                          -                          [Using                           Cloud front                          with S3](https://docs.aws.amazon.com/Amazon cloud front/latest/Developer guide/Download dists3And custom origins.html)

137.A solutions architect is optimizing a website for an upcoming musical event. Videos of t he performances will be streamed in real time and then will be available on demand. The event is expected to attract a global online audience. Which service will improve the performance of both the real-time and on-demand streaming?

A ��Amazon Cloud front

B ��AWS Global Accelerator C ��Amazon Route 53

D ��Amazon S3 Transfer Acceleration

�𰸣�A

������ Correct Answer: A. Amazon Cloud front Detailed Explanation: Amazon Cloud front is a Content Delivery Network (CDN) that caches content at edge  locations worldwide,  reducing  latency for global audiences.  For  real-time  streaming,  Cloud front

supports  live  streaming  via  protocols  like  HLS/DASH,  delivering  content  efficiently  from  the  nearest  edge  location.  For on-demand streaming, it caches videos at edge locations, allowing faster access for repeated views. While other services  like Global Accelerator (optimizes TCP/UDP traffic) or S3 Transfer Acceleration (speeds up uploads to S3) have niche uses, Cloud front uniquely addresses both real-time and on-demand streaming performance for global users by leveraging AWS ��s edge network. Reference Links: - [Amazon Cloud front for Streaming](https://aws.amazon.com/cloud front/streaming/) - [AWS Content Delivery Overview](https://aws.amazon.com/cdn/)

138.An ecommerce company hosts its analytics application in the AWS Cloud. The application generates about 300 MB of data each month. The data is stored in JSON format. The company is evaluating a disaster recovery solution to back up the data. The data  must  be  accessible  in  milliseconds  if it  is  needed,  and the  data  must  be  kept  for 30  days. Which solution  meets these requirements MOST cost-effectively?

A ��Amazon Open search Service (Amazon Elastic search Service)

B ��Amazon S3 Glacier  C ��Amazon S3 Standard

D ��Amazon RDS for Postgresql

�𰸣�C

���� �� Correct Answer: C. Amazon S3 Standard Detailed Explanation: Amazon S3 Standard is the most cost-effective solution here  because:  1.   Millisecond  Access:  S3  Standard  provides  instant  data  retrieval,  meeting  the  "accessible  in  milliseconds" requirement. 2. Cost-Efficiency: At 300 MB/month, storage costs are extremely low (~$$0.023/month*). Glacier is cheaper but has slow retrieval, while Open search/RDS require expensive compute resources. 3. JSON Support: S3 natively stores JSON files without requiring special formatting. 4. Retention Control: Use S3 Lifecycle Policies to automatically delete files after 30 days, enforcing the retention requirement. 5.  Durability: S3 offers 99.999999999%  (11 9's) data durability, ideal for backups. Other options fail  because:  - A  (Open search):  Requires  expensive  cluster  maintenance  for  small  data.  -  B  (Glacier):  Retrieval  takes minutes/hours, violating millisecond access. - D (RDS): Designed for transactional databases, not JSON backups; more expensive.
*Based    on     S3    Standard     storage     cost:    $$0.023/GB-month     (us-east-1)     Reference     Links:    -     S3    Storage     Classes: https://aws.amazon.com/s3/storage-classes/                             -                              S3                              Lifecycle                              Policies:
https://docs.aws.amazon.com/AmazonS3/latest/user guide/object-lifecycle-mgmt.html

139.A company has a small Python application that processes JSON documents and outputs the results to an on-premises SQL database. The application runs thousands of times each day. The company wants to move the application to the AWS Cloud. The company needs a highly available solution that maximizes scalability and minimizes operational overhead. Which solution will meet these requirements?

A ��Place the JSON documents in an Amazon S3 bucket. Run the Python code on multiple Amazon EC2 instances to process the documents. Store the results in an Amazon Aurora DB cluster.

B��Place the JSON documents in an Amazon S3 bucket. Create an AWS Lambda function that runs the Python code to process the documents as they arrive in the S3 bucket. Store the results in an Amazon Aurora DB cluster.

C ��Place the JSON documents in an Amazon Elastic Block Store (Amazon EBS) volume. Use the EBS Multi-Attach feature to attach the volume to multiple Amazon EC2 instances. Run the Python code on the EC2 instances to process the documents. Store the results on an Amazon RDS DB instance.

D ��Place the JSON documents in an Amazon Simple Queue Service (Amazon SQS) queue as messages. Deploy the Python code as a container on an Amazon Elastic Container Service (Amazon ECS) cluster that is configured with the Amazon EC2 launch type. Use the container to process the SQS messages. Store the results on an Amazon RDS DB instance.

�𰸣�B

���� �� Correct Answer B Detailed Explanation The best solution is Option B because it uses fully managed AWS services that minimize operational overhead and scale automatically. Here ��s why: 1. Amazon S3 stores the JSON documents securely and durably. When new files arrive, S3 can automatically trigger an AWS Lambda function. 2. AWS  Lambda runs the Python code without requiring servers. It scales automatically to handle thousands of executions per day (no need to manage EC2 instances). You only pay for the compute time used. 3. Amazon Aurora (a managed database)  replaces the on -  premises SQL database, providing high availability, scalability, and reliability with minimal administration. Other options are less optimal: - Option A/C/ D rely on EC2, EBS, or ECS with EC2, which require manual scaling, server management, and higher operational effort. - Option C uses EBS Multi - Attach, which is not ideal for distributed processing and adds complexity. - Option D uses SQS and ECS, which are less efficient than S3+Lambda for event - driven workflows. Reference Links - [AWS Lambda](https://aws.amazon.com/lambda/) - [Amazon S3](https://aws.amazon.com/s3/) - [Amazon Aurora](https://aws.amazon.com/rds/aurora/)

140.A company wants to use high performance computing (HPC) infrastructure on AWS for financial risk modeling. The company �� s HPC workloads run on Linux. Each HPC workflow runs on hundreds of Amazon EC2 Spot Instances, is short-lived, and generates    thousands of output files that are ultimately stored in persistent storage for analytics and long-term future use. The company    seeks  a  cloud  storage  solution  that  permits  the  copying  of  on-premises  data  to  long-term  persistent  storage  to  make  data    available for processing by all EC2 instances. The solution should also be a high performance file system that is integrated with    persistent storage to read and write datasets and output files. Which combination of AWS services meets these requirements?

A ��Amazon FSx for Lustre integrated with Amazon S3

B ��Amazon FSx for Windows File Server integrated with Amazon S3

C ��Amazon S3 Glacier integrated with Amazon Elastic Block Store (Amazon EBS)

D ��Amazon S3 bucket with a VPC endpoint integrated with an Amazon Elastic Block Store (Amazon EBS) General Purpose SSD (gp2) volume

�𰸣�A

���� �� Correct Answer: A. Amazon FSx for Lustre integrated with Amazon S3 Detailed Explanation: Amazon FSx for Lustre is specifically designed for high-performance computing (HPC) workloads like financial risk modeling. Here's why this combination works  best:  1.  High-Performance  File  System:  FSx  for  Lustre  provides  the  speed  needed  for  processing  large  datasets  across hundreds  of  EC2  instances  simultaneously,  which   is  critical  for  short-lived   HPC  workflows.  2.  Seamless  S3   Integration:   It automatically synchronizes with Amazon S3 (persistent storage). You can copy on-premises data to S3 first, then FSx for Lustre can  instantly  "stage" this data  into a super-fast file system for computation. After  processing,  results  get written  back to S3 automatically. 3.  Linux  Compatibility:  FSx  for  Lustre  works  natively  with  Linux-based  EC2  instances,  matching  the  company's infrastructure. 4. Spot Instance Friendly: The temporary nature of Spot Instances aligns perfectly with FSx for Lustre's ability to quickly handle data processing bursts. Other options fail because: - B uses Windows-based file servers (not Linux-optimized) - C uses  Glacier  (too  slow  for  processing)  and  EBS  (not  shareable)  -  D  uses  basic  S3/EBS  without  high-performance  file  system capabilities                   Reference                   Links:                   -                    [FSx                   for                   Lustre                   with                   S3
integration](https://docs.aws.amazon.com/fsx/latest/Lustre guide/fsx-aws-s3.html)          -           [HPC           on          AWS           best practices](https://aws.amazon.com/hpc/)

141.A company is building a containerized application on premises and decides to move the application to AWS. The application will have thousands of users soon after it is deployed. The company is unsure how to manage the deployment of containers at scale. The company needs to deploy the containerized application in a highly available architecture that minimizes operational overhead. Which solution will meet these requirements?

A ��Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the AWS Fargate launch type to run the containers. Use target tracking to scale automatically based on demand.

B ��Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the Amazon EC2 launch type to run the containers. Use target tracking to scale automatically based on demand.

C ��Store container images in a repository that runs on an Amazon EC2 instance. Run the containers on EC2 instances that are spread across multiple Availability Zones. Monitor the average CPU utilization in Amazon Cloud watch. Launch new EC2 instances as needed.

D ��Create an Amazon EC2 Amazon Machine Image (AMI) that contains the container image. Launch EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon Cloud watch alarm to scale out EC2 instances when the average CPU utilization threshold is breached.

�𰸣�A

�� �� �� Correct Answer: A  Detailed  Explanation: The  best  solution  is  A  because  it  uses  AWS  Fargate,  which  is  a  serverless compute engine for containers. With Fargate, the company doesn ��t need to manage the underlying EC2 instances (like patching, scaling, or securing servers), minimizing operational overhead. Amazon ECS (Elastic Container Service) with Fargate automatically deploys containers across  multiple Availability Zones  (AZs) for  high  availability. Target tracking scaling  adjusts the  number of containers based on demand, ensuring the application handles thousands of users smoothly. Option B uses the EC2 launch type, which requires manual management of EC2 instances (e.g., scaling the cluster), increasing operational work. Options C and D rely on  manually   managing   EC2  instances  and  scaling,  which  is  error-prone  and  adds  complexity.  Reference   Links:  -   [AWS Fargate](https://aws.amazon.com/fargate/)   -   [Amazon   ECS   with   Fargate](https://aws.amazon.com/ecs/)   -   [Target   Tracking Scaling](https://docs.aws.amazon.com/auto scaling/ec2/user guide/as-scaling-target-tracking.html)

142.A company has two applications: a sender application that sends messages with payloads to be processed and a processing application  intended  to  receive  the  messages  with  payloads.  The  company  wants  to  implement  an  AWS  service  to  handle messages between the two applications. The sender application can send about 1,000 messages each hour. The messages may take  up to  2  days  to  be  processed:  If  the  messages  fail  to  process,  they  must  be  retained  so  that  they  do  not  impact  the processing of any remaining messages. Which solution meets these requirements and is the MOST operationally efficient?

A ��Set up an Amazon EC2 instance running a Redis database. Configure both applications to use the instance. Store, process, and delete the messages, respectively.

B ��Use an Amazon Kinesis data stream to receive the messages from the sender application. Integrate the processing application with the Kinesis Client Library (KCL).

C �� Integrate the sender and processor applications with an Amazon Simple Queue Service (Amazon SQS) queue. Configure a dead-letter queue to collect the messages that failed to process.

D ��Subscribe the processing application to an Amazon Simple Notification Service (Amazon SNS) topic to receive notifications to

process. Integrate the sender application to write to the SNS topic.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The best solution is Amazon SQS with a Dead-Letter Queue (DLQ). Here's why: 1. Message Retention: SQS keeps messages for up to 14 days by default, easily meeting the 2-day processing requirement. 2. Error Handling: If the processing app fails to handle a message, SQS automatically retries it. After a defined number of failed attempts, messages move to a DLQ. This ensures failed messages don't block others (operationally efficient). 3. Scalability: SQS handles 1,000  messages/hour  effortlessly  without  server  management.  4.   Decoupling:  SQS  fully   decouples  sender/receiver   apps, allowing independent scaling. Other options are less suitable: - A (Redis on EC2): Requires manual server/database management (not operationally efficient). - B (Kinesis): Designed for real-time streaming, not long retention (default 24h). Overkill for 1k/hour.
- D (SNS): Messages aren't persisted if the processor is unavailable. Requires extra SQS queues to work reliably. Reference link: - [Amazon                                                                                               SQS                                                                                                 Dead-Letter Queues](https://docs.aws.amazon.com/Aws simple queue service/latest/Sqs developer guide/sqs-dead-letter-queues.html)         - [SQS vs SNS vs Kinesis](https://aws.amazon.com/messaging/sqs-vs-sns-vs-kinesis/)

143.A solutions architect must design a solution that uses Amazon Cloud front with an Amazon S3 origin to store a static website. The company ��s security policy requires that all website traffic be inspected by AWS WAF. How should the solutions architect comply with these requirements?

A ��Configure an S3 bucket policy to accept requests coming from the AWS WAF Amazon Resource Name (ARN) only.

B ��Configure Amazon Cloud front to forward all incoming requests to AWS WAF before requesting content from the S3 origin.

C ��Configure a security group that allows Amazon Cloud front IP addresses to access Amazon S3 only. Associate AWS WAF to Cloud front.

D ��Configure Amazon Cloud front and Amazon S3 to use an origin access identity (OAI) to restrict access to the S3 bucket. Enable AWS WAF on the distribution.

�𰸣�D

������ Correct Answer D Detailed Explanation To meet the security requirements, the solution must ensure: 1. All traffic passes through AWS WAF: AWS WAF is directly integrated with Cloud front. By enabling AWS WAF on the Cloud front distribution, all incoming requests are automatically inspected by WAF before reaching the S3 origin. 2. Restrict direct access to the S3 bucket: Using an Origin Access Identity (OAI) ensures that the S3 bucket can only be accessed via Cloud front. This prevents users from bypassing Cloud front (and WAF) by accessing the S3 bucket directly. - Why not A/B/C? - A: S3 bucket policies cannot reference AWS WAF ARNs directly. Instead, access should be restricted via OAI. - B: Cloud front does not "forward requests to WAF"��WAF is natively integrated with Cloud front distributions. - C: Security groups do not apply to S3 (they are for EC2/network interfaces). S3        access        control         uses         bucket        policies         or        OAI.         Reference        Links         -         [AWS        WAF        with Cloud front](https://docs.aws.amazon.com/waf/latest/developer guide/cloud front-features.html)       -         [Using        OAI       with S3](https://docs.aws.amazon.com/Amazon cloud front/latest/Developer guide/private-content-restricting-access-to-s3.html)

144.Organizers for a global event want to put daily reports online as static HTML  pages. The pages are expected to generate millions of views from users around the world. The files are stored in an Amazon S3 bucket. A solutions architect has been asked to design an efficient and effective solution. Which action should the solutions architect take to accomplish this?

A ��Generate pre signed URLs for the files.

B ��Use cross-Region replication to all Regions.

C ��Use the geo proximity feature of Amazon Route 53.

D ��Use Amazon Cloud front with the S3 bucket as its origin.

�𰸣�D

������ Correct Answer: D. Use Amazon Cloud front with the S3 bucket as its origin. Detailed Explanation: Amazon Cloud front is a Content Delivery Network (CDN) that caches static content (like HTML pages) at edge locations globally. When users request the files, Cloud front serves them from the nearest edge location, reducing latency and improving speed for millions of global users. This is far more efficient than serving files directly from S3, which lacks built - in global caching. Why not other options? - A. Pre signed  URLs:  These  are  for  temporary  access  to  private  S3  files,  not  for  public  static  websites.  They  add  unnecessary complexity for public content. - B. Cross - Region Replication: Copying S3 files to all regions is costly and doesn ��t solve latency issues on its own. Cloud front ��s edge network is faster and cheaper. - C. Route 53 Geo proximity: This DNS feature routes users to the  nearest  AWS   region,   not  edge  locations.   It  �� s  not  a  substitute  for  CDN  caching.   Reference:  [Amazon  Cloud front Overview](https://aws.amazon.com/cloud front/)                    [AWS                    S3                   Static                    Website                    with
Cloud front](https://docs.aws.amazon.com/Amazon cloud front/latest/Developer guide/Getting started.Simple distribution.html)

145.A  company  runs  a  production  application  on  a  fleet  of  Amazon  EC2  instances.  The  application  reads  the  data  from  an Amazon SQS queue and processes the messages in parallel. The  message volume is unpredictable and often has intermittent traffic. This application should continually process messages without any downtime. Which solution meets these requirements MOST cost-effectively?

A ��Use Spot Instances exclusively to handle the maximum capacity required.

B ��Use Reserved Instances exclusively to handle the maximum capacity required.

C ��Use Reserved Instances for the baseline capacity and use Spot Instances to handle additional capacity.

D ��Use Reserved Instances for the baseline capacity and use On-Demand Instances to handle additional capacity.

�𰸣�D

���� �� Correct Answer:  D  Detailed  Explanation: The  best approach here is to use  Reserved  Instances (RIs) for the baseline (minimum) traffic and On-Demand Instances to handle unpredictable spikes. Here's why: 1. Reserved Instances are cheaper for steady, predictable workloads (baseline capacity). They ��re prepaid and lock in lower costs for long-term use. 2. On-Demand Instances are more expensive but always available. They ��re ideal for handling sudden, unpredictable traffic spikes without risking interruptions. Why not Spot Instances (Options A/C)? Spot Instances are the cheapest but can be terminated abruptly if AWS needs  capacity  back.  For  a  production  app  requiring  24/7  uptime,  Spot  Instances  are  risky �� even  with  fault-tolerant architectures. A single  interruption could cause delays or downtime. Option  B (only  RIs)  is  bad  because you ��d  overpay for unused  capacity  during  low-traffic  periods.  Option  D  balances  cost  and  reliability:  RIs  save  money  on  the  baseline,  while On-Demand        ensures        spikes        are        handled         without        interruptions.        Reference:         [AWS        EC2        Pricing Models](https://aws.amazon.com/ec2/pricing/)

146.A security team wants to limit access to specific services or actions in all of the team ��s AWS accounts. All accounts belong to a large organization in AWS Organizations. The solution must be scalable and there must be a single point where permissions can be maintained. What should a solutions architect do to accomplish this?

A ��Create an ACL to provide access to the services or actions.

B ��Create a security group to allow accounts and attach it to user groups.

C ��Create cross-account roles in each account to deny access to the services or actions.

D ��Create a service control policy in the root organizational unit to deny access to the services or actions.

�𰸣�D

�� �� ��  Correct  Answer:   D   Detailed  Explanation:  To  centrally   manage  permissions  across   multiple  AWS  accounts   in  an organization, the  best  solution  is  to  use  Service  Control  Policies  (SCPs)  in  AWS  Organizations.  SCPs  act  as  guardrails  for  all accounts in the organization. By creating an SCP in the root organizational unit (OU), you can define permissions that apply to all accounts under that OU. This ensures a single  point of control for permissions and scales automatically as new accounts are added.  SCPs  can  explicitly  deny  access  to  specific  services  or  actions,  overriding  any  permissive  IAM  policies  in  individual accounts. Other options fail because: - A (ACLs): ACLs (like S3 bucket ACLs) are resource-specific, not account-wide, and cannot centrally restrict services/actions. - B (Security Groups): Security Groups control network traffic for resources (e.g., EC2 instances), not  account-level  service  permissions.  -  C  (Cross-Account  Roles):  Cross-account  roles  enable  access  delegation  but  require
manual     setup     in      each     account,     which      is     not      scalable.     Reference      Link:      [AWS     Service     Control      Policies (SCPs)](https://docs.aws.amazon.com/organizations/latest/user guide/orgs_manage_policies_scps.html)

147.A company is concerned about the security of its public web application due to recent web attacks. The application uses an Application Load Balancer (ALB). A solutions architect must reduce the risk of DDoS attacks against the application. What should the solutions architect do to meet this requirement?

A ��Add an Amazon Inspector agent to the ALB.

B ��Configure Amazon Macie to prevent attacks.

C ��Enable AWS Shield Advanced to prevent attacks.

D ��Configure Amazon Guard duty to monitor the ALB.

�𰸣�C

������ Correct Answer: C. Enable AWS Shield Advanced to prevent attacks. Detailed Explanation: To protect a web application
using an Application Load Balancer (ALB) from DDoS attacks, AWS Shield Advanced is the most appropriate solution. Here ��s why:
- AWS Shield Advanced is a managed service specifically designed to protect AWS resources, including ALBs, from DDoS attacks. It provides enhanced detection, automatic mitigations, and 24/7 access to the AWS DDoS Response Team. This directly addresses the requirement to reduce DDoS risks. Why the other options are incorrect: - A. Add an Amazon Inspector agent to the ALB: Amazon Inspector scans EC2 instances and container images for vulnerabilities but does not protect against DDoS attacks. ALBs do not support Inspector agents, making this option irrelevant. - B. Configure Amazon Macie to prevent attacks: Amazon Macie focuses  on discovering and  protecting sensitive  data  (e.g.,  in  S3  buckets)  using  machine  learning.  It  does  not  mitigate  DDoS attacks. - D. Configure Amazon Guard duty to monitor the ALB: Guard duty detects threats like unauthorized access or malicious activity by analyzing logs and network traffic. While it can alert you to suspicious behavior, it does not actively block DDoS attacks. Reference     Links:      -      [AWS      Shield     Advanced](https://aws.amazon.com/shield/)      -      [AWS      DDoS      Protection      Best Practices](https://aws.amazon.com/answers/security/aws-ddos-best-practices/)

148.A company ��s web  application is  running on Amazon  EC2  instances  behind an Application  Load  Balancer. The company

recently  changed  its  policy,  which   now   requires  the   application  to   be  accessed  from  one  specific   country  only.  Which configuration will meet this requirement?

A ��Configure the security group for the EC2 instances.

B ��Configure the security group on the Application Load Balancer.

C ��Configure AWS WAF on the Application Load Balancer in a VPC.

D ��Configure the network ACL for the subnet that contains the EC2 instances.

�𰸣�C

���� �� Correct Answer: C Detailed Explanation: To restrict access to a web application based on a specific country, AWS WAF (Web Application Firewall) is the appropriate solution. AWS WAF allows you to create geographic match conditions to block or allow requests originating from specific countries. Here ��s why the other options are incorrect: - A/B (Security Groups): Security Groups control traffic via IP/port/protocol rules but cannot filter based on geographic location. - D (Network ACLs): Network ACLs operate at the subnet level and also use IP-based rules, lacking geographic filtering capabilities. - C (AWS WAF): By attaching AWS WAF to the Application Load Balancer (ALB), you can block non-compliant country traffic before it reaches the EC2 instances. AWS WAF ��s geographic rules are applied at the ALB level, ensuring only permitted countries access the application. Reference Link:                                             [AWS                                            WAF                                             Geographic                                             Match
Conditions](https://docs.aws.amazon.com/waf/latest/developer guide/waf-rule-statement-type-geo-match.html)

149.A company provides an API to its users that automates inquiries for tax computations based on item prices. The company experiences a larger number of inquiries during the holiday season only that cause slower response times. A solutions architect needs to design a solution that is scalable and elastic. What should the solutions architect do to accomplish this?

A �� Provide an API  hosted on an Amazon  EC2 instance. The  EC2 instance performs the required computations when the API request is made.

B ��Design a REST API using Amazon API Gateway that accepts the item names. API Gateway passes item names to AWS Lambda for tax computations.

C ��Create an Application Load Balancer that has two Amazon EC2 instances behind it. The EC2 instances will compute the tax on the received item names.

D �� Design a REST API using Amazon API Gateway that connects with an API hosted on an Amazon EC2 instance. API Gateway accepts and passes the item names to the EC2 instance for tax computations.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The correct answer is B because using Amazon API Gateway with AWS Lambda provides a scalable and serverless architecture. Here's why: 1. Automatic Scaling: Both API Gateway and Lambda automatically scale to handle traffic spikes (like holiday seasons) without manual intervention. Lambda functions spin up additional instances as needed and shut down when idle. 2. Cost-Effective: You only pay for the compute time Lambda consumes (per millisecond), avoiding costs for idle EC2 instances during low-traffic periods. 3. Simplified Architecture: API Gateway manages API requests and directly triggers Lambda for tax computations, eliminating the need to provision/manage servers (EC2) or load balancers (ALB). Options A/C/D rely on EC2 instances, which require manual scaling configurations and incur costs even when unused. Lambda's serverless approach better matches the requirement for elasticity and scalability during unpredictable traffic spikes. Reference

Links: - [AWS Lambda](https://aws.amazon.com/lambda/) - [Amazon API Gateway](https://aws.amazon.com/api-gateway/)

150.A  solutions  architect   is  creating  a   new  Amazon  Cloud front  distribution  for  an  application.  Some  of  the   information submitted  by  users  is  sensitive.  The  application  uses  HTTPS  but  needs  another  layer  of  security.  The  sensitive  information should.be  protected  throughout  the  entire  application  stack,  and  access  to  the  information  should  be  restricted  to  certain applications. Which action should the solutions architect take?

A ��Configure a Cloud front signed URL.

B ��Configure a Cloud front signed cookie.

C ��Configure a Cloud front field-level encryption profile.

D��Configure Cloud front and set the Origin Protocol Policy setting to HTTPS Only for the Viewer Protocol Policy.

�𰸣�C

���� �� Correct Answer: C. Configure a Cloud front field-level encryption profile. Detailed Explanation: The question requires protecting sensitive data throughout the entire application stack and restricting access to specific applications.  Here ��s why field-level encryption (C) is the best choice: 1. How It Works: - Field-level encryption encrypts specific sensitive fields (e.g., credit card numbers, passwords) at the edge (Cloud front) before sending the data to the origin server (backend). - Only applications with  the  private  key  can  decrypt  the  data,  ensuring  end-to-end  protection.  2.  Why  Not  Other  Options:  -  A/B  (Signed URLs/Cookies): These control who can access content (e.g., paid users) but don ��t encrypt data end-to-end. Sensitive data could still be exposed in backend systems. - D (HTTPS Only): HTTPS encrypts data in transit, but the data is decrypted at the origin server.  It doesn ��t add an extra encryption  layer for sensitive fields or  restrict decryption to specific apps. 3.  Key  Benefit: - Field-level encryption ensures sensitive data is never exposed in plain text��even to AWS services like logs or the origin server. Only  authorized  apps  with  the  decryption  key  can  read  it.  Example:  Imagine  users  submit  credit  card  info  via  HTTPS.  With field-level encryption: - Cloud front encrypts the card number using a public key. - The encrypted value (e.g., `xyz123`) is sent to the backend. - Only an app with the private key (e.g., a payment service) can decrypt `xyz123` back to the actual card number. Reference:                                                      [Amazon                                                     Cloud front                                                      Field-Level
Encryption](https://docs.aws.amazon.com/Amazon cloud front/latest/Developer guide/field-level-encryption.html)

151.A gaming company hosts a browser-based application on AWS. The users of the application consume a  large number of videos  and  images  that  are  stored  in  Amazon  S3.  This  content  is  the  same  for  all  users.  The  application  has  increased  in popularity, and millions of users worldwide accessing these  media files. The company wants to provide the files to the users while reducing the load on the origin. Which solution meets these requirements MOST cost-effectively?

A ��Deploy an AWS Global Accelerator accelerator in front of the web servers.

B ��Deploy an Amazon Cloud front web distribution in front of the S3 bucket.

C ��Deploy an Amazon Elastic ache for Redis instance in front of the web servers.

D ��Deploy an Amazon Elastic ache for Memcached instance in front of the web servers.

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: The best solution is to use Amazon Cloud front, a Content Delivery Network (CDN).  Cloud front  caches  static  content  (like  videos  and  images)  at  edge  locations  worldwide.  When   users  request  files,

Cloud front serves them from the nearest edge location instead of fetching from the origin (S3) every time. This reduces load on S3, lowers latency for global users, and cuts costs (Cloud front ��s data transfer pricing is often cheaper than S3 for high volumes). Other options are less suitable: - A (Global Accelerator) optimizes traffic routing but doesn ��t cache content. - C/D (Elastic ache) are  for  caching  dynamic  data  (e.g.,  database  queries),  not  static  files,  and  require  code  changes.  Reference  Link:  [Amazon Cloud front](https://aws.amazon.com/cloud front/)

152.A company has a multi-tier application that runs six front-end web servers in an Amazon EC2 Auto Scaling group in a single Availability Zone behind an Application Load Balancer (ALB). A solutions architect needs to modify the infrastructure to be highly available  without  modifying  the  application.  Which  architecture  should  the  solutions  architect  choose  that   provides  high availability?

A ��Create an Auto Scaling group that uses three instances across each of two Regions.

B ��Modify the Auto Scaling group to use three instances across each of two Availability Zones.

C ��Create an Auto Scaling template that can be used to quickly create more instances in another Region.

D ��Change the ALB in front of the Amazon EC2 instances in a round-robin configuration to balance traffic to the web tier.

�𰸣�B

������ Correct Answer: B Detailed Explanation: High availability (HA) in AWS is achieved by distributing resources across multiple Availability Zones (AZs), which are physically separate data centers. The original setup uses only one AZ, so a failure in that AZ would take down all 6 web servers. Option B fixes this by modifying the Auto Scaling group to launch 3 instances in each of 2 AZs (total 6). The Application  Load  Balancer  (ALB)  automatically  routes  traffic  to  healthy  instances  in  both  AZs. This  requires  no application  code  changes,  as  the  ALB  handles  traffic  distribution.  -  Why  not  A/C?  Using  multiple  AWS  Regions  (A/C)  adds complexity (e.g., data replication, DNS routing) and isn ��t needed for basic HA. - Why not D? The ALB already balances traffic; the problem      is      the       single      AZ,       not      the      load-balancing       method.       Reference      Links:       [AWS      Auto       Scaling Groups](https://docs.aws.amazon.com/auto scaling/ec2/user guide/auto-scaling-groups.html)                 [Application                  Load
Balancer](https://aws.amazon.com/elastic load balancing/application-load-balancer/)

153.An ecommerce company has an order-processing application that uses Amazon API Gateway and an AWS Lambda function. The application stores data in an Amazon Aurora Postgresql database. During a recent sales event, a sudden surge in customer orders occurred. Some customers experienced timeouts, and the application did not process the orders of those customers. A solutions architect determined that the CPU utilization and memory utilization were high on the database because of a large number  of  open  connections.  The  solutions  architect  needs  to  prevent  the  timeout  errors  while  making  the  least  possible changes to the application. Which solution will meet these requirements?

A ��Configure provisioned concurrency for the Lambda function. Modify the database to be a global database in multiple AWS Regions.

B��Use Amazon RDS Proxy to create a proxy for the database. Modify the Lambda function to use t he RDS Proxy endpoint instead of the database endpoint.

C��Create a read replica for the database in a different AWS Region. Use query string parameters in API Gateway to route traffic to the read replica.

D �� Migrate the  data from Aurora  Postgresql to Amazon  Dynamo db  by  using AWS  Database  Migration Service  (AWS  DMS). Modify the Lambda function to use the Dynamo db table.

�𰸣�B

�� �� �� Correct Answer:  B  Detailed  Explanation: The  main  issue  here  is  too  many open database connections causing  high CPU/ memory usage on the Aurora database. When many Lambda functions connect directly to the database simultaneously, it can overload the database with too many connections (since each Lambda might open a new connection). RDS Proxy acts like a "connection pool manager" for your database: 1. It maintains a pool of reusable database connections 2. Instead of each Lambda function  opening  a  new  connection  every  time,  they  share  connections from the  pool  3.  This  reduces  the  total  number  of connections hitting the database 4. It also helps handle connection spikes more smoothly Why other options aren't ideal: - A) Provisioned  concurrency  helps  Lambda  start  faster  but  doesn't  fix  database  connection  issues.  Making  a  global  database  is overkill. - C) Read replicas help with read - heavy workloads, but order processing needs writes (which can't go to read replicas). -
D) Migrating to Dynamo db would require major application changes, which the question says to avoid. Reference Links: - [AWS RDS Proxy documentation](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/rds - proxy.html) - [When to use RDS Proxy](https://aws.amazon.com/rds/proxy/)

154.An application runs on Amazon EC2 instances in private subnets. The application needs to access an Amazon Dynamo db table. What is the MOST secure way to access the table while ensuring that the traffic does not leave the AWS network?

A ��Use a VPC endpoint for Dynamo db.

B ��Use a NAT gateway in a public subnet.

C ��Use a NAT instance in a private subnet.

D ��Use the internet gateway attached to the VPC.

�𰸣�A

������ Correct Answer A �� Use a VPC endpoint for Dynamo db. Detailed Explanation When an application in a private subnet needs to access Amazon Dynamo db, the most secure and efficient method is to use a VPC endpoint. Here's why: 1. Traffic Stays Within the AWS Network: A VPC endpoint creates a private connection between your VPC and Dynamo db. This means traffic never leaves the AWS network, avoiding exposure to the public internet. 2.  No Need for NAT Gateways/Instances or  Internet Gateways: -  NAT Gateway (Option  B):  Requires traffic to route through a public subnet, which adds complexity and potential latency. While NAT gateways allow private subnets to access the internet, Dynamo db is an AWS service�� using a VPC endpoint avoids unnecessary internet routing. - NAT Instance (Option C): Similar to a NAT gateway but less scalable and managed by you (not AWS). It also requires public subnet access. - Internet Gateway (Option D): Only allows public subnet resources to access the internet. Private subnets can ��t use it directly. 3. Enhanced Security: VPC endpoints use AWS Identity and Access Management (IAM) policies and security groups to restrict access to the Dynamo db table. This minimizes attack surfaces compared to routing traffic        through        the         internet        or         NAT        devices.         Reference        Link         [AWS        VPC         Endpoints        for Dynamo db](https://docs.aws.amazon.com/amazon dynamo db/latest/developer guide/vpc-endpoints-dynamo db.html)

155.An entertainment company  is  using Amazon  Dynamo db to  store  media  metadata. The  application  is  read  intensive  and experiencing delays. The  company does  not  have staff to  handle  additional operational overhead and  needs to  improve the performance efficiency of Dynamo db without reconfiguring the application. What should a solutions architect recommend to meet this requirement?

A ��Use Amazon Elastic ache for Redis.

B ��Use Amazon Dynamo db Accelerator (DAX).

C ��Replicate data by using Dynamo db global tables.

D ��Use Amazon Elastic ache for Memcached with Auto Discovery enabled.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution here is Amazon Dynamo db Accelerator (DAX). DAX is an in - memory caching service specifically designed for Dynamo db. It reduces read latency by caching frequently accessed data, which is ideal for read - intensive workloads like this media metadata application. Since DAX is fully managed and integrates seamlessly with Dynamo db, it requires no application reconfiguration��the application only needs to point to the DAX endpoint instead of the Dynamo db endpoint. This meets the requirement of minimizing operational overhead. Other options: - A/D (Elastic ache): While Elastic ache (Redis/Memcached) can cache data, it requires manual application changes to implement caching logic (e.g., checking the cache before querying Dynamo db), which violates the "no reconfiguration" requirement. - C (Global Tables): Global Tables improve availability and disaster recovery by replicating data across regions but do not directly address read latency in a single region. Reference Link: [Amazon Dynamo db Accelerator (DAX)](https://aws.amazon.com/dynamo db/dax/)

156.A company ��s infrastructure consists of Amazon EC2 instances and an Amazon RDS DB instance in a single AWS Region. The company  wants  to   back  up  its  data  in  a  separate   Region.  Which  solution  will   meet  these   requirements  with  the   LEAST operational overhead?

A ��Use AWS Backup to copy EC2 backups and RDS backups to the separate Region.

B ��Use Amazon Data Lifecycle Manager (Amazon DLM) to copy EC2 backups and RDS backups to the separate Region.

C ��Create Amazon Machine Images (AMIs) of the EC2 instances. Copy the AMIs to the separate Region. Create a read replica for t he RDS DB instance in the separate Region.

D �� Create Amazon  Elastic  Block Store (Amazon EBS) snapshots. Copy the EBS snapshots to the separate Region. Create RDS snapshots. Export the RDS snapshots to Amazon S3. Configure S3 Cross-Region Replication (CRR) to the separate Region.

�𰸣�A

������Correct Answer: A Detailed Explanation: AWS Backup is a centralized service that automates backups across multiple AWS services, including EC2 and RDS. By configuring a backup plan in AWS Backup, you can automatically copy backups to a separate Region with minimal manual effort. This reduces operational overhead compared to other options that require manual steps or combining  multiple  services  (like  creating  AMIs,  managing  snapshots,  or  setting  up  S3  Cross-Region  Replication  separately). Options C and D involve complex, multi-step processes, while Option B (Amazon DLM) only handles EC2 EBS snapshots, not RDS. AWS   Backup   provides   a   unified,   automated   solution   for   both   services.   Reference   Links:   -   [AWS   Backup   Cross-Region Copy](https://docs.aws.amazon.com/aws-backup/latest/dev guide/cross-region-copy.html)      -       [AWS        Backup      Supported Services](https://aws.amazon.com/backup/features/)

157.A solutions architect  needs to securely store a database  user  name and  password that an application  uses to access an Amazon RDS DB instance. The application that accesses the database runs on an Amazon EC2 instance. The solutions architect wants to create a secure parameter in AWS Systems Manager Parameter Store. What should the solutions architect do to meet this requirement?

A ��Create an IAM role that has read access to the Parameter Store parameter. Allow Decrypt access to an AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this IAM role to the EC2 instance.

B �� Create  an  IAM  policy  that  allows  read  access  to  the  Parameter  Store  parameter.  Allow  Decrypt  access  to  an  AWS  Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this IAM policy to the EC2 instance.

C ��Create an IAM trust relationship between the Parameter Store parameter and the EC2 instance. Specify Amazon RDS as a principal in the trust policy.

D��Create an IAM trust relationship between the DB instance and the EC2 instance. Specify Systems Manager as a principal in the trust policy.

�𰸣�A

���� �� Correct Answer: A Detailed Explanation: To securely store and access sensitive data like database credentials, the EC2 instance needs permissions to retrieve the parameter from AWS Systems Manager Parameter Store and decrypt it using the AWS KMS key. Here's why A is correct: 1. **IAM Role for EC2** : - IAM roles (not policies directly) are assigned to EC2 instances. The role must include permissions to read the Parameter Store parameter (`ssm:Getparameter`) and decrypt it using the KMS key (`kms:Decrypt`).   -   Example    policy   snippet:   ```json    {   "Effect":    "Allow",   "Action" :    [    "ssm:Getparameter"   ],   "Resource": "arn:aws:ssm:region:account-id:parameter/parameter-name"  },  {   "Effect":  "Allow",  "Action" :  [  "kms:Decrypt"  ],   "Resource": "arn:aws:kms:region:account-id:key/key-id" } ``` 2. **Why Other Options Are Incorrect** : - B: Assigning an IAM policy directly to the EC2 instance isn't possible. Policies must be attached to roles. - C/D: Trust relationships are used for roles to define who can assume the role, not for granting access to Parameter Store or RDS. For a beginner: Think of the IAM role as a "security badge" the EC2 instance wears. This badge gives it permission to fetch the password (from Parameter Store) and unlock it (using KMS). Options    C/D     are     like     giving    the     badge     to    the     wrong     people.     Reference     Links:     -     [AWS    SSM     Parameter Store](https://docs.aws.amazon.com/systems-manager/latest/user guide/systems-manager-parameter-store.html)  -   [IAM  Roles for EC2](https://docs.aws.amazon.com/AWSEC2/latest/User guide/iam-roles-for-amazon-ec2.html)

158.A company has a legacy data processing application that runs on Amazon EC2 instances. Data is processed sequentially, but the order of results does not matter. The application uses a monolithic architecture. The only way that the company can scale the application to meet increased demand  is to increase the size of the instances. The company ��s developers  have decided to rewrite the application to use a micro services architecture on Amazon Elastic Container Service (Amazon ECS). What should a solutions architect recommend for communication between the micro services?

A ��Create an Amazon Simple Queue Service (Amazon SQS) queue. Add code to the data producers, and send data to the queue. Add code to the data consumers to process data from the queue.

B ��Create an Amazon Simple Notification Service (Amazon SNS) topic. Add code to the data producers, and publish notifications to the topic. Add code to the data consumers to subscribe to the topic.

C ��Create an AWS Lambda function to pass messages. Add code to the data producers to call the Lambda function with a data object. Add code to the data consumers to receive a data object that is passed from the Lambda function.

D��Create an Amazon Dynamo db table. Enable Dynamo db Streams. Add code to the data producers to insert data into the table. Add code to the data consumers to use the Dynamo db Streams API to detect new table entries and retrieve the data.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best choice is Amazon SQS because it allows asynchronous communication between micro services, decoupling producers (data senders) and consumers (data processors). Since the order of results doesn �� t matter, SQS ��s standard queue is ideal. Producers send data to the queue, and consumers process messages independently,

enabling horizontal scaling by adding more ECS tasks to handle the workload. Why not SNS (B)? SNS broadcasts messages to all subscribers, which is unnecessary here since each message only needs to be processed once. Why not Lambda (C)? Lambda isn �� t  designed  for  direct  message  passing  between  micro services  and  would  add  unnecessary  complexity.  Why  not  Dynamo db Streams (D)? While Dynamo db Streams can trigger actions on data changes, it ��s less efficient for high-throughput messaging compared to SQS. Reference Link: [Amazon SQS](https://aws.amazon.com/sqs/)

159.A company wants to migrate its MySQL database from on premises to AWS. The company recently experienced a database outage that significantly impacted the business. To ensure this does not happen again, the company wants a reliable database solution  on AWS that  minimizes  data  loss  and  stores  every  transaction  on  at  least  two  nodes.  Which  solution  meets  these requirements?

A ��Create an Amazon RDS DB instance with synchronous replication to three nodes in three Availability Zones.

B ��Create an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data.

C �� Create an Amazon  RDS  MySQL  DB  instance and then create a  read  replica in a separate AWS  Region that synchronously replicates the data.

D �� Create  an Amazon  EC2  instance with  a  MySQL  engine  installed that triggers  an AWS  Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance.

�𰸣�B

������ Correct Answer B Detailed Explanation The best choice is B because Amazon RDS Multi-AZ ensures high availability and data durability for MySQL databases. Here ��s why: - Synchronous Replication: Multi-AZ uses synchronous replication to a standby instance in a different Availability Zone (AZ). Every transaction is committed to both the primary and standby nodes before being acknowledged, minimizing data loss. - Automatic Failover: If the primary node fails, RDS automatically switches to the standby node,  reducing  downtime.  -  Durability:  Data  is  stored  redundantly  across  two  nodes  (primary  +  standby),  meeting  the requirement of storing transactions on at least two nodes. Other options are less optimal: - A (three  nodes):  RDS for MySQL doesn ��t natively support three-node synchronous replication (this is more typical for Amazon Aurora). - C (cross-region read replica): Read replicas use asynchronous replication, risking data loss during outages. - D (EC2 + Lambda): This adds unnecessary complexity    and     relies    on     custom     code    instead     of    AWS-managed     redundancy.     Reference     Link    [Amazon     RDS Multi-AZ](https://docs.aws.amazon.com/Amazon rds/latest/User guide/Concepts.MultiAZ.html)

160.A company is building a new dynamic ordering website. The company wants to minimize server maintenance and patching. The website  must  be  highly available and must scale  read and write capacity as quickly as  possible to meet changes in  user demand. Which solution will meet these requirements?

A �� Host static  content  in Amazon S3.  Host  dynamic content  by  using Amazon API Gateway and AWS  Lambda.  Use  Amazon Dynamo db with on-demand capacity for the database. Configure Amazon Cloud front to deliver the website content.

B ��Host static content in Amazon S3. Host dynamic content by using Amazon API Gateway and AWS Lambda. Use Amazon Aurora with Aurora Auto Scaling for the database. Configure Amazon Cloud front to deliver the website content.

C �� Host all the website content on Amazon  EC2  instances. Create an Auto Scaling group to scale the  EC2  instances.  Use an Application Load Balancer to distribute traffic. Use Amazon Dynamo db with provisioned write capacity for the database.

D �� Host all the website content on Amazon  EC2 instances. Create an Auto Scaling group to scale the  EC2  instances.  Use an Application Load Balancer to distribute traffic. Use Amazon Aurora with Aurora Auto Scaling for the database.

�𰸣�A

���� �� Correct Answer A Detailed Explanation The question asks for a solution that minimizes server maintenance/patching,
ensures high availability, and scales read/write capacity rapidly. Here's why Option A is the best choice: 1. No server maintenance:
- S3 (static content) and Lambda (dynamic content) are fully managed by AWS, eliminating server upkeep. - Dynamo db (on - demand)  automatically  scales  without  capacity  planning,  unlike  Aurora  (Option  B)  or  EC2  (Options  C/D).  2.  Rapid  scaling:  - Dynamo db on - demand scales  read/write capacity instantly to handle traffic spikes. Aurora (Options B/D) scales slower and requires read replicas for read - heavy workloads. - Lambda and API Gateway auto - scale to handle thousands of concurrent requests. 3. High availability: - S3, Cloud front, Lambda, and Dynamo db are inherently highly available across AWS Availability Zones.  -  EC2  -  based  solutions  (Options  C/D)  require  manual  setup  for  redundancy  and  load  balancing.  4.  Performance:  - Cloud front  caches  static  content globally,  reducing  latency.  -  EC2  -  based  options  (C/D)  introduce  overhead  from  managing servers, scaling delays, and patching. Why other options fail: - Option B: Aurora requires more manual scaling and maintenance than  Dynamo db.  -  Options  C/D:   EC2   requires  server   management,  patching,  and  slower  scaling  compared  to  serverless. Reference          Links         -          [AWS          Serverless](https://aws.amazon.com/serverless/)          -          [Dynamo db          On          - Demand](https://aws.amazon.com/dynamo db/pricing/on                       -                       demand/)                       -                        [Lambda Scaling](https://docs.aws.amazon.com/lambda/latest/dg/invocation - scaling.html)

161.A company has an AWS account used for software engineering. The AWS account has access to the company ��s on-premises data  center through  a  pair  of  AWS  Direct  Connect  connections.  All  non-VPC  traffic  routes  to  the  virtual  private  gateway.  A development team recently created an AWS Lambda function through the console. The development team needs to allow the function to access a database that  runs in a  private subnet in the company ��s  data  center. Which solution will  meet these requirements?

A ��Configure the Lambda function to run in the VPC with the appropriate security group.

B ��Set up a VPN connection from AWS to the data center. Route the traffic from the Lambda function through the VPN.

C �� Update the  route tables  in the VPC to allow the  Lambda function to access the on-premises data center through  Direct Connect.

D ��Create an Elastic IP address. Configure the Lambda function to send traffic through the Elastic IP address without an elastic network interface.

�𰸣�A

������ Correct Answer: A Detailed Explanation: When a Lambda function is created, it runs in a default VPC-less environment and cannot  directly  access  resources  in  a VPC  or  on-premises  network. To  enable  access to the on-premises  database  (in  a private subnet), the Lambda must be configured to run within the VPC. Here's why: 1. VPC Attachment: By deploying the Lambda in the VPC (Option A), it gains an elastic network interface (ENI) in the specified subnet. This allows it to follow VPC routing rules, which are already configured to route on-premises traffic through the Direct Connect connections and virtual private gateway. 2. Security Groups: A security group acts as a virtual firewall. Configuring it properly ensures the Lambda can communicate with the on-premises database  over the  required  ports  (e.g.,  TCP  3306  for  MySQL).  3.  Why Other  Options  Fail:  -  Option  B:  A VPN  is unnecessary because Direct Connect is already provisioned. This adds cost/complexity. - Option C: VPC route tables alone won't help  if the  Lambda  isn't  in the VPC.  Routing  rules  only apply to  resources  inside the VPC. - Option  D:  Elastic  IPs don't work without a VPC ENI. Lambda outside a VPC can't route through Direct Connect. Key Take away: Lambda must be VPC-attached to leverage existing Direct Connect routing. Security groups control access permissions.

162.A company runs an application using Amazon ECS. The application creates resized versions of an original image and then

makes Amazon S3 API calls to store the resized images in Amazon S3. How can a solutions architect ensure that the application has permission to access Amazon S3?

A ��Update the S3 role in AWS IAM to allow read/write access from Amazon ECS, and then relaunch the container.

B ��Create an IAM role with S3 permissions, and then specify that role as the task ro learn in the task definition.

C ��Create a security group that allows access from Amazon ECS to Amazon S3, and update the launch configuration used by the ECS cluster.

D ��Create an IAM user with S3 permissions, and then relaunch the Amazon EC2 instances for the ECS cluster while logged in as this account.

�𰸣�B

������ Correct Answer: B Detailed Explanation: To allow an Amazon ECS task to access Amazon S3 securely, you need to assign an IAM role with the required S3 permissions directly to the ECS task. Here's why: 1. IAM Roles for Tasks (Option B): - ECS tasks can  assume  an  IAM  role  defined  in  the  task  definition  (task ro learn).  This  role  grants  permissions  (like  S3  access)  to  the containers in the task. - This is the recommended approach because it follows the principle of least privilege��only the specific task gets the permissions it needs, not the entire EC2 instance or cluster. 2. Why Other Options Are Incorrect: - Option A: S3 doesn ��t have "roles" to modify. IAM roles control access to S3, but you don ��t "update the S3 role"��you attach policies to roles assigned  to  services  like  ECS.  -  Option  C:  Security  groups  control  network  traffic  (e.g.,  inbound/outbound  rules),  not  API permissions. S3 access relies on IAM policies, not network rules. - Option D: Using IAM users for EC2 instances is insecure and outdated. Roles are temporary and more secure. Also, relaunching EC2 instances is unnecessary. Reference Links: - [IAM Roles for Amazon ECS Tasks](https://docs.aws.amazon.com/Amazon ecs/latest/developer guide/task-iam-roles.html) -  [Amazon S3 Access Control](https://docs.aws.amazon.com/AmazonS3/latest/user guide/access-control-overview.html)

163.A company has a Windows-based application that must be migrated to AWS. The application requires the use of a shared Windows file system attached to multiple Amazon EC2 Windows instances that are deployed across multiple Availability Zone: What should a solutions architect do to meet this requirement?

A ��Configure AWS Storage Gateway in volume gateway mode. Mount the volume to each Windows instance.

B ��Configure Amazon FSx for Windows File Server. Mount the Amazon FSx file system to each Windows instance.

C �� Configure  a file system  by  using Amazon  Elastic  File  System  (Amazon  EFS).  Mount  the  EFS  file  system to each Windows instance.

D��Configure an Amazon Elastic Block Store (Amazon EBS) volume with the required size. Attach each EC2 instance to the volume. Mount the file system within the volume to each Windows instance.

�𰸣�B

�� �� �� Correct Answer:  B  Detailed  Explanation:  The  correct  answer  is  B  because  Amazon  FSx  for  Windows  File  Server  is specifically designed for Windows-based applications that require a shared file system.  It supports the Server  Message  Block (SMB) protocol, integrates with Active Directory, and provides multi-AZ redundancy for high availability. This makes it ideal for Windows  workloads  needing  shared  storage  across  multiple  Availability  Zones  (AZs).  Why  not  other  options?  -  A  (Storage Gateway Volume Gateway): This is meant for hybrid cloud setups (on-premises + AWS) and provides block storage via iSCSI, not a fully managed shared file system for cross-AZ EC2 instances. - C (Amazon EFS): EFS uses the NFS protocol and is optimized for

Linux, not Windows. - D (Amazon EBS): EBS volumes are block storage tied to a single AZ and cannot be attached to multiple instances       across        AZs        simultaneously.        Reference        Link:        [Amazon        FSx       for        Windows        File        Server Documentation](https://docs.aws.amazon.com/fsx/latest/Windows guide/what-is.html)

164.A company uses Amazon S3 as its data lake. The company has a new partner that must use SFTP to upload data files. A solutions architect needs to implement a highly available SFTP solution that minimizes operational overhead. Which solution will meet these requirements?

A ��Use AWS Transfer Family to configure an SFTP-enabled server with a publicly accessible endpoint. Choose the S3 data lake as the destination.

B��Use Amazon S3 File Gateway as an SFTP server. Expose the S3 File Gateway endpoint URL to the new partner. Share the S3 File Gateway endpoint with the new partner.

C��Launch an Amazon EC2 instance in a private subnet in a Vp instruct the new partner to upload files to the EC2 instance by using a VPN. Run a cron job script, on the EC2 instance to upload files to the S3 data lake.

D ��Launch Amazon EC2 instances in a private subnet in a VPC. Place a Network Load Balancer (NLB) in front of the EC2 instances. Create an SFTP  listener  port  for the  NLB.  Share the  NLB  hostname  with  the  new  partner.  Run  a  cron  job script  on the  EC2 instances to upload files to the S3 data lake.

�𰸣�A

������ Correct Answer: A Detailed Explanation: AWS Transfer Family is a fully managed service that supports SFTP, FTPS, and FTP protocols. It directly integrates with Amazon S3, eliminating the need to manage servers, scripts, or infrastructure. The service automatically  provides   high  availability,  scalability,  and  security  (via  managed   keys  or  custom  certificates).  This   minimizes operational overhead compared to self-managed options like EC2 instances (C/D) or S3 File Gateway (B), which require manual setup, maintenance, and HA configurations. Option A allows the partner to upload files via SFTP directly to S3 without additional code or infrastructure management. Reference Link: https://aws.amazon.com/aws-transfer-family/

165.A company  has a web  application that  is  based  on Java  and  PHP. The  company  plans  to  move the  application from on premises to AWS. The company needs the ability to test new site features frequently. The company also needs a highly available and managed solution that requires minimum operational overhead. Which solution will meet these requirements?

A��Create an Amazon S3 bucket. Enable static web hosting on the S3 bucket. Upload the static content to the S3 bucket. Use AWS Lambda to process all dynamic content.

B ��Deploy the web application to an AWS Elastic Beanstalk environment. Use URL swapping to switch between multiple Elastic Beanstalk environments for feature testing.

C ��Deploy the web application to Amazon EC2 instances that are configured with Java and PHP. Use Auto Scaling groups and an Application Load Balancer to manage the website ��s availability.

D��Container ize the web application. Deploy the web application to Amazon EC2 instances. Use the AWS Load Balancer Controller to dynamically route traffic between containers that contain the new site features for testing.

�𰸣�B

������ Correct Answer B �� Deploy the web application to an AWS Elastic Beanstalk environment. Use URL swapping to switch

between multiple Elastic Beanstalk environments for feature testing. Detailed Explanation Elastic Beanstalk is a fully managed AWS service that automatically handles deployment, capacity provisioning, load balancing, scaling, and health monitoring. This minimizes operational overhead, aligning with the  requirement for a  "managed solution with  minimum operational effort." - Java/PHP Support:  Elastic  Beanstalk  natively  supports  Java  and  PHP,  so  no  code  refactoring  is  needed.  -  High  Availability:  It automatically configures  load balancing and auto - scaling, ensuring  high availability. -  Feature Testing:  URL swapping (blue - green deployment) allows seamless testing of new features in a separate environment. You can test new features in a staging environment and  instantly switch traffic to it without downtime. Other  options fail  because: - A: S3+Lambda only works for static+serverless apps, not traditional Java/PHP apps. - C: EC2 requires manual infrastructure management (not fully managed). -
D: Containerization adds complexity and EC2 management (not "minimum operational overhead"). Reference Link [AWS Elastic Beanstalk](https://aws.amazon.com/elastic beanstalk/)            [Blue           -            Green            Deployments            with            Elastic Beanstalk](https://docs.aws.amazon.com/elastic beanstalk/latest/dg/using - features.Cname swap.html)

166.A company has an ordering application that stores customer information in Amazon RDS for MySQL. During regular business hours, employees  run one-time queries for  reporting  purposes. Timeouts are occurring during order  processing  because the reporting queries are taking a long time to run. The company needs to eliminate the timeouts without preventing employees from performing queries. What should a solutions architect do to meet these requirements?

A ��Create a read replica. Move reporting queries to the read replica.

B ��Create a read replica. Distribute the ordering application to the primary DB instance and the read replica. C ��Migrate the ordering application to Amazon Dynamo db with on-demand capacity.

D ��Schedule the reporting queries for non-peak hours.

�𰸣�A

������ Correct Answer: A. Create a read replica. Move reporting queries to the read replica. Detailed Explanation: The problem occurs  because  reporting  queries  (read-heavy  operations)  are  competing  with  order-processing  transactions   (write-heavy operations) on the same RDS instance, causing resource contention and timeouts. Here's why Option A works: 1. Read Replica Purpose: A read replica is a copy of the primary database that handles read-only traffic. By redirecting reporting queries to the read replica, you offload read operations from the primary database. 2. Isolate Workloads: - The primary DB instance handles critical  write  operations  (order  processing),  ensuring  no  timeouts.  -  The  read  replica  handles  non-critical  read  operations (reporting),  eliminating competition for  resources.  3.  No  Downtime  or  App  Changes:  Employees  can  still  run  queries  during business hours without affecting order processing. No code changes are required��only redirect reporting queries to the replica endpoint. Other options fail because: - B: Distributing writes across instances would complicate the application and isn't standard practice for RDS. - C: Migrating to Dynamo db is unnecessary; the issue is about read/write separation, not database type. - D: Restricting queries to off-peak  hours doesn't meet the requirement for real-time  reporting during business  hours.  Reference: [AWS RDS Read Replicas](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_Read repl.html)

167.A company is  running a batch application on Amazon  EC2 instances. The application consists of a backend with  multiple Amazon RDS databases. The application is causing a high number of reads on the databases. A solutions architect must reduce the number of database reads while ensuring high availability. What should the solutions architect do to meet this requirement?

A ��Add Amazon RDS read replicas.

B ��Use Amazon Elastic ache for Redis.

C ��Use Amazon Route 53 DNS caching

D ��Use Amazon Elastic ache for Memcached.

�𰸣�A

���� �� Correct Answer: A. Add Amazon RDS read replicas. Detailed Explanation: The best solution here is A because adding Amazon RDS read replicas directly addresses both requirements: reducing database read load and ensuring high availability. - Reducing Reads:  Read replicas allow the application to offload read traffic from the primary database to one or more replica instances. This distributes read requests, reducing pressure on the primary database. - High Availability: RDS read replicas can be promoted to a standalone database if the primary instance fails, improving fault tolerance. They can also be deployed across multiple Availability Zones (AZs) for redundancy. While caching solutions  like Elastic ache (B/D) can reduce database  reads  by storing frequently accessed data, they introduce additional complexity (e.g., cache invalidation, data consistency) and may not inherently ensure high availability unless explicitly configured (e.g., Redis Multi-AZ). Route 53 (C) is unrelated to database read optimization. For a batch application, read replicas are a simpler, more direct way to scale read capacity without modifying the application logic. Caching is better suited for repetitive read patterns (e.g., static or semi-static data), which the question does not         explicitly          mention.          Reference          Links:          -          [Using          Read           Replicas         to          Offload          Read Traffic](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_Read repl.html)   -   [RDS    Read    Replicas   and    High Availability](https://aws.amazon.com/rds/features/read-replicas/)

168.A company  needs to  run  a  critical application on AWS. The company  needs to  use Amazon  EC2  for the  application ��s database. The database must be highly available and must fail over automatically if a disruptive event occurs. Which solution will meet these requirements?

A �� Launch two EC2 instances, each in a different Availability Zone in the same AWS Region. Install the database on both EC2 instances. Configure the EC2 instances as a cluster. Set up database replication.

B ��Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an Amazon Machine Image (AMI) to back up the data. Use AWS Cloud formation to automate provisioning of the EC2 instance if a disruptive event occurs.

C �� Launch two  EC2  instances,  each  in  a different AWS  Region.  Install the  database  on  both  EC2  instances. Set  up  database replication. Fail over the database to a second Region.

D��Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an Amazon Machine Image (AMI) to back up the data. Use EC2 automatic recovery to recover the instance if a disruptive event occurs.

�𰸣�A

���� �� Correct Answer: A Detailed Explanation: To achieve high availability and automatic failover for a database running on Amazon EC2, the best approach is to distribute the database across multiple Availability Zones (AZs) within the same AWS Region.
- Option A  uses two  EC2  instances  in  different  AZs  (physically  separate  data  centers)  and  configures them  as  a  cluster with database  replication. This  ensures that if one AZ fails, the database  in the other AZ  can take over automatically,  minimizing downtime.  Database  replication  keeps  the  data  synchronized  between  instances,  enabling  seamless  failover.  -  Why  Other Options Are Incorrect: - Option B relies on a single EC2 instance and AMI backups. Recovery via Cloud formation is manual and slow, leading to downtime during failures. - Option C uses  multiple AWS  Regions, which adds complexity and higher  latency. Cross-Region failover is typically for disaster recovery, not routine high availability. - Option D depends on EC2 automatic recovery, which only restarts the instance on the same hardware or AZ. It doesn ��t address AZ-level failures or ensure data redundancy. Reference  Links:  -   [AWS  Availability  Zones](https://aws.amazon.com/about-aws/global-infrastructure/regions_az/)  -   [Amazon EC2                                                                                                                                                                                                                                 High
Availability](https://docs.aws.amazon.com/white papers/latest/real-time-communication-on-aws/high-availability-for-amazon-ec

2.html)

169.A company ��s order system sends requests from clients to Amazon EC2 instances. The EC2 instances process the orders and then store the orders in a database on Amazon RDS. Users report that they must re process orders when the system fails. The company wants a resilient solution that can process orders automatically if a system outage occurs. What should a solutions architect do to meet these requirements?

A��Move the EC2 instances into an Auto Scaling group. Create an Amazon Event bridge (Amazon Cloud watch Events) rule to target an Amazon Elastic Container Service (Amazon ECS) task.

B ��Move the EC2 instances into an Auto Scaling group behind an Application Load Balancer (ALB). Update the order system to send messages to the ALB endpoint.

C��Move the EC2 instances into an Auto Scaling group. Configure the order system to send messages to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the EC2 instances to consume messages from the queue.

D �� Create  an Amazon  Simple  Notification  Service  (Amazon  SNS) topic.  Create an AWS  Lambda function,  and subscribe the function to the SNS topic. Configure the order system to send messages to the SNS topic. Send a command to the EC2 instances to process the messages by using AWS Systems Manager Run Command.

�𰸣�C

������ Correct Answer: C. Move the EC2 instances into an Auto Scaling group. Configure the order system to send messages to    an Amazon Simple Queue Service  (Amazon SQS) queue. Configure the  EC2  instances to consume  messages from the queue.    Explanation: The core problem is that orders might be lost during system failures, forcing users to re process them manually. Here �� s why Option C solves this: 1. Amazon SQS acts as a buffer. When the order system sends messages to SQS, they ��re stored    durably (even if EC2 instances crash). 2. Auto Scaling ensures EC2 instances automatically restart/replace failed instances. New      instances will  resume  processing  messages from SQS. 3.  Message  retention: SQS  keeps  messages until they �� re  successfully    processed and deleted. Even if all EC2 instances go down temporarily, no orders are lost. Other options fail because: - A/B rely on    direct  requests (ALB/ ECS) without message  persistence.  If processing fails  mid-way, the order is  lost. -  D uses SNS (pub/sub),   which doesn ��t guarantee  message  retention if Lambda/EC2 fails. Systems  Manager  Run Command requires instances to be    online.   Reference   Link:   [Amazon   SQS    Features](https://aws.amazon.com/sqs/features/)    (Durable   messaging,    retention,    auto-retry)

170.A company runs an application on a large fleet of Amazon EC2 instances. The application reads and writes entries into an Amazon Dynamo db table. The size of the Dynamo db table continuously grows, but the application needs only data from the last
30 days. The company needs a solution that minimizes cost and development effort. Which solution meets these requirements?

A ��Use an AWS Cloud formation template to deploy the complete solution. Redeploy the Cloud formation stack every 30 days, and delete the original stack.

B ��Use an EC2 instance that runs a monitoring application from AWS Marketplace. Configure the monitoring application to use Amazon Dynamo db Streams to store the timestamp when a new item is created in the table. Use a script that runs on the EC2 instance to delete items that have a timestamp that is older than 30 days.

C ��Configure Amazon Dynamo db Streams to invoke an AWS Lambda function when a new item is created in the table. Configure the Lambda function to delete items in the table that are older than 30 days.

D ��Extend the application to add an attribute that has a value of the current timestamp plus 30 days to each new item that is

created in the table. Configure Dynamo db to use the attribute as the TTL attribute.

�𰸣�D

������ Correct Answer D Detailed Explanation The best solution is option D because it uses Dynamo db's built - in Time to Live (TTL) feature. TTL automatically deletes items that are older than a specified timestamp, requiring minimal development effort and no additional infrastructure costs. Here's why it works: 1. How TTL Works: When you enable TTL on a Dynamo db table, you specify an attribute that stores a timestamp. Dynamo db automatically checks and deletes items where this timestamp is older than the current time. This process is fully managed by AWS, so there's no need for scripts, EC2 instances, or Lambda functions. 2. Minimal Development Effort: The application only needs to add an attribute (e.g., `expiration_time`) to each new item with a value of "current timestamp + 30 days." No complex code or additional services are required. 3. Cost - Effective: TTL incurs no extra costs beyond standard Dynamo db write/read operations. Unlike options B or C (which require running EC2 instances or Lambda functions), TTL operates server lessly within Dynamo db. 4. Why Other Options Are Less Ideal: - Option A: Redeploying Cloud formation stacks every 30 days is impractical and risky (e.g., accidental data deletion). - Option B: Running a dedicated EC2 instance for cleanup  introduces ongoing costs and  maintenance. - Option C: While serverless,  using  Lambda and  Dynamo db Streams adds complexity (writing/deploying code) and minor costs compared to TTL. Reference Link [Amazon Dynamo db Time to Live (TTL)](https://docs.aws.amazon.com/amazon dynamo db/latest/developer guide/TTL.html)

171.A company runs a containerized application on a Ku bernet es cluster in an on-premises data center. The company is using a MongoDB database for data storage. The company wants to migrate some of these environments to AWS, but no code changes or deployment method changes are possible at this time. The company needs a solution that minimizes operational overhead. Which solution meets these requirements?

A ��Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes for compute and MongoDB on EC2 for data storage.

B ��Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute and Amazon Dynamo db for data storage

C ��Use Amazon Elastic Ku bernet es Service (Amazon EKS) with Amazon EC2 worker nodes for compute and Amazon Dynamo db for data storage.

D �� Use  Amazon  Elastic  Ku bernet es  Service  (Amazon  EKS)  with  AWS  Fargate  for  compute  and  Amazon  Document db  (with MongoDB compatibility) for data storage.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The company's requirements are to migrate their existing Ku bernet es (container) setup and MongoDB database to AWS without any code or deployment changes, while minimizing operational overhead. Here's why option D is the best choice:  1. Amazon EKS with AWS Fargate: - EKS maintains Ku bernet es compatibility (no deployment changes needed). - Fargate handles serverless container execution (no EC2 management = low operational overhead). 2. Amazon Document db: - Fully managed MongoDB-compatible database service. - Requires no code changes to migrate from MongoDB. - Handles  maintenance/backups  automatically.  Other  options  fail  because:  -  A/C:  Require  EC2  management  (high  operational overhead) - B/C: Use Dynamo db (not MongoDB-compatible, requires code changes) - A/B: Use ECS instead of Ku bernet es (would require  deployment  changes)  Reference  Links:  -  [Amazon  EKS  with  Fargate](https://aws.amazon.com/eks/features/fargate/)  - [Amazon Document db MongoDB compatibility](https://aws.amazon.com/document db/mongodb-compatibility/)

172.A telemarketing company is designing its customer call center functionality on AWS. The company needs a solution that provides multiple speaker recognition and generates transcript files. The company wants to query the transcript files to analyze the  business  patterns. The transcript  files  must  be  stored for 7 years for  auditing  purposes. Which  solution will  meet these

requirements?

A �� Use Amazon  Re k ognition for  multiple speaker recognition. Store the transcript files in Amazon S3.  Use  machine  learning models for transcript file analysis.

B ��Use Amazon Transcribe for multiple speaker recognition. Use Amazon Athena for transcript file analysis.

C �� Use Amazon Translate for multiple speaker recognition. Store the transcript files in Amazon Redshift. Use SQL queries for transcript file analysis.

D ��Use Amazon Re k ognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use Amazon T extract for transcript file analysis.

�𰸣�B

�� �� ��  Correct  Answer:  B  Detailed  Explanation:  The  correct  answer  is  B  because:  1.  Amazon  Transcribe  is  designed  for speech-to-text   conversion   and   supports   speaker   dia rization   (identifying   multiple   speakers   in   audio),   which   meets   the requirement for recognizing multiple speakers. 2. Amazon S3 (implied in the answer) is the best storage solution for transcript files. It ��s cost-effective, durable, and supports lifecycle policies to retain files for 7 years (e.g., using S3 Glacier for long-term storage). 3. Amazon Athena allows querying transcript files directly in S3 using SQL, making it easy to analyze business patterns without needing complex machine learning models. Other options are incorrect: - A   �� D: Amazon Re k ognition (for image/video analysis) and T extract (for document text extraction) don ��t handle speech or speaker recognition. - C: Amazon Translate is for language  translation,  not  speech-to-text,  and   Redshift   is  unsuitable  for  raw  text  file  storage.   Reference   Links:  -   [Amazon Transcribe Speaker  Dia rization](https://docs.aws.amazon.com/transcribe/latest/dg/speaker-dia rization.html) -  [Amazon Athena Query                       S3](https://aws.amazon.com/athena/)                        -                       [Amazon                        S3                        Lifecycle
Policies](https://docs.aws.amazon.com/AmazonS3/latest/user guide/object-lifecycle-mgmt.html)

173.A company hosts its application on AWS. The company uses Amazon Cognito to manage users. When users log in to the application, the application fetches required data from Amazon Dynamo db by using a REST API that is hosted in Amazon API Gateway. The company wants an AWS managed solution that will control access to the REST API to reduce development efforts. Which solution will meet these requirements with the LEAST operational overhead?

A ��Configure an AWS Lambda function to be an authorizer in API Gateway to validate which user made the request.

B ��For each user, create and assign an API key that must be sent with each request. Validate the key by using an AWS Lambda function.

C ��Send the user ��s email address in the header with every request. Invoke an AWS Lambda function to validate that the user with that email address has proper access.

D ��Configure an Amazon Cognito user pool authorizer in API Gateway to allow Amazon Cognito to validate each request.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The correct answer is D because using an Amazon Cognito user pool authorizer    with API Gateway allows the company to leverage AWS's managed service (Cognito) to validate user requests automatically. Here �� s why: - Least Operational Overhead: Amazon Cognito is fully managed by AWS, so there ��s no need to write or maintain custom     Lambda code for authentication (unlike options A, B, and C). API Gateway natively integrates with Cognito to validate JSON Web     Tokens (JWTs) from authenticated users, ensuring secure, zero-code authorization. - AWS-Managed Solution: Cognito handles

token  generation,  validation,   and  user  session  management,  eliminating  the   need  for  manual  token  checks  or  API   key management (options B and C require custom validation logic). - Security: JWTs signed by Cognito are cryptographically secure, unlike sending email addresses in headers (option C), which can be spoofed. API keys (option B) are static and less secure for user-specific  access.  In  contrast,  options  A,   B,  and  C   require  writing  and  maintaining   Lambda  functions,  which   increases development   effort    and    operational    complexity.    Reference    Link:    [Amazon    API    Gateway    integration   with   Amazon Cognito](https://docs.aws.amazon.com/api gateway/latest/developer guide/api gateway-integrate-with-cognito.html)

174.A company is developing a marketing communications service that targets mobile app users. The company needs to send confirmation messages with Short Message Service (SMS) to its users. The users must be able to reply to the SMS messages. The company must store the responses for a year for analysis. What should a solutions architect do to meet these requirements?

A ��Create an Amazon Connect contact flow to send the SMS messages. Use AWS Lambda to process the responses.

B ��Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.

C��Use Amazon Simple Queue Service (Amazon SQS) to distribute the SMS messages. Use AWS Lambda to process the responses.

D ��Create an Amazon Simple Notification Service (Amazon SNS) FI FO topic. Subscribe an Amazon Kinesis data stream to the SNS topic for analysis and archiving.

�𰸣�B

������ Correct Answer: B Detailed Explanation: Amazon Pinpoint is designed for targeted messaging (like SMS campaigns) and can  handle two-way SMS communication, allowing  users to  reply. When  replies  come  in,  Pinpoint  can  send these events to Amazon Kinesis Data Streams.  Kinesis enables real-time data processing and can archive responses to Amazon S3 (via  Kinesis Data Firehose) for long-term storage (1 year retention), which aligns with the requirements. Other options lack native two-way SMS handling or efficient data archiving for analysis. Key reasons: 1. Pinpoint handles SMS sending/receiving natively. 2. Kinesis streams enable real-time processing and durable storage. 3. This combo directly supports replies and 1-year retention without extra                 glue                  code.                  Reference                  Links:                  -                 Amazon                  Pinpoint                  SMS:
https://docs.aws.amazon.com/pinpoint/latest/developer guide/channels-sms.html           -            Kinesis             Data           Streams: https://aws.amazon.com/kinesis/data-streams/

175.A company is planning to move its data to an Amazon S3 bucket. The data must be encrypted when it is stored in the S3 bucket. Additionally, the encryption key must be automatically rotated every year. Which solution will meet these requirements with the LEAST operational overhead?

A �� Move the data to the S3 bucket.  Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3).  Use the built-in key rotation behavior of SSE-S3 encryption keys.

B��Create an AWS Key Management Service (AWS KMS) customer managed key. Enable automatic key rotation. Set the S3 bucket �� s default encryption behavior to use the customer managed KMS key. Move the data to the S3 bucket.

C��Create an AWS Key Management Service (AWS KMS) customer managed key. Set the S3 bucket ��s default encryption behavior to use the customer managed KMS key. Move the data to the S3 bucket. Manually rotate the KMS key every year.

D �� Encrypt the data with customer key material before moving the data to the S3 bucket. Create an AWS  Key Management Service (AWS KMS) key without key material. Import the customer key material into the KMS key. Enable automatic key rotation.

�𰸣�A

������ Correct Answer: B Detailed Explanation: The question requires encrypting data in Amazon S3 with automatic key rotation every year while minimizing operational overhead. Here ��s why Option B is correct: 1. AWS KMS Customer Managed Key: By creating a  customer-managed  key  in  AWS  KMS,  the  company  retains  control  over  the  encryption  key  while  leveraging  AWS services for management. 2. Automatic Key Rotation: AWS KMS supports automatic annual rotation for customer-managed keys. When enabled, KMS generates a new cryptographic key material every year, while retaining old versions to decrypt existing data. This meets the requirement for annual rotation without manual intervention. 3. S3 Default Encryption: Setting the S3 bucket ��s default encryption to use the KMS key ensures all uploaded objects are automatically encrypted with the specified key. Why other options are incorrect: - Option A: SSE-S3 (Amazon S3-managed keys) does not support automatic key rotation. While AWS manages the underlying keys, there ��s  no guarantee of annual rotation, and the rotation process  is  not user-configurable. - Option C: Manual rotation adds operational overhead, violating the "least operational effort" requirement. - Option D: KMS keys with imported external key material cannot use automatic rotation, making this option invalid. Reference Links: - [AWS KMS Key Rotation](https://docs.aws.amazon.com/kms/latest/developer guide/rotate-keys.html)          -           [Amazon           S3           Default Encryption](https://docs.aws.amazon.com/AmazonS3/latest/user guide/default-encryption.html)

176.The  customers  of  a  finance  company  request  appointments  with  financial  advisors  by  sending  text  messages.  A  web application  that  runs  on  Amazon  EC2  instances  accepts  the  appointment  requests.  The  text  messages  are  published  to  an Amazon Simple Queue Service (Amazon SQS) queue through the web application. Another application that runs on EC2 instances then sends  meeting  invitations and meeting confirmation email  messages to the customers. After successful scheduling, this application stores the meeting information in an Amazon Dynamo db database. As the company expands, customers report that their meeting invitations are taking longer to arrive. What should a solutions architect recommend to resolve this issue?

A ��Add a Dynamo db Accelerator (DAX) cluster in front of the Dynamo db database.

B ��Add an Amazon API Gateway API in front of the web application that accepts the appointment requests.

C ��Add an Amazon Cloud front distribution. Set the origin as the web application that accepts the appointment requests.

D ��Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto Scaling group to scale based on the depth of the SQS queue.

�𰸣�D

������ Correct Answer D Explanation The delay in sending meeting invitations occurs because the EC2 instances processing SQS messages can't keep up with increasing demand. Here's why option D is the best solution: - Problem Cause: The SQS queue is likely accumulating more messages than the current EC2 instances can process quickly. - Auto Scaling: By configuring Auto Scaling based on SQS queue depth, AWS automatically adds more EC2 instances when the queue grows (high demand) and removes them when the queue shrinks. This ensures the system scales dynamically to handle workload spikes. - Direct Fix: Scaling the processing application directly addresses the bottleneck (slow message processing), unlike other options that target unrelated components  like  caching  (DAX/Cloud front)  or   request  handling  (API  Gateway).   Reference   Links  -   [AWS  Auto  Scaling  with SQS](https://docs.aws.amazon.com/auto scaling/ec2/user guide/as-using-sqs-queue.html)                         -                         [SQS-Based
Scaling](https://aws.amazon.com/blogs/aws/auto-scaling-based-on-amazon-sqs/)

177.An online retail company has more than 50 million active customers and receives more than 25,000 orders each day. The company collects purchase data for customers and stores this data in Amazon S3. Additional customer data is stored in Amazon RDS. The company wants to make all the data available to various teams so that the teams can perform analytics. The solution must  provide  the  ability  to  manage  fine-grained  permissions  for  the  data  and  must  minimize  operational  overhead.  Which solution will meet these requirements?

A ��Migrate the purchase data to write directly to Amazon RDS. Use RDS access controls to limit access.

B ��Schedule an AWS Lambda function to periodically copy data from Amazon RDS to Amazon S3. Create an AWS Glue crawler. Use Amazon Athena to query the data. Use S3 policies to limit access.

C ��Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation. Use Lake Formation access controls to limit access.

D ��Create an Amazon Redshift cluster. Schedule an AWS Lambda function to periodically copy data from Amazon S3 and Amazon RDS to Amazon Redshift. Use Amazon Redshift access controls to limit access.

�𰸣�C

���� �� Correct Answer C Detailed Explanation The correct solution is C because AWS Lake Formation is designed to simplify	 building and securing data  lakes.  Here's why:  1.  Unified  Data Access -  Lake  Formation can ingest data from both Amazon S3	 (where purchase data is stored) and Amazon RDS (customer data) using an AWS Glue JDBC connection. This avoids manual data	 movement. - It centralizes all data in a "data lake," making it easily accessible for analytics. 2. Fine-Grained Permissions - Lake	 Formation allows granular access controls (e.g., column-level or row-level permissions) across S3 and RDS data, which S3 policies	 or Redshift alone cannot achieve. 3. Minimal Operational Overhead - Lake Formation automates tasks like data cataloging (via	 AWS Glue) and security management, reducing the need for manual scripting or scheduling (unlike options B and D, which rely	 on Lambda functions). Why Other Options Fail - A: Migrating S3 data to RDS would be inefficient and costly for analytics. RDS	 lacks fine-grained permissions for analytics use cases. - B: Using Lambda to copy data adds operational complexity. S3 policies	 alone can ��t enforce granular permissions across combined S3/RDS datasets. - D: Redshift requires frequent data loading and isn �� t ideal for ad-hoc analytics on raw data. Managing Redshift permissions is less flexible than Lake Formation. Reference Links -	 [AWS                    Lake                    Formation](https://aws.amazon.com/lake-formation/)                    -                    [AWS                    Glue
Connections](https://docs.aws.amazon.com/glue/latest/dg/connection-using.html)

178.A company hosts a marketing website in an on-premises data center. The website consists of static documents and runs on a single server. An administrator updates the website content infrequently and uses an SFTP client to upload new documents. The company decides to  host its website on AWS and to  use Amazon Cloud front. The company ��s  solutions architect creates a Cloud front distribution. The solutions architect must design the most cost-effective and resilient architecture for website hosting to serve as the Cloud front origin. Which solution will meet these requirements?

A ��Create a virtual server by using Amazon Light sail. Configure the web server in the Light sail instance. Upload website content by using an SFTP client.

B ��Create an AWS Auto Scaling group for Amazon EC2 instances. Use an Application Load Balancer. Upload website content by using an SFTP client.

C ��Create a private Amazon S3 bucket. Use an S3 bucket policy to allow access from a Cloud front origin access identity (OAI). Upload website content by using the AWS CLI.

D ��Create a public Amazon S3 bucket. Configure AWS Transfer for SFTP. Configure the S3 bucket for website hosting. Upload website content by using the SFTP client.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The best solution is to use a private Amazon S3 bucket with Cloud front Origin

Access  Identity  (OAI)  for  security   and  cost  efficiency.  Here's  why:  1.  Cost-Effective:  S3  is  cheaper  than   running  servers (EC2/Light sail)  for  static  content.  No  compute  costs,  only  storage/data  transfer  fees.  2.  Resilient:  S3  automatically  provides 99.999999999% durability and 99.99% availability, eliminating server maintenance. 3. Security: A private bucket + OAI ensures only  Cloud front  can  access  the  content,  unlike  public  buckets  (Option  D)  which  risk  exposure.  4.  Scalability:  S3  seamlessly handles traffic spikes without infrastructure changes. While Option C uses AWS CLI instead of SFTP, it's still acceptable because: - AWS CLI is free and simple for infrequent uploads - Migrating to modern tools like CLI aligns with cloud best practices - Options with  SFTP  (A/D)  either  add  server  costs  (A)  or  paid  services  (D's  AWS  Transfer)   Reference   Links:  -   [S3  Static  Website Hosting](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Website hosting.html)             -              [Cloud front             OAI Setup](https://docs.aws.amazon.com/Amazon cloud front/latest/Developer guide/private-content-restricting-access-to-s3.html)

179.A company wants to manage Amazon Machine Images (AMIs). The company currently copies AMIs to the same AWS Region where  the  AMIs  were  created.  The  company  needs  to  design  an  application  that  captures  AWS  API  calls  and  sends  alerts whenever the Amazon EC2 Createimage API operation is called within the company ��s account. Which solution will meet these requirements with the LEAST operational overhead?

A ��Create an AWS Lambda function to query AWS Cloud trail logs and to send an alert when a Createimage API call is detected.

B ��Configure AWS Cloud trail with an Amazon Simple Notification Service (Amazon SNS) notification that occurs when updated logs are sent to Amazon S3. Use Amazon Athena to create a new table and to query on Createimage when an API call is detected.

C ��Create an Amazon Event bridge (Amazon Cloud watch Events) rule for the Createimage API call. Configure the target as an Amazon Simple Notification Service (Amazon SNS) topic to send an alert when a Createimage API call is detected.

D ��Configure an Amazon Simple Queue Service (Amazon SQS) FI FO queue as a target for AWS Cloud trail logs. Create an AWS Lambda function to send an alert to an Amazon Simple Notification Service (Amazon SNS) topic when a Createimage API call is detected.

�𰸣�C

�� �� �� Correct Answer:  C.  Create an Amazon  Event bridge  (Amazon  Cloud watch  Events)  rule for the  Createimage API  call.
Configure the target as an Amazon SNS topic to send an alert when a Createimage API call is detected. Detailed Explanation: Amazon Event bridge (formerly Cloud watch Events) is designed to monitor AWS API calls in real - time via AWS Cloud trail. When the Createimage API is called, Cloud trail automatically logs this event, and Event bridge can directly detect it using a predefined event pattern. By creating an Event bridge rule that triggers an Amazon SNS topic, the solution sends alerts immediately without needing  custom  code,  scheduled  queries,  or  manual  log  analysis.  This  approach  minimizes  operational  overhead  because Event bridge natively integrates with Cloud trail and SNS, requiring only configuration steps (no code, tables, or queues). Other options (A, B, D) involve extra components like Lambda functions, Athena queries, or SQS queues, which add complexity and maintenance.                   Reference                   Links:                  -                   [AWS                   Event bridge                   +                   Cloud trail
Integration](https://docs.aws.amazon.com/event bridge/latest/user guide/eb  -  cloud trail.html)  -   [Using  Event bridge  to  Trigger SNS](https://docs.aws.amazon.com/event bridge/latest/user guide/eb - sns.html)

180.A company owns an asynchronous API that is used to ingest user requests and, based on the request type, dispatch requests to the appropriate micro service for processing. The company is using Amazon API Gateway to deploy the API front end, and an AWS  Lambda  function  that  invokes  Amazon  Dynamo db  to  store  user  requests  before  dispatching  them  to  the  processing micro services.  The  company   provisioned  as  much  Dynamo db  throughput   as  its  budget  allows,  but  the  company   is  still experiencing availability issues and is losing user requests. What should a solutions architect do to address this issue without impacting existing users?

A ��Add throttling on the API Gateway with server-side throttling limits.

B ��Use Dynamo db Accelerator (DAX) and Lambda to buffer writes to Dynamo db.

C ��Create a secondary index in Dynamo db for the table with the user requests.

D ��Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to Dynamo db.

�𰸣�D

���� �� Correct Answer D Detailed Explanation The issue arises because the system is losing user requests when Dynamo db cannot handle write operations due to throughput limits.  Here ��s why Option  D is the best solution:  1. Problem with  Direct Writes to Dynamo db: Lambda functions writing directly to Dynamo db risk throttling if the table ��s provisioned throughput is exceeded. This  leads to failed  requests  and  data  loss.  2.  How  SQS  +  Lambda  Helps:  -  Buffering  with  SQS:  Instead  of  writing directly to  Dynamo db, the  Lambda  function  can first send  requests to  an Amazon SQS  queue. SQS  acts  as a  buffer, storing messages even during traffic spikes. - Controlled Processing: A separate Lambda function can read messages from the SQS queue at a controlled rate (within Dynamo db ��s provisioned throughput limits) and write them to Dynamo db. This avoids throttling and ensures no requests are lost. 3. Why Other Options Fail: - A (API Gateway Throttling): Throttling at the API Gateway rejects excess requests, worsening the  problem  (more  lost  requests).  -  B  (Dynamo db  Accelerator -  DAX):  DAX  caches  read  operations,  not writes. It doesn ��t address write throttling. - C (Secondary Index): Secondary indexes consume additional throughput, increasing costs        and         potentially        worsening         throttling.         Reference         Links        -         [Using         Amazon        SQS         with Lambda](https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html)  -   [Dynamo db   Best   Practices:   Using  SQS  to   Buffer Writes](https://docs.aws.amazon.com/amazon dynamo db/latest/developer guide/bp-partition-key-design.html#bp-partition-key- partitioning)

181.A company needs to move data from an Amazon EC2 instance to an Amazon S3 bucket. The company must ensure that no API calls and no data are routed through public internet routes. Only the EC2 instance can have access to upload data to the S3 bucket. Which solution will meet these requirements?

A ��Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is located. Attach a resource policy to the S3 bucket to only allow the EC2 instance ��s IAM role for access.

B ��Create a gateway VPC endpoint for Amazon S3 in the Availability Zone where the EC2 instance is located. Attach appropriate security groups to the endpoint. Attach a resource policy to the S3 bucket to only allow the EC2 instance ��s IAM role for access.

C ��Run t he nslookup tool from inside the EC2 instance to obtain the private IP address of the S3 bucket ��s service API endpoint. Create a route in the VPC route table to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance ��s IAM role for access.

D ��Use the AWS provided, publicly available ip-ranges.json file to obtain the private IP address of the S3 bucket ��s service API endpoint. Create a route in the VPC route table to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance ��s IAM role for access.

�𰸣�A

������ Correct Answer: B Detailed Explanation: To securely transfer data from an EC2 instance to an S3 bucket without using the public internet, you need a Gateway VPC Endpoint (not an Interface Endpoint). Here's why: 1. Gateway VPC Endpoint: - Designed specifically for S3 and Dynamo db. - Routes traffic to S3 through AWS's private network (no public internet). - No security groups needed (Gateway Endpoints work by updating route tables). 2. Why Option B is Correct: - A Gateway Endpoint ensures private connectivity. - While Gateway Endpoints don't use security groups, the mention in the option might be misleading, but the core

setup  (Gateway  Endpoint + S3  bucket  policy)  is  valid. - The  S3  bucket  policy  restricts  access to the  EC2  instance's  IAM  role, ensuring  only  authorized  access.   3.  Why  Option  A   is   Incorrect:  -   Interface   Endpoints  (used  for   most  AWS  services)   are unnecessary for S3. - Interface Endpoints incur costs and add complexity, while Gateway Endpoints are free and simpler for S3. Key Take away: Use a Gateway VPC Endpoint for S3 to keep traffic private, and lock down the S3 bucket with IAM policies. Ignore the security group mention in Option B��it ��s a distractor; the critical part is using the correct endpoint type. Reference Links: - [AWS  VPC  Endpoints  for  S3](https://docs.aws.amazon.com/AmazonS3/latest/user guide/private link-interface-endpoints.html)  - [S3 Bucket Policies](https://docs.aws.amazon.com/AmazonS3/latest/user guide/using-iam-policies.html)

182.A solutions architect is designing the architecture of a new application being deployed to the AWS Cloud. The application will run on Amazon EC2 On-Demand Instances and will automatically scale across multiple Availability Zones. The EC2 instances will scale up and down frequently throughout the day. An Application  Load  Balancer  (ALB) will  handle the  load distribution. The architecture needs to support distributed session data management. The company is willing to make changes to code if needed. What should the solutions architect do to ensure that the architecture supports distributed session data management?

A ��Use Amazon Elastic ache to manage and store session data.

B ��Use session affinity (sticky sessions) of the ALB to manage session data.

C ��Use Session Manager from AWS Systems Manager to manage the session.

D ��Use the Get session token API operation in AWS Security Token Service (AWS STS) to manage the session.

�𰸣�A

������ Correct Answer: A Detailed Explanation: To handle distributed session data management in a scalable AWS environment where EC2 instances frequently scale up/down across Availability Zones, Amazon Elastic ache (option A) is the best choice. Here's why:  1.  Centralized  Session  Storage:  Elastic ache  (using   Redis  or   Memcached)  stores  session  data  in  a  shared,  in-memory database. This allows any  EC2  instance to access the session data, even  if the original  instance is terminated during scaling. Storing sessions  locally on an  instance (e.g.,  using sticky sessions)  risks data loss when  instances scale down. 2. Auto Scaling Compatibility: Since the application uses On-Demand Instances that scale frequently, sticky sessions (option B) would fail if an instance holding a session is terminated.  Elastic ache decouples session data from individual instances, ensuring continuity. 3. Why Other Options  Fail: - Option B (Sticky Sessions): Ties  users to specific instances, which breaks during scaling. - Option C (Session Manager): Manages server access for admins, not application user sessions. - Option D (STS Get session token): Provides temporary security credentials, unrelated to application sessions. 4. Code Changes: The company must update the application to store/retrieve session data from Elastic ache instead of local instance storage. This is a common and manageable adjustment for distributed  systems.  Reference  Links:  -  [Amazon  Elastic ache  Use  Cases](https://aws.amazon.com/elastic ache/use-cases/)  - [Session                                                            Management                                                             for                                                             Auto
Scaling](https://docs.aws.amazon.com/white papers/latest/session-manager-intro/distributed-session-management.html)

183.A company offers a food delivery service that is growing rapidly. Because of the growth, the company ��s order processing system is experiencing scaling problems during peak traffic hours. The current architecture includes the following:   ? A group of Amazon EC2 instances that run in an Amazon EC2 Auto Scaling group to collect orders from the application   ? Another group of EC2 instances that run in an Amazon EC2 Auto Scaling group to fulfill orders The order collection process occurs quickly, but the order fulfillment process can take longer. Data must not be lost because of a scaling event. A solutions architect must ensure that the order collection process and the order fulfillment process can both scale properly during peak traffic hours. The solution must optimize utilization of the company ��s AWS resources. Which solution meets these requirements?

A��Use Amazon Cloud watch metrics to monitor the CPU of each instance in the Auto Scaling groups. Configure each Auto Scaling group ��s minimum capacity according to peak workload values.

B ��Use Amazon Cloud watch metrics to monitor the CPU of each instance in the Auto Scaling groups. Configure a Cloud watch alarm  to  invoke  an  Amazon  Simple  Notification  Service  (Amazon  SNS)  topic  that  creates  additional  Auto  Scaling  groups  on demand.

C��Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to poll their respective queue. Scale the Auto Scaling groups based on notifications that the queues send.

D��Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to poll their respective queue. Create a metric based on a backlog per instance calculation. Scale the Auto Scaling groups based on this metric.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The key here is to decouple the order collection and fulfillment processes while ensuring no data loss during scaling. Using two SQS queues separates the workloads, allowing each part to scale independently. Why D is correct: - SQS queues act as buffers, storing orders safely even if fulfillment instances are busy/scaling. - The "backlog per instance" metric (e.g., Approximate number of Messages visible divided by instance count) directly measures how much work each instance is handling. This is more precise than generic CPU metrics for this workflow. - Auto Scaling can then add fulfillment instances when backlog per instance is high, and remove them when low, optimizing resource usage. Why not others: A) CPU metrics don't directly reflect order processing workload B) Creating new Auto Scaling groups adds unnecessary complexity C) SQS notifications              alone              don't               provide              granular              scaling              metrics               Reference              link: https://docs.aws.amazon.com/auto scaling/ec2/user guide/as-using-sqs-queue.html

184.A company hosts multiple  production applications. One of the applications consists of resources from Amazon EC2, AWS Lambda, Amazon  RDS, Amazon Simple  Notification  Service  (Amazon SNS), and Amazon Simple  Queue Service  (Amazon SQS) across multiple AWS Regions. All company resources are tagged with a tag name of ��application �� and a value that corresponds to each application. A solutions architect must provide the quickest solution for identifying all of the tagged components. Which solution meets these requirements?

A ��Use AWS Cloud trail to generate a list of resources with the application tag.

B ��Use the AWS CLI to query each service across all Regions to report the tagged components.

C ��Run a query in Amazon Cloud watch Logs Insights to report on the components with the application tag.

D ��Run a query with the AWS Resource Groups Tag Editor to report on the resources globally with the application tag.

�𰸣�D

���� �� Correct Answer:  D  Detailed  Explanation: The AWS  Resource Groups Tag  Editor  is specifically designed to search for resources across all AWS services and  regions  based on their tags. This  makes it the quickest and most efficient solution for identifying all resources tagged with "application". - Option A (Cloud trail) tracks API activity but isn't built for real-time resource tagging queries. - Option B (AWS CLI) would require manually querying every service in every region, which is time-consuming. - Option C (Cloud watch Logs Insights) analyzes logs, not resource metadata like tags. - Option D (Tag Editor) directly solves the problem     by     enabling      a     global,     tag-based     search      in     one      step.     Reference:      [AWS     Resource     Groups      Tag Editor](https://docs.aws.amazon.com/ARG/latest/user guide/tag-editor.html)

185.A company needs to export its database once a day to Amazon S3 for other teams to access. The exported object size varies between  2 GB and 5 GB. The S3 access  pattern for the data  is variable and changes  rapidly. The  data  must  be  immediately available and  must  remain accessible for  up to 3  months. The  company  needs the  most  cost-effective solution that will  not increase retrieval time. Which S3 storage class should the company use to meet these requirements?

A ��S3 Intelligent-Tiering

B ��S3 Glacier Instant Retrieval C ��S3 Standard

D ��S3 Standard-Infrequent Access (S3 Standard-IA)

�𰸣�A

�� �� ��  Correct  Answer:  A.  S3  Intelligent-Tiering  Explanation:  The  company  needs  a  cost-effective  solution  for  data  with unpredictable  access  patterns  while  ensuring  immediate  availability.  Here's  why  S3  Intelligent-Tiering  is  the  best  choice:  1. Automatic Cost Optimization: S3 Intelligent-Tiering automatically moves objects between Frequent Access and Infrequent Access tiers  based on  usage. This avoids overpaying for  underutilized data (like S3 Standard) or incurring  high  retrieval fees  (like S3 Standard-IA)  if  access  suddenly  spikes.  2.  Immediate  Availability:   Unlike  Glacier   Instant   Retrieval  (which  is  for  archives), Intelligent-Tiering ��s Frequent Access tier provides the same low-latency performance as S3 Standard, meeting the "immediately available" requirement. 3. No Retrieval Fees: S3 Intelligent-Tiering has no retrieval charges, unlike S3 Standard-IA or Glacier. This is critical for rapidly changing access patterns where frequent retrievals could inflate costs. 4. 3-Month Retention: Since the data is stored for up to 3 months, Intelligent-Tiering ��s 30-day auto-tiering window won��t cause issues. Objects accessed at least once in 30 days stay in the Frequent tier; others move to Infrequent (still millisecond access). Alternatives like S3 Standard (C) are too expensive for variable access, S3 Standard-IA (D) adds retrieval fees, and Glacier (B) isn ��t designed for rapid access changes. Intelligent-Tiering        balances       cost        and        performance        dynamically.        Reference        Links:        -        [Amazon        S3 Intelligent-Tiering](https://aws.amazon.com/s3/storage-classes/intelligent-tiering/)            -             [S3            Storage             Classes Comparison](https://aws.amazon.com/s3/storage-classes/)

186.A company is developing a new mobile app. The company must implement proper traffic filtering to protect its Application Load  Balancer (ALB) against common application-level attacks, such as cross-site scripting or SQL injection. The company has minimal infrastructure and operational staff. The company needs to reduce its share of the responsibility in managing, updating, and securing servers for its AWS environment. What should a solutions architect recommend to meet these requirements?

A ��Configure AWS WAF rules and associate them with the ALB.

B ��Deploy the application using Amazon S3 with public hosting enabled.

C ��Deploy AWS Shield Advanced and add the ALB as a protected resource.

D��Create a new ALB that directs traffic to an Amazon EC2 instance running a third-party firewall, which then passes the traffic to the current ALB.

�𰸣�A

���� �� Correct Answer: A Detailed Explanation: The best solution is to use AWS WAF (Web Application Firewall) because it directly  protects the ALB against common application-level attacks  like SQL injection and cross-site scripting (XSS). AWS WAF allows the company to define customizable rules to filter malicious traffic without managing servers or infrastructure. Since AWS

WAF is a fully managed service, it reduces operational complexity, aligning with the company's goal to minimize management responsibilities. Other options, like AWS Shield Advanced or a third-party firewall on EC2, either address different threats (DDoS) or add unnecessary complexity. Public S3 hosting is irrelevant here, as the app already uses an ALB. Reference Links: - [AWS WAF
Overview](https://aws.amazon.com/waf/)                      -                       [Protecting                      ALBs                       with                      AWS
WAF](https://docs.aws.amazon.com/waf/latest/developer guide/web-acl-alb-usage.html)

187.A company ��s reporting system delivers hundreds of .csv files to an Amazon S3 bucket each day. The company must convert these files to Apache  Parquet format and  must store the files  in a transformed data  bucket. Which solution will  meet these requirements with the LEAST development effort?

A ��Create an Amazon EMR cluster with Apache Spark installed. Write a Spark application to transform the data. Use EMR File System (EMRFS) to write files to the transformed data bucket.

B ��Create an AWS Glue crawler to discover the data. Create an AWS Glue extract, transform, and load (ETL) job to transform the data. Specify the transformed data bucket in the output step.

C ��Use AWS Batch to create a job definition with Bash syntax to transform the data and output the data to the transformed data bucket. Use the job definition to submit a job. Specify an array job as the job type.

D ��Create an AWS Lambda function to transform the data and output the data to the transformed data bucket. Configure an event notification for the S3 bucket. Specify the Lambda function as the destination for the event notification.

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: AWS Glue is a fully managed ETL (Extract, Transform, Load) service designed specifically  for  data   preparation  tasks   like  converting  file  formats.   Here's   why  Option   B   requires  the   least  effort:  1.  No Infrastructure Management: AWS Glue is serverless, so you don't need to provision clusters (unlike EMR in Option A) or manage Lambda functions (Option D). 2. Built-in Format Support: AWS Glue natively supports converting between formats like CSV and Parquet  through  simple  configuration,  eliminating  the  need  to  write  custom  conversion  code  (required  in  Lambda/Docker solutions). 3. Automated Schema Discovery: The Glue crawler automatically detects the CSV file schema, reducing manual work compared to other options where you'd need to define schemas programmatically. 4. Visual ETL Editor: Glue Studio provides a low-code interface to configure transformations without writing Spark/Scala code (unlike Option A's custom Spark application). Options like Lambda (D) would require writing Python code for conversion and handling Lambda's 15-minute timeout, while AWS Batch  (C)  would  need  custom  Bash/Python  scripts  and  container  management.  Glue  (B)  achieves  the  same  result  through configuration-driven workflows.

188.A company has 700 TB of backup data stored in network attached storage (NAS) in its data center. This backup data need to be accessible for infrequent regulatory requests and must be retained 7 years. The company has decided to migrate this backup data from its data center to AWS. The migration must be complete within 1 month. The company has 500 Mbps of dedicated bandwidth on its public internet connection available for data transfer. What should a solutions architect do to migrate and store the data at the LOWEST cost?

A ��Order AWS Snowball devices to transfer the data. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.

B ��Deploy a VPN connection between the data center and Amazon VPC. Use the AWS CLI to copy the data from on premises to Amazon S3 Glacier.

C ��Provision a 500 Mb ps AWS Direct Connect connection and transfer the data to Amazon S3. Use a lifecycle policy to transition

the files to Amazon S3 Glacier Deep Archive.

D��Use AWS Data sync to transfer the data and deploy a Data sync agent on premises. Use the Data sync task to copy files from the on-premises NAS storage to Amazon S3 Glacier.

�𰸣�A

���� �� Correct Answer: A Detailed Explanation: The best solution here is A: Use AWS Snowball with S3 Glacier Deep Archive lifecycle  policy.  Here's why:  1.  Data Transfer Challenge: - 700TB over 500Mbps  internet would take ~154 days (calculated via [AWS  Data  Transfer  Calculator](https://aws.amazon.com/snowball/resources/calculator/)),  exceeding  the  1-month  deadline.  - Snowball devices can physically transfer 80TB each,  needing ~9 devices (700TB   �� 80TB).  AWS  can ship  multiple devices  in parallel, easily completing within 1 month. 2. Cost Optimization: - Snowball has a fixed $$300/device fee (much cheaper than 154 days of internet transfer costs). - S3 Glacier Deep Archive ($$0.00099/GB - month) is AWS's cheapest storage for 7 - year retention. 3. Why Other Options Fail: - B (VPN + CLI): 500Mbps internet is too slow (same 154 - day math). Glacier has higher API costs for direct uploads. - C (Direct Connect): Same bandwidth bottleneck as B. Direct Connect has $$0.03/GB data - out fees + $$500/month port charge. - D (Data sync): While Data sync accelerates transfers, 500Mbps still can't move 700TB in  1 month. Data sync      can't       write       directly       to       Glacier       (needs       S3       first).       Reference        Links:      -        [AWS       Snowball Pricing](https://aws.amazon.com/snowball/pricing/) - [S3 Storage Classes](https://aws.amazon.com/s3/storage-classes/) - [Data Transfer Time Calculator](https://aws.amazon.com/snowball/resources/calculator/)

189.A company has a serverless website with millions of objects in an Amazon S3 bucket. The company uses the S3 bucket as the origin for an Amazon Cloud front distribution. The company did not set encryption on the S3 bucket before the objects were loaded. A solutions architect  needs to enable encryption for all existing objects and for all objects that are added to the S3 bucket in the future. Which solution will meet these requirements with the LEAST amount of effort?

A �� Create a new S3 bucket. Turn on the default encryption settings for the new S3 bucket.  Download all existing objects to temporary local storage. Upload the objects to the new S3 bucket.

B ��Turn on the default encryption settings for the S3 bucket. Use the S3  Inventory feature to create a  .csv file that lists the unencrypted objects. Run an S3 Batch Operations job that uses the copy command to encrypt those objects.

C ��Create a new encryption key by using AWS Key Management Service (AWS KMS). Change the settings on the S3 bucket to use server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Turn on versioning for the S3 bucket.

D��Navigate to Amazon S3 in the AWS Management Console. Browse the S3 bucket ��s objects. Sort by the encryption field. Select each  unencrypted object.  Use the  Modify  button to apply default encryption settings to every  unencrypted object  in the S3 bucket.

�𰸣�B

���� �� Correct Answer:  B  Detailed  Explanation: The  best solution  is to enable default encryption for the S3  bucket (which
automatically encrypts new objects) and use S3 Batch Operations to encrypt existing objects. Here's why: 1. Default Encryption: Enabling this ensures all new objects uploaded to the bucket are automatically encrypted. No code or manual effort is needed for future uploads. 2. S3 Inventory + Batch Operations: - S3 Inventory generates a list (CSV file) of all unencrypted objects. - S3 Batch Operations processes this CSV file to copy (re-upload) each object back to the same bucket with encryption applied. This is done  automatically  at  scale,  avoiding  manual  work  for  millions  of  objects.  Other  options  are  worse:  -  A  requires  manually re uploading  all  objects   (impractical  for  millions).  -  C  only  handles  future  objects  (existing  ones  stay  unencrypted).  -   D  is impossible       to       do       manually       for       millions       of        objects.       Reference       Links:       -        [Amazon       S3       Default Encryption](https://docs.aws.amazon.com/AmazonS3/latest/user guide/bucket-encryption.html)              -              [S3               Batch

Operations](https://docs.aws.amazon.com/AmazonS3/latest/user guide/batch-ops.html)

190.A company runs a global web application on Amazon EC2 instances behind an Application Load Balancer. The application stores data in Amazon Aurora. The company needs to create a disaster recovery solution and can tolerate up to 30 minutes of downtime and potential data loss. The solution does not need to handle the load when the primary infrastructure is healthy. What should a solutions architect do to meet these requirements?

A ��Deploy the application with the required infrastructure elements in place. Use Amazon Route 53 to configure active-passive failover. Create an Aurora Replica in a second AWS Region.

B ��Host a scaled-down deployment of the application in a second AWS Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora Replica in the second Region.

C ��Replicate the primary infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora database that is restored from the latest snapshot.

D ��Back up data with AWS Backup. Use the backup to create the required infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-passive failover. Create an Aurora second primary instance in the second Region.

�𰸣�A

���� �� Correct Answer: A Detailed Explanation: The company needs a disaster recovery (DR) solution that tolerates up to 30 minutes of downtime and potential data loss. This aligns with a "warm standby" strategy, where a scaled-down but functional copy of the infrastructure exists in a secondary AWS Region. Here ��s why Option A is the best choice: 1. Active-Passive Failover with Route 53: - Route 53 ��s active-passive routing ensures traffic goes to the primary Region by default. If the primary Region fails (detected via health checks), Route 53 automatically reroutes traffic to the passive standby environment in the secondary Region. This meets the requirement to avoid handling load when the primary is healthy. 2. Aurora Replica in a Second Region: - Creating an Aurora  Replica  in the secondary  Region  enables  asynchronous cross-Region  replication. This  minimizes  data  loss (since replication is near real-time) and allows fast promotion of the replica to a primary database during failover. While some data loss  might occur (due to replication lag), it ��s  acceptable  per the requirements. 3. Why Other Options  Fail: - Option  B (active-active) would distribute traffic to both Regions, violating the "no load handling when primary is healthy" requirement. - Option C (active-active + restored snapshot) introduces longer recovery times (restoring snapshots takes time) and higher data loss  (snapshots  are  periodic).  -  Option  D  (AWS  Backup  +  manual  restore)  would  exceed  the  30-minute  downtime  limit,  as restoring      backups      and      rebuilding       infrastructure      is      slow.       Reference      Links:      -       [Amazon      Aurora      Global Database](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/aurora-global-database.html)      -      [Route       53 Active-Passive Failover](https://docs.aws.amazon.com/Route53/latest/Developer guide/dns-failover-types.html)

191.A company ��s application is having performance issues. The application is stateful and needs to complete in-memory tasks on Amazon EC2 instances. The company used AWS Cloud formation to deploy infrastructure and used the M5 EC2 instance family. As traffic increased, the application performance degraded. Users are  reporting delays when the users attempt to access the application. Which solution will resolve these issues in the MOST operationally efficient way?

A �� Replace the EC2 instances with T3  EC2 instances that run in an Auto Scaling group. Make the changes by using the AWS Management Console.

B��Modify the Cloud formation templates to run the EC2 instances in an Auto Scaling group. Increase the desired capacity and the maximum capacity of the Auto Scaling group manually when an increase is necessary.

C��Modify the Cloud formation templates. Replace the EC2 instances with R5 EC2 instances. Use Amazon Cloud watch built-in EC2

memory metrics to track the application performance for future capacity planning.

D �� Modify the Cloud formation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the Amazon Cloud watch agent on the EC2 instances to generate custom application latency metrics for future capacity planning.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The application's performance issues are likely due to insufficient memory (since M5 instances are general-purpose, and R5 is memory-optimized). Here ��s why D is the best choice: 1. Switch to R5 Instances: The R5 instance family is optimized for memory-intensive workloads. Since the application is stateful and relies on in-memory tasks, upgrading to R5 directly addresses the root cause (memory bottlenecks). 2. Custom Metrics via Cloud watch Agent: The built-in Cloud watch EC2 memory metrics (used in option C) only track system-level memory usage. However, the application might have unique latency patterns or application-specific memory requirements. Deploying the Cloud watch agent allows the company to collect  custom  metrics  (e.g.,  application  latency,  in-memory  cache  usage)  for  precise  monitoring  and  scaling  decisions.  3. Operational  Efficiency: Option  D enables automated, data-driven capacity planning using custom  metrics. This avoids manual scaling (as in option B) and ensures future scaling aligns with actual application behavior. Options A (T3 instances) are burstable and  unsuitable for  sustained  memory  workloads.  Option  C  lacks  custom  metrics,  which  are  critical  for  stateful  applications. Reference   Links:   -    [Amazon   EC2    Instance   Types](https://aws.amazon.com/ec2/instance-types/)   -    [Custom   Metrics   with Cloud watch Agent](https://docs.aws.amazon.com/Amazon cloud watch/latest/monitoring/Install-Cloud watch-Agent.html)

192.A solutions architect is designing a new API using Amazon API Gateway that will receive requests from users. The volume of requests  is  highly  variable;  several  hours  can  pass  without  receiving  a  single  request.  The  data  processing  will  take  place asynchronously, but should  be completed within  a few seconds after  a  request  is  made.  Which  compute service  should the solutions architect have the API invoke to deliver the requirements at the lowest cost?

A ��An AWS Glue job

B ��An AWS Lambda function

C ��A containerized service hosted in Amazon Elastic Ku bernet es Service (Amazon EKS)

D ��A containerized service hosted in Amazon ECS with Amazon EC2

�𰸣�B

������Correct Answer B ��An AWS Lambda function Detailed Explanation AWS Lambda is the most cost - effective choice for this scenario because: 1. Pay - per - Use Pricing: Lambda charges only when the function runs. Since the API might go hours without requests, Lambda avoids costs during idle periods (unlike EC2/EKS/ECS, which require paying for running servers). 2. Automatic Scaling:  Lambda instantly scales to handle unpredictable traffic spikes without manual configuration. 3.  Fast Execution: While Lambda has a "cold start" delay (~100ms - 2s), the problem states processing should complete within "a few seconds," which Lambda  easily  meets  (functions  can  run  up  to  15  minutes).  4.  Async  Processing:   Lambda  natively  supports  asynchronous invocation (e.g., via Event bridge or SQS), decoupling the API response from backend processing. Other options are less optimal: - AWS Glue (A):  Designed for batch  ETL jobs, not real - time APIs. - EKS (C) /  ECS with  EC2 (D):  Require paying for always - on container        hosts,        increasing        costs        for        sporadic        workloads.        Reference        Links         -        [AWS        Lambda Pricing](https://aws.amazon.com/lambda/pricing/)                                -                                  [Asynchronous                                 Lambda
Invocations](https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html)

193.A company runs an application on a group of Amazon Linux EC2 instances. For compliance reasons, the company must retain all application log files for 7 years. The  log files will  be analyzed  by a reporting tool that must  be able to access all the files

concurrently. Which storage solution meets these requirements MOST cost-effectively?

A ��Amazon Elastic Block Store (Amazon EBS)

B ��Amazon Elastic File System (Amazon EFS) C ��Amazon EC2 instance store

D ��Amazon S3

�𰸣�D

���� �� Correct Answer:  D. Amazon S3  Detailed Explanation: Amazon S3 is the most cost-effective solution because it offers durable, scalable object storage designed for long-term retention. Since the logs must be retained for 7 years, S3 ��s lifecycle policies can automatically transition data to lower-cost storage tiers like S3 Glacier Deep Archive, reducing costs significantly over time. S3 also allows concurrent access by multiple reporting tools via its API or services like Amazon Athena. Other options are less suitable: - A. EBS: Designed for block storage attached to a single EC2 instance, not cost-effective for long-term retention or multi-access. -  B.  EFS:  A shared file  system  but  more expensive than S3 for  large-scale,  infrequently  accessed  data. - C.  EC2 Instance Store: Temporary  storage  (data  lost  if  the  instance  stops),  unsuitable  for  compliance.  Reference  Links:  [Amazon  S3 Storage                     Classes](https://aws.amazon.com/s3/storage-classes/)                      [Amazon                      S3                      Lifecycle
Management](https://docs.aws.amazon.com/AmazonS3/latest/user guide/object-lifecycle-mgmt.html)

194.A company has hired an external vendor to perform work in the company ��s AWS account. The vendor uses an automated tool that is hosted in an AWS account that the vendor owns. The vendor does not have IAM access to the company ��s AWS account. How should a solutions architect grant this access to the vendor?

A ��Create an IAM role in the company ��s account to delegate access to the vendor ��s IAM role. Attach the appropriate IAM policies to t he role for the permissions that the vendor requires.

B��Create an IAM user in the company ��s account with a password that meets the password complexity requirements. Attach the appropriate IAM policies to the user for the permissions that the vendor requires.

C ��Create an IAM group in the company ��s account. Add the tool ��s IAM user from the vendor account to the group. Attach the appropriate IAM policies to the group for the permissions that the vendor requires.

D ��Create a new identity provider by choosing  ��AWS account �� as the provider type in the IAM console. Supply the vendor ��s AWS account ID and user name. Attach the appropriate IAM policies to the new provider for the permissions that the vendor requires.

�𰸣�A

������ Correct Answer A Detailed Explanation The best solution is to create an IAM role in the company ��s AWS account that allows the vendor ��s AWS account (and its IAM roles/users) to assume this role. This is done by configuring the role ��s trust policy to specify the vendor ��s AWS account ID. Here ��s why this works: 1. No shared credentials: The vendor doesn ��t need a password or access key (unlike Option B, which creates an IAM user). 2. Cross-account access: The vendor ��s automated tool (in their own AWS account) can assume the IAM role in the company ��s account temporarily. This is safer than Option C (groups can ��t span AWS accounts) or Option D (misuse of identity providers). 3. Least privilege: Attach IAM policies to t he role to grant only the permissions the vendor needs.  Example: - Company Account ID: `123456789012` - Vendor Account ID: `987654321098` - The role �� s trust  policy would  include: ```json  {  "Version":  "2012-10-17",  "Statement":  [{  "Effect":  "Allow",  "Principal":  {  "AWS":

"arn:aws:iam::987654321098:root"  },  "Action" :  "sts:Assume role"  }]  } ```  Reference  Links  -  [AWS  IAM  Roles  for  Cross-Account Access](https://docs.aws.amazon.com/IAM/latest/User guide/tutorial_cross-account-with-roles.html)            -             [Assume role API](https://docs.aws.amazon.com/STS/latest/Api reference/API_Assume role.html)

195.A media company collects and analyzes user activity data on premises. The company wants to migrate this capability to AWS. The user activity data store will continue to grow and will be petabytes in size. The company needs to build a highly available data ingestion solution that facilitates on-demand analytics of existing data and new data with SQL. Which solution will meet these requirements with the LEAST operational overhead?

A ��Send activity data to an Amazon Kinesis data stream. Configure the stream to deliver the data to an Amazon S3 bucket.

B ��Send activity data to an Amazon Kinesis Data Firehose delivery stream. Configure the stream to deliver the data to an Amazon Redshift cluster.

C �� Place activity data in an Amazon S3 bucket. Configure Amazon S3 to run an AWS Lambda function on the data as the data arrives in the S3 bucket.

D ��Create an ingestion service on Amazon EC2 instances that are spread across multiple Availability Zones. Configure the service to forward data to an Amazon RDS Multi-AZ database.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution here is Option B: Send activity data to an Amazon Kinesis Data Firehose delivery stream and configure it to deliver data to an Amazon Redshift cluster. Here's why: 1. Kinesis Data Firehose: This is a fully managed service that automatically scales to handle data ingestion without requiring manual intervention (like adjusting shards in Kinesis Data Streams). It handles buffering, batching, compression, and encryption, minimizing operational overhead. 2. Amazon Redshift: A petabyte-scale data warehouse optimized for SQL-based analytics. Firehose can automatically load data into Redshift, making it immediately available for querying. While Redshift requires some cluster management, it ��s designed for large-scale analytics and integrates seamlessly with Firehose. 3. Why Other Options Are Less Optimal: - Option A uses Kinesis Data Streams  (requires  manual  shard  management) and S3. While S3  is  scalable,  querying  data  directly via Athena/Redshift Spectrum adds extra steps compared to Redshift ��s native SQL support. - Option C uses S3 and Lambda. Lambda isn ��t ideal for petabyte-scale data due to execution time/memory limits and cost at scale. - Option D uses EC2 (high operational effort) and RDS (not designed for petabyte-scale analytics).  Firehose (fully  managed) +  Redshift (purpose-built for SQL analytics) provides the least operational overhead while meeting scalability, availability, and SQL requirements. Reference Links: - [Amazon Kinesis Data Firehose](https://aws.amazon.com/kinesis/data-firehose/) - [Amazon Redshift](https://aws.amazon.com/redshift/)

196.A company needs to retain its AWS Cloud trail logs for 3 years. The company is enforcing Cloud trail across a set of AWS accounts by using AWS Organizations from the parent account. The Cloud trail target S3 bucket is configured with S3 Versioning enabled. An S3 Lifecycle policy is in place to delete current objects after 3 years. After the fourth year of use of the S3 bucket, the S3 bucket metrics show that the number of objects has continued to rise. However, the number of new Cloud trail logs that are delivered to the S3 bucket has remained consistent. Which solution will delete objects that are older than 3 years in the MOST cost-effective manner?

A ��Configure the organization ��s centralized Cloud trail trail to expire objects after 3 years.

B ��Configure the S3 Lifecycle policy to delete previous versions as well as current versions.

C ��Create an AWS Lambda function to enumerate and delete objects from Amazon S3 that are older than 3 years.

D ��Configure the parent account as the owner of all objects that are delivered to the S3 bucket.

�𰸣�B

������ Correct Answer B �� Configure the S3 Lifecycle policy to delete previous versions as well as current versions. Detailed Explanation When S3 Versioning is enabled, overwriting or deleting an object creates a  new version  instead of  permanently removing  it. The original  problem describes  a  lifecycle  policy that  deletes only the current versions of objects  after 3 years. However, older  non - current versions  (created by Cloud trail  updates or deletions) are  not automatically deleted. Over time, these non - current versions accumulate, causing the bucket ��s object count to grow even if new logs remain consistent. Option B fixes this  by updating the S3  Lifecycle  policy to delete both current and non - current versions after 3 years. This ensures all versions (old and new) are automatically purged, aligning with the retention requirement. - Why not A? Cloud trail itself doesn ��t manage  object  expiration;  this  is  handled  by  S3  lifecycle  policies.  -  Why  not  C?  A  Lambda  function  would  work  but  adds unnecessary complexity and cost compared to S3 ��s built - in lifecycle rules. - Why not D? Object ownership settings don ��t impact lifecycle policies or version cleanup. In short, S3 lifecycle policies are the simplest, most cost - effective way to automate deletion of both current and non - current object versions after a specified period. Reference Links - [S3 Lifecycle for Versioned Objects](https://docs.aws.amazon.com/AmazonS3/latest/user guide/lifecycle  -  configuration  -  examples.html)  -  [S3  Versioning Basics](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Versioning.html)

197.A company has an API that receives real-time data from a fleet of monitoring devices. The API stores this data in an Amazon RDS DB instance for later analysis. The amount of data that the monitoring devices send to the API fluctuates. During periods of heavy traffic, the API often returns timeout errors. After an inspection of the logs, the company determines that the database is not capable of processing the volume of write traffic that comes from the API. A solutions architect must minimize the number of connections to the database and must ensure that data is not lost during periods of heavy traffic. Which solution will meet these requirements?

A ��Increase the size of the DB instance to an instance type that has more available memory.

B ��Modify the DB instance to be a Multi-AZ DB instance. Configure the application to write to all active RDS DB instances.

C �� Modify the API to write  incoming  data to an Amazon Simple Queue Service  (Amazon SQS) queue.  Use  an AWS  Lambda function that Amazon SQS invokes to write data from the queue to the database.

D ��Modify the API to write incoming data to an Amazon Simple Notification Service (Amazon SNS) topic. Use an AWS Lambda function that Amazon SNS invokes to write data from the topic to the database.

�𰸣�C

������ Correct Answer C Explanation The problem here is that the database can't handle high write traffic, leading to timeouts. Option C uses Amazon SQS (Simple Queue Service) as a buffer between the API and the database. Here's why it works: - SQS acts as a buffer: During traffic spikes, the API writes data to the SQS queue instead of directly to the database. This decouples the API from the database, preventing timeouts. - Lambda scales automatically: AWS Lambda can process messages from SQS in parallel, automatically scaling to handle the load. This reduces the number of direct connections to the database (since Lambda manages connections efficiently). -  No data loss: SQS  retains  messages for  up to  14 days by default, ensuring data isn ��t  lost even  if processing is delayed. Other options fail because: - A: Scaling the DB might help temporarily but doesn ��t address connection limits or traffic spikes. - B: Multi - AZ improves availability (not scalability) and still writes directly to the DB. - D: SNS doesn ��t buffer    messages    like    SQS.     If    Lambda    fails,     data    could    be     lost.    Reference     Links    -    [Amazon    SQS     +    Lambda Integration](https://docs.aws.amazon.com/lambda/latest/dg/with                -                sqs.html)                -                 [SQS                vs. SNS](https://aws.amazon.com/sqs/faqs/#:~:text=Amazon%20SNS%20is%20a%20pub/sub%20service%2C%20while%20Amazon% 20SQS%20is%20a%20message%20queueing%20service.)

198.A  company  manages  its  own  Amazon   EC2  instances  that  run   MySQL  databases.  The  company  is  manually  managing replication and scaling as demand  increases or decreases. The company  needs a  new solution that simplifies the  process of adding or removing compute capacity to or from its database tier as needed. The solution also must offer improved performance, scaling, and durability with minimal effort from operations. Which solution meets these requirements?

A ��Migrate the databases to Amazon Aurora Serverless for Aurora MySQL.

B ��Migrate the databases to Amazon Aurora Serverless for Aurora Postgresql.

C ��Combine the databases into one larger MySQL database. Run the larger database on larger EC2 instances.

D ��Create an EC2 Auto Scaling group for the database tier. Migrate the existing databases to the new environment.

�𰸣�A

������ Correct Answer A �� Migrate the databases to Amazon Aurora Serverless for Aurora MySQL. Detailed Explanation The company  is  manually  managing  MySQL  databases  on  EC2  instances,  which  is  time-consuming  and  error-prone. They  need  a solution  that  automates  scaling,  improves  performance,  and  ensures  durability  with  minimal  operational  effort.  -  Why  A  is correct: Amazon Aurora Serverless is a fully managed, auto-scaling database service. It automatically adjusts compute capacity based  on  demand  (scaling  up/down  seamlessly),  eliminating  manual  scaling.  Aurora  MySQL  is  compatible  with  MySQL,  so migration  is  straightforward.  Aurora  also  provides  high  durability  (6  copies  of  data  across  3  Availability  Zones)  and  better performance than standard MySQL. Since the company already uses MySQL, migrating to Aurora MySQL minimizes changes to their application. - Why B is incorrect: While Aurora Serverless for Postgresql also offers auto-scaling, the company uses MySQL. Switching to Postgresql would require significant application changes, which isn��t mentioned in the problem statement. - Why C is incorrect: Consolidating databases into a larger EC2 instance doesn ��t solve the manual scaling issue. Scaling vertically (using larger instances) still requires downtime and manual intervention, and it doesn ��t improve durability or automate scaling. - Why D is incorrect: EC2 Auto Scaling is designed for stateless workloads (e.g., web servers). Databases are stateful, and Auto Scaling won ��t automatically handle replication, failover, or data consistency. This approach would still require manual effort to manage the database  layer.  Reference  Links  -  [Amazon  Aurora  Serverless](https://aws.amazon.com/rds/aurora/serverless/)  -  [Amazon Aurora Features](https://aws.amazon.com/rds/aurora/features/)

199.A company is concerned that two NAT instances in use will no longer be able to support the traffic needed for the company �� s  application.  A  solutions  architect  wants  to  implement  a  solution  that  is  highly  available,  fault  tolerant,  and  automatically  scalable. What should the solutions architect recommend?

A ��Remove the two NAT instances and replace them with two NAT gateways in the same Availability Zone.

B ��Use Auto Scaling groups with Network Load Balancers for the NAT instances in different Availability Zones. C ��Remove the two NAT instances and replace them with two NAT gateways in different Availability Zones.

D ��Replace the two NAT instances with Spot Instances in different Availability Zones and deploy a Network Load Balancer.

�𰸣�C

������ Correct Answer: C Detailed Explanation: NAT Gateway is a managed AWS service that is inherently highly available and scalable within an Availability Zone (AZ). By deploying NAT Gateways in different AZs, the solution ensures fault tolerance across AZs. Unlike NAT instances (which require manual scaling and HA setup), NAT Gateways automatically handle traffic spikes and

provide redundancy within each AZ. Using two NAT Gateways in separate AZs allows resources in each AZ to use their local NAT Gateway, preventing cross-AZ traffic bottlenecks. This setup meets all requirements: high availability (HA), fault tolerance, and automatic                scalability.                 Reference                  Links:                -                  [NAT                Gateway                 vs.                 NAT Instance](https://docs.aws.amazon.com/vpc/latest/user guide/vpc-nat-comparison.html)      -      [High      Availability      for       NAT Gateway](https://docs.aws.amazon.com/vpc/latest/user guide/nat-gateway-scenarios.html)

200.An application runs on an Amazon EC2 instance that has an Elastic IP address in VPC A. The application requires access to a database in VPC B. Both VPCs are in the same AWS account. Which solution will provide the required access MOST securely?

A ��Create a DB instance security group that allows all traffic from the public IP address of the application server in VPC A.

B ��Configure a VPC peering connection between VPC A and VPC B.

C ��Make the DB instance publicly accessible. Assign a public IP address to the DB instance.

D ��Launch an EC2 instance with an Elastic IP address into VPC B. Proxy all requests through the new EC2 instance.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution is to configure a VPC peering connection between VPC A and VPC B. VPC peering allows private, secure communication between resources in t he two VPCs using AWS's internal network (no public internet exposure). Since  both VPCs are in the same AWS account, this setup is simple and secure. - Option A is risky because it relies on the EC2 instance ��s public IP (Elastic IP). If the database ��s security group allows traffic from this public IP, the database is still indirectly exposed to the internet, increasing vulnerability. - Option C is highly insecure �� publicly exposing the database invites potential attacks. - Option D adds unnecessary complexity and cost (managing a proxy server) while still using public  IPs, which  is  less  secure than direct  private  routing via VPC  peering.  Key  Concept: VPC  peering  enables  private,  low  - latency communication between VPCs. It ��s like building a private tunnel between two isolated networks, keeping traffic off the public         internet         and          reducing         exposure         to          threats.         Reference          Links:         -          [VPC         Peering Basics](https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html)          -            [Security           Groups           Best Practices](https://docs.aws.amazon.com/vpc/latest/user guide/VPC_Security groups.html)

201.A company runs demonstration environments for its customers on Amazon EC2 instances. Each environment is isolated in its own  VPC.  The  company �� s  operations  team  needs  to  be  notified  when  RDP  or  SSH  access  to  an  environment  has  been established.

A ��Configure Amazon Cloud watch Application Insights to create AWS Systems Manager Ops items when RDP or SSH access is detected.

B ��Configure the EC2 instances with an IAM instance profile that has an IAM role with the Amazon ssm managed instance core policy attached.

C��Publish VPC flow logs to Amazon Cloud watch Logs. Create required metric filters. Create an Amazon Cloud watch metric alarm with a notification action for when the alarm is in the ALARM state.

D �� Configure  an Amazon  Event bridge  rule  to  listen for  events of type  EC2  Instance  State-change  Notification.  Configure  an Amazon Simple Notification Service (Amazon SNS) topic as a target. Subscribe the operations team to the topic.

������ Correct Answer: C Detailed Explanation: To detect RDP/SSH access to EC2 instances and notify the operations team, VPC Flow  Logs and Cloud watch provide the  most direct solution.  Here ��s  why:  1. VPC  Flow  Logs track  network traffic  in a VPC, including connection attempts on specific ports (e.g., port 22 for SSH, port 3389 for RDP). By publishing these logs to Cloud watch Logs, you can analyze them for specific access patterns. 2. Metric Filters in Cloud watch allow you to search log data for specific patterns  (e.g.,  `REJECT`/`ACCEPT`  events  on  RDP/SSH  ports).  When  a  match  is  found,  Cloud watch  can  trigger  an  alarm.  3. Cloud watch Alarms can send notifications via Amazon SNS when the alarm enters the `ALARM` state (e.g., when RDP/SSH access is detected). The operations team can subscribe to this SNS topic for  real-time alerts. Why other options are  incorrect: - A: Cloud watch Application  Insights  focuses  on  application  performance  monitoring,  not  network-level  access  events.  -  B:  IAM instance profiles enable Systems Manager (SSM) access but do not monitor RDP/SSH connections. - D: EC2 state-change events (e.g.,        instance         start/stop)        do        not         track        RDP/SSH         logins.        Reference         Links:         -        [VPC         Flow Logs](https://docs.aws.amazon.com/vpc/latest/user guide/flow-logs.html)                     -                     [Cloud watch                      Metric
Filters](https://docs.aws.amazon.com/Amazon cloud watch/latest/logs/Monitoring log data.html)                 -                 [Cloud watch
Alarms](https://docs.aws.amazon.com/Amazon cloud watch/latest/monitoring/Alarm that sends email.html)

202.A company is  building a  new web-based customer  relationship management application. The application will  use several Amazon EC2 instances that are backed by Amazon Elastic Block Store (Amazon EBS) volumes behind an Application Load Balancer (ALB). The application will also use an Amazon Aurora database. All data for the application must be encrypted at rest and in transit. Which solution will meet these requirements?

A ��Use AWS Key Management Service (AWS KMS) certificates on the ALB to encrypt data in transit. Use AWS Certificate Manager (ACM) to encrypt the EBS volumes and Aurora database storage at rest.

B��Use the AWS root account to log in to the AWS Management Console. Upload the company ��s encryption certificates. While in the root account, select the option to turn on encryption for all data at rest and in transit for the account.

C �� Use AWS Key Management Service (AWS KMS) to encrypt the EBS volumes and Aurora database storage at rest. Attach an AWS Certificate Manager (ACM) certificate to the ALB to encrypt data in transit.

D ��Use Bit locker to encrypt all data at rest. Import the company ��s TLS certificate keys to AWS Key Management Service (AWS KMS) Attach the KMS keys to the ALB to encrypt data in transit.

�𰸣�C

������ Correct Answer: C Detailed Explanation: To meet the requirements of encrypting data at rest and in transit: 1. Encryption at Rest (for EBS volumes and Aurora): - AWS KMS (Key Management Service) is used to manage encryption keys. - EBS volumes and Aurora databases can be encrypted using KMS keys. When you enable encryption for EBS or Aurora, AWS automatically uses KMS to handle the encryption keys. 2.  Encryption in Transit (for t he ALB): - AWS Certificate Manager (ACM) provides SSL/TLS certificates. - You attach an ACM certificate to the ALB to encrypt traffic between clients (users) and the ALB using HTTPS. Why Other Options Are Wrong: - A: Mixes up KMS and ACM roles. KMS doesn't provide certificates for ALB, and ACM doesn't encrypt storage (it handles SSL/TLS for transit). - B: The root account can't enable encryption for all services automatically. Encryption must be configured per service (EBS, Aurora, ALB). - D: Bit locker is a Windows tool (not AWS - native), and KMS isn't designed to store       TLS       certificates        (use       ACM        instead).       Reference        Links:        -       [Encrypting        EBS       Volumes        with KMS](https://docs.aws.amazon.com/AWSEC2/latest/User guide/Ebs encryption.html)         -          [Encrypting         Aurora          with KMS](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/Overview.Encryption.html)  -  [Using  ACM  Certificates with ALB](https://docs.aws.amazon.com/elastic load balancing/latest/application/create-https-listener.html)

203.A company is moving its on-premises Oracle database to Amazon Aurora Postgresql. The database has several applications that write to the  same tables. The  applications  need to  be  migrated  one  by  one  with  a  month  in  between  each  migration. Management has expressed concerns that the database has a high number of reads and writes. The data must be kept in sync

across both databases throughout the migration. What should a solutions architect recommend?

A ��Use AWS Data sync for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a change data capture (CDC) replication task and a table mapping to select all tables.

B ��Use AWS Data sync for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a full load plus change data capture (CDC) replication task and a table mapping to select all tables.

C��Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS) using a memory optimized replication instance. Create a full load plus change data capture (CDC) replication task and a table mapping to select all tables.

D �� Use  the  AWS  Schema  Conversion  Tool  with  AWS  Database  Migration  Service  (AWS  DMS)  using  a  compute  optimized replication instance. Create a full load plus change data capture (CDC) replication task and a table mapping to select the largest tables.

�𰸣�C

������ Correct Answer C Detailed Explanation The correct approach uses AWS Schema Conversion Tool (SCT) to handle schema and code differences between Oracle and Postgresql. Since the migration involves high read/write activity, a memory-optimized replication instance in AWS DMS is ideal for efficiently processing CDC (Change Data Capture) to keep data in sync. The full load + CDC  task  ensures  the  initial  data  transfer  (full  load)  and  ongoing  replication  of  changes  during  the  month-long  migration. Selecting all tables  in the  mapping ensures complete synchronization. - Why not A/B?  Data sync is for bulk data transfer, not schema conversion or continuous  replication. - Why  not  D? Compute-optimized  instances are for CPU-heavy tasks,  not CDC. Selecting        only         "largest         tables"         would         leave         data         inconsistent.        Reference         Links         -         [AWS SCT](https://aws.amazon.com/dms/schema-conversion-tool/)                                  -                                  [AWS                                   DMS
CDC](https://docs.aws.amazon.com/dms/latest/user guide/CHAP_Task.CDC.html)

204.A company has a three-tier application for image sharing. The application uses an Amazon EC2 instance for the front-end layer, another EC2 instance for the application layer, and a third EC2 instance for a MySQL database. A solutions architect must design a scalable and highly available solution that requires the least amount of change to the application. Which solution meets these requirements?

A ��Use Amazon S3 to host the front-end layer. Use AWS Lambda functions for the application layer. Move the database to an Amazon Dynamo db table. Use Amazon S3 to store and serve users �� images.

B ��Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the application layer. Move the database to an Amazon RDS DB instance with multiple read replicas to serve users �� images.

C �� Use Amazon S3 to host the front-end layer. Use a fleet of EC2 instances in an Auto Scaling group for the application layer. Move the database to a memory optimized instance type to store and serve users �� images.

D ��Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the application layer. Move the database to an Amazon RDS Multi-AZ DB instance. Use Amazon S3 to store and serve users �� images.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The question requires a solution that's scalable, highly available, and minimizes application changes. Here's why option D is best: 1. Front end   �� Application Layers: Elastic Beanstalk with load balancing and
Multi - AZ automatically handles scaling, redundancy, and failover for EC2 - based layers. This requires minimal code changes

since it's a managed platform - as - a - service (PaaS). 2. Database: Migrating MySQL to Amazon RDS Multi - AZ ensures automatic failover  and  high  availability  without  altering  database  queries  (since  RDS  is  MySQL  -  compatible).  3.  Image  Storage:  Using Amazon S3 for images  replaces the database/EC2 storage with a scalable, durable service. This only requires changing where images  are  stored/retrieved  in  the  application  code.  Other  options  fail  because:  -  A  forces  serverless  (Lambda)  and  NoSQL (Dynamo db),  requiring  major  code  rewrites. -  B  incorrectly  uses  RDS  replicas  for  image  storage  instead of S3. - C  keeps the database    on    a    single    EC2    instance    (not     HA)    and    doesn't    use    S3    optimally.    Reference     Links:    -    [AWS    Elastic Beanstalk](https://aws.amazon.com/elastic beanstalk/) - [Amazon RDS Multi - AZ](https://aws.amazon.com/rds/features/multi - az/) - [Amazon S3](https://aws.amazon.com/s3/)

205.An application running on an Amazon EC2 instance in VPC-A needs to access files in another EC2 instance in VPC-B. Both VPCs are in separate AWS accounts. The  network administrator needs to design a solution to configure secure access to EC2 instance in VPC-B from VPC-A. The connectivity should not have a single point of failure or bandwidth concerns. Which solution will meet these requirements?

A ��Set up a VPC peering connection between VPC-A and VPC-B.

B ��Set up VPC gateway endpoints for the EC2 instance running in VPC-B.

C ��Attach a virtual private gateway to VPC-B and set up routing from VPC-A.

D ��Create a private virtual interface (V IF) for the EC2 instance running in VPC-B and add appropriate routes from VPC-A.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The correct solution is to set up a VPC peering connection between VPC-A and VPC-B.  Here ��s why:  1. VPC  Peering  Basics: VPC peering allows direct private network connectivity between two VPCs, even across different AWS accounts. Traffic stays within the AWS network, avoiding exposure to the public internet. 2. Security: Since communication occurs over AWS ��s private infrastructure, it ��s inherently more secure than public internet-based solutions. 3. No Single Point of Failure: AWS ��s backbone network is highly redundant. VPC peering doesn ��t rely on physical hardware (like VPNs) or dedicated connections (like Direct Connect), eliminating single points of failure. 4. Bandwidth: Peering uses AWS ��s high-speed internal network, so bandwidth isn ��t throttled (unlike VPNs, which have throughput limits). Why other options are incorrect: - B (Gateway Endpoints): These only work for AWS services like S3/Dynamo db, not for EC2 instances. - C (Virtual Private Gateway): This would require a VPN connection, which has bandwidth limits and potential single points of failure. - D (Private VIF): Requires Direct  Connect   (physical  cabling),  which   introduces  complexity  and   potential  single   points  of  failure   unless   redundant connections                are                configured.                Reference                Links:                -                 [AWS                VPC                Peering Guide](https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html)         -          [VPC         Peering          vs         VPN Comparison](https://aws.amazon.com/blogs/aws/vpc-peering-private-connectivity-between-vpcs/)

206.A company wants to experiment with individual AWS accounts for its engineer team. The company wants to be notified as soon as the Amazon EC2 instance usage for a given month exceeds a specific threshold for each account. What should a solutions architect do to meet this requirement MOST cost-effectively?

A ��Use Cost Explorer to create a daily report of costs by service. Filter the report by EC2 instances. Configure Cost Explorer to send an Amazon Simple Email Service (Amazon SES) notification when a threshold is exceeded.

B ��Use Cost Explorer to create a monthly report of costs by service. Filter the report by EC2 instances. Configure Cost Explorer to send an Amazon Simple Email Service (Amazon SES) notification when a threshold is exceeded.

C ��Use AWS Budgets to create a cost budget for each account. Set the period to monthly. Set the scope to EC2 instances. Set an

alert threshold for the budget. Configure an Amazon Simple  Notification Service (Amazon SNS) topic to receive a notification when a threshold is exceeded.

D ��Use AWS Cost and Usage Reports to create a report with hourly granularity. Integrate the report data with Amazon Athena. Use Amazon Event bridge to schedule an Athena query. Configure an Amazon Simple Notification Service (Amazon SNS) topic to receive a notification when a threshold is exceeded.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The best solution is C because AWS Budgets is specifically designed to monitor costs and send alerts when thresholds are exceeded. Here's why it works: 1. AWS Budgets lets you create custom budgets tied to EC2 usage (scoped to "EC2 instances") and set monthly thresholds. 2. It integrates natively with Amazon SNS (Simple Notification Service) to send alerts instantly when the threshold is crossed. 3. This approach is fully automated, requires no code, and avoids unnecessary complexity or costs (unlike options like Athena or hourly reports in other answers). Other options are less optimal: - A/B: Cost Explorer is for cost analysis, not real-time alerts. It doesn't natively support automated SES/SNS notifications. - D: Using Athena and Event bridge adds complexity and costs for querying/storing reports, which is overkill for a simple alert. AWS Budgets is         the          simplest,         most          cost-effective         tool          for         this          scenario.          Reference          Links:         [AWS Budgets](https://aws.amazon.com/aws-cost-management/aws-budgets/)                [AWS                Budgets                vs.                Cost Explorer](https://docs.aws.amazon.com/cost-management/latest/user guide/budgets-compare.html)

207.A solutions architect needs to design a new micro service for a company��s application. Clients must be able to call an HTTPS endpoint to reach the micro service. The micro service also must use AWS Identity and Access Management (IAM) to authenticate calls. The solutions architect will write the logic for this micro service by using a single AWS Lambda function that is written in Go 1.x. Which solution will deploy the function in the MOST operationally efficient way?

A ��Create an Amazon API Gateway REST API. Configure the method to use the Lambda function. Enable IAM authentication on the API.

B ��Create a Lambda function URL for the function. Specify AWS_IAM as the authentication type.

C ��Create an Amazon Cloud front distribution. Deploy the function to Lambda@Edge. Integrate IAM authentication logic into the Lambda@Edge function.

D �� Create  an  Amazon   Cloud front  distribution.  Deploy  the  function  to   Cloud front   Functions.  Specify  AWS_IAM  as  the authentication type.

�𰸣�A

������ Correct Answer: B Detailed Explanation: The most operationally efficient solution is to use a Lambda Function URL with AWS_IAM  authentication.  Here's  why:  -  Lambda  Function  URLs  provide  a  direct  HTTPS  endpoint  for  your  Lambda  function without  needing  to  configure  additional  services  like  API  Gateway  or  Cloud front.  -  By  setting  the  authentication  type  to AWS_IAM, AWS automatically handles IAM-based request signing and validation. This means the Lambda function itself doesn ��t need extra code to authenticate requests��AWS manages it natively. - This approach requires fewer steps than setting up API Gateway (Option A) or Lambda@Edge (Options C/D), making it simpler and faster to deploy. Options A, C, and D involve extra components  (API  Gateway,  Cloud front) that  add  complexity  without  providing  additional  benefits  for  this  specific  use  case. Reference  Links:  -   [AWS  Lambda  Function  URLs](https://docs.aws.amazon.com/lambda/latest/dg/lambda-urls.html)  -   [IAM Authentication                                                                                              for                                                                                              Lambda
URLs](https://aws.amazon.com/blogs/aws/announcing-aws-lambda-function-urls-built-in-https-endpoints-for-single-function-mi cro services/)

208.A  company  previously   migrated  its  data  warehouse  solution  to  AWS.  The  company  also   has  an  AWS  Direct  Connect connection. Corporate office users query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 50 MB and each webpage sent by the visualization tool is approximately 500 KB. Result sets returned by the data warehouse are not cached. Which solution provides the LOWEST data transfer egress cost for the company?

A ��Host the visualization tool on premises and query the data warehouse directly over the internet.

B ��Host the visualization tool in the same AWS Region as the data warehouse. Access it over the internet.

C ��Host the visualization tool on premises and query the data warehouse directly over a Direct Connect connection at a location in the same AWS Region.

D ��Host the visualization tool in the same AWS Region as the data warehouse and access it over a Direct Connect connection at a location in the same Region.

�𰸣�D

������Correct Answer: D Detailed Explanation: To minimize data transfer egress costs, the key is to: 1. Keep the visualization tool and  data  warehouse  in  the  same  AWS  Region  (eliminates  cross-region  data  transfer  fees)  2.  Use  AWS  Direct  Connect  for corporate office  access  (cheaper  than  standard  internet  egress)  Here's  why  option  D  wins:  -  The  50MB  query  results  move between services within the same AWS  Region (free/no egress charges) - Only the 500KB webpage needs to transfer out via Direct Connect (lower cost than internet egress pricing) - Direct Connect has predictable pricing vs. variable internet data transfer rates Other options have higher costs: A/C:  Move 50MB  results over internet/Direct Connect (large volume)  B:  Uses internet egress for both 50MB and 500KB D optimally contains the large dataset within AWS while using cost-efficient Direct Connect for the               smaller                webpage.                Reference                 Links:               AWS                 Data               Transfer                Pricing: https://aws.amazon.com/ec2/pricing/on-demand/#Data_Transfer                         Direct                         Connect                          Pricing:
https://aws.amazon.com/directconnect/pricing/

209.An online  learning company is  migrating to the AWS Cloud. The company maintains  its student  records  in a  Postgresql database. The company needs a solution in which its data is available and online across multiple AWS Regions at all times. Which solution will meet these requirements with the LEAST amount of operational overhead?

A ��Migrate the Postgresql database to a Postgresql cluster on Amazon EC2 instances.

B ��Migrate the Postgresql database to an Amazon RDS for Postgresql DB instance with the Multi-AZ feature turned on.

C ��Migrate the Postgresql database to an Amazon RDS for Postgresql DB instance. Create a read replica in another Region.

D��Migrate the Postgresql database to an Amazon RDS for Postgresql DB instance. Set up DB snapshots to be copied to another Region.

�𰸣�C

������ Correct Answer C Detailed Explanation The requirement is to keep student records available and online across multiple AWS Regions with the least operational overhead. Here's why Option C is the best choice: 1. Option A (EC2 Postgresql cluster) - Managing a self - hosted Postgresql cluster on EC2 requires manual setup for replication, failover, and maintenance. This adds significant operational complexity. 2. Option B (RDS  Multi - AZ) - RDS  Multi - AZ ensures high availability within a single AWS Region  by  replicating  data to a standby  instance  in  a different Availability Zone.  However,  it does  not  provide  cross -  region

availability. 3. Option C (RDS with cross - region read replica) - Amazon RDS allows creating a read replica in another AWS Region.
This replica is updated asynchronously but stays online and can be promoted to a standalone database if the primary Region fails.
- RDS automates replication, backups, and maintenance, minimizing operational effort. 4. Option D (RDS snapshots copied to another Region) - Snapshots are backups, not live databases. Restoring a snapshot in another Region takes time, meaning the data      is       not       "available      at       all       times"      in       multiple       Regions.       Reference       Link       [Amazon       RDS       Read Replicas](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_Read repl.html)

210.A company hosts its web application on AWS using seven Amazon EC2 instances. The company requires that the IP addresses of all healthy EC2 instances be returned in response to DNS queries. Which policy should be used to meet this requirement?

A ��Simple routing policy

B ��Latency routing policy

C ��Multivalue routing policy D ��Geolocation routing policy

�𰸣�C

���� �� Correct Answer: C. Multivalue routing policy Detailed Explanation: The Multivalue routing policy is designed to return multiple  healthy  resource  records  (e.g.,  EC2  instance  IPs)  in  response  to  a  DNS  query.  It  performs  health  checks  on  each configured record and only returns values that are healthy. This makes it ideal when you want to distribute traffic across multiple healthy endpoints, like the 7 EC2 instances in this scenario. Other options: - A. Simple routing policy: Returns only 1 record (not multiple IPs) - B. Latency routing: Routes based on network latency (not quantity of healthy instances) - D. Geolocation routing: Routes      based       on      user       location       (not      health       status)       Reference      Link:       [Amazon       Route      53       Routing Policies](https://docs.aws.amazon.com/Route53/latest/Developer guide/routing-policy.html)

211.A medical research lab produces data that is related to a new study. The lab wants to make the data available with minimum latency to clinics across the country for their on-premises, file-based applications. The data files are stored  in an Amazon S3 bucket  that   has   read-only   permissions   for  each   clinic.  What  should   a  solutions   architect   recommend  to   meet  these requirements?

A ��Deploy an AWS Storage Gateway file gateway as a virtual machine (VM) on premises at each clinic

B ��Migrate the files to each clinic ��s on-premises applications by using AWS Data sync for processing.

C ��Deploy an AWS Storage Gateway volume gateway as a virtual machine (VM) on premises at each clinic. D ��Attach an Amazon Elastic File System (Amazon EFS) file system to each clinic ��s on-premises servers.

�𰸣�A

������ Correct Answer A �� Deploy an AWS Storage Gateway file gateway as a virtual machine (VM) on premises at each clinic Detailed  Explanation The  key  requirements  are:  1.  Low-latency  access  to S3  data for on-premises, file-based  applications. 2. Read-only permissions for clinics. 3. No need to migrate/copy data to clinics (keep it centralized in S3). Why Option A works best:
- AWS Storage Gateway  File Gateway creates  a  local cache  at each clinic,  enabling fast access to frequently  used files while keeping data in S3. - Clinics access files via standard protocols (NFS/SMB) like a local file share. - S3 permissions are enforced automatically    - clinics only see what they ��re allowed to read. - No data duplication    - all clinics work with the same S3

bucket. Why other options fail: - B (Data sync) requires copying data to each clinic, creating latency and management overhead. - C  (Volume Gateway)  provides  block storage  (disks),  not file-level  access.  -  D  (EFS)  requires  cloud  connectivity and doesn ��t integrate  directly  with  on-premises  servers  as  seamlessly  as  Storage  Gateway.  Reference  Links  -  [AWS  Storage  Gateway  File
Gateway](https://aws.amazon.com/storage gateway/file/)                  -                  [S3                  Integration                  with                  File
Gateway](https://docs.aws.amazon.com/storage gateway/latest/user guide/Creating an s3Fileshare.html)

212.A company is using a content management system that runs on a single Amazon EC2 instance. The EC2 instance contains both the web server and the database software. The company must make its website platform highly available and must enable the website to scale to meet user demand. What should a solutions architect recommend to meet these requirements?

A �� Move the database to Amazon  RDS,  and enable  automatic  backups.  Manually  launch  another  EC2  instance in the same Availability Zone. Configure an Application Load Balancer in the Availability Zone, and set the two instances as targets.

B �� Migrate the database to an Amazon Aurora instance with a read replica in the same Availability Zone as the existing EC2 instance. Manually launch another EC2 instance in the same Availability Zone. Configure an Application Load Balancer, and set the two EC2 instances as targets.

C��Move the database to Amazon Aurora with a read replica in another Availability Zone. Create an Amazon Machine Image (AMI) from the EC2 instance. Configure an Application Load Balancer in two Availability Zones. Attach an Auto Scaling group that uses the AMI across two Availability Zones.

D ��Move the database to a separate EC2 instance, and schedule backups to Amazon S3. Create an Amazon Machine Image (AMI) from the original EC2 instance. Configure an Application Load Balancer in two Availability Zones. Attach an Auto Scaling group that uses the AMI across two Availability Zones.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The correct answer is C because it addresses both high availability and scalability effectively. Here's why: 1. High Availability - Database: Moving to Amazon Aurora with a read replica in another Availability Zone (AZ) ensures the database is redundant across AZs. If one AZ fails, the replica can take over. - Web Servers: Using an Auto Scaling group across two AZs ensures EC2 instances (web servers) are distributed. If one AZ goes down, instances in the other AZ keep the website running. - Application Load Balancer: Distributes traffic across instances in both AZs, improving fault tolerance. 2. Scalability - Auto Scaling: Automatically launches or terminates EC2 instances based on demand, ensuring the website scales to handle traffic spikes. - AMI (Amazon Machine Image): Ensures new instances launched by Auto Scaling are pre - configured with the  web  server  setup,  reducing  deployment  time.  Options  A  and  B  fail  because  they  keep  resources  in  a  single  AZ,  risking downtime if that AZ fails. Option D leaves the database on a single EC2 instance, which is not highly available. Only Option C ensures  redundancy for  both  the  database  and web servers  across AZs while  enabling  automatic scaling.  Reference  Links:  - [Amazon      Aurora       High      Availability](https://aws.amazon.com/rds/aurora/high       -       availability/)      -       [Auto       Scaling Groups](https://docs.aws.amazon.com/auto scaling/ec2/user guide/Auto scaling group.html)            -            [Application             Load Balancer](https://aws.amazon.com/elastic load balancing/application - load - balancer/)

213.A company is launching an application on AWS. The application uses an Application Load Balancer (ALB) to direct traffic to at least two Amazon EC2 instances in a single target group. The instances are in an Auto Scaling group for each environment. The company requires a development environment and a production environment. The production environment will have periods of high traffic. Which solution will configure the development environment MOST cost-effectively?

A ��Reconfigure the target group in the development environment to have only one EC2 instance as a target.

B ��Change the ALB balancing algorithm to least outstanding requests.

C ��Reduce the size of the EC2 instances in both environments.

D ��Reduce the maximum number of EC2 instances in the development environment ��s Auto Scaling group.

�𰸣�D

���� �� Correct Answer: D Detailed Explanation: The goal is to minimize costs for the development (dev) environment while maintaining functionality. Here's why option D is the best choice:  1. Auto Scaling Group (ASG)  Basics: - An ASG manages EC2 instances based on scaling policies. The maximum setting limits how many instances it can launch. - In production, the ASG might scale up during traffic spikes (high max), but dev doesn ��t need this. Reducing the dev ASG ��s max ensures it never uses more instances than necessary. 2. Why Not Other Options: - A: Using only 1 EC2 instance violates the requirement of "at least two instances" for the ALB target group (stated in the question). - B: Changing the ALB algorithm affects traffic distribution, not cost. -
C: Smaller instances save costs, but the question focuses on dev - specific changes (option C affects both environments). 3. Cost Savings with Option D: - By setting the dev ASG ��s max to 2 (instead of a higher number like 4), you prevent unnecessary scaling and reduce the number of running instances. - Example: If dev ��s ASG has min = 2, desired = 2, max = 2, it runs exactly 2 instances 24/7               (no                scaling               costs).                Reference               Links:                -                [AWS               Auto               Scaling Groups](https://docs.aws.amazon.com/auto scaling/ec2/user guide/Auto scaling group.html)       -        [Cost       Optimization       for EC2](https://aws.amazon.com/ec2/cost-optimization/)

214.A company runs a web application on Amazon EC2 instances in multiple Availability Zones. The EC2 instances are in private subnets. A solutions architect implements an internet-facing Application Load Balancer (ALB) and specifies the EC2 instances as the target group. However, the internet traffic is not reaching the EC2 instances. How should the solutions architect reconfigure the architecture to resolve this issue?

A ��Replace the ALB with a Network Load Balancer. Configure a NAT gateway in a public subnet to allow internet traffic.

B �� Move the EC2 instances to public subnets. Add a rule to the EC2 instances �� security groups to allow outbound traffic to
0.0.0.0/0.

C ��Update the route tables for the EC2 instances �� subnets to send 0.0.0.0/0 traffic through the internet gateway route. Add a rule to the EC2 instances �� security groups to allow outbound traffic to 0.0.0.0/0.

D ��Create public subnets in each Availability Zone. Associate the public subnets with the ALB. Update the route tables for the public subnets wit h a route to the private subnets.

�𰸣�D

���� �� Correct Answer: D Detailed Explanation: The EC2 instances are in private subnets, which cannot directly communicate with the internet.  However, an internet-facing Application Load  Balancer (ALB) must be deployed in  public subnets to accept traffic from the internet. Here's why option D works: 1. ALB Placement: The ALB needs public subnets to receive internet traffic. If the ALB was originally configured without public subnets (e.g., placed in private subnets), it cannot accept external requests. Creating public subnets for the ALB fixes this. 2. Routing: The ALB acts as a bridge between the internet (public subnets) and the EC2 instances (private subnets). The ALB forwards traffic to the private instances via their private IP addresses. The route tables for the  public subnets  must  direct  internet-bound traffic  (0.0.0.0/0)  to  an  internet gateway, while the  private subnets  route internal traffic through the VPC ��s local network. 3. No NAT Required: The EC2 instances in private subnets don ��t need internet access themselves (they only receive traffic via the ALB), so a NAT gateway isn ��t required here. Options A, B, and C are incorrect because: - A: A Network Load Balancer (NLB) isn��t necessary for HTTP/HTTPS traffic, and a NAT gateway is irrelevant here. - B: Moving instances to public subnets exposes them directly to the internet, which violates the security best practice of keeping

backend servers in private subnets. - C: Private subnets should NOT have a route to the internet gateway (this would make them
public).                        Reference                         Links:                         -                         [Application                         Load                         Balancer
Basics](https://docs.aws.amazon.com/elastic load balancing/latest/application/introduction.html)            -            [VPC            Subnet Routing](https://docs.aws.amazon.com/vpc/latest/user guide/VPC_Route_Tables.html)

215.A company runs analytics software on Amazon EC2 instances. The software accepts job requests from users to process data that  has  been  uploaded to  Amazon S3.  Users  report  that some submitted  data  is  not  being  processed  Amazon  Cloud watch reveals  that  the  EC2  instances  have  a  consistent  CPU  utilization  at  or  near  100%.  The  company  wants  to  improve  system performance and scale the system based on user load. What should a solutions architect do to meet these requirements?

A ��Create a copy of the instance. Place all instances behind an Application Load Balancer.

B ��Create an S3 VPC endpoint for Amazon S3. Update the software to reference the endpoint.

C ��Stop the EC2 instances. Modify the instance type to one with a more powerful CPU and more memory. Restart the instances.

D �� Route incoming requests to Amazon Simple Queue Service (Amazon SQS). Configure an EC2 Auto Scaling group based on queue size. Update the software to read from the queue.

�𰸣�D

���� �� Correct Answer:  D Detailed  Explanation: The EC2 instances are overwhelmed (100% CPU) and can't process all user requests, causing some data to be unprocessed. Option D solves this by: 1. Buffering requests: Using SQS as a queue ensures no requests  are  lost,  even   if  all  instances  are   busy.  2.  Auto  Scaling:  Configuring  Auto  Scaling   based  on  the  SQS  queue  size automatically  adds/removes  EC2  instances  based  on  actual  workload.  If  the  queue  grows  (indicating  high  demand),  more instances launch. If the queue shrinks, excess instances terminate to save costs. 3. Decoupling: Separates the request submission (users) from processing (EC2), making the system more resilient. Other options fail because: - A adds load balancing but doesn ��t fix overloading or enable automatic scaling. - B optimizes S3 access but doesn ��t address CPU overload. - C upgrades instance size (vertical   scaling)    but   doesn   �� t    enable    automatic   scaling    or   handle    request    spikes.    Reference   Links:   -    [Amazon SQS](https://aws.amazon.com/sqs/) - [EC2 Auto Scaling](https://aws.amazon.com/ec2/auto scaling/)

216.A company is implementing a shared storage solution for a media application that is hosted in the AWS Cloud. The company needs the ability to  use SMB clients to access  data. The  solution  must  be  fully  managed.  Which AWS solution  meets these requirements?

A �� Create an AWS Storage Gateway volume gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.

B ��Create an AWS Storage Gateway tape gateway. Configure tapes to use Amazon S3. Connect the application server to the tape gateway.
C �� Create  an Amazon  EC2  Windows  instance.  Install  and configure a Windows file share  role on the  instance.  Connect the application server to the file share.

D ��Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server. Connect the application server to the file system.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The correct answer is D because Amazon FSx for Windows File Server is a fully managed  AWS  service  that  provides  shared  storage  using  the  SMB  protocol.  It  is  designed  specifically  for  Windows-based applications and supports seamless integration with SMB clients, which matches the requirement. - Option A (Storage Gateway Volume Gateway) is  incorrect  because Storage Gateway is  primarily  used for  hybrid cloud setups (connecting on-premises to AWS) and is not a fully managed solution for cloud-native applications. - Option B (Tape Gateway) is unrelated to file sharing and is  used for  backup/archival. -  Option  C  (EC2 Windows  instance)  requires  manual  setup  and  management,  so  it ��s  not fully managed. - Option D (FSx for Windows) is fully managed, supports SMB, and requires no infrastructure management. Reference Links:      -      [Amazon       FSx      for      Windows      File       Server](https://aws.amazon.com/fsx/windows/)      -      [AWS      Storage Gateway](https://aws.amazon.com/storage gateway/)

217.A company ��s security team requests that network traffic be captured in VPC Flow Logs. The logs will be frequently accessed for 90 days and then accessed intermittently. What should a solutions architect do to meet these requirements when configuring the logs?

A ��Use Amazon Cloud watch as the target. Set the Cloud watch log group with an expiration of 90 days

B ��Use Amazon Kinesis as the target. Configure the Kinesis stream to always retain the logs for 90 days.

C ��Use AWS Cloud trail as the target. Configure Cloud trail to save to an Amazon S3 bucket, and enable S3 Intelligent-Tiering.

D �� Use Amazon S3  as the target.  Enable  an  S3  Lifecycle  policy  to  transition the  logs  to S3  Standard-Infrequent Access  (S3 Standard-IA) after 90 days.

�𰸣�D

������ Correct Answer: D Detailed Explanation: To meet the requirements of frequently accessing VPC Flow Logs for 90 days and then intermittently afterward, the best approach is to use Amazon S3 as the target with a Lifecycle policy: 1. Why S3? VPC  Flow Logs can be configured to send logs directly to an Amazon S3 bucket. S3 is cost-effective for storage and allows lifecycle policies to automate transitioning data between storage classes. 2. Lifecycle Policy for Access Patterns - For the first 90 days, logs are accessed frequently. S3 Standard (default storage class) is ideal for this phase because it ��s optimized for frequent access. - After 90 days, access becomes intermittent. Using a lifecycle policy to transition logs to S3 Standard-Infrequent Access (S3 Standard-IA) reduces costs while keeping the logs available for occasional retrieval. 3. Why Other Options Are Incorrect - (A) Cloud watch: Logs expire after 90 days (deleted entirely), but the requirement is to retain them for intermittent access beyond 90 days. - (B) Kinesis: VPC Flow Logs cannot send data directly to Kinesis. - (C) Cloud trail: Cloud trail is for auditing API activity, not VPC  Flow Logs. Reference  Links:  -   [VPC  Flow  Logs  Destinations](https://docs.aws.amazon.com/vpc/latest/user guide/flow-logs-s3.html)  -   [S3 Lifecycle Policies](https://docs.aws.amazon.com/AmazonS3/latest/user guide/object-lifecycle-mgmt.html)

218.An Amazon EC2 instance is located in a private subnet in a new VPC. This subnet does not have outbound internet access, but the EC2 instance needs the ability to download monthly security updates from an outside vendor. What should a solutions architect do to meet these requirements?

A ��Create an internet gateway, and attach it to the VPC. Configure the private subnet route table to use the internet gateway as the default route.

B ��Create a NAT gateway, and place it in a public subnet. Configure the private subnet route table to use the NAT gateway as the default route.

C ��Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the NAT instance as the default route.

D ��Create an internet gateway, and attach it to the VPC. Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the internet gateway as the default route.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The EC2 instance is in a private subnet without direct internet access. To allow it to download updates from the internet while remaining private, a NAT gateway must be used. Here ��s why: - A NAT gateway (placed in a public subnet) allows instances in a private subnet to initiate outbound traffic to the internet (e.g., downloading updates) but blocks unsolicited inbound traffic. - The private subnet ��s route table must direct internet-bound traffic (0.0.0.0/0) to the NAT gateway. - Option A is incorrect because attaching an internet gateway directly to the private subnet would make it public, exposing the instance to the internet. - Option C/D are incorrect because a NAT instance must reside in a public subnet (with        internet       gateway        access)        to        work        properly.        Reference        Links:        -        [AWS        NAT        Gateway Documentation](https://docs.aws.amazon.com/vpc/latest/user guide/vpc-nat-gateway.html)         -         [Public          vs         Private Subnets](https://docs.aws.amazon.com/vpc/latest/user guide/VPC_Scenario2.html)

219.A solutions architect needs to design a system to store client case files. The files are core company assets and are important. The number of files will grow over time. The files must be simultaneously accessible from multiple application servers that run on Amazon EC2 instances. The solution must have built-in redundancy. Which solution meets these requirements?

A ��Amazon Elastic File System (Amazon EFS)

B ��Amazon Elastic Block Store (Amazon EBS) C ��Amazon S3 Glacier Deep Archive

D ��AWS Backup

�𰸣�A

������ Correct Answer: A. Amazon Elastic File System (Amazon EFS) Detailed Explanation: Amazon EFS is the best choice here because it meets all the requirements: 1. Shared Access: EFS provides a shared network file system that multiple EC2 instances can access simultaneously via the NFS protocol, perfect for applications needing concurrent file access. 2. Built-in Redundancy: EFS  automatically  stores  data  redundantly  across  multiple  Availability  Zones  (AZs)  in  a  region,  ensuring  high  availability  and durability. 3.  Scalability:  EFS  scales  automatically  as  files grow, with  no  need  to  provision  storage  in  advance.  4.  Core  Asset Protection: As a fully managed service, EFS handles maintenance, security patches, and backups (via lifecycle policies), making it ideal for  critical  company data.  Other  options fail  key  requirements:  -  EBS  (B)  is  limited  to  single  EC2  instance  attachments (without complex setups) and lacks cross-AZ redundancy. - S3 Glacier (C) is for archival storage with slow retrieval times, not active  file   access.  -  AWS   Backup  (D)  is  a  backup  tool,   not  a   primary  storage  solution.  Reference  Links:  -  [Amazon  EFS Features](https://aws.amazon.com/efs/features/)                          -                           [AWS                          Storage                          Services
Overview](https://aws.amazon.com/products/storage/)

220.A solutions architect has created two IAM policies: Policy1 and Policy2. Both policies are attached to an IAM group.        A cloud engineer is added as an IAM user to the IAM group. Which action will the cloud engineer be able to perform?

A ��Deleting IAM users

B ��Deleting directories

C ��Deleting Amazon EC2 instances

D ��Deleting logs from Amazon Cloud watch Logs

�𰸣�C

������ Correct Answer C �� Deleting Amazon EC2 instances Detailed Explanation The cloud engineer can delete EC2 instances because: 1. Policy1 explicitly allows all EC2 actions (ec2:*) on all resources (*). This includes permissions to terminate/delete EC2 instances. 2.  Policy2 only grants limited Cloud watch  Logs  permissions (logs:Create log group and logs:Put log events) and does not grant delete permissions for Cloud watch Logs or other services. 3. AWS follows a permissions union model: when multiple policies are attached, the effective permissions are the SUM of all allowed actions (unless an explicit "Deny" exists). Since there's no explicit denial for  EC2 termination  in  either  policy,  Policy1's  broad  EC2  permissions  enable this  action. Other  options are incorrect because: - A (Delete IAM users): Requires iam:Delete user permission, which isn't granted in either policy. - B (Delete directories): Typically requires directory service permissions (e.g., AWS Directory Service), which aren't mentioned. - D (Delete Cloud watch logs):  Requires  logs:Delete log group/logs:Delete log stream  permissions, which  Policy2 doesn't  include.  Reference Links                                               -                                                [IAM                                                 Policy                                                Evaluation
Logic](https://docs.aws.amazon.com/IAM/latest/User guide/reference_policies_evaluation-logic.html)        -         [EC2         Actions Permissions](https://docs.aws.amazon.com/AWSEC2/latest/Api reference/ec2-api-permissions.html)

221.A company is reviewing a recent migration of a three-tier application to a VPC. The security team discovers that the principle of least privilege is not being applied to Amazon EC2 security group ingress and egress rules between the application tiers. What should a solutions architect do to correct this issue?

A ��Create security group rules using the instance ID as the source or destination.

B ��Create security group rules using the security group ID as the source or destination.

C ��Create security group rules using the VPC CIDR blocks as the source or destination.

D ��Create security group rules using the subnet CIDR blocks as the source or destination.

�𰸣�B

���� �� Correct Answer  B �� Create  security group  rules using the security group  ID as the source or destination.  Detailed Explanation Security groups act as virtual firewalls for EC2 instances. By referencing security group IDs (not instance IDs or IP ranges)   in   rules,   you   enforce   the   principle   of   least   privilege:   1.   Logical   Grouping:   Instances   in   the   same   tier   (e.g., web/app/database) share a security group. Rules using security group IDs automatically allow only instances in that group, even if  IPs change. 2.  Precision:  Unlike subnet/VPC  CIDR  blocks  (which  allow  all  instances  in  a  network  range), security group  IDs restrict access only to authorized groups. 3. Dynamic Updates: If instances are added/removed, rules using security group IDs stay valid without  manual  IP  updates.  Example:  -  A  database-tier  security  group  should  only  allow  inbound traffic  from the app-tier     security     group     (SG-App),      not     the     entire      VPC/subnet.     Reference      Links     -     [AWS      Security     Groups Documentation](https://docs.aws.amazon.com/vpc/latest/user guide/VPC_Security groups.html)       -       [Principle       of       Least Privilege](https://docs.aws.amazon.com/IAM/latest/User guide/best-practices.html#grant-least-privilege)

222.A company has an ecommerce checkout workflow that writes an order to a database and calls a service to  process the payment.  Users  are  experiencing  timeouts  during  the  checkout  process.  When  users  resubmit  the  checkout  form,  multiple unique orders are created for the same desired transaction. How should a solutions architect refactor this workflow to prevent the creation of multiple orders?

A��Configure the web application to send an order message to Amazon Kinesis Data Firehose. Set the payment service to retrieve the message from Kinesis Data Firehose and process the order.

B��Create a rule in AWS Cloud trail to invoke an AWS Lambda function based on the logged application path request. Use Lambda to query the database, call the payment service, and pass in the order information.

C �� Store the order in the database. Send a  message that includes the order number to Amazon Simple  Notification Service (Amazon SNS). Set the payment service to poll Amazon SNS, retrieve the message, and process the order.

D �� Store  the order  in the  database.  Send  a  message that  includes the  order  number to  an Amazon Simple  Queue Service (Amazon SQS) FI FO queue. Set the payment service to retrieve the message and process the order. Delete the message from the queue.

�𰸣�D

���� �� Correct Answer: D Detailed Explanation: The issue occurs because when a user resubmits the checkout form after a timeout, the system creates duplicate orders. This happens because the original order was already saved in the database, but the payment service didn ��t respond in time, causing the user to retry. The solution must ensure that even if the payment service is slow  or fails temporarily, the  order  is  processed  exactly  once.  Why  Option  D  Works:  -  Amazon  SQS  FI FO  queues  guarantee exactly-once  processing and strict  message ordering.  Here ��s  how  it  solves the  problem:  1. Order Creation: When the  user submits the checkout form, the order is saved in the database with a unique order number. 2.  Message Sent to SQS  FI FO: A message containing the order number is sent to an SQS FI FO queue. The queue uses the order number as a de duplication ID, ensuring  that  even  if  the  user  resubmits  the  form,  duplicate  messages  for  the  same  order  are  automatically  discarded.  3. Payment  Processing:  The  payment  service  retrieves  the  message  from  the  queue  and  processes  the  payment.  If  processing succeeds, the message is deleted from the queue. If it fails (e.g., due to a timeout), the message becomes visible again in the queue after a visibility timeout, allowing retries. 4. No Duplicate Orders: Since the queue ensures only one message per order is processed, and the payment service deletes the message only after successful processing, the same order won ��t be reprocessed even if the user resubmits. Why Other Options Fail: - A (Kinesis Data Firehose): Designed for streaming data to storage/analytics (e.g., S3), not for ensuring exactly-once processing. It doesn ��t handle message de duplication or retries effectively. - B (Cloud trail + Lambda): Cloud trail logs API calls with delays, making it unsuitable for real-time workflows. Lambda would need to check the database for duplicates, but race conditions could still occur. - C (SNS): SNS broadcasts messages to all subscribers but doesn ��t guarantee message retention or de duplication. If the payment service is offline, messages are lost, leading to unprocessed orders. Key Take away: SQS F IFO queues solve the problem by ensuring each order is processed exactly once, even if the user retries. The payment service retries failed messages automatically, and duplicates are blocked at the queue level. Reference Links: - [Amazon SQS    FIFO     Queues](https://docs.aws.amazon.com/Aws simple queue service/latest/Sqs developer guide/FIFO-queues.html)    - [Exactly-Once                                                            Processing                                                             in                                                             SQS
FIFO](https://aws.amazon.com/blogs/aws/amazon-sqs-introduces-fifo-queues-with-exactly-once-processing/)

223.A company is  building a solution that will  report Amazon  EC2 Auto Scaling  events across all the applications in an AWS account. The company needs to use a serverless solution to store the EC2 Auto Scaling status data in Amazon S3. The company then will use the data in Amazon S3 to provide near-real-time updates in a dashboard. The solution must not affect the speed of EC2 instance launches. How should the company move the data to Amazon S3 to meet these requirements?

A ��Use an Amazon Cloud watch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.

B��Launch an Amazon EMR cluster to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.

C ��Create an Amazon Event bridge rule to invoke an AWS Lambda function on a schedule. Configure the Lambda function to send the EC2 Auto Scaling status data directly to Amazon S3.

D ��Use a bootstrap script during the launch of an EC2 instance to install Amazon Kinesis Agent. Configure Kinesis Agent to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.

�𰸣�A

������ Correct Answer: A. Use an Amazon Cloud watch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.  Detailed Explanation: The problem requires a serverless solution to capture EC2 Auto Scaling  events,  store them  in  S3,  and  enable  near-real-time  dashboards  without  slowing  down  EC2  instance  launches. Here's why Option A works  best:  1.  Cloud watch  Metric Streams automatically collect AWS service  metrics  (like Auto Scaling group status) in real-time. This is serverless and avoids manual data collection. 2. Kinesis Data Firehose buffers and batches data before writing to S3. This  improves efficiency and  reduces costs compared to direct writes  (e.g., Option C ��s  Lambda-to-S3 approach). 3. No impact on EC2 launch speed because the solution operates externally. Options like D (using bootstrap scripts) would slow down instance launches by running extra code during startup. Why other options fail: - B: Amazon EMR is overkill for simple data collection and isn ��t serverless. - C: Scheduled Lambda invocations can ��t capture events in real-time. Direct S3 writes lack Firehose ��s buffering/retry features. - D: Bootstrap scripts delay instance launches, violating the requirement. Reference Links:                                           -                                             [Amazon                                            Cloud watch                                            Metric
Streams](https://docs.aws.amazon.com/Amazon cloud watch/latest/monitoring/Cloud watch-Metric-Streams.html)    -    [Amazon Kinesis Data Firehose](https://aws.amazon.com/kinesis/data-firehose/)

224.A company has an application that places hundreds of .csv files into an Amazon S3 bucket every hour. The files are 1 GB in size. Each time a file is uploaded, the company needs to convert the file to Apache Parquet format and place the output file into an S3 bucket. Which solution will meet these requirements with the LEAST operational overhead?

A��Create an AWS Lambda function to download the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Invoke the Lambda function for each S3 PUT event.

B��Create an Apache Spark job to read the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the Spark job.

C ��Create an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application places the .csv files. Schedule an AWS Lambda function to periodically use Amazon Athena to query the AWS Glue table, convert the query results into Parquet format, and place the output files into an S3 bucket.

D��Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the ETL job.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The best solution is D because AWS Glue is a fully managed ETL service designed to handle  large-scale data transformations with  minimal operational overhead.  Here's why:  1. AWS Glue  ETL Job: -  Glue can directly read CSV files from S3, convert them to Parquet, and write the output back to S3. - It automatically scales to handle hundreds of files and large data sizes (1 GB  per file) without manual intervention. 2.  Event-Driven with  Lambda: - A  Lambda function triggers the Glue job each time a new CSV file is uploaded (via S3  PUT events). -  Lambda acts only as a lightweight "trigger," avoiding heavy processing tasks (unlike Option A/B, where Lambda/Spark would handle resource-intensive work). 3. Serverless    ��  Managed:   -   Glue   handles   infrastructure,   scaling,   and  job   execution,  eliminating  the   need   to   manage servers/clusters  (unlike  Spark  in  Option   B).  -   No   need  for  complex  scheduling  or  Athena  queries   (Option  C),  which   add

unnecessary steps. Why Not Other Options? - A: Lambda has a 15-minute timeout and limited memory, making it unsuitable for large 1 GB file processing. - B: Managing Spark jobs (e.g., on EMR) adds operational complexity. - C: Athena is for querying, not ETL;         this          approach          is          inefficient         and          slow.          Reference          Links:         -          [AWS          Glue          ETL Jobs](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-jobs-job.html)    -    [Serverless     Data    Processing    with     AWS Glue](https://aws.amazon.com/glue/)

225.A  company  is  implementing  new  data  retention  policies  for  all  databases  that  run  on  Amazon  RDS  DB  instances.  The company must retain daily backups for a minimum  period of 2 years. The  backups must be consistent and restorable. Which solution should a solutions architect recommend to meet these requirements?

A��Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign t he RDS DB instances to the backup plan.

B��Configure a backup window for the RDS DB instances for daily snapshots. Assign a snapshot retention policy of 2 years to each RDS DB instance. Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule snapshot deletions.

C��Configure database transaction logs to be automatically backed up to Amazon Cloud watch Logs with an expiration period of 2 years.
D ��Configure an AWS Database Migration Service (AWS DMS) replication task. Deploy a replication instance, and configure a change data capture (CDC) task to stream database changes to Amazon S3 as the target. Configure S3 Lifecycle policies to delete the snapshots after 2 years.

�𰸣�A

���� �� Correct Answer A  Detailed  Explanation The correct answer is A  because AWS  Backup provides a centralized way to manage backups across AWS services, including Amazon RDS. By creating a backup plan with a daily schedule and setting the expiration  period to  2 years, the solution  ensures consistent,  restorable  backups that  meet  the  retention  requirement. AWS Backup  automatically  handles  the  lifecycle  of  backups,  simplifying  compliance  and  reducing  manual  effort.  Why  the  other options are incorrect: - B: RDS automated backups have a maximum retention period of 35 days. While manual snapshots can be retained longer, managing them  manually or with Amazon DLM (designed for  EC2 snapshots) is error-prone and not natively integrated with RDS for automated daily backups. - C: Cloud watch Logs are for monitoring, not database backups. Transaction logs alone cannot restore a full database without a base backup, and Cloud watch lacks native tools for point-in-time recovery. - D: AWS DMS replicates data changes but does not create restorable backups. S3 Lifecycle policies manage object expiration but cannot   guarantee   transactional    consistency   or    full   database    recovery.    Reference    Link   -    [AWS    Backup   for   Amazon RDS](https://docs.aws.amazon.com/aws-backup/latest/dev guide/working-with-backups.html)

226.A company ��s compliance team needs to move its file shares to AWS. The shares run on a Windows Server SMB file share. A self-managed on-premises Active Directory controls access to the files and folders. The company wants to use Amazon FSx for Windows File Server as  part of the solution. The company must ensure that the on-premises Active  Directory groups restrict access to the FSx for Windows File Server SMB compliance shares, folders, and files after the move to AWS. The company has created an FSx for Windows File Server file system. Which solution will meet these requirements?

A ��Create an Active Directory Connector to connect to the Active Directory. Map the Active Directory groups to IAM groups to restrict access.

B ��Assign a tag with a Restrict tag key and a Compliance tag value. Map the Active Directory groups to IAM groups to restrict access.

C ��Create an IAM service-linked role that is linked directly to FSx for Windows File Server to restrict access.

D ��Join the file system to the Active Directory to restrict access.

�𰸣�D

���� �� Correct Answer: D Detailed Explanation: To meet the requirements, the FSx for Windows File Server file system must integrate with the existing on-premises Active Directory (AD). Here ��s why: - Option D is correct because joining the FSx file system to the existing AD ensures that file/folder permissions (managed by AD groups) are directly inherited. FSx for Windows uses Windows-native SMB permissions, which rely on AD for authentication and authorization. This setup allows the on-premises AD groups to control access to shares, folders, and files without needing IAM mapping. - Why other options are wrong: - Option A: AWS Directory Service AD Connector acts as a proxy to on-premises AD but doesn ��t store directory data. FSx for Windows requires a full AD join,  not just a  connector. - Option  B: Tags  and  IAM groups are  unrelated to  SMB file  permissions,  which depend on AD, not IAM. - Option C:  IAM roles manage AWS service  permissions (e.g., creating/ modifying FSx resources), not file-level    access    via    SMB.     Reference     Links:    -    [Amazon     FSx    for    Windows     File    Server     Integration    with    Active Directory](https://docs.aws.amazon.com/fsx/latest/Windows guide/aws-ad-integration.html)  -  [How  FSx  for  Windows  Works with Active Directory](https://aws.amazon.com/fsx/windows/features/active-directory/)

227.A company plans to use Amazon Elastic ache for its multi-tier web application. A solutions architect creates a Cache VPC for the Elastic ache cluster and an App VPC for the application ��s Amazon EC2 instances. Both VPCs are in the us-east-1 Region. The solutions architect must implement a solution to provide the application ��s EC2 instances with access to the Elastic ache cluster. Which solution will meet these requirements MOST cost-effectively?

A��Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the Elastic ache cluster ��s security group to allow inbound connection from the application ��s security group.

B ��Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route traffic through the Transit VPC. Configure an  inbound  rule for the  Elastic ache cluster's security group to allow  inbound connection from the application ��s security group.

C��Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the peering connection ��s security group to allow inbound connection from the application ��s security group.

D ��Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route traffic through the Transit VPC. Configure an inbound rule for the Transit VPC ��s security group to allow inbound connection from the application ��s security group.

�𰸣�A

������ Correct Answer: A Detailed Explanation: To enable EC2 instances in t he App VPC to access the Elastic ache cluster in the Cache VPC cost-effectively, the  best solution  is to use VPC  peering  (free within the same AWS  Region) and configure  proper security group rules. Here ��s why: 1. VPC peering directly connects two VPCs in the same Region at no extra cost (unlike Transit VPC/VPN, which incurs hourly charges for VPN gateways or third-party appliances). 2. Route tables in both VPCs must include routes  for  the  peering  connection  to  allow  traffic  flow.  3.  The  Elastic ache  cluster �� s  security  group  must  allow  inbound connections from the EC2 instances �� security group. This ensures only authorized instances can access the cluster. Options B and D involve a Transit VPC, which is unnecessarily complex and costly for connecting just two VPCs. Option C incorrectly refers to  a   "peering  connection  �� s  security  group,"  which   doesn  �� t  exist �� security  groups   are  attached  to   resources  (like EC2/Elastic ache),             not             the              peering             connection             itself.              Reference:             [Amazon              VPC Peering](https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html)               [Security               Groups              for

Elastic ache](https://docs.aws.amazon.com/Amazon elastic ache/latest/red-ug/accessing-elastic ache.html)

228.A company  has a web application  hosted over  10 Amazon  EC2  instances with traffic  directed  by Amazon  Route 53. The company occasionally experiences a timeout error when attempting to browse the application. The networking team finds that some DNS queries return IP addresses of unhealthy instances, resulting in the timeout error. What should a solutions architect implement to overcome these timeout errors?

A ��Create a Route 53 simple routing policy record for each EC2 instance. Associate a health check with each record.

B ��Create a Route 53 failover routing policy record for each EC2 instance. Associate a health check with each record.

C ��Create an Amazon Cloud front distribution with EC2 instances as its origin. Associate a health check with the EC2 instances.

D ��Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances. Route to the ALB from Route 53.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The timeout errors occur because Route 53 occasionally returns IP addresses of	 unhealthy EC2 instances. To fix this, the best solution is to use an Application Load Balancer (ALB) with health checks. Here ��s	 why: 1. How ALB Works: - The ALB automatically distributes traffic across multiple EC2 instances. - It constantly performs health	 checks on the EC2 instances. If an instance fails the check, the ALB stops sending traffic to it. 2. Route 53 + ALB: - Route 53 is	 configured to route traffic to the ALB ��s  DNS name (not individual  EC2 instances). - Users always connect to the ALB, which	 forwards requests only to healthy EC2 instances. - Since the ALB handles health checks, Route 53 no longer needs to manage	 instance health or return individual EC2 IPs. Why Other Options Fail: - A/B: Configuring health checks for each EC2 instance in	 Route 53 is complex and doesn ��t scale well. It also doesn ��t balance traffic efficiently. - C: Cloud front caches content but doesn �� t  directly  solve  the  issue  of  unhealthy  EC2  instances  causing  timeouts.   Reference  Link:  [Application  Load  Balancer  Health	 Checks](https://docs.aws.amazon.com/elastic load balancing/latest/application/load-balancer-target-groups.html#health-checks)

229.A solutions architect needs to design a highly available application consisting of web, application, and database tiers. HTTPS content  delivery  should   be  as   close  to  the  edge   as  possible,  with  the   least  delivery  time.  Which  solution   meets  these requirements and is MOST secure?

A �� Configure  a  public  Application  Load  Balancer  (ALB)  with  multiple  redundant  Amazon  EC2  instances  in  public  subnets. Configure Amazon Cloud front to deliver HTTPS content using the public ALB as the origin.

B ��Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon Cloud front to deliver HTTPS content using the EC2 instances as the origin.

C �� Configure  a  public  Application  Load  Balancer  (ALB)  with  multiple  redundant  Amazon  EC2  instances  in  private  subnets. Configure Amazon Cloud front to deliver HTTPS content using the public ALB as the origin.

D ��Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon Cloud front to deliver HTTPS content using the EC2 instances as the origin.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The correct solution uses Cloud front (a Content Delivery Network) to cache and deliver  HTTPS  content  from  edge  locations,  reducing  latency.  The  EC2  instances  are  placed  in  private  subnets  for  security, ensuring they aren't directly exposed to the internet. The Application Load Balancer (ALB) in a public subnet acts as the origin for

Cloud front, handling traffic routing and SSL termination. This setup combines edge caching (via Cloud front), security (private EC2  instances),  and  scalability  (ALB).  Options  A  and  D  expose  EC2  instances  in  public  subnets,  which  is  insecure.  Option  B incorrectly uses EC2 instances (in private subnets) as the origin, which Cloud front cannot access directly without a public-facing component   like   an   ALB.   Reference   Links:   -   [AWS   Cloud front](https://aws.amazon.com/cloud front/)   -   [Application   Load
Balancer](https://aws.amazon.com/elastic load balancing/application-load-balancer/)                   -                   [AWS                   Subnet
Design](https://docs.aws.amazon.com/vpc/latest/user guide/VPC_Scenario2.html)

230.A company  has a  popular gaming  platform  running  on AWS. The  application  is sensitive to  latency  because  latency  can impact the user experience and introduce unfair advantages to some players. The application is deployed in every AWS Region. It runs on Amazon  EC2  instances  that are  part  of Auto Scaling groups configured  behind Application  Load  Balancers  (ALBs).  A solutions architect  needs to implement a  mechanism to  monitor the  health of the application and redirect traffic to healthy endpoints. Which solution meets these requirements?

A ��Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.

B ��Create an Amazon Cloud front distribution and specify the ALB as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic.

C ��Create an Amazon Cloud front distribution and specify Amazon S3 as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic.

D ��Configure an Amazon Dynamo db database to serve as the data store for the application. Create a Dynamo db Accelerator (DAX) cluster to act as the in-memory cache for Dynamo db hosting the application data.

�𰸣�A

������Correct Answer: A Detailed Explanation: The question describes a latency-sensitive gaming application deployed across all AWS Regions using EC2 instances behind ALBs. The key requirements are health monitoring and automatic traffic redirection to healthy endpoints. Here's why AWS Global Accelerator (Option A) is the best solution: 1. Low Latency Routing: Global Accelerator uses  AWS's  global   network  to  route  traffic  to  the  nearest   healthy  regional  endpoint,   reducing  latency  for  geographically distributed  users. This  aligns with the gaming app's  sensitivity to  latency.  2.  Health  Checks   ��  Failover:  Global  Accelerator continuously monitors the health of ALBs in all regions. If an ALB becomes unhealthy, traffic is automatically rerouted to the next closest healthy region. This ensures minimal disruption to players. 3. Regional Endpoints: By attaching ALBs as regional endpoints, Global Accelerator  directs traffic to  healthy  instances  behind ALBs,  which  already  perform  instance-level  health  checks.  This creates a multi-layered health monitoring system (instance-level via ALB, region-level via Global Accelerator). Why other options are incorrect: - Option B/C (Cloud front): Cloud front is a CDN designed for caching static content. While it can accelerate dynamic content, it lacks  built-in multi-regional failover capabilities.  Lambda-based traffic optimization adds unnecessary complexity. - Option  D  (DAX):  DAX  is  a  database  cache  for  Dynamo db  and  does  nothing  to  address  traffic  routing  or  health  monitoring. Reference                                          Link:                                          [AWS                                          Global                                          Accelerator
Documentation](https://docs.aws.amazon.com/global-accelerator/latest/dg/introduction-how-it-works.html)

231.A company has one million users that use its mobile app. The company must analyze the data usage in near-real time. The company also must encrypt the data in near-real time and must store the data in a centralized location in Apache Parquet format for further processing. Which solution will meet these requirements with the LEAST operational overhead?

A ��Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data. Invoke an AWS Lambda function to send the data to the Kinesis Data Analytics application.

B ��Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data. Invoke an AWS Lambda function to send the data to the EMR cluster.

C ��Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data.

D �� Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The best solution is D because it uses Amazon Kinesis Data Firehose and Kinesis
Data  Analytics,   both  fully   managed   services,  minimizing  operational  overhead.  Here  �� s  why:  1.  Kinesis  Data  Firehose: Automatically ingests streaming data, encrypts it in transit/at rest (via AWS  KMS), converts it to Apache Parquet format, and stores it in Amazon S3. No servers or shard management is needed. 2. Kinesis Data Analytics: A serverless service that analyzes data in  real time  using SQL or Apache  Flink.  It  integrates directly with  Firehose,  avoiding  manual  infrastructure setup. Other options  (A/B/C)  involve  services  like  Kinesis  Data  Streams  (requires  shard  management)  or  Amazon  EMR  (requires  cluster provisioning/ maintenance),      which      add      operational      complexity.      Reference      Links:      -      [Amazon       Kinesis      Data Firehose](https://aws.amazon.com/kinesis/data-firehose/)                      -                       [Amazon                       Kinesis                       Data
Analytics](https://aws.amazon.com/kinesis/data-analytics/)

232.A gaming company has a web application that displays scores. The application runs on Amazon  EC2 instances  behind an Application Load Balancer. The application stores data in an Amazon RDS for MySQL database. Users are starting to experience long  delays  and   interruptions  that  are  caused   by  database   read   performance.  The  company  wants  to   improve  the   user experience while  minimizing changes to the application ��s architecture. What should a solutions architect do to meet these requirements?

A ��Use Amazon Elastic ache in front of the database.

B ��Use RDS Proxy between the application and the database.

C ��Migrate the application from EC2 instances to AWS Lambda.

D ��Migrate the database from Amazon RDS for MySQL to Amazon Dynamo db.

�𰸣�A

������ Correct Answer A �� Use Amazon Elastic ache in front of the database. Detailed Explanation The users are experiencing delays due to database read performance issues. Since the goal is to improve performance with minimal architectural changes, the best solution is to reduce the number of direct read requests to the database. Amazon Elastic ache is a managed caching service (supports Redis or Memcached). By placing it in front of t he RDS database: 1. Frequently accessed data (like game scores) is stored in the cache. 2. The application checks the cache first��if the data exists (a cache hit), it avoids querying the database. 3. This reduces load on t he RDS database, speeds up read operations, and requires only minor code changes (e.g., adding cache logic to the app). Why not other options? - B (RDS Proxy): Primarily solves connection management issues (e.g., for serverless apps like Lambda), not read performance. - C (Lambda): Migrating to serverless would require major code/architecture changes.
- D (Dynamo db): Switching from SQL to NoSQL would require rewriting the data model and app logic, which violates the minimal changes requirement. Reference Link [AWS Elastic ache Documentation](https://aws.amazon.com/elastic ache/)

233.An ecommerce company has noticed performance degradation of its Amazon RDS based web application. The performance

degradation  is  attributed to  an  increase  in the  number  of  read-only  SQL  queries triggered  by  business  analysts. A  solutions architect needs to solve the problem with minimal changes to the existing web application. What should the solutions architect recommend?

A ��Export the data to Amazon Dynamo db and have the business analysts run their queries.

B ��Load the data into Amazon Elastic ache and have the business analysts run their queries.

C ��Create a read replica of the primary database and have the business analysts run their queries.

D ��Copy the data into an Amazon Redshift cluster and have the business analysts run their queries.

�𰸣�C

������ Correct Answer C Detailed Explanation The best solution is C: Create a read replica of the primary database and have the business analysts run their queries. Here's why: 1. Problem Context: The primary database (Amazon RDS) is overloaded due to read-only queries from business analysts. The goal is to reduce load on the primary database with minimal application changes. 2. Why Read Replicas? - Offload Read Traffic: Read replicas are exact copies of the primary database that handle read-only queries. This reduces the load on the primary instance. - Minimal Changes: The web application only needs a small configuration change to direct read queries to the replica ��s endpoint. No code rewrite or data migration is required. - AWS-Managed: RDS automates replication, so there ��s no need for manual data syncing or infrastructure management. 3. Why Other Options Are Less Ideal: - A (Dynamo db):  Switching  to  a   NoSQL  database  would  require  rewriting  queries  and  altering  the  data  model,  which   is  not ��minimal change. ��- B (Elastic ache): Caching works for repetitive queries, not ad-hoc analytics. It also requires application code changes to implement caching logic. - D (Redshift): Redshift is for analytics, not real-time application queries. Migrating data and rewriting         SQL          queries         would          be          time-consuming.          Reference         Link          [Amazon          RDS          Read Replicas](https://aws.amazon.com/rds/features/read-replicas/)

234.A company is using a centralized AWS account to store log data in various Amazon S3 buckets. A solutions architect needs to ensure that the data is encrypted at rest before the data is uploaded to the S3 buckets. The data also must be encrypted in transit. Which solution meets these requirements?

A ��Use client-side encryption to encrypt the data that is being uploaded to the S3 buckets.

B ��Use server-side encryption to encrypt the data that is being uploaded to the S3 buckets.

C��Create bucket policies that require the use of server-side encryption with S3 managed encryption keys (SSE-S3) for S3 uploads.

D ��Enable the security option to encrypt the S3 buckets through the use of a default AWS Key Management Service (AWS KMS) key.

�𰸣�A

������Correct Answer: A Explanation: The question requires data to be encrypted before it is uploaded to S3 (encryption at rest) and  encrypted  during transfer  (encryption  in  transit).  Here �� s  why  A  is  correct:  -  Client-side  encryption  means  the  data  is encrypted on the client ��s side (e.g., using AWS KMS or a custom key) before it is sent to S3. This ensures encryption at rest is applied before upload, meeting the requirement. - Encryption in transit is automatically handled if the client uses HTTPS (SSL/TLS) to upload data to S3, which is the default for AWS SDKs/CLI. Why other options are wrong: - B/D (Server-side encryption): Data is encrypted after it arrives in S3, not before upload. - C (Bucket policies enforcing SSE-S3): Bucket policies only enforce encryption after               upload               (server-side),                not               before.                Reference                Links:               -               [Client-Side

Encryption](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Using client side encryption.html)      -       [Encryption      in Transit](https://docs.aws.amazon.com/AmazonS3/latest/user guide/security-best-practices.html#transit)

235.A solutions architect observes that a nightly batch processing job is automatically scaled up for 1 hour before the desired Amazon EC2 capacity is reached. The peak capacity is the   �� same every night and the batch jobs always start at 1 AM. The solutions architect needs to find a cost-effective solution that will allow for the desired EC2 capacity to be reached quickly and allow the Auto Scaling group to scale down after the batch jobs are complete. What should the solutions architect do to meet these requirements?

A ��Increase the minimum capacity for the Auto Scaling group.

B ��Increase the maximum capacity for the Auto Scaling group.

C ��Configure scheduled scaling to scale up to the desired compute level.

D ��Change the scaling policy to add more EC2 instances during each scaling operation.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The best solution is to configure scheduled scaling. Since the batch jobs start at the same time every night (1 AM) and require a predictable peak capacity, scheduled scaling allows the Auto Scaling group to automatically  increase the  number of  EC2  instances to the  desired  level  exactly when  needed. This  avoids the  1-hour  delay caused by reactive scaling policies (like dynamic scaling based on metrics), ensuring capacity is ready at 1 AM. After the batch jobs finish, the Auto Scaling group can scale back down, optimizing costs. Options A and B don ��t address the scaling delay, and Option  D  might  not  guarantee  timely  scaling  since  it  still  relies  on  reactive  policies.   Reference   Link:   [AWS  Scheduled Scaling](https://docs.aws.amazon.com/auto scaling/ec2/user guide/schedule_time.html)

236.A company serves a dynamic website from a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB). The website needs to support multiple languages to serve customers around the world. The website ��s architecture is running in the us-west-1 Region and is exhibiting high request latency for users that are located in other parts of the world. The website needs to serve requests quickly and efficiently regardless of a user ��s location. However, the company does not want to recreate the existing architecture across multiple Regions. What should a solutions architect do to meet these requirements?

A ��Replace the existing architecture with a website that is served from an Amazon S3 bucket. Configure an Amazon Cloud front distribution with the S3 bucket as the origin. Set the cache behavior settings to cache based on the Accept-Language request header.

B��Configure an Amazon Cloud front distribution with the ALB as the origin. Set the cache behavior settings to cache based on the Accept-Language request header.

C��Create an Amazon API Gateway API that is integrated with the ALB. Configure the API to use the HTTP integration type. Set up an API Gateway stage to enable the API cache based on the Accept-Language request header.

D��Launch an EC2 instance in each additional Region and configure NGINX to act as a cache server for that Region. Put all the EC2 instances and the ALB behind an Amazon Route 53 record set with a geolocation routing policy.

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: The best solution is to use Amazon Cloud front, a Content Delivery Network

(CDN), to cache content at edge locations globally. By setting the ALB as the origin, Cloud front caches dynamic content closer to users,  reducing  latency for  international  users without  rebuilding the  architecture  in  multiple  regions.  Caching  based  on the `Accept-Language`  header  ensures  localized  content  (e.g.,  English,  Spanish)  is  stored  separately,  improving  efficiency.  Other options: A uses S3 for static websites,  but the problem involves dynamic content. C uses API Gateway for APIs, which  is  less efficient for general website caching. D requires managing EC2 instances globally, conflicting with the no recreate architecture requirement.                             Reference                              Links:                             -                              [Amazon                             Cloud front
Caching](https://docs.aws.amazon.com/Amazon cloud front/latest/Developer guide/Header behavior.html)   -    [Cloud front    with ALB
Integration](https://aws.amazon.com/blogs/networking-and-content-delivery/dynamic-whole-site-delivery-with-amazon-cloudfr ont/)

237.A rapidly growing ecommerce company is running its workloads in a single AWS Region. A solutions architect must create a disaster recovery (DR) strategy that includes a different AWS Region. The company wants its database to be up to date in the DR Region with the least possible latency. The remaining infrastructure in the DR Region needs to run at reduced capacity and must be able to scale up if necessary. Which solution will meet these requirements with the LOWEST recovery time objective (RTO)?

A ��Use an Amazon Aurora global database with a pilot light deployment.

B ��Use an Amazon Aurora global database with a warm standby deployment.

C ��Use an Amazon RDS Multi-AZ DB instance with a pilot light deployment.

D ��Use an Amazon RDS Multi-AZ DB instance with a warm standby deployment.

�𰸣�B

������ Correct Answer B �� Use an Amazon Aurora global database with a warm standby deployment. Detailed Explanation The requirements are: 1. Database must be up-to-date in the DR Region with minimal latency   �� Aurora Global Database replicates data across regions with typical replication lag under 1 second, ensuring near-real-time data sync. 2. Infrastructure in DR must run at reduced capacity but scale when needed   �� A warm standby keeps non-database infrastructure (e.g., application servers) running at minimal capacity, so they can scale quickly during a disaster. This minimizes Recovery Time Objective (RTO) compared to a pilot light (where non-database components are stopped and require manual activation). 3. Lowest RTO  �� Aurora Global Database enables fast cross-region failover (seconds to minutes), and warm standby reduces startup delays for non-database components. Why Not Other Options? - A (Aurora + Pilot Light): Pilot light requires starting non-database infrastructure from scratch,  increasing  RTO.  -  C/D  (RDS  Multi-AZ  +  Pilot  Light/Warm  Standby):  RDS  Multi-AZ  only  works  within  a  single  region. Cross-region replication for RDS (e.g., read replicas) has higher latency than Aurora Global Database, making the database less up-to-date                 in                  DR.                  Reference                 Links                  -                  [Amazon                 Aurora                  Global
Database](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/aurora-global-database.html)         -          [Disaster Recovery Options for AWS](https://aws.amazon.com/blogs/aws/disaster-recovery-of-workloads-on-aws-part-1-strategies/)

238.A  company  runs  an  application  on  Amazon  EC2  instances.  The  company  needs  to  implement  a  disaster  recovery  (DR) solution for the application. The DR solution needs to have a recovery time objective (RTO) of less than 4 hours. The DR solution also needs to use the fewest possible AWS resources during normal operations. Which solution will meet these requirements in the MOST operationally efficient way?

A ��Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS Lambda and custom scripts.

B ��Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate

infrastructure deployment in the secondary Region by using AWS Cloud formation.

C ��Launch EC2 instances in a secondary AWS Region. Keep the EC2 instances in the secondary Region active at all times.

D ��Launch EC2 instances in a secondary Availability Zone. Keep the EC2 instances in the secondary Availability Zone active at all times.

�𰸣�B

�� �� �� Correct Answer:  B  Detailed  Explanation:  The  best  solution  is  B  because  it  uses  AWS  Cloud formation  to  automate infrastructure deployment in the secondary Region. AWS Cloud formation provides a standardized, repeatable, and managed way to deploy resources, which is more operationally efficient than custom scripts (Option A). This ensures fast recovery (RTO  ��4 hours) while  minimizing resource usage during normal operations since resources in the secondary Region are only deployed during a disaster. Options C and D are less efficient because they require keeping active EC2 instances running in a secondary Region  or Availability  Zone, which  consumes  more  resources.  Option  A �� s  reliance  on  custom  scripts  adds  complexity  and maintenance  overhead  compared  to  Cloud formation �� s  infrastructure-as-code  approach.  Reference  Links:  -  [AWS  Disaster Recovery       Options](https://aws.amazon.com/blogs/aws/new-aws-cloud formation-updates/)       -        [AWS       Cloud formation Benefits](https://aws.amazon.com/cloud formation/features/)

239.A company runs an internal browser-based application. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales up to 20 instances during work hours, but scales down to 2 instances overnight. Staff are complaining that the application is very slow when the day begins, although it runs well by mid-morning. How should the scaling be changed to address the staff complaints and keep costs to a minimum?

A ��Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens.

B ��Implement a step scaling action triggered at a lower CPU threshold, and decrease the cool down period.

C ��Implement a target tracking action triggered at a lower CPU threshold, and decrease the cool down period.

D ��Implement a scheduled action that sets the minimum and maximum capacity to 20 shortly before the office opens.

�𰸣�C

������ Correct Answer: A. Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens. Explanation: The issue occurs because the Auto Scaling group scales down to 2 instances overnight. When staff start using the app in the morning, the sudden surge in traffic overwhelms the 2 instances, causing slow performance. By the time CPU-based scaling metrics (like target tracking or step scaling) detect high load and trigger scaling, there ��s already a delay, leading to poor user experience. Why A is the best choice: - A scheduled action proactively scales the Auto Scaling group to 20 instances before the office opens. This ensures enough instances are already running when demand spikes, eliminating the lag caused by reactive scaling  (like target tracking or step scaling). - Cost  remains  minimal: The Auto Scaling group  still scales down to 2  instances overnight, so there ��s no unnecessary cost outside work hours. Why C (target tracking) is less effective here: - Target tracking reacts after the load increases, which means instances are added after users already experience slowness. Even with a lower CPU threshold   or   shorter    cool down,   there   �� s   still   a    delay   in   scaling    up.   Reference:    -   [AWS   Auto    Scaling   Scheduled Scaling](https://docs.aws.amazon.com/auto scaling/ec2/user guide/schedule_time.html)      -       [AWS      Auto      Scaling      Target Tracking](https://docs.aws.amazon.com/auto scaling/ec2/user guide/target-tracking.html)

240.A company provides an online service for  posting video content and transcoding it for  use  by any mobile  platform. The

application architecture uses Amazon Elastic File System (Amazon EFS) Standard to collect and store the videos so that multiple Amazon EC2 Linux instances can access the video content for processing. As the popularity of the service has grown over time, the storage costs have become too expensive. Which storage solution is MOST cost-effective?

A ��Use AWS Storage Gateway for files to store and process the video content.

B ��Use AWS Storage Gateway for volumes to store and process the video content.

C ��Use Amazon EFS for storing the video content. Once processing is complete, transfer the files to Amazon Elastic Block Store (Amazon EBS).

D ��Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block Store (Amazon EBS) volume attached to the server for processing.

�𰸣�D

���� �� Correct Answer D Detailed Explanation The most cost-effective solution is D because: 1. Amazon S3 is cheaper than Amazon EFS for storing large amounts of data (like videos). S3 is designed for object storage with lower costs, especially at scale.
2.  EBS  volumes  (attached to  EC2  instances)  are  cheaper  than  EFS  for  temporary  processing.  Since  EBS  is  block  storage  (not shared), you can move only the files needed for processing to an EBS volume, reducing costs compared to keeping all data on EFS.
3. EFS is expensive for large-scale storage (e.g., video files). By using S3 for long-term storage and EBS for temporary processing, you avoid paying for EFS's high per-GB cost. 4. Data transfer from S3 to EC2/EBS is free within the same AWS Region, minimizing extra  costs.  Why  not  the  other  options?  -  A/B   (Storage  Gateway):  Storage  Gateway  is  designed  for   hybrid  cloud  setups (connecting on-premises systems to AWS), not for purely cloud-native applications. It adds unnecessary complexity and costs. - C (EFS + EBS): Keeping videos on EFS still incurs high storage costs. Transferring to EBS after processing doesn��t solve the core issue of expensive EFS storage. - D avoids long-term  EFS costs while leveraging S3 ��s affordability and EBS ��s  low-cost temporary processing.       Reference       Link       -       [Amazon       S3       Pricing](https://aws.amazon.com/s3/pricing/)       -       [Amazon       EFS Pricing](https://aws.amazon.com/efs/pricing/) - [Amazon EBS Pricing](https://aws.amazon.com/ebs/pricing/)

241.A company has an application that is backed by an Amazon Dynamo db table. The company ��s compliance requirements specify that database backups must be taken every month, must be available for 6 months, and must be retained for 7 years. Which solution will meet these requirements?

A ��Create an AWS Backup plan to back up the Dynamo db table on the first day of each month. Specify a lifecycle policy that transitions the backup to cold storage after 6 months. Set the retention period for each backup to 7 years.

B ��Create a Dynamo db on-demand backup of the Dynamo db table on the first day of each month. Transition the backup to Amazon S3 Glacier Flexible Retrieval after 6 months. Create an S3 Lifecycle policy to delete backups that are older than 7 years.

C �� Use  the  AWS  SDK  to  develop  a  script  that  creates  an  on-demand  backup  of  the  Dynamo db  table.  Set  up  an  Amazon Event bridge rule that runs the script on the first day of each month. Create a second script that will run on the second day of each month to transition Dynamo db backups that are older than 6 months to cold storage and to delete backups that are older than 7 years.

D ��Use the AWS CLI to create an on-demand backup of the Dynamo db table. Set up an Amazon Event bridge rule that runs the command on the first day of each  month with a cron expression. Specify  in the command to transition the  backups to cold storage after 6 months and to delete the backups after 7 years.

�𰸣�A

�� �� ��  Correct Answer  A  Detailed  Explanation  The  correct  solution  is  A  because  AWS  Backup  provides  a  fully  managed,
automated way to handle Dynamo db backups while meeting all compliance requirements. Here ��s why: 1. Monthly Backups: AWS Backup can create a scheduled backup plan to automatically take backups on the first day of each month. 2. 6 - Month Availability: AWS Backup ��s lifecycle policies allow backups to be transitioned to cold storage (e.g., AWS Glacier) after 6 months. This ensures backups remain accessible but at a lower cost. 3. 7 - Year Retention: AWS Backup lets you set a retention period of up to 7 years, after which  backups are automatically deleted. Options  B, C, and  D  involve  manual scripting,  custom  lifecycle policies, or  incorrect assumptions  (e.g.,  Dynamo db  backups aren ��t  directly stored  in S3 for  lifecycle  policies). AWS  Backup simplifies compliance by handling scheduling, transitions to cold storage, and retention automatically. Reference Links -  [AWS Backup  for  Dynamo db](https://docs.aws.amazon.com/aws-backup/latest/dev guide/Dynamo db.html)  -  [AWS  Backup  Lifecycle Policies](https://docs.aws.amazon.com/aws-backup/latest/dev guide/lifecycle.html)

242.A company is using Amazon Cloud front with its website. The company has enabled logging on the Cloud front distribution, and logs are saved in one of the company ��s Amazon S3 buckets. The company needs to perform advanced analyses on the logs and build visualizations. What should a solutions architect do to meet these requirements?

A ��Use standard SQL queries in Amazon Athena to analyze the Cloud front logs in the S3 bucket. Visualize the results with AWS Glue.

B��Use standard SQL queries in Amazon Athena to analyze the Cloud front logs in the S3 bucket. Visualize the results with Amazon Quick sight.

C ��Use standard SQL queries in Amazon Dynamo db to analyze the Cloud front logs in the S3 bucket. Visualize the results with AWS Glue.

D ��Use standard SQL queries in Amazon Dynamo db to analyze the Cloud front logs in the S3 bucket. Visualize the results with Amazon Quick sight.

�𰸣�B

������ Correct Answer: B. Use standard SQL queries in Amazon Athena to analyze the Cloud front logs in the S3 bucket. Visualize the results with Amazon Quick sight. Detailed Explanation: 1. Why Athena? Cloud front logs are stored as raw files in Amazon S3. Amazon Athena is a serverless query service that lets you analyze data in S3 using standard SQL. It directly reads log files (like CSV, JSON)  without  needing  to  load  data  into  a  database.  Dynamo db  (options  C  and  D)  is  a  NoSQL  database,  not  designed  for querying S3 logs. 2. Why Quick sight? Amazon Quick sight is a business intelligence (BI) tool for creating interactive visualizations (charts, dashboards) from Athena query results. AWS Glue (options A and C) is an ETL (Extract,Transform, Load) service for data preparation, not visualization. 3. Workflow: - Cloud front logs  �� S3  �� Athena (SQL queries)   �� Quick sight (visualizations). - No     need      to      move     data      or      use      Dynamo db/Glue.      Reference     Links:      -      [Analyze      Cloud front     logs      with Athena](https://docs.aws.amazon.com/athena/latest/ug/cloud front-logs.html)                                  -                                   [Quick sight
visualization](https://docs.aws.amazon.com/quick sight/latest/user/getting-started.html)

243.A company runs a fleet of web servers using an Amazon RDS for Postgresql DB instance. After a routine compliance check, the company sets a standard that requires a recovery point objective (RPO) of less than 1 second for all its production databases. Which solution meets these requirements?

A ��Enable a Multi-AZ deployment for the DB instance.

B ��Enable auto scaling for the DB instance in one Availability Zone.

C ��Configure the DB instance in one Availability Zone, and create multiple read replicas in a separate Availability Zone.

D ��Configure the DB instance in one Availability Zone, and configure AWS Database Migration Service (AWS DMS) change data capture (CDC) tasks.

�𰸣�A

������Correct Answer: A. Enable a Multi-AZ deployment for the DB instance. Detailed Explanation: The Recovery Point Objective (RPO) defines the maximum acceptable amount of data loss measured in time. To achieve an RPO of less than 1 second, the database must replicate transactions to a standby copy synchronously (instantly), ensuring zero data loss if the primary database fails. - Option A (Correct): Multi-AZ deployments use synchronous replication to a standby replica in another Availability Zone. Every write operation is confirmed only after it is committed to both the primary and standby databases. This guarantees an RPO of 0 seconds (no data loss) during a failure. - Option B (Wrong): Auto-scaling in one AZ only handles performance scaling (e.g., adding read replicas for read-heavy workloads). It does not provide disaster recovery or synchronous replication. If the AZ fails, all data since the last backup is lost, leading to a high RPO. - Option C (Wrong): Read replicas in RDS use asynchronous replication, meaning there is a small delay (replication lag) between the primary and replicas. If the primary fails, the replicas might miss recent  transactions,   resulting   in  an   RPO  greater  than   1   second.  -  Option   D   (Wrong):  AWS   DMS   CDC   replicates   data asynchronously. Even though it can capture changes quickly, it is not designed for real-time disaster recovery. The RPO depends on        replication        latency,        which        might        exceed         1        second.        Reference         Links:        -        [Amazon        RDS Multi-AZ](https://aws.amazon.com/rds/features/multi-az/)                       -                        [Amazon                        RDS                        Read
Replicas](https://aws.amazon.com/rds/features/read-replicas/)               -                [AWS                DMS                Change                Data Capture](https://docs.aws.amazon.com/dms/latest/user guide/CHAP_Task.CDC.html)

244.A company runs a web application that is deployed on Amazon EC2 instances in the private subnet of a VPC. An Application Load  Balancer  (ALB) that  extends  across the  public subnets  directs web traffic to the  EC2  instances. The  company wants to implement new security measures to restrict inbound traffic from the ALB to the EC2 instances while preventing access from any other source inside or outside the private subnet of the EC2 instances. Which solution will meet these requirements?

A ��Configure a route in a route table to direct traffic from the internet to the private IP addresses of the EC2 instances.

B ��Configure the security group for the EC2 instances to only allow traffic that comes from the security group for the ALB. C ��Move the EC2 instances into the public subnet. Give the EC2 instances a set of Elastic IP addresses.

D ��Configure the security group for the ALB to allow any TCP traffic on any port.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution is to configure the EC2 instances' security group to only allow inbound traffic from the ALB's  security group. Security groups act  as virtual firewalls for AWS  resources.  By  setting the  EC2 security group rule to reference the ALB ��s security group (instead of IP addresses), traffic is automatically restricted to only the ALB. This works  because AWS tracks the ALB's  underlying  infrastructure  and ensures only traffic originating from the ALB  is permitted. Other options either expose the EC2 instances to unwanted traffic (like Option C) or mis configure routing/security rules          (like         Options          A          and          D).          Reference          Link:          [Security          Groups          for          ALB          and EC2](https://docs.aws.amazon.com/elastic load balancing/latest/application/load-balancer-update-security-groups.html)

245.A  research company runs experiments that are  powered  by a simulation application and a visualization application. The simulation  application   runs  on   Linux  and  outputs   intermediate  data  to  an   NFS  share  every   5   minutes.  The  visualization application is a Windows desktop application that displays the simulation output and requires an SMB file system. The company

maintains two synchronized file systems. This strategy is causing data duplication and inefficient resource usage. The company needs to migrate the applications to AWS without making code changes to either application. Which solution will meet these requirements?

A ��Migrate both applications to AWS Lambda. Create an Amazon S3 bucket to exchange data between the applications.

B ��Migrate both applications to Amazon Elastic Container Service (Amazon ECS). Configure Amazon FSx File Gateway for storage.

C �� Migrate the simulation application to  Linux Amazon EC2 instances.  Migrate the visualization application to Windows  EC2 instances. Configure Amazon Simple Queue Service (Amazon SQS) to exchange data between the applications.

D �� Migrate the simulation application to Linux Amazon  EC2 instances.  Migrate the visualization application to Windows EC2 instances. Configure Amazon FSx for NetApp ONTAP for storage.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The company needs to migrate both applications to AWS without code changes. The simulation app requires Linux and NFS, while the visualization app needs Windows and SMB. Using Amazon FSx for NetApp ONTAP solves the problem because it supports multi - protocol file storage. This means both NFS (for Linux EC2) and SMB (for Windows  EC2)  can  access  the  same  file  system  simultaneously,  eliminating  data  duplication.  The  other  options  either  use incompatible storage (S3/SQS), require code changes, or don ��t resolve the dual - protocol requirement. FSx ONTAP allows both apps to read/write to a single shared storage layer, maintaining compatibility with their original OS/file system requirements. Reference Link: [Amazon FSx for NetApp ONTAP](https://aws.amazon.com/fsx/netapp-ontap/)

246.As part of budget planning, management wants a report of AWS billed items listed by user. The data will be used to create department budgets. A solutions architect needs to determine the most efficient way to obtain this report information. Which solution meets these requirements?

A ��Run a query with Amazon Athena to generate the report.

B ��Create a report in Cost Explorer and download the report.

C ��Access the bill details from the billing dashboard and download the bill.

D ��Modify a cost budget in AWS Budgets to alert with Amazon Simple Email Service (Amazon SES).

�𰸣�B

������ Correct Answer B Detailed Explanation To generate a report of AWS costs grouped by user for department budgets, AWS Cost Explorer (Option B) is the most efficient solution. Here ��s why: 1. Cost Explorer allows you to view and analyze costs filtered by tags. If your AWS resources (e.g., EC2 instances, S3 buckets) are tagged with user/department identifiers, Cost Explorer can group costs by these tags. You can then generate and download a customized report directly. 2. Option C (billing dashboard) provides a general cost breakdown but lacks granularity unless cost allocation tags are enabled. Without proper tagging, it won �� t show costs by user. 3. Option A (Athena) requires manually setting up queries on raw billing data stored in S3, which is more complex and time - consuming. 4. Option D (Budgets with SES) focuses on alerts for budget thresholds, not detailed reporting. Key Assumption: The organization uses cost allocation tags (e.g., `user:john`) to track resource ownership. If tags aren ��t set up, enabling     them      is     a      prerequisite     for      any      user     -      specific     cost      reporting.      Reference      Link     [AWS      Cost Explorer](https://aws.amazon.com/aws-cost-management/aws-cost-explorer/)

247.A company hosts its static website  by using Amazon S3. The company wants to add a contact form to its webpage. The contact form will have dynamic server-side components for users to input their name, email address, phone number, and user message. The company anticipates that there will  be fewer than  100 site visits each  month. Which solution will  meet these requirements MOST cost-effectively?

A ��Host a dynamic contact form page in Amazon Elastic Container Service (Amazon ECS). Set up Amazon Simple Email Service (Amazon SES) to connect to any third-party email provider.

B ��Create an Amazon API Gateway endpoint with an AWS Lambda backend that makes a call to Amazon Simple Email Service (Amazon SES).

C ��Convert the static webpage to dynamic by deploying Amazon Light sail. Use client-side scripting to build the contact form. Integrate the form with Amazon Work mail.

D ��Create a t2.micro Amazon EC2 instance. Deploy a LAMP (Linux, Apache, MySQL, PHP/Perl/Python) stack to host the webpage. Use client-side scripting to build the contact form. Integrate the form with Amazon Work mail.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The most cost-effective solution is B because it uses serverless technologies (API Gateway and Lambda) that charge only when the contact form is used. Since the company expects fewer than 100 visits/month, these services will incur minimal or no cost (within AWS Free Tier limits). Here��s why: - API Gateway handles HTTP requests from the website ��s contact form. - Lambda processes the form data (e.g., validating inputs) and triggers Amazon SES to send emails. SES is  inexpensive for low-volume email sending. -  No servers (like EC2 or  ECS) are  running 24/7, avoiding fixed costs. Other options are less optimal: - A (ECS): Running containers continuously costs more than serverless. - C (Light sail): Converting the entire static site to dynamic is unnecessary and adds complexity. - D (EC2): A t2.micro instance has a monthly cost even with minimal    usage.    Reference    Links:    -    [AWS    Lambda    Pricing](https://aws.amazon.com/lambda/pricing/)    -    [Amazon    SES Pricing](https://aws.amazon.com/ses/pricing/)

248.A company  has a static website that  is  hosted on Amazon Cloud front  in front of Amazon S3. The static website  uses  a database backend. The company notices that the website does not reflect updates that have been made in the website ��s Git repository. The company checks the continuous integration and continuous delivery (CI/CD) pipeline between the Git repository and  Amazon  S3.  The  company  verifies  that  the  web hooks  are  configured  properly  and  that  the  CI/CD  pipeline  is  sending messages that indicate successful deployments. A solutions architect needs to implement a solution that displays the updates on the website. Which solution will meet these requirements?

A ��Add an Application Load Balancer.

B ��Add Amazon Elastic ache for Redis or Memcached to the database layer of the web application. C ��Invalidate the Cloud front cache.

D ��Use AWS Certificate Manager (ACM) to validate the website ��s SSL certificate.

�𰸣�C

������ Correct Answer: C. Invalidate the Cloud front cache. Explanation: When you update files in an Amazon S3 bucket that's behind Amazon Cloud front, Cloud front might still serve the old (cached) version of the files until the cache expires. This happens because Cloud front caches content at its edge locations to improve performance. Even if the CI/CD pipeline successfully deploys

updates to S3, Cloud front's cache might not immediately reflect these changes. Here's a simple analogy: Imagine Cloud front as a librarian who  keeps copies of  popular  books  (your website files)  on a shelf.  Even  if the original  book  (S3) gets  updated, the librarian (Cloud front) won ��t know to replace the shelf copy unless you explicitly tell them to check for updates. Invalidate the Cloud front cache is like asking the librarian to clear the shelf and grab the latest version of the book from the original storage (S3). The other options are unrelated to the problem: - A (Application Load Balancer): Adds traffic distribution but doesn ��t fix caching. - B (Elastic ache): Caches database queries, not static website files. - D (ACM SSL validation): Ensures HTTPS works but doesn         ��    t          affect          content          updates.          Reference          Link:          [Invalidating          Cloud front          Cached Objects](https://docs.aws.amazon.com/Amazon cloud front/latest/Developer guide/Invalidation.html)

249.A company wants to migrate a Windows-based application from on premises to the AWS Cloud. The application has three tiers: an  application tier, a  business tier, and  a database tier with  Microsoft  SQL Server. The company wants to  use  specific features of SQL Server such as native backups and Data Quality Services. The company also needs to share files for processing between the tiers. How should a solutions architect design the architecture to meet these requirements?

A ��Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing between the tiers.

B ��Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file sharing between the tiers.

C ��Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use Amazon Elastic File System (Amazon EFS) for file sharing between the tiers.

D �� Host the application tier and the  business tier on Amazon  EC2  instances.  Host the database tier on Amazon  RDS.  Use  a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume for file sharing between the tiers.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The company needs to migrate a Windows-based application with specific SQL Server features (native backups, Data Quality Services) and cross-tier file sharing. Here's why Option B is the best choice: 1. Why EC2 for the database tier (SQL Server): Amazon RDS for SQL Server has limitations on certain features like native backups (e.g., `BACKUP/RESTORE` commands) and  Data Quality Services  (DQS).  Using  EC2  allows full  control over the SQL  Server  instance, enabling these  specific features.  2. Why  FSx for Windows  File  Server for file sharing: - Amazon  FSx for Windows  File  Server provides  fully  managed,  native  Windows  file  sharing  (SMB   protocol),  which  is  ideal  for  Windows-based  applications.  - Alternatives are unsuitable: Option A's FSx File Gateway is designed for hybrid cloud storage (caching on-premises data), not pure AWS environments. Option C's  EFS  is  Linux-oriented  (NFS protocol) and  incompatible with Windows. Option  D's  EBS  is  block storage for single-instance  use,  not  multi-tier  file  sharing.  3.  Why  not  RDS  (Options  C/D):  RDS  abstracts  away  the  OS  layer, preventing  direct  access  to  SQL  Server  features  like  native  backups  or  DQS.  EC2  is  required  for  full  administrative  control. Reference   Links:  -   [Amazon   FSx  for  Windows  File  Server](https://aws.amazon.com/fsx/windows/)  -   [RDS  for  SQL  Server Limitations](https://docs.aws.amazon.com/Amazon rds/latest/User guide/CHAP_Sql server.html)    -    [EC2    vs.    RDS    for    SQL Server](https://aws.amazon.com/blogs/database/migrating-microsoft-sql-server-workloads-to-amazon-rds-or-amazon-ec2/)

250.A company is migrating a Linux-based web server group to AWS. The web servers must access files in a shared file store for some content. The company must not make any changes to the application. What should a solutions architect do to meet these requirements?

A ��Create an Amazon S3 Standard bucket with access to the web servers.

B ��Configure an Amazon Cloud front distribution with an Amazon S3 bucket as the origin.

C ��Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on all web servers.

D ��Configure a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume to all web servers.

�𰸣�C

�� �� �� Correct Answer: C  Detailed  Explanation: The  correct  solution  is to  use Amazon  Elastic  File  System  (EFS)  because  it provides a scalable, fully managed network file system (NFS) that multiple Linux-based servers can access simultaneously. Since the company cannot  modify the application,  EFS is  ideal  because  it works  like a traditional file system. The web servers can mount the EFS file system just like a local drive, requiring no changes to the application code. - Why not S3 (A/B): Amazon S3 is object storage, not a file system. Applications need to use API calls to access S3, which would require code changes. - Why not EBS  (D): An  EBS  volume  can only  be  attached to  one  EC2  instance  at  a time,  making  it  unsuitable  for  shared  access  across multiple  servers.  -  EFS  (C)  solves  the  problem  seamlessly  by  offering  shared,  low-latency  file  storage  compatible  with  Linux servers via standard NFS protocols. Reference Link: [Amazon EFS Overview](https://aws.amazon.com/efs/)

251.A company has an AWS Lambda function that needs read access to an Amazon S3 bucket that is located in the same AWS account. Which solution will meet these requirements in the MOST secure manner?

A ��Apply an S3 bucket policy that grants read access to the S3 bucket.

B ��Apply an IAM role to the Lambda function. Apply an IAM policy to t he role to grant read access to the S3 bucket.

C��Embed an access key and a secret key in the Lambda function ��s code to grant the required IAM permissions for read access to the S3 bucket.

D ��Apply an IAM role to the  Lambda function. Apply an  IAM policy to t he role to grant read access to all S3 buckets in the account.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The most secure way to grant a Lambda function read access to an S3 bucket in the same AWS account is by using an IAM role attached to the Lambda function with a policy that grants read access specifically to the  required  S3  bucket.  Here's  why:  1.  Principle  of  Least  Privilege:  Option  B  ensures  the  Lambda  function  only  gets  the permissions it needs (read access to the target bucket). Option D is insecure because granting access to all S3 buckets in the account  violates  this   principle.   2.  Temporary  Credentials:   IAM   roles   provide  temporary  security  credentials,  which  are automatically rotated and more secure than long - term credentials like access keys (Option C), which risk exposure if embedded in code. 3. Centralized Management: Managing permissions via IAM roles (Option B) is cleaner and more scalable than bucket policies (Option A). While bucket policies can work, they are typically used for cross - account access or public buckets. For same - account access,  IAM  roles are the recommended  best practice. 4. Avoid  Hardcoding Secrets: Option C is  highly insecure, as hardcoded access  keys can  leak through code repositories or  logs,  leading to  potential account compromise.  Reference  Link: [AWS Security Best Practices for Lambda](https://docs.aws.amazon.com/lambda/latest/dg/security - best - practices.html)

252.A company hosts a web application on multiple Amazon EC2 instances. The EC2 instances are in an Auto Scaling group that scales  in  response  to  user  demand. The  company wants to optimize cost savings without  making  a  long-term  commitment. Which EC2 instance purchasing option should a solutions architect recommend to meet these requirements?

A ��Dedicated Instances only

B ��On-Demand Instances only

C ��A mix of On-Demand Instances and Spot Instances

D ��A mix of On-Demand Instances and Reserved Instances

�𰸣�C

���� �� Correct Answer: C. A mix of On-Demand Instances and Spot Instances Detailed Explanation: The best choice here is C because  Spot  Instances  provide  up  to  90%  cost  savings  compared  to  On-Demand  Instances,  which  aligns  with  the  goal  of optimizing costs. Since the application uses Auto Scaling, it can handle temporary interruptions of Spot Instances (if reclaimed by AWS)  by  automatically  replacing  them.  On-Demand  Instances  ensure  a  baseline  capacity  for  reliability,  while  Spot  Instances handle variable/scaling workloads. Reserved Instances (Option D) require a 1- or 3-year commitment, which conflicts with the no long-term commitment  requirement.  Dedicated  Instances  (A) are expensive and  unnecessary  here, and  pure On-Demand  (B) doesn��t maximize savings. Reference Links: - AWS Spot Instances: [link](https://aws.amazon.com/ec2/spot/) - Auto Scaling with Spot: [link](https://docs.aws.amazon.com/auto scaling/ec2/user guide/asg-instance-purchasing-options.html)

253.A company has an on-premises volume backup solution that has reached its end of life. The company wants to use AWS as part of a new backup solution and wants to maintain local access to all the data while it is backed up on AWS. The company wants  to  ensure  that  the  data  backed  up  on  AWS  is  automatically  and  securely  transferred.  Which  solution  meets  these requirements?

A ��Use AWS Snowball to migrate data out of the on-premises solution to Amazon S3. Configure on-premises systems to mount the Snowball S3 endpoint to provide local access to the data.

B��Use AWS Snowball Edge to migrate data out of the on-premises solution to Amazon S3. Use the Snowball Edge file interface to provide on-premises systems with local access to the data.

C ��Use AWS Storage Gateway and configure a cached volume gateway. Run the Storage Gateway software appliance on premises and configure a percentage of data to cache locally. Mount the gateway storage volumes to provide local access to the data.

D ��Use AWS Storage Gateway and configure a stored volume gateway. Run the Storage Gateway software appliance on premises and map the gateway storage volumes to on-premises storage. Mount the gateway storage volumes to provide local access to the data.

�𰸣�D

���� �� Correct Answer: D Detailed Explanation: The company needs to maintain local access to all data while automatically backing it up to AWS securely. Option D (Stored Volume Gateway) is correct because: 1. Local Access to Full Data: The stored volume gateway keeps the entire dataset on - premises, allowing instant access to all data locally. 2. Automatic AWS Backup: It asynchronously  copies snapshots of the on -  premises  data to Amazon  S3,  ensuring  backups  are automatically  and securely transferred to AWS. 3. No Data Caching Trade - offs: Unlike cached volumes (Option C), which store only a subset of data locally, stored volumes  retain the full dataset on - premises, meeting the requirement for uninterrupted local access. Why not other options? - A/B (Snowball): Snowball is for one - time data transfers, not ongoing backups. - C (Cached Gateway): Only keeps frequently  accessed  data  locally,  violating  the  all  data  access  requirement.   Reference  Link:   [AWS  Storage  Gateway  Stored Volumes](https://docs.aws.amazon.com/storage gateway/latest/user guide/Storage gateway concepts.html#storage   -   gateway   -
volumes)

254.An application that is hosted on Amazon EC2 instances needs to access an Amazon S3 bucket. Traffic must not traverse the internet. How should a solutions architect configure access to meet these requirements?

A ��Create a private hosted zone by using Amazon Route 53.

B ��Set up a gateway VPC endpoint for Amazon S3 in the VPC.

C ��Configure the EC2 instances to use a NAT gateway to access the S3 bucket.

D ��Establish an AWS Site-to-Site VPN connection between the VPC and the S3 bucket.

�𰸣�B

������Correct Answer: B. Set up a gateway VPC endpoint for Amazon S3 in the VPC. Detailed Explanation: To allow EC2 instances
in a VPC to access an S3 bucket without internet traffic, a gateway VPC endpoint is the right solution. Here's why: 1. How it works:
- A gateway VPC endpoint creates a private connection between your VPC and S3. - Traffic stays entirely within the AWS network, avoiding the  public  internet.  2.  Why  not  other  options:  -  A  (Route  53  Private  Hosted  Zone):  This  manages  DNS  for  private domains, but S3 access doesn ��t require DNS changes. It doesn ��t solve the routing issue. - C (NAT Gateway): A NAT gateway lets private  EC2  instances  access  the  internet,  but  traffic  still  goes  through  the  public  internet,  violating  the  requirement.  -  D (Site-to-Site VPN): VPNs connect your VPC to on-premises  networks,  not to AWS services  like S3. S3 doesn ��t support VPN connections directly. 3. Benefits of a Gateway VPC Endpoint: - Security: No exposure to the public internet. - Simplicity: No need for NAT, VPNs, or public IPs. - Cost: No data transfer charges for traffic between the VPC and S3. Reference Links: - [AWS VPC Endpoints    for    S3](https://docs.aws.amazon.com/vpc/latest/private link/vpc-endpoints-s3.html)    -     [Gateway    vs.     Interface Endpoints](https://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/)

255.An  ecommerce company stores terabytes of customer  data  in the AWS  Cloud. The  data  contains  personally  identifiable information (P II). The company wants to use the data in three applications. Only one of the applications needs to process the P II. The P II must be removed before the other two applications process the data. Which solution will meet these requirements with the LEAST operational overhead?

A ��Store the data in an Amazon Dynamo db table. Create a proxy application layer to intercept and process the data that each application requests.

B ��Store the data in an Amazon S3 bucket. Process and transform the data by using S3 Object Lambda before returning the data to the requesting application.

C ��Process the data and store the transformed data in three separate Amazon S3 buckets so that each application has its own custom dataset. Point each application to its respective S3 bucket.

D ��Process the data and store the transformed data in three separate Amazon Dynamo db tables so that each application has its own custom dataset. Point each application to its respective Dynamo db table.

�𰸣�B

�� �� ��  Correct  Answer:  B  Detailed  Explanation:  Amazon  S3  Object  Lambda  allows  you  to  process  data  on-the-fly  when applications retrieve it from S3.  Instead of maintaining multiple copies of data (like in Options C/D) or building a proxy layer (Option A), you  create  a  single  Lambda  function  to  remove  P II  dynamically.  This  approach  avoids  data  duplication,  reduces storage  costs,   and   leverages  AWS-managed   scaling.   Only  the   application   needing   P II   receives   raw   data,   while   others automatically get filtered data via the same S3  bucket. This  serverless  design  minimizes  operational tasks  like  infrastructure management. Reference Links: - [S3 Object Lambda Overview](https://aws.amazon.com/s3/features/object-lambda/) - [How S3 Object Lambda Works](https://docs.aws.amazon.com/AmazonS3/latest/user guide/transforming-objects.html)

256.A development team has launched a new application that is hosted on Amazon EC2 instances inside a development VPC. A solutions architect needs to create a new VPC in the same account. The new VPC will be peered with the development VPC. The VPC CIDR block for the development VPC is 192.168.0.0/24. The solutions architect needs to create a CIDR block for the new VPC. The CIDR block must be valid for a VPC  peering connection to the development VPC. What is the Smallest CIDR  block that meets these requirements?

A ��10.0.1.0/32

B ��192.168.0.0/24 C ��192.168.1.0/32 D ��10.0.1.0/24

�𰸣�D

���� �� Correct Answer: D. 10.0.1.0/24 Detailed Explanation: To establish a VPC peering connection, the two VPCs must have non-overlapping CIDR blocks. The existing development VPC uses 192.168.0.0/24 (covers IPs from 192.168.0.0 to 192.168.0.255). The new VPC ��s CI DR must avoid this range entirely. Let ��s evaluate the options: - A. 10.0.1.0/32: Invalid. A VPC requires a CIDR block between /16 and /28. A /32 (a single IP address) is too small and not allowed for a VPC. - B. 192.168.0.0/24: Invalid. This CIDR overlaps with the development VPC ��s CI DR, making peering impossible. - C. 192.168.1.0/32: Invalid. A /32 is too small for a VPC.  Even  if  the  size  were  valid,  192.168.1.0/24  (a  /24  range)  would  work,  but  this  option �� s  /32  size  disqualifies  it.  -  D.
10.0.1.0/24: Valid. This CIDR is in the 10.0.0.0/8 private range and does not overlap with 192.168.0.0/24. The /24 size is allowed for  a VPC  (minimum /28,  maximum  /16).  Key  Take away:  VPC  peering  requires  non-overlapping  CIDR  blocks.  While  a  /28  is technically the smallest allowed CIDR for a VPC, none of the options provide a valid smaller CIDR. Among the choices, D is the only           valid           and           non-overlapping           option.           Reference            Links:           -           [AWS           VPC           Peering Requirements](https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-basics.html)          -           [VPC          CIDR           Block Rules](https://docs.aws.amazon.com/vpc/latest/user guide/VPC_Subnets.html#vpc-sizing-ipv4)

257.A company deploys an application on five Amazon EC2 instances. An Application Load Balancer (ALB) distributes traffic to the instances  by using a target group. The average CPU usage on each of the instances is  below  10% most of the time, with occasional surges to 65%. A solutions architect needs to implement a solution to automate the scalability of the application. The solution must optimize the cost of the architecture and must ensure that the application has enough CPU resources when surges occur. Which solution will meet these requirements?

A ��Create an Amazon Cloud watch alarm that enters the ALARM state when the Cpu utilization metric is less than 20%. Create an AWS Lambda function that the Cloud watch alarm invokes to terminate one of the EC2 instances in the ALB target group.

B ��Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set a target tracking scaling policy that is based on the As g average cpu utilization metric. Set the minimum instances to 2, the desired capacity to 3, the maximum instances to 6, and the target value to 50%. Add the EC2 instances to the Auto Scaling group.

C��Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set the minimum instances to 2, the desired capacity to 3, and the maximum instances to 6. Add the EC2 instances to the Auto Scaling group.

D ��Create two Amazon Cloud watch alarms. Configure the first Cloud watch alarm to enter the ALARM state when the average

Cpu utilization  metric  is  below  20%.  Configure  the  second  Cloud watch  alarm  to  enter  the  ALARM  state  when  the  average Cpu utilization matric is above 50%. Configure the alarms to publish to an Amazon Simple  Notification Service (Amazon SNS) topic to send an email message. After receiving the message, log in to decrease or increase the number of EC2 instances that are running.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The correct solution is B because it uses EC2 Auto Scaling with a target tracking scaling policy based on the As g average cpu utilization metric.  Here ��s why this works best:  1. Cost Optimization: - The Auto Scaling group reduces the number of instances when CPU usage is low (below 50%). By setting the minimum instances to 2, the company avoids over provisioning and saves costs during periods of low traffic. The existing setup (5 instances running constantly) is wasteful because CPU  usage is mostly below  10%. 2. Automated Scalability: - The target tracking policy automatically adds instances when the average CPU  usage across the group exceeds 50%  (e.g., during surges). This ensures the application  has enough resources to handle traffic spikes. No manual intervention is needed. 3. Alignment with Requirements: - The existing ALB and target group are  reused,  avoiding  unnecessary  complexity. - The  maximum  instances  limit  (6)  prevents  over provisioning during extreme surges, keeping costs predictable. Why Other Options Fail: - A and D rely on manual actions (Lambda termination or email alerts), which are not fully automated. - C lacks a scaling policy, so the Auto Scaling group won ��t adjust instance counts dynamically. - B is the only option that balances automation, cost, and performance. Reference Link: [AWS Auto Scaling Target Tracking Policies](https://docs.aws.amazon.com/auto scaling/ec2/user guide/as-scaling-target-tracking.html)

258.A company is running a critical business application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances run in an Auto Scaling group and access an Amazon RDS DB instance. The design did not pass an operational review because the EC2 instances and the DB instance are all located in a single Availability Zone. A solutions architect must update the design to use a second Availability Zone. Which solution will make the application highly available?

A �� Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.

B �� Provision two subnets that extend across  both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.

C �� Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.

D �� Provision  a  subnet  that  extends  across  both  Availability  Zones.  Configure  the  Auto  Scaling  group  to  distribute the  EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.

�𰸣�C

���� �� Correct Answer C Detailed Explanation To achieve high availability, the solution must ensure redundancy across two Availability Zones (AZs) for both the EC2 instances and t he RDS database. Here ��s why Option C is correct: 1. EC2 Instances and Auto Scaling Group: - You need two subnets, each in a separate AZ. - The Auto Scaling group must launch EC2 instances across these subnets. This ensures that if one AZ fails, instances in the other AZ can still handle traffic. 2. RDS Database: - A single - AZ RDS instance is a single point of failure. - Enabling Multi - AZ deployment for RDS automatically provisions a standby replica in a different AZ. During failures, RDS fails over to the standby, minimizing downtime. Why Other Options Fail: - A: Uses Multi - AZ for EC2 but leaves RDS in a single AZ (no database redundancy). - B: Incorrectly claims subnets extend across both AZs (subnets are confined to one AZ). - D: Repeats the subnet/AZ mistake in B and incorrectly describes t he RDS configuration. Reference Links - [Auto Scaling Groups and AZs](https://docs.aws.amazon.com/auto scaling/ec2/user guide/auto - scaling -  benefits.html) -  [RDS Multi - AZ Deployments](https://docs.aws.amazon.com/Amazon rds/latest/User guide/Concepts.MultiAZ.html)

259.A research laboratory needs to process approximately 8 TB of data. The laboratory requires sub-millisecond latencies and a minimum throughput  of  6  GBps for the  storage  subsystem.  Hundreds  of Amazon  EC2  instances  that  run  Amazon  Linux  will distribute and process the data. Which solution will meet the performance requirements?

A��Create an Amazon FSx for NetApp ONTAP file system. Sat each volume ��tiering policy to ALL. Import the raw data into the file system. Mount the fila system on the EC2 instances.

B �� Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses  persistent SSD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.

C ��Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent HDD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.

D��Create an Amazon FSx for NetApp ONTAP file system. Set each volume ��s tiering policy to NONE. Import the raw data into the file system. Mount the file system on the EC2 instances.

�𰸣�B

�� �� �� Correct Answer:  B  Detailed  Explanation:  The  scenario  requires  sub-millisecond  latency  and  6  GBps  throughput  for processing 8 TB of data across hundreds of EC2 instances. Here ��s why Amazon FSx for Lustre with persistent SSD storage (Option B) is the best fit: 1. High Throughput   �� Low Latency: - FSx for Lustre is purpose-built for high-performance computing (HPC) and parallel workloads. It delivers sub-millisecond latencies and scales to hundreds of GBps of throughput when configured with SSD  storage. This  meets  the  6  GBps  requirement.  -  Persistent  SSD  storage  ensures  consistent  performance  for  long-running workloads. 2. Integration with Amazon S3: - FSx for Lustre can directly import/export data from/to Amazon S3, making it easy to stage raw data in S3 (a cost-effective storage layer) and process it at high speed using Lustre. 3. Why Other Options Fail: - Options A/D (FSx for NetApp ONTAP): NetApp ONTAP is designed for enterprise NAS workloads (e.g., file sharing), not HPC. It can��t match Lustre ��s throughput or latency for parallel processing. - Option C (FSx for Lustre with HDD): HDD-based Lustre is cheaper but maxes  out  at  ~1.25  GBps   per  file  system,  far  below  the  6  GBps   requirement.  Reference   Links:  -   [Amazon  FSx  for  Lustre Performance](https://aws.amazon.com/fsx/lustre/features/)        -          [FSx         for         Lustre        vs.          FSx         for         NetApp ONTAP](https://aws.amazon.com/fsx/compare/)

260.A company needs to migrate a legacy application from an on-premises data center to the AWS Cloud because of hardware capacity constraints. The application runs 24 hours a day, 7 days a week. The application ��s database storage continues to grow over time. What should a solutions architect do to meet these requirements MOST cost-effectively?

A ��Migrate the application layer to Amazon EC2 Spot Instances. Migrate the data storage layer to Amazon S3.

B ��Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon RDS On-Demand Instances.

C ��Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon Aurora Reserved Instances.

D ��Migrate the application layer to Amazon EC2 On-Demand Instances. Migrate the data storage layer to Amazon RDS Reserved Instances.

�� �� ��  Correct  Answer:  C  Detailed  Explanation:  The  application  runs  24/7,  so  using  EC2  Reserved  Instances  (instead  of On-Demand or Spot) for the application layer reduces costs significantly because Reserved Instances offer steep discounts (up to 72%)  for  long-term  commitments  (1-3  years).  Spot  Instances  (Option  A)  are  risky  for  24/7  workloads  because  they  can  be interrupted. The database requires scalable storage and runs continuously. Amazon Aurora Reserved  Instances (Option C) are ideal here because: 1. Aurora is a managed database service optimized for scalability and cost-effectiveness (up to 1/10th the cost of commercial databases). 2. Reserved Instances for Aurora lower database costs compared to On-Demand (Option B/D) while handling storage growth automatically. S3 (Option A) isn ��t suitable for transactional databases. RDS On-Demand (Options B/D) is more expensive than Aurora Reserved. Reference Links: - [EC2 Pricing](https://aws.amazon.com/ec2/pricing/) - [Amazon Aurora Pricing](https://aws.amazon.com/rds/aurora/pricing/)

261.A university research laboratory needs to migrate 30 TB of data from an on-premises Windows file server to Amazon FSx for Windows  File  Server.  The  laboratory  has  a  1  Gb ps  network  link  that  many  other  departments  in  the  university  share.  The laboratory wants to implement a data migration service that will maximize the performance of the data transfer. However, the laboratory  needs  to  be  able  to  control  the  amount  of  bandwidth  that  the  service  uses  to  minimize  the  impact  on  other departments. The data migration must take place within the next 5 days. Which AWS solution will meet these requirements?

A ��AWS Snow cone

B ��Amazon FSx File Gateway

C ��AWS Data sync

D ��AWS Transfer Family

�𰸣�C

���� �� Correct Answer: C. AWS Data sync Detailed Explanation: AWS Data sync is the best choice here because it's specifically designed for fast, secure, and automated data transfers between on-premises storage and AWS storage services like Amazon FSx. Here's why it fits perfectly: 1. Bandwidth Control: Data sync lets you limit network bandwidth usage during transfers. This means the lab can prevent overwhelming their shared 1 Gb ps connection, minimizing impact on other departments. 2. Transfer Speed: Data sync uses  parallel processing and built-in compression to maximize transfer efficiency. Even with bandwidth limits, it can complete 30 TB in 5 days. For example, at 500 Mbps (half their capacity), it would take about 5.5 days without compression - but Data sync's  compression  could  reduce  this  below  5  days.  3.  Direct  FSx  Support:  Data sync  natively  supports  Amazon  FSx  for Windows  File  Server,  ensuring  compatibility  and  optimized  transfers.  4.  No  Physical  Shipping:  Unlike  Snow cone  (Option  A), Data sync doesn't  require waiting for  hardware  delivery,  making  it  faster for their  5-day  deadline.  Other  options fall short: - Snow cone (A): Physical device shipping would likely exceed 5 days. - FSx File Gateway (B): Better for ongoing hybrid access than one-time migration. - Transfer Family (D): Designed for FTP-style transfers, not optimized bulk data migration. Reference Links: - [AWS             Data sync              Features](https://aws.amazon.com/data sync/features/)              -              [Data sync              Bandwidth Limits](https://docs.aws.amazon.com/data sync/latest/user guide/bandwidth-limits.html)

262.A  company wants to  create a  mobile  app that  allows  users  to stream  slow-motion video  clips  on their  mobile  devices. Currently, the app captures video clips and uploads the video clips in raw format into an Amazon S3 bucket. The app retrieves these video clips directly from the S3 bucket. However, the videos are large in their raw format. Users are experiencing issues with buffering and playback on mobile devices. The company wants to implement solutions to maximize the performance and scalability of the app while  minimizing operational overhead. Which combination of solutions will  meet these  requirements? (Choose two.)

A ��Deploy Amazon Cloud front for content delivery and caching.

B ��Use AWS Data sync to replicate the video files across AW'S Regions in other S3 buckets.

C ��Use Amazon Elastic Transcoder to convert the video files to more appropriate formats.

D ��Deploy an Auto Sealing group of Amazon EC2 instances in Local Zones for content delivery and caching.

E ��Deploy an Auto Scaling group of Amazon EC2 instances to convert the video files to more appropriate formats.

�𰸣�A

���� �� Correct Answer: A. Deploy Amazon Cloud front for content delivery and caching. C. Use Amazon Elastic Transcoder to convert the video files to more appropriate formats. Detailed Explanation: The buffering and playback issues arise because the raw video files are too large for smooth streaming on mobile devices. Here ��s how the solutions address the problem: 1. Amazon Cloud front (Option A): - Cloud front is a Content Delivery Network (CDN) that caches video files at edge locations closer to users. This  reduces  latency  and  improves  playback  speed,  minimizing  buffering.  -  It  offloads  traffic  from  the  S3  bucket,  improving scalability without operational effort. 2. Amazon Elastic Transcoder (Option C): - Raw video files are inefficient for streaming. Elastic Transcoder  converts them  into  optimized  formats  (e.g.,  HLS  or  MP4)  with  smaller  sizes  and  adaptive  bit rates.  -  This reduces  file  sizes,  making  streaming  faster  and  compatible  with  mobile  devices,  while  being  a  fully  managed  service  (low operational overhead). Why other options are incorrect: - Option B (Data sync): Replicating files across regions doesn ��t solve buffering; it increases storage costs. - Option D/E (EC2 Auto Scaling): Using EC2 for transcoding or delivery requires managing servers, which adds operational complexity compared to  managed services  like Cloud front and  Elastic Transcoder.  Reference Links:              -               [Amazon              Cloud front](https://aws.amazon.com/cloud front/)               -               [Amazon               Elastic Transcoder](https://aws.amazon.com/elastic transcoder/)

263.A company is launching a new application deployed on an Amazon Elastic Container Service (Amazon ECS) cluster and is using the Fargate launch type for ECS tasks. The company is monitoring CPU and memory usage because it is expecting high traffic to the application upon its launch. However, the company wants to reduce costs when utilization decreases. What should a solutions architect recommend?

A ��Use Amazon EC2 Auto Scaling to scale at certain periods based on previous traffic patterns.

B ��Use an AWS Lambda function to scale Amazon ECS based on metric breaches that trigger an Amazon Cloud watch alarm.

C ��Use Amazon EC2 Auto Scaling with simple scaling policies to scale when ECS metric breaches trigger an Amazon Cloud watch alarm.

D �� Use  AWS  Application  Auto  Scaling  with  target  tracking  policies  to  scale  when  ECS  metric  breaches  trigger  an  Amazon Cloud watch alarm.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The correct answer is D: Use AWS Application Auto Scaling with target tracking policies to scale when ECS metric breaches trigger an Amazon Cloud watch alarm. Here's why: 1. AWS Application Auto Scaling is the native service designed to automatically adjust resources for services like Amazon ECS, Amazon Dynamo db, or AWS Lambda. For  ECS  Fargate,  this service scales the  number  of tasks  (containers)  based  on  metrics  like  CPU  or  memory  usage.  2.  Target Tracking Scaling Policies simplify scaling by automatically adjusting resources to maintain a specific target value (e.g., 70% CPU utilization). You don't need to manually define Cloud watch alarms or write custom scaling logic (unlike Option B). This reduces operational overhead and ensures cost efficiency during low - traffic periods. 3. Why Not Other Options? - A/C: EC2 Auto Scaling scales EC2 instances, but Fargate tasks aren't tied to EC2 instances. Fargate abstracts server management, so these options are

irrelevant. - B: While Lambda could scale ECS, it requires custom code and manual alarm setup. Application Auto Scaling is a managed,       more        reliable       solution.        Reference        Link:       [AWS        Application       Auto       Scaling        for       Amazon ECS](https://docs.aws.amazon.com/Amazon ecs/latest/developer guide/service-auto-scaling.html)

264.A company recently created a disaster recovery site in a different AWS Region. The company needs to transfer large amounts of  data  back  and  forth  between  NFS  file  systems  in  the  two  Regions  on  a  periodic  basis.  Which  solution  will  meet  these requirements with the LEAST operational overhead?

A ��Use AWS Data sync.

B ��Use AWS Snowball devices.

C ��Set up an SFTP server on Amazon EC2.

D ��Use AWS Database Migration Service (AWS DMS).

�𰸣�A

���� �� Correct Answer: A.  Use AWS  Data sync. Detailed  Explanation: AWS  Data sync is the best choice for transferring large amounts of data between NFS file systems across AWS Regions with minimal operational effort. It is a fully managed service that automates data transfers, supports scheduling, and optimizes speed and security.  Unlike Snowball (physical device  handling), SFTP (manual server management), or DMS (database-specific), Data sync requires no infrastructure setup, reduces manual tasks, and  natively  integrates  with  NFS  file  systems.  This  makes  it  the  most  efficient  and  low-maintenance  solution  for  periodic cross-region  transfers.   Reference  Links:  -   [AWS  Data sync  Overview](https://aws.amazon.com/data sync/)  -   [NFS  Support   in Data sync](https://docs.aws.amazon.com/data sync/latest/user guide/creating-nfs-location.html)

265.A company is designing a shared storage solution for a gaming application that is hosted in the AWS Cloud. The company needs the ability to  use SMB clients to access  data. The  solution  must  be  fully  managed.  Which AWS solution  meets these requirements?

A ��Create an AWS Data sync task that shares the data as a mountable file system. Mount the file system to the application server.

B �� Create  an Amazon  EC2 Windows  instance.  Install  and  configure a Windows file  share  role on the  instance.  Connect the application server to the file share.

C ��Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server. Connect the application server to the file system.

D ��Create an Amazon S3 bucket. Assign an IAM role to the application to grant access to the S3 bucket. Mount the S3 bucket to the application server.

�𰸣�C

���� �� Correct Answer: C Detailed Explanation: The correct answer is C. Create an Amazon FSx for Windows File Server file system. Here's why: 1. SMB Protocol Requirement: The question specifies the need for SMB (Server Message Block) client access. Amazon FSx for Windows File Server is specifically designed to support SMB, making it the ideal choice for Windows-based file sharing. 2. Fully Managed Service: Amazon FSx is a fully managed AWS service. This means AWS handles maintenance, updates, backups, and scaling automatically,  meeting the  requirement for a fully  managed  solution. 3.  Eliminating  Other  Options: - A (Data sync): Data sync is for data migration/synchronization, not a persistent shared file system. It doesn ��t natively support SMB

clients.  -   B   (EC2  Windows   Instance):  This   requires   manual  setup   (installing/configuring  a  file  share  role)  and  ongoing management, violating the fully managed requirement. - D (S3): S3 is object storage, not a file system. While tools like S3FS can mount it as a file system, S3 lacks native SMB support and isn ��t optimized for low-latency file operations like gaming apps. Reference  Links:  -   [Amazon  FSx  for  Windows   File  Server](https://aws.amazon.com/fsx/windows/)  -   [AWS  Storage  Services Overview](https://aws.amazon.com/products/storage/)

266.A company wants to run an in-memory database for a latency-sensitive application that runs on Amazon EC2 instances. The application processes more than 100,000 transactions each minute and requires high network throughput. A solutions architect needs   to   provide   a   cost-effective   network   design   that   minimizes   data   transfer   charges.   Which   solution   meets   these requirements?

A ��Launch all EC2 instances in the same Availability Zone within the same AWS Region. Specify a placement group with cluster strategy when launching EC2 instances.

B ��Launch all EC2 instances in different Availability Zones within the same AWS Region. Specify a placement group with partition strategy when launching EC2 instances.

C ��Deploy an Auto Scaling group to launch EC2 instances in different Availability Zones based on a network utilization target.

D ��Deploy an Auto Scaling group with a step scaling policy to launch EC2 instances in different Availability Zones.

�𰸣�A

�� �� �� Correct Answer: A  Detailed  Explanation:  The  correct  answer  is  A  because  launching  all  EC2  instances  in  the  same Availability Zone (AZ) with a cluster placement group minimizes network latency and maximizes throughput. This setup ensures instances are physically close, reducing delays for the latency-sensitive in-memory database. Additionally, data transfer within the same AZ incurs no charges, avoiding cross-AZ data transfer costs. - Cluster placement groups optimize for low latency and high throughput by grouping instances in the same AZ and near each other. - Cross-AZ deployments (Options B, C, D) introduce higher latency and costs due to inter-AZ data transfer fees. - While Auto Scaling (Options C, D) improves fault tolerance, the question    prioritizes    cost-effective    network    performance    over    scaling    or     redundancy.    Reference:    [AWS     Placement Groups](https://docs.aws.amazon.com/AWSEC2/latest/User guide/placement-groups.html)            [AWS            Data            Transfer Pricing](https://aws.amazon.com/ec2/pricing/on-demand/#Data_Transfer)

267.A company that primarily runs its application servers on premises has decided to migrate to AWS. The company wants to minimize its need to scale its Internet Small Computer Systems Interface (iSCSI) storage on premises. The company wants only its recently accessed data to remain stored locally. Which AWS solution should the company use to meet these requirements?

A ��Amazon S3 File Gateway

B ��AWS Storage Gateway Tape Gateway

C ��AWS Storage Gateway Volume Gateway stored volumes D ��AWS Storage Gateway Volume Gateway cached volumes
�𰸣�D

������ Correct Answer: D. AWS Storage Gateway Volume Gateway cached volumes Detailed Explanation: The company needs to reduce on-premises iSCSI storage scaling while keeping recently accessed data locally. AWS Storage Gateway's Volume Gateway

offers two modes: stored volumes and cached volumes. - Cached Volumes (Option D): Stores all data in AWS (like Amazon S3) and keeps only frequently accessed (recent) data cached locally. This minimizes on-premises storage needs because most data stays in the cloud. It uses iSCSI, matching the company's block storage requirement. - Stored Volumes (Option C): Keeps all data on-premises and backs it up to AWS. This would require more local storage, which doesn ��t  align with the goal of reducing on-premises scaling. - File Gateway (Option A): For file storage (NFS/SMB), not iSCSI/block storage. - Tape Gateway (Option B): For  backups  to  virtual  tapes,   not  active   iSCSI  storage.  Cached  Volumes   optimize  local  storage   by   keeping  only   hot  data on-premises,      making      Option      D      the      correct       choice.      Reference      Link:       [AWS      Storage      Gateway      Volume Gateway](https://aws.amazon.com/storage gateway/volume-gateway/)

268.A solutions architect needs to optimize storage costs. The solutions architect must identify any Amazon S3 buckets that are no longer being accessed or are rarely accessed. Which solution will accomplish this goal with the LEAST operational overhead?

A ��Analyze bucket access patterns by using the S3 Storage Lens dashboard for advanced activity metrics.

B ��Analyze bucket access patterns by using the S3 dashboard in the AWS Management Console.

C��Turn on the Amazon Cloud watch Bucketsize bytes metric for buckets. Analyze bucket access patterns by using the metrics data with Amazon Athena.

D ��Turn on AWS Cloud trail for S3 object monitoring. Analyze bucket access patterns by using Cloud trail logs that are integrated with Amazon Cloud watch Logs.

�𰸣�A

������ Correct Answer A Explanation The correct answer is A because Amazon S3 Storage Lens is a fully managed, auto-enabled tool that provides a centralized dashboard to analyze all S3 buckets across your AWS accounts. It automatically tracks advanced metrics  like  access  patterns,  activity  trends,  and  storage  usage  without   manual  configuration.  This   minimizes  operational overhead  compared  to  options  B,  C,  or  D,  which  require  setting  up  additional  services  (Cloud watch,  Athena,  Cloud trail)  or analyzing raw logs manually. S3 Storage Lens gives ready-to-use insights, making it the simplest solution for identifying rarely accessed  buckets.  Reference  Links  -  [Amazon  S3  Storage  Lens](https://aws.amazon.com/s3/storage-lens/)  -  [S3  Storage  Lens Metrics](https://docs.aws.amazon.com/AmazonS3/latest/user guide/storage-lens.html)

269.A company sells datasets to customers who do research in artificial intelligence and machine learning (AI/ML). The datasets are large, formatted files that are stored in an Amazon S3 bucket in the us-east-1 Region. The company hosts a web application that the customers use to purchase access to a given dataset. The web application is deployed on multiple Amazon EC2 instances behind an Application Load Balancer. After a purchase is made, customers receive an S3 signed URL that allows access to the files. The customers are distributed across North America and Europe. The company wants to reduce the cost that is associated with data  transfers  and  wants  to   maintain  or  improve   performance.  What  should  a  solutions  architect  do  to   meet  these requirements?

A �� Configure S3 Transfer Acceleration  on the  existing  S3  bucket.  Direct  customer  requests  to  the  S3  Transfer  Acceleration endpoint. Continue to use S3 signed URLs for access control.

B �� Deploy  an  Amazon  Cloud front  distribution  with  the  existing  S3  bucket  as  the  origin.  Direct  customer  requests  to  the Cloud front URL. Switch to Cloud front signed URLs for access control.

C ��Set up a second S3 bucket in the eu-central-1 Region with S3 Cross-Region Replication between the buckets. Direct customer requests to the closest Region. Continue to use S3 signed URLs for access control.

D ��Modify the web application to enable streaming of the datasets to end users. Configure the web application to read the data from the existing S3 bucket. Implement access control directly in the application.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution is to use Amazon Cloud front, a content delivery network (CDN), which  caches  datasets  at  edge  locations  closer  to  users  in  North  America  and  Europe.  This  reduces  data  transfer  costs  by minimizing cross - region traffic from the S3 bucket in us - east - 1. Cloud front also improves performance by serving files from the  nearest  edge  location.  Switching  to  Cloud front  signed  URLs  replaces  S3  signed  URLs,  maintaining  access  control  while aligning with the CDN setup. Other options like S3 Transfer Acceleration (A) focus on uploads, cross - region replication (C) incurs higher  storage/replication  costs,  and  app  -  level  streaming  (D)  doesn ��t  reduce  transfer  costs.  Reference  Links:  -  [Amazon Cloud front](https://aws.amazon.com/cloud front/)                   -                   [Reducing                    S3                    Data                   Transfer
Costs](https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloud front-a-match-made-in-the-cl oud/)                         -                         [Cloud front                        vs                         S3                         Cross                         -                          Region
Replication](https://docs.aws.amazon.com/Amazon cloud front/latest/Developer guide/How cloud front works.html)

270.A company is using AWS to design a web application that will process insurance quotes. Users will request quotes from the application. Quotes must be separated by quote type, must be responded to within 24 hours, and must not get lost. The solution must maximize operational efficiency and must minimize maintenance. Which solution meets these requirements?

A��Create multiple Amazon Kinesis data streams based on the quote type. Configure the web application to send messages to the proper data stream. Configure each backend group of application servers to use the Kinesis Client Library (KCL) to pool messages from its own data stream.

B �� Create  an AWS  Lambda  function  and  an  Amazon  Simple  Notification  Service  (Amazon  SNS)  topic  for  each  quote  type. Subscribe the  Lambda  function  to  its  associated  SNS  topic.  Configure the  application  to  publish  requests  for  quotes  to  the appropriate SNS topic.

C ��Create a single Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon Simple Queue Service (Amazon SQS) queues to the SNS topic. Configure SNS message filtering to publish messages to the proper SQS queue based on the quote type. Configure each backend application server to use its own SQS queue.

D��Create multiple Amazon Kinesis Data Firehose delivery streams based on the quote type to deliver data streams to an Amazon Open search Service cluster. Configure the application to send messages to the proper delivery stream. Configure each backend group of application servers to search for the messages from Open search Service and process them accordingly.

�𰸣�C

���� �� Correct Answer: C Detailed Explanation: The best solution is C because it uses Amazon SNS with message filtering to route quotes to separate Amazon SQS queues based on their type. Here ��s why this works: 1. Separation by Quote Type: SNS message filtering uses  message attributes (e.g., quote_type) to automatically route messages to the correct SQS queue. Each backend server processes messages from its dedicated queue, ensuring quotes are handled by type. 2. No Lost Messages: SQS queues  are  durable  and  store  messages  for  up  to  14  days  by  default,  easily  meeting  the  24  -  hour  response  requirement. Messages remain in the queue until processed successfully. 3. Operational Efficiency   �� Low  Maintenance:  SNS and SQS are fully managed services. AWS handles scaling, availability, and infrastructure maintenance. There ��s no need to manage servers (like  in  Option A with  Kinesis  shards)  or  deploy  separate  Lambda  functions  per  quote  type  (like  in  Option  B).  4.  Scalability: Backend servers can independently poll their SQS queues, enabling parallel processing and easy scaling. Why Other Options Fail:
- A: Managing multiple Kinesis streams adds complexity (e.g., shard management) and is overkill for non - real - time processing.
- B: Deploying a Lambda function per quote type increases maintenance and costs. - D: Using Open search for message retrieval

is    inefficient     and     not    designed     for     queue    -     based     processing.     Reference     Links:    -     [Amazon     SNS     Message Filtering](https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html)         -          [Fanout          with          SNS          and SQS](https://aws.amazon.com/blogs/aws/queues-and-notifications-now-best-friends/)

271.A company has an application that runs on several Amazon EC2 instances. Each EC2 instance has multiple Amazon Elastic Block Store (Amazon EBS) data volumes attached to it. The application ��s EC2 instance configuration and data need to be backed up nightly. The application also needs to be recoverable in a different AWS Region. Which solution will meet these requirements in the MOST operationally efficient way?

A��Write an AWS Lambda function that schedules nightly snapshots of the application ��s EBS volumes and copies the snapshots to a different Region.

B �� Create  a  backup  plan  by  using  AWS  Backup  to  perform  nightly  backups.  Copy  the  backups  to  another  Region.  Add  the application ��s EC2 instances as resources.

C �� Create a  backup  plan  by  using  AWS  Backup  to  perform  nightly  backups.  Copy  the  backups  to  another  Region.  Add  the application ��s EBS volumes as resources.

D��Write an AWS Lambda function that schedules nightly snapshots of the application's EBS volumes and copies the snapshots to a different Availability Zone.

�𰸣�B

�� �� �� Correct Answer  C  Detailed  Explanation  The  question  asks for the  most  operationally  efficient  way to  back  up  EC2 instances (configuration and data) and enable cross-region recovery. Here ��s why Option C is correct: 1. AWS Backup Simplifies Management: AWS Backup is a managed service that automates backups of EBS volumes and EC2 instance configurations (via AMIs). It handles scheduling, retention, and cross-region copying without requiring custom code. 2. Focus on EBS Volumes: The question specifies that each EC2 instance has multiple EBS volumes. To back up the data (stored on EBS), you must explicitly add the EBS volumes to the backup plan. AWS Backup will automatically include the EC2 instance configuration (AMI) when backing up EBS volumes attached to an EC2 instance. 3. Why Option B is Incorrect: Adding EC2 instances as resources in AWS Backup might seem logical, but this approach focuses on AMIs (instance configurations) and not necessarily all attached EBS volumes. To ensure all  EBS volumes are backed up, they must be explicitly added as  resources. 4. Operational  Efficiency: Option C avoids manual scripting (Lambda) and ensures all EBS volumes are included. AWS Backup ��s cross-region copy feature also simplifies disaster               recovery.               Reference                Links               -                [AWS                Backup               for                EC2               and EBS](https://docs.aws.amazon.com/aws-backup/latest/dev guide/working-with-aws-backup.html)  -  [Cross-Region  Copy  in  AWS Backup](https://docs.aws.amazon.com/aws-backup/latest/dev guide/cross-region-copy.html)

272.A company is building a mobile app on AWS. The company wants to expand its reach to millions of users. The company needs to build a platform so that authorized users can watch the company ��s content on their mobile devices. What should a solutions architect recommend to meet these requirements?

A ��Publish content to a public Amazon S3 bucket. Use AWS Key Management Service (AWS KMS) keys to stream content.

B ��Set up IPsec VPN between the mobile app and the AWS environment to stream content.

C ��Use Amazon Cloud front. Provide signed URLs to stream content.

D ��Set up AWS Client VPN between the mobile app and the AWS environment to stream content.

�𰸣�C

������ Correct Answer: C Detailed Explanation: To securely stream content to millions of mobile users, Amazon Cloud front (a content delivery network) paired with signed URLs is ideal. Cloud front ensures fast, global content delivery, while signed URLs grant temporary, secure access to authorized users. - Why not A? Public S3 buckets expose data to everyone, and KMS alone doesn ��t  restrict access. - Why not  B/D? VPNs are impractical for millions of users and aren ��t designed for  public content streaming. - Why C? Cloud front scales globally, reduces latency, and signed URLs enforce time-limited, permission-based access. Reference                                Links:                               -                                [Amazon                                Cloud front                                Signed
URLs](https://docs.aws.amazon.com/Amazon cloud front/latest/Developer guide/private-content-signed-urls.html)       -        [AWS Content Delivery](https://aws.amazon.com/cloud front/)

273.A company has an on-premises MySQL database used by the global sales team with infrequent access patterns. The sales team requires the database to have minimal downtime. A database administrator wants to migrate this database to AWS without selecting  a  particular  instance  type  in  anticipation  of  more  users  in  the  future.  Which  service  should  a  solutions  architect recommend?

A ��Amazon Aurora MySQL

B ��Amazon Aurora Serverless for MySQL C ��Amazon Redshift Spectrum

D ��Amazon RDS for MySQL

�𰸣�B

�� �� �� Correct Answer:  B.  Amazon Aurora  Serverless for  MySQL  Explanation:  The  key  requirements  here  are:  1.  Minimal downtime during migration and operation. 2. No need to select instance types (to handle future user growth automatically). 3. Infrequent access patterns (cost-efficient scaling). Why B is correct: - Amazon Aurora Serverless automatically scales database capacity  up  or  down  based  on  demand.  This  eliminates the  need  to  manually  choose  or  resize  instances,  aligning  with  the requirement to avoid selecting instance types. - It automatically pauses during inactivity (cost-saving for infrequent access) and resumes  instantly,  ensuring  minimal  downtime.  -  Aurora  Serverless  is  fully  compatible  with  MySQL,  making  migration  from on-premises MySQL straightforward. Why other options are incorrect: - A (Aurora MySQL)   �� D  (RDS  MySQL):  Both  require selecting instance types and manual scaling, which could lead to downtime during future scaling. - C (Redshift Spectrum): A data querying   tool    for   S3,    not    a   transactional    database    replacement    for   MySQL.   Reference   Links:   -    [Amazon    Aurora Serverless](https://aws.amazon.com/rds/aurora/serverless/)

274.A  company  experienced  a  breach  that  affected  several  applications  in  its  on-premises  data  center.  The  attacker  took advantage of vulnerabilities  in the custom applications that were  running  on the servers. The company  is  now  migrating  its applications to run on Amazon EC2 instances. The company wants to implement a solution that actively scans for vulnerabilities on the EC2 instances and sends a report that details the findings. Which solution will meet these requirements?

A ��Deploy AWS Shield to scan the EC2 instances for vulnerabilities. Create an AWS Lambda function to log any findings to AWS Cloud trail.

B �� Deploy Amazon Macie and AWS  Lambda functions to scan the EC2 instances for vulnerabilities.  Log any findings to AWS Cloud trail.

C ��Turn  on Amazon Guard duty.  Deploy the Guard duty agents to the  EC2  instances.  Configure an AWS  Lambda function to

automate the generation and distribution of reports that detail the findings.

D ��Turn on Amazon Inspector. Deploy the Amazon Inspector agent to the EC2 instances. Configure an AWS Lambda function to automate the generation and distribution of reports that detail the findings.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The correct solution is Amazon Inspector because it is specifically designed to automatically scan Amazon EC2 instances for vulnerabilities (like software vulnerabilities, network exposure, or deviations from security best practices). Here ��s why: - Amazon Inspector requires installing an agent on EC2 instances to perform continuous vulnerability scanning of the operating system and applications. - It generates detailed reports (findings) about vulnerabilities, which can be automated using AWS Lambda to process these findings and distribute reports (e.g., via S3, email, etc.). - Other options are incorrect: - AWS Shield (A) protects against DDoS attacks, not vulnerability scanning. - Amazon Macie (B) scans S3 for sensitive data, not EC2 vulnerabilities. - Amazon Guard duty (C) detects threats using logs and ML but does not require agents or perform vulnerability scans. Reference Link: - [Amazon Inspector](https://aws.amazon.com/inspector/)

275.A company uses an Amazon  EC2  instance to  run a script to  poll for and  process  messages  in  an Amazon Simple  Queue Service (Amazon SQS) queue. The company wants to reduce operational costs while maintaining its ability to process a growing number of messages that are added to the queue. What should a solutions architect recommend to meet these requirements?

A ��Increase the size of the EC2 instance to process messages faster.

B ��Use Amazon Event bridge to turn off the EC2 instance when the instance is underutilized.

C ��Migrate the script on the EC2 instance to an AWS Lambda function with the appropriate runtime.

D ��Use AWS Systems Manager Run Command to run the script on demand.

�𰸣�C

���� �� Correct Answer: C. Migrate the script on the EC2 instance to an AWS Lambda function with the appropriate runtime. Detailed  Explanation:  The  company  currently  uses  an  EC2  instance  to  poll  and  process  messages  from  an  SQS  queue.  EC2 instances incur costs even when idle (you pay for the instance's uptime). Scaling with EC2 requires manual adjustments (e.g., resizing instances or adding more instances), which is inefficient for variable workloads. By migrating to AWS Lambda: 1. Cost Reduction: Lambda charges based on execution time and number of requests, not uptime. If there are no messages, there are no costs. 2. Automatic Scaling: Lambda automatically scales to match the SQS queue depth. For example, if 1,000 messages arrive, Lambda processes them concurrently (up to account limits), eliminating bottlenecks. 3. SQS Integration: Lambda supports event source mapping for SQS.  Lambda polls the queue and triggers your function whenever  messages are available, replacing the custom script. Other options fail to meet both requirements: - A: Larger EC2 instances cost more and don ��t auto-scale. - B: Turning  off  EC2  saves  costs  but  risks  delayed  processing  during  spikes.  -  D:  Running  scripts  on  demand  adds  operational complexity    and    doesn   �� t    guarantee     real-time    scaling.     Reference    Link:    -     [Using    AWS    Lambda    with    Amazon SQS](https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html)                                             -                                              [Lambda
Pricing](https://aws.amazon.com/lambda/pricing/)

276.A company uses a legacy application to produce data in CSV format. The legacy application stores the output data in Amazon S3. The  company  is  deploying  a  new  commercial  off-the-shelf  (COTS)  application  that  can  perform  complex  SQL  queries  to analyze data that is stored in Amazon Redshift and Amazon S3 only. However, the COTS application cannot process the .csv files that the legacy application produces. The company cannot update the legacy application to produce data in another format. The company needs to implement a solution so that the COTS application can use the data that the legacy application produces.

Which solution will meet these requirements with the LEAST operational overhead?

A ��Create an AWS Glue extract, transform, and load (ETL) job that runs on a schedule. Configure the ETL job to process the .csv files and store the processed data in Amazon Redshift.

B ��Develop a Python script that runs on Amazon EC2 instances to convert the .csv files to .sql files. Invoke the Python script on a cron schedule to store the output files in Amazon S3.

C ��Create an AWS Lambda function and an Amazon Dynamo db table. Use an S3 event to invoke the Lambda function. Configure the Lambda function to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in the Dynamo db table.

D ��Use Amazon Event bridge to launch an Amazon EMR cluster on a weekly schedule. Configure the EMR cluster to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in an Amazon Redshift table.

�𰸣�A

���� �� Correct Answer: A Detailed Explanation: The best solution is Option A because it uses AWS Glue, a fully managed ETL service, to automatically convert the CSV files into a format compatible with Amazon Redshift. Here's why: 1. Least Operational Overhead: AWS Glue  is serverless �� no servers to manage.  It  handles scaling, scheduling, and  monitoring automatically. This reduces the effort  needed compared to managing  EC2  instances  (Option  B),  Lambda/Dynamo db  (Option C), or  EMR clusters (Option  D). 2.  Direct  Integration with  Redshift: The COTS application already supports Amazon  Redshift.  By transforming and loading the CSV data into Redshift, the COTS app can immediately query the data without additional changes. 3. Automated Scheduling: AWS Glue jobs can run on a schedule (e.g., hourly/daily), ensuring the latest CSV data is processed and available in Redshift. This avoids manual intervention or complex cron setups (as in Option B). Why Other Options Are Less Ideal: - Option B: Running a custom script on  EC2  requires  managing servers, scaling, and error handling, which adds operational complexity. - Option C: Storing data in Dynamo db doesn ��t align with the COTS app ��s supported data sources (Redshift/S3). Dynamo db is a NoSQL  database,  not  ideal  for  SQL  queries.  -  Option  D:  EMR  clusters  are  powerful  for  big  data  but  require  manual  setup, configuration,     and     termination.     Weekly      batches      may     also      cause     data     delays.      Reference      Links:     -      [AWS Glue](https://aws.amazon.com/glue/) - [Amazon Redshift](https://aws.amazon.com/redshift/)

277.A company has hundreds of Amazon EC2 Linux-based instances in the AWS Cloud. Systems administrators have used shared SSH keys to manage the instances. After a recent audit, the company ��s security team is mandating the removal of all shared keys. A solutions architect  must design a solution that  provides secure access to the  EC2  instances. Which solution will  meet this requirement with the LEAST amount of administrative overhead?

A ��Use AWS Systems Manager Session Manager to connect to the EC2 instances.

B ��Use AWS Security Token Service (AWS STS) to generate one-time SSH keys on demand.

C ��Allow shared SSH access to a set of bastion instances. Configure all other instances to allow only SSH access from the bastion instances.

D ��Use an Amazon Cognito custom authorizer to authenticate users. Invoke an AWS Lambda function to generate a temporary SSH key.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution is to use AWS Systems Manager Session Manager. Here's why

it's ideal for a beginner: 1. No SSH Keys Needed: Session Manager lets you connect to EC2 instances directly through the AWS Management Console or CLI without  needing SSH  keys. This completely removes the shared SSH  key problem. 2.  IAM-Based Access: You control access using AWS IAM policies instead of managing keys. For example, you can grant permissions like Only developers in Team A can connect to Server X. 3. Automatic Security: All sessions are encrypted and logged automatically in AWS Cloud trail,  improving security without extra setup. 4. Zero  Maintenance:  Unlike  bastion  hosts  (Option  C)  or  custom  Lambda solutions  (Option  D),  Session  Manager  requires  no  servers to  maintain  or complex code to write.  Other  options fall short: - Option B/C still involve SSH keys (temporary or shared). - Option D adds complexity with Cognito/Lambda. Reference Link: [AWS Session Manager Documentation](https://docs.aws.amazon.com/systems-manager/latest/user guide/session-manager.html)

278.A company is using a fleet of Amazon EC2 instances to ingest data from on-premises data sources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 instance is rebooted, the data in-flight is lost. The company ��s data science team  wants to  query  ingested  data  in  near-real  time.  Which  solution  provides  near-real-time  data  querying  that  is scalable with minimal data loss?

A ��Publish data to Amazon Kinesis Data Streams, Use Kinesis Data Analytics to query the data.

B �� Publish data to Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon Redshift to query the data.

C��Store ingested data in an EC2 instance store. Publish data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data.

D ��Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Publish data to Amazon Elastic ache for Redis. Subscribe to the Redis channel to query the data.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution is to use Amazon Kinesis Data Streams (KDS) for real-time data ingestion and Kinesis Data Analytics for near-real-time querying. Here ��s why: 1. Minimal Data Loss: When EC2 instances reboot, data stored locally (e.g., instance store, EBS) is lost. However, if data is directly sent to Kinesis Data Streams immediately upon ingestion, it persists in the stream even if an EC2 instance fails or reboots. KDS retains data for up to 365 days, ensuring durability.
2. Near-Real-Time Querying: Kinesis Data Analytics allows running SQL queries directly on streaming data in real time. This meets the data science team ��s requirement for near-real-time analysis. 3. Scalability: KDS automatically scales to handle high ingestion rates (like 1 MB/s). Data Analytics also scales seamlessly to process the data stream. Why Other Options Fail: - Option B: Kinesis Data Firehose batches data before delivering it to Redshift, introducing latency (not near-real-time). - Option C: Storing data in an EC2 instance store is risky (data loss on reboot) and Athena queries S3 data, which has latency. - Option D:  EBS volumes are durable, but Elastic ache for Redis is not designed for complex queries or large-scale analytics. Reference Links: - [Amazon Kinesis Data                Streams](https://aws.amazon.com/kinesis/data-streams/)                 -                 [Amazon                 Kinesis                 Data Analytics](https://aws.amazon.com/kinesis/data-analytics/)

279.What should a solutions architect do to ensure that all objects uploaded to an Amazon S3 bucket are encrypted?

A ��Update the bucket policy to deny if the Put object does not have an s3:x-amz-acl header set.

B ��Update the bucket policy to deny if the Put object does not have an s3:x-amz-acl header set to private.

C ��Update the bucket policy to deny if the Put object does not have an aws:Securetransport header set to true.

D ��Update the bucket policy to deny if the Put object does not have an x-amz-server-side-encryption header set.

�𰸣�D

������ Correct Answer: D Detailed Explanation: To ensure all objects uploaded to an Amazon S3 bucket are encrypted, you need to enforce server-side encryption (SSE) during upload. Option D uses a bucket policy to block uploads (Put object) that don ��t include the  x-amz-server-side-encryption  header.  This  header  is  required  to  enable  SSE,  ensuring  S3  automatically  encrypts objects using AWS-managed keys (SSE-S3) or a KMS key (SSE-KMS). Why not other options? - A/B: These relate to ACLs (access control lists), which  manage  permissions,  not encryption. - C: This enforces  HTTPS (secure transport), which  protects data  in transit but doesn ��t ensure encryption at rest. - D: Directly mandates encryption for stored objects via the encryption header. Reference:                        [Amazon                         S3                         Default                         Encryption                        vs.                         Bucket
Policies](https://docs.aws.amazon.com/AmazonS3/latest/user guide/bucket-encryption.html)

280.A solutions architect is designing a multi-tier application for a company. The application's users upload images from a mobile device. The application generates a thumbnail of each image and returns a message to the user to confirm that the image was uploaded successfully. The thumbnail generation can take up to 60 seconds, but the company wants to provide a faster response time to its users to  notify them that the original image was  received. The solutions architect  must design the application to asynchronously  dispatch  requests  to  the  different  application  tiers.  What  should  the  solutions  architect  do  to  meet  these requirements?

A��Write a custom AWS Lambda function to generate the thumbnail and alert the user. Use the image upload process as an event source to invoke the Lambda function.

B ��Create an AWS Step Functions workflow. Configure Step Functions to handle the orchestration between the application tiers and alert the user when thumbnail generation is complete.

C ��Create an Amazon Simple Queue Service (Amazon SQS) message queue. As images are uploaded, place a message on the SQS queue for thumbnail generation. Alert the user through an application message that the image was received.

D ��Create Amazon Simple Notification Service (Amazon SNS) notification topics and subscriptions. Use one subscription with the application to generate the thumbnail after the  image  upload  is complete.  Use a second subscription to  message the  user's mobile app by way of a push notification after thumbnail generation is complete.

�𰸣�C

������ Correct Answer C Detailed Explanation The key requirement is to decouple the image upload confirmation (immediate response) from the thumbnail generation  (asynchronous, slower  process).  Here's  why Option C  is the  best fit:  1.  Immediate Response: When a user uploads an image, the application can immediately send a confirmation message. This avoids waiting for the thumbnail generation (up to 60 seconds). 2. Asynchronous Processing: Using Amazon SQS, the application places a message in the queue when the image is uploaded. A separate worker process (e.g., AWS Lambda, EC2 instances) consumes messages from the queue to generate thumbnails. This decouples the front end (user interaction) from the backend (thumbnail processing).
3. Scalability: SQS handles varying workloads, ensuring thumbnail generation scales automatically with the number of uploads. Why Not Other Options? - A:  Lambda could work, but if the upload process directly invokes Lambda, the user might wait for Lambda to complete (defeating the immediate response requirement). S3 event triggers to Lambda are better but still require proper  decoupling.  -   B:  Step   Functions  adds   unnecessary  complexity  for  a  simple  workflow   (upload    ��  thumbnail    �� notification). -  D: SNS  is  a  pub/sub service,  not  ideal for  sequential tasks.  It  might trigger  multiple  subscribers,  complicating workflow order. Reference Link [Amazon SQS Documentation](https://aws.amazon.com/sqs/)

281.A company ��s facility has badge readers at every entrance throughout the building. When badges are scanned, the readers send a message over HTTPS to indicate who attempted to access that particular entrance. A solutions architect must design a

system to  process  these  messages  from  the  sensors.  The  solution  must  be  highly  available,  and  the  results  must  be  made available for the company ��s security team to analyze. Which system architecture should the solutions architect recommend?

A��Launch an Amazon EC2 instance to serve as the HTTPS endpoint and to process the messages. Configure the EC2 instance to save the results to an Amazon S3 bucket.

B��Create an HTTPS endpoint in Amazon API Gateway. Configure the API Gateway endpoint to invoke an AWS Lambda function to process the messages and save the results to an Amazon Dynamo db table.

C �� Use Amazon Route 53 to direct incoming sensor messages to an AWS Lambda function. Configure the Lambda function to process the messages and save the results to an Amazon Dynamo db table.

D ��Create a gateway VPC endpoint for Amazon S3. Configure a Site-to-Site VPN connection from the facility network to the VPC so that sensor data can be written directly to an S3 bucket by way of the VPC endpoint.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution here is to use Amazon API Gateway as the HTTPS endpoint, which is fully managed and highly available by default. When a badge is scanned, the message is sent to API Gateway, which then triggers an AWS Lambda function. Lambda is serverless, so it automatically scales to handle the incoming requests without any server  management.  The   processed  data  is  stored  in  Amazon   Dynamo db,   a  NoSQL  database  that   provides   low-latency performance  and  scalability. This  setup  ensures  high  availability,  as  all  components  (API  Gateway,  Lambda,  Dynamo db)  are managed AWS services with built-in redundancy and fault tolerance. Option C is incorrect because Route 53 is a DNS service and cannot directly invoke Lambda. Option A requires managing EC2 instances, which adds operational overhead. Option D uses S3 for direct writes via VPN, which is  less efficient for real-time  processing compared to Dynamo db.  Reference Link: -  [AWS API Gateway](https://aws.amazon.com/api-gateway/)       -       [AWS       Lambda](https://aws.amazon.com/lambda/)       -        [Amazon Dynamo db](https://aws.amazon.com/dynamo db/)

282.A company wants to implement a disaster recovery plan for its primary on-premises file storage volume. The file storage volume is mounted from an Internet Small Computer Systems Interface (iSCSI) device on a local storage server. The file storage volume holds hundreds of terabytes (TB) of data. The company wants to ensure that end users retain immediate access to all file types from the on-premises systems without experiencing latency. Which solution will meet these requirements with the LEAST amount of change to the company's existing infrastructure?

A �� Provision an Amazon S3 File Gateway as a virtual machine (VM) that is hosted on premises. Set the local cache to 10 TB. Modify existing applications to access the files through the NFS protocol. To recover from a disaster, provision an Amazon EC2 instance and mount the S3 bucket that contains the files.

B �� Provision an AWS Storage Gateway tape gateway. Use a data backup solution to back up all existing data to a virtual tape library. Configure the data backup solution to run nightly after the initial backup is complete. To recover from a disaster, provision an Amazon EC2 instance and restore the data to an Amazon Elastic Block Store (Amazon EBS) volume from the volumes in the virtual tape library.

C��Provision an AWS Storage Gateway Volume Gateway cached volume. Set the local cache to 10 TB. Mount the Volume Gateway cached volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.

D ��Provision an AWS Storage Gateway Volume Gateway stored volume with the same amount of disk space as the existing file

storage volume. Mount the Volume Gateway stored volume to the existing file server by using iSCSI, and copy all files to the storage volume.  Configure scheduled  snapshots of the storage volume. To  recover from a disaster,  restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.

�𰸣�D

������ Correct Answer: D Explanation: The company needs a disaster recovery solution that minimizes infrastructure changes and ensures  immediate access to files without  latency.  Here's  why  Option  D  is the  best  choice:  1.  Volume  Gateway  (Stored Volume): - Stores the entire dataset  locally on the existing iSCSI device, so  users access files directly from the on -  premises server with no latency. - Automatically backs up data to AWS via scheduled snapshots (stored in Amazon S3). - Requires minimal changes because it uses the existing iSCSI connection�� no application modifications are needed. 2. Disaster Recovery Process: - If the on - premises system fails, restore the latest snapshot to an Amazon EBS volume and attach it to an EC2 instance. Users can then access the data via the EC2 instance. Other options (A, B, C) either require protocol changes (NFS), use slower tape backups, or rely on limited local caching (risking latency for large datasets). Option D maintains the existing iSCSI setup while seamlessly integrating     with      AWS      for      backups     and      recovery.      Reference      Links:      -      [AWS      Storage      Gateway     Volume Gateway](https://aws.amazon.com/storage gateway/volume     -     gateway/)      -     [Disaster      Recovery     with      AWS     Storage
Gateway](https://docs.aws.amazon.com/storage gateway/latest/user guide/Disaster recovery.html)

283.A company is hosting a web application from an Amazon S3 bucket. The application uses Amazon Cognito as an identity provider to authenticate users and return a JSON Web Token (JWT) that provides access to protected resources that are stored in another S3 bucket. Upon deployment of the application, users report errors and are unable to access the protected content. A solutions architect must resolve this issue by providing proper permissions so that users can access the protected content. Which solution meets these requirements?

A ��Update the Amazon Cognito identity pool to assume the proper IAM role for access to the protected content.

B ��Update the S3 ACL to allow the application to access the protected content.

C ��Redeploy the application to Amazon S3 to prevent eventually consistent reads in the S3 bucket from affecting the ability of users to access the protected content.

D �� Update the Amazon Cognito  pool to use custom attribute  mappings within the identity pool and grant users the  proper permissions to access the protected content.

�𰸣�A

���� �� Correct Answer: A Detailed Explanation: When users authenticate with Amazon Cognito, they receive temporary AWS credentials through an identity pool. These credentials allow users to access AWS resources (like the protected S3 bucket) based on the permissions defined in the IAM role associated with the identity pool. If the identity pool is configured with the wrong IAM role (or a role lacking permissions for the S3 bucket), users will  be denied access.  Updating the identity pool to use the correct IAM role ensures that authenticated users inherit the proper permissions to access the protected content. Other options are incorrect because: - B: S3 ACLs are outdated and less flexible than IAM policies. The root issue is IAM permissions, not bucket ACLs. - C: S3 now provides strong read-after-write consistency, so eventual consistency is not the problem. - D: Custom attribute mappings define  how  user  attributes are  mapped,  but they don ��t  directly  grant  resource access.  Reference  Link:  [Amazon Cognito Identity Pools](https://docs.aws.amazon.com/cognito/latest/developer guide/identity-pools.html)

284.A solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 instances contain highly sensitive data and run in a private subnet. According to company policy, the EC2 instances that run in the VPC can access only approved third-party software repositories on the internet for software product updates that use the third party ��s URL. Other internet

traffic must be blocked. Which solution meets these requirements?

A ��Update the route table for the private subnet to route the outbound traffic to an AWS Network Firewall firewall. Configure domain list rule groups.

B ��Set up an AWS WAF web ACL. Create a custom set of rules that filter traffic requests based on source and destination IP address range sets.

C ��Implement strict inbound security group rules. Configure an outbound rule that allows traffic only to the authorized software repositories on the internet by specifying the URLs.

D �� Configure an Application Load  Balancer (ALB) in front of the  EC2 instances.  Direct all outbound traffic to the ALB.  Use a URL-based rule listener in the ALB ��s target group for outbound access to the internet.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The correct solution is to use AWS Network Firewall with domain list rule groups. Here's why: 1. Private Subnet Requirements: EC2 instances in a private subnet cannot access the internet directly. All outbound traffic must pass through a security gateway (like Network Firewall). 2. Domain-Based Filtering: Network Firewall allows you to create  rules  based  on  domain  names  (URLs).  This  matches  the  requirement  to  permit  traffic  only  to  approved  third-party repositories using their URLs, even if their  IP addresses change over time. 3. Traffic Control: By updating the private subnet's route table to send outbound traffic to  Network  Firewall, you ensure all  internet-bound traffic  is  inspected.  Domain  list  rule groups block all traffic except to the approved domains. Why other options are incorrect: -  B. AWS WAF: WAF  protects web applications (e.g., ALB) but filters HTTP/HTTPS traffic based on IPs/patterns, not domain URLs. It can't control general outbound traffic from  EC2 instances. - C. Security Groups: Security groups use  IP/port rules, not URLs. Approved repositories might use dynamic IPs, making this unreliable. - D. ALB: ALB handles inbound traffic (requests to your EC2 instances), not outbound traffic from         instances         to          the         internet.          Reference         Links:          -         [AWS          Network         Firewall          Domain Filtering](https://docs.aws.amazon.com/network-firewall/latest/developer guide/domain-lists.html)   -   [AWS    Network   Firewall Basics](https://aws.amazon.com/network-firewall/features/)

285.A company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts the website on Amazon S3 and integrates the website with an API that handles sales requests. The company hosts the API on three Amazon EC2 instances behind an Application Load Balancer (ALB). The API consists of static and dynamic front-end content along with backend workers that process sales requests asynchronously. The company is expecting a significant and sudden increase in the number of sales requests during events for the  launch of new products. What should a solutions architect  recommend to ensure that all the requests are processed successfully?

A��Add an Amazon Cloud front distribution for the dynamic content. Increase the number of EC2 instances to handle the increase in traffic.

B ��Add an Amazon Cloud front distribution for the static content. Place the EC2 instances in an Auto Scaling group to launch new instances based on network traffic.

C ��Add an Amazon Cloud front distribution for the dynamic content. Add an Amazon Elastic ache instance in front of the ALB to reduce traffic for the API to handle.

D ��Add an Amazon Cloud front distribution for the static content. Add an Amazon Simple Queue Service (Amazon SQS) queue to receive requests from the website for later processing by the EC2 instances.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The best solution is to use Amazon Cloud front for static content (which offloads traffic from the EC2 instances and improves performance) and add an Amazon SQS queue to handle the sudden spike in sales requests. Here ��s why: 1. Static content on Cloud front: The website ��s static content (e.g., images, CSS, Javascript) can be cached and delivered globally via Cloud front,  reducing  load on the  backend  EC2 instances. 2. SQS for asynchronous  processing: The backend workers process sales requests asynchronously. During traffic spikes, an SQS queue acts as a buffer. Requests from the website  are  sent  to  the  queue,  and  EC2  instances  process  them  at  their  own  pace.  This  decouples  the  front end  (request submission) from the backend (request processing), ensuring no requests are lost even if the system is temporarily overloaded. Options A and C are incorrect because Cloud front is not ideal for dynamic content (which requires real-time processing). Option B uses Auto Scaling (good for scaling EC2 instances) but lacks a buffer like SQS to handle sudden bursts, risking request failures during       extreme        spikes.        Reference        Links:       -        [Amazon        SQS](https://aws.amazon.com/sqs/)       -        [Amazon Cloud front](https://aws.amazon.com/cloud front/)

286.A security audit reveals that Amazon EC2 instances are not being patched regularly. A solutions architect needs to provide a solution that will run regular security scans across a large fleet of EC2 instances. The solution should also patch the EC2 instances on a regular schedule and provide a report of each instance ��s patch status. Which solution will meet these requirements?

A ��Set up Amazon Macie to scan the EC2 instances for software vulnerabilities. Set up a cron job on each EC2 instance to patch the instance on a regular schedule.

B��Turn on Amazon Guard duty in the account. Configure Guard duty to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Session Manager to patch the EC2 instances on a regular schedule.

C ��Set up Amazon Detective to scan the EC2 instances for software vulnerabilities. Set up an Amazon Event bridge scheduled rule to patch the EC2 instances on a regular schedule.

D ��Turn on Amazon Inspector in the account. Configure Amazon Inspector to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Patch Manager to patch the EC2 instances on a regular schedule.

�𰸣�D

�� �� ��  Correct  Answer:  D  Detailed  Explanation:  The  correct  solution  is  Option  D  because  it  uses  Amazon  Inspector  for vulnerability  scanning  and  AWS  Systems   Manager  Patch  Manager  for   automated  patching,  which  together   meet  all  the requirements:  1.  Amazon  Inspector:  -  Automatically  scans  EC2  instances  for  software  vulnerabilities  (e.g.,  missing  patches, misconfigurations). - Generates detailed reports about the security findings, including patch status. 2. AWS Systems  Manager Patch Manager: - Automates the process of patching EC2 instances on a predefined schedule. - Provides compliance reports to track which  instances are  patched and which  need attention. Why other options are incorrect: - Option A: Amazon  Macie  is designed  for  data  privacy  and  sensitive  data  discovery  (e.g.,  in  S3  buckets),  not  vulnerability  scanning.  Using  cron  jobs  for patching is manual and error-prone. - Option B: Amazon Guard duty detects threats like malicious API activity but doesn ��t scan for software vulnerabilities. Session  Manager is for SSH access,  not patching. - Option C: Amazon  Detective analyzes security incidents  but  doesn �� t  perform  vulnerability  scans.  Event bridge  can  trigger  events  but  lacks  built-in  patching  automation. Reference     Links:      -      [Amazon      Inspector](https://aws.amazon.com/inspector/)      -      [AWS     Systems      Manager      Patch Manager](https://aws.amazon.com/systems-manager/patch-manager/)

287.A company is planning to store data on Amazon RDS DB instances. The company must encrypt the data at rest. What should a solutions architect do to meet this requirement?

A ��Create a key in AWS Key Management Service (AWS KMS). Enable encryption for the DB instances.

B ��Create an encryption key. Store the key in AWS Secrets Manager. Use the key to encrypt the DB instances.

C ��Generate a certificate in AWS Certificate Manager (ACM). Enable SSL/TLS on the DB instances by using the certificate.

D �� Generate a certificate  in AWS  Identity and Access  Management  (IAM).  Enable SSL/TLS on the  DB  instances  by  using the certificate.

�𰸣�A

������ Correct Answer: A Detailed Explanation: To encrypt data at rest in Amazon RDS, you need to use AWS Key Management Service  (KMS).  Here's  why: - AWS  KMS  (Option  A)  is the  service  specifically designed for  managing  encryption  keys  used  to encrypt data at rest. When you enable encryption for an RDS instance, AWS automatically uses KMS to handle the encryption keys. - Secrets Manager (Option B) stores credentials like passwords, not encryption keys for data-at-rest. - SSL/TLS certificates (Options C and D) encrypt data in transit (during transmission), not data at rest. ACM and IAM don��t handle disk-level encryption. Reference                                                               Link:                                                               [Amazon                                                               RDS
Encryption](https://docs.aws.amazon.com/Amazon rds/latest/User guide/Overview.Encryption.html)

288.A company must migrate 20 TB of data from a data center to the AWS Cloud within 30 days. The company ��s network bandwidth  is  limited  to  15  Mbps  and  cannot  exceed  70%  utilization.  What  should  a  solutions  architect  do  to  meet  these requirements?

A ��Use AWS Snowball.

B ��Use AWS Data sync.

C ��Use a secure VPN connection.

D ��Use Amazon S3 Transfer Acceleration.

�𰸣�A

������Correct Answer A ��Use AWS Snowball. Detailed Explanation The company needs to transfer 20 TB of data within 30 days. With a network bandwidth of 15 Mbps (70% usable = 10.5 Mbps), transferring 20 TB over the internet would take approximately 176 days (calculated below), which far exceeds the 30-day requirement. Network-based solutions like AWS Data sync (B), VPN (C), or S3 Transfer Acceleration (D) are impractical due to bandwidth limitations. AWS Snowball (A) is designed for large-scale offline data transfers. It physically transports storage devices to AWS, bypassing slow internet connections. This method ensures the data is transferred securely and within the required time frame. Calculation: - 20 TB = 160,000,000 Megabits (1 TB = 8,000,000 Megabits). - Time = Total Data / Bandwidth = 160,000,000 Mb / 10.5 Mbps   �� 15,238,095 seconds   �� 176  days. Reference Links             -              [AWS              Snowball](https://aws.amazon.com/snowball/)              -              [AWS              Data              Transfer Options](https://aws.amazon.com/answers/data-transfer/)

289.A company needs to provide its employees with secure access to confidential and sensitive files. The company wants to ensure that the files can  be accessed only  by authorized users. The files  must  be downloaded securely to the employees �� devices. The files are stored in an on-premises Windows file server. However, due to an increase in remote usage, the file server  is running out of capacity. . Which solution will meet these requirements?

A ��Migrate the file server to an Amazon EC2 instance in a public subnet. Configure the security group to limit inbound traffic to the employees �� IP addresses.

B �� Migrate the  files to an Amazon  FSx for  Windows  File  Server file system.  Integrate  the Amazon  FSx  file  system with the on-premises Active Directory. Configure AWS Client VPN.

C ��Migrate the files to Amazon S3, and create a private VPC endpoint. Create a signed URL to allow download.

D ��Migrate the files to Amazon S3, and create a public VPC endpoint. Allow employees to sign on with AWS IAM Identity Center (AWS Single Sign-On).

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: The best solution is B because it addresses all the requirements securely and efficiently: - Amazon FSx for Windows File Server is a fully managed service that replicates the on-premises Windows file server environment, ensuring compatibility and scalability (solving the capacity issue). - Integration with on-premises Active Directory allows employees to use their existing credentials for authorization, ensuring only authorized users access files. - AWS Client VPN establishes a secure, encrypted connection between remote employees and the AWS environment, preventing exposure of the file system to the public internet. This keeps downloads secure and limits access to authenticated users. Other options fall short:
- A uses a public subnet, exposing the file server to unnecessary risk. - C and D rely on Amazon S3, which isn ��t designed for traditional  file  server  workflows.  Signed  URLs  (C)  or  public  endpoints  (D)  add  complexity  or  security  risks  compared  to  a dedicated        VPN        and        AD        integration.         Reference        Links:        -         [Amazon        FSx         for        Windows        File Server](https://aws.amazon.com/fsx/windows/) - [AWS Client VPN](https://aws.amazon.com/vpn/client-vpn/)

290.A company ��s application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. On the first day of every month at midnight, the application becomes much slower when the month-end financial calculation batch runs. This causes the CPU utilization of the EC2 instances to  immediately  peak  to  100%,  which  disrupts  the  application.  What  should  a  solutions  architect  recommend  to  ensure the application is able to handle the workload and avoid downtime?

A ��Configure an Amazon Cloud front distribution in front of the ALB.

B ��Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization.

C ��Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule.

D ��Configure Amazon Elastic ache to remove some of the workload from the EC2 instances.

�𰸣�C

������ Correct Answer: C. Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule. Explanation: The problem occurs predictably every month at a specific time (midnight on the first day). A scheduled scaling policy in EC2 Auto Scaling allows you to automatically increase the number of EC2 instances before the expected high-load event (monthly batch job). This way, the additional  instances  are already  running when the  CPU-heavy  batch  process  starts,  preventing  CPU  from spiking to 100% and avoiding application disruption. Other options: - A (Cloud front) is for caching static content, not dynamic batch processing. - B (Simple scaling policy) reacts after CPU spikes, which is too late to prevent slowdowns. - D (Elastic ache) reduces     database     load     but     won't     help     with     CPU-intensive     batch     calculations.     Reference:      [AWS     Scheduled Scaling](https://docs.aws.amazon.com/auto scaling/ec2/user guide/schedule_time.html)

291.A company wants to give a customer the ability to use on-premises Microsoft Active Directory to download files that are stored in Amazon S3. The customer ��s application uses an SFTP client to download the files. Which solution will  meet these

requirements with the LEAST operational overhead and no changes to the customer ��s application?

A ��Set up AWS Transfer Family with SFTP for Amazon S3. Configure integrated Active Directory authentication.

B �� Set  up  AWS  Database  Migration  Service  (AWS  DMS)  to  synchronize  the  on-premises  client  with  Amazon  S3.  Configure integrated Active Directory authentication.

C ��Set up AWS Data sync to synchronize between the on-premises location and the S3 location by using AWS IAM Identity Center (AWS Single Sign-On).

D ��Set up a Windows Amazon EC2 instance with SFTP to connect the on-premises client with Amazon S3. Integrate AWS Identity and Access Management (IAM).

�𰸣�A

������ Correct Answer: A Detailed Explanation: AWS Transfer Family is a fully managed service that provides secure file transfers directly into and out of Amazon S3 using SFTP, FTPS, or FTP protocols. Here's why this is the best solution: 1. SFTP Support: The customer's existing SFTP client works  immediately without any  modifications. 2. Active  Directory  Integration: Transfer  Family natively  integrates  with   Microsoft  Active  Directory  for  authentication,  meeting  the  requirement  to  use  on  -  premises  AD credentials. 3. S3 Integration: Files are stored directly in S3 buckets, eliminating the need for data synchronization. 4. Managed Service: AWS handles server maintenance, security patches, and scaling, minimizing operational overhead. Other options are less suitable:  -  B  (DMS):  Designed  for  database  migrations,  not  file  downloads  via  SFTP.  -  C  (Data sync):  Focuses  on  bulk  data synchronization, not SFTP file access. - D (EC2):  Requires managing servers and SFTP software, increasing operational burden.
Reference Link: [AWS Transfer Family Features](https://aws.amazon.com/aws - transfer - family/features/)

292.A company is experiencing sudden increases in demand. The company needs to provision large Amazon EC2 instances from an Amazon Machine Image (AMI). The instances will run in an Auto Scaling group. The company needs a solution that provides minimum initialization latency to meet the demand. Which solution meets these requirements?

A ��Use the aws ec2 register-image command to create an AMI from a snapshot. Use AWS Step Functions to replace the AMI in the Auto Scaling group.

B��Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot. Provision an AMI by using the snapshot. Replace the AMI in the Auto Scaling group with the new AMI.

C �� Enable AMI creation and define lifecycle rules in Amazon Data Lifecycle Manager (Amazon DLM). Create an AWS Lambda function that modifies the AMI in the Auto Scaling group.

D ��Use Amazon Event bridge to invoke AWS Backup lifecycle policies that provision AMIs. Configure Auto Scaling group capacity limits as an event source in Event bridge.

�𰸣�B

������ Correct Answer: B Detailed Explanation: When EC2 instances are launched from an AMI backed by an EBS snapshot, the EBS volume  must  be  initialized  (data  is  lazily  loaded  from  the  snapshot).  This  can  cause  delays.  Amazon  EBS  Fast  Snapshot Restore (FSR) solves this by pre-warming the snapshot in a specific Availability Zone, allowing new EBS volumes to be created instantly from the snapshot. By enabling FSR on the snapshot used for the AMI, the Auto Scaling group can launch instances with minimal  initialization  latency.  This  ensures  instances  are  ready  faster  during  sudden  demand  spikes.  Other  options  involve unnecessary complexity (Step Functions, Lambda, Event bridge) or don ��t address the core issue of snapshot initialization delays.

Reference:                                [Amazon                               EBS                                Fast                                Snapshot                                Restore
Documentation](https://docs.aws.amazon.com/AWSEC2/latest/User guide/ebs-fast-snapshot-restore.html)

293.A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for storage. The application tier is hosted on Amazon EC2 instances. The company ��s IT security guidelines mandate that the database credentials be encrypted and rotated every 14 days. What should a solutions architect do to meet this requirement with the LEAST operational effort?

A��Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Associate the secret with the Aurora DB cluster. Configure a custom rotation period of 14 days.

B ��Create two parameters in AWS Systems Manager Parameter Store: one for the user name as a string parameter and one that uses the Secure string type for the  password. Select AWS  Key  Management  Service  (AWS  KMS)  encryption for the  password parameter, and load these  parameters in the application tier. Implement an AWS Lambda function that rotates the password every 14 days.

C��Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in all EC2 instances of t he application tier. Restrict the access to the file on the file system so that the application can read the file and that only super users can modify the file. Implement an AWS Lambda function that rotates the key in Aurora every 14 days and writes new credentials into the file.

D��Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon S3 bucket that the application uses to load the credentials. Download the file to the application regularly to ensure that the correct credentials are used. Implement an AWS Lambda function that rotates the Aurora credentials every 14 days and uploads these credentials to the file in the S3 bucket.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution is to use AWS Secrets Manager with a 14 - day rotation period. Here's why: 1. Automated Rotation: Secrets Manager can automatically rotate database credentials (like Aurora MySQL) without custom code. It handles the rotation process securely. 2. KMS Integration: Secrets Manager uses AWS KMS to encrypt secrets by default, meeting the encryption requirement. 3. Least Effort: No need to build Lambda functions (like Options B/C/ D) or manage file systems/S3 buckets. Secrets Manager natively integrates with Aurora DB clusters. 4. Central Management: Credentials are stored  in Secrets  Manager  (not in  Parameter Store/S3/EFS), which  is  purpose  -  built for secrets  management. Other options require  manual  coding  (Lambda),  file  permissions  management  (EFS/S3),  or  lack  native  rotation  support  (Parameter  Store). Secrets Manager minimizes operational overhead while fulfilling security requirements. Reference Links: - [AWS Secrets Manager Rotation](https://docs.aws.amazon.com/secrets manager/latest/user guide/rotating-secrets.html)  -  [AWS  Secrets   Manager    & Aurora
Integration](https://aws.amazon.com/blogs/security/rotate-amazon-aurora-mysql-credentials-with-aws-secrets-manager/)

294.A company has deployed a web application on AWS. The company hosts the backend database on Amazon RDS for MySQL with a primary DB instance and five read replicas to support scaling needs. The read replicas must lag no more than 1 second behind the primary DB instance. The database routinely runs scheduled stored procedures. As traffic on the website increases, the replicas experience additional lag during periods of peak load. A solutions architect must reduce the replication lag as much as  possible. The  solutions  architect  must  minimize  changes  to the  application  code and  must  minimize  ongoing  operational overhead. Which solution will meet these requirements?

A ��Migrate the database to Amazon Aurora MySQL. Replace the read replicas with Aurora Replicas, and configure Aurora Auto Scaling. Replace the stored procedures with Aurora MySQL native functions.

B ��Deploy an Amazon Elastic ache for Redis cluster in front of the database. Modify the application to check the cache before the application queries the database. Replace the stored procedures with AWS Lambda functions.

C �� Migrate  the  database to  a  MySQL  database  that  runs  on  Amazon  EC2  instances.  Choose  large,  compute  optimized  EC2 instances for all replica nodes. Maintain the stored procedures on the EC2 instances.

D ��Migrate the database to Amazon Dynamo db. Provision a large number of read capacity units (RCUs) to support the required throughput, and configure on-demand capacity scaling. Replace the stored procedures with Dynamo db streams.

�𰸣�A

������ Correct Answer: A Detailed Explanation: Amazon Aurora MySQL is designed to handle high-performance workloads with lower replication lag compared to standard RDS MySQL. Aurora Replicas have minimal lag (often milliseconds) because they use a  shared  storage  architecture,  which  avoids  the   physical   replication  delays  of  traditional   MySQL.  Aurora  Auto  Scaling automatically adjusts the  number of replicas during  peak traffic,  reducing the  load on  individual  replicas and minimizing  lag. Replacing stored procedures with Aurora-native functions (if compatible) can further optimize performance with minimal code changes. Other options require more code changes (B, D) or increase operational complexity (C).  Reference Links: -  [Amazon Aurora   Replicas](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/Aurora.Replication.html)   -   [Aurora   Auto Scaling](https://aws.amazon.com/rds/aurora/auto scaling/)

295.A solutions architect must create a disaster recovery (DR) plan for a high-volume software as a service (SaaS) platform. All data for the platform is stored in an Amazon Aurora MySQL DB cluster. The DR plan must replicate data to a secondary AWS Region. Which solution will meet these requirements MOST cost-effectively?

A �� Use MySQL binary log replication to an Aurora cluster in the secondary Region. Provision one DB instance for the Aurora cluster in the secondary Region.

B ��Set up an Aurora global database for the DB cluster. When setup is complete, remove the DB instance from the secondary Region.

C ��Use AWS Database Migration Service (AWS DMS) to continuously replicate data to an Aurora cluster in the secondary Region. Remove the DB instance from the secondary Region.

D ��Set up an Aurora global database for the DB cluster. Specify a minimum of one DB instance in the secondary Region.

�𰸣�B

���� �� Correct Answer D Explanation The correct solution is D because an Aurora Global Database requires at least one DB instance in the secondary Region to maintain replication and ensure data is available for disaster recovery (DR). Here ��s why: - Aurora Global  Database  replicates  data  at the  storage  layer to the  secondary  Region,  but  it  still  needs  a  DB  instance  in  the secondary Region to manage replication and apply changes. Without an instance, the secondary cluster cannot function, and replication may stop. - Option B is incorrect because removing the DB instance in the secondary Region would break the Aurora Global Database setup. Aurora clusters must have at least one DB instance to exist. - Option D is cost - effective because you can use a smaller instance (e.g., a reader instance) in the secondary Region for replication. This minimizes costs while ensuring data is ready for DR. In contrast, options like AWS DMS (C) or manual replication (A) add complexity and cost. Aurora Global Database is the native, low - latency, and most cost - efficient way to replicate data across Regions for DR. Reference Links - [AWS Aurora Global   Database](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/aurora-global-database.html)  -   [Disaster Recovery for Aurora](https://aws.amazon.com/rds/aurora/disaster-recovery/)

296.A company has a custom application with embedded credentials that retrieves information from an Amazon RDS MySQL DB instance.  Management says the application must  be  made  more secure with the  least amount of  programming effort. What should a solutions architect do to meet these requirements?

A ��Use AWS Key Management Service (AWS KMS) to create keys. Configure the application to load the database credentials from AWS KMS. Enable automatic key rotation.

B ��Create credentials on t he RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Create an AWS Lambda function that rotates the credentials in Secret Manager.

C ��Create credentials on t he RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Set up a credentials rotation schedule for the application user in t he RDS for MySQL database using Secrets Manager.

D��Create credentials on t he RDS for MySQL database for the application user and store the credentials in AWS Systems Manager Parameter Store. Configure the application to load the database credentials from Parameter Store. Set up a credentials rotation schedule for the application user in t he RDS for MySQL database using Parameter Store.

�𰸣�C

���� �� Correct Answer: C Detailed Explanation: The best solution is C because it securely stores credentials in AWS Secrets Manager and uses its built-in rotation feature for RDS MySQL. Here's why: 1. Problem with Embedded Credentials: Hardcoding credentials in t he app is risky (e.g., leaks, no rotation). The goal is to eliminate this. 2. Secure Storage: AWS Secrets Manager securely stores database credentials (encrypted at rest/in transit). The app fetches them programmatically, removing hardcoded values.  3.  Automatic  Rotation:  Secrets  Manager  natively  supports  rotating  RDS  MySQL  credentials.  You  simply  configure  a rotation schedule�� no custom code (like Lambda in Option B) is needed. This minimizes programming effort. 4. Why Not Other Options: - A: KMS manages encryption keys, not secrets. It doesn ��t store/rotate credentials. - B: Using a custom Lambda for rotation adds unnecessary coding effort. Secrets Manager handles this automatically. - D: Parameter Store can store secrets but lacks  native  RDS  credential  rotation.  You ��d  have  to  build  rotation  logic,  increasing  effort.  For  a  Beginner:  Imagine  Secrets Manager as a locked vault. Instead of writing your database password on a sticky note (embedded credentials), you store it in the vault. The vault can automatically change the password every few days (rotation), and your app just asks the vault for the latest password. No extra coding��just configure the vault to handle rotations. This is safer and simpler. Reference Links: - [AWS Secrets Manager       Overview](https://aws.amazon.com/secrets-manager/)       -        [Rotating        RDS       Credentials        with       Secrets
Manager](https://docs.aws.amazon.com/secrets manager/latest/user guide/rotating-secrets-strategies.html)

297.A media company hosts its website on AWS. The website application ��s architecture includes a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB) and a database that is hosted on Amazon Aurora. The company ��s cybersecurity team reports that the application is vulnerable to SQL injection. How should the company resolve this issue?

A ��Use AWS WAF in front of the ALB. Associate the appropriate web ACLs with AWS WAF.

B ��Create an ALB listener rule to reply to SQL injections with a fixed response.

C ��Subscribe to AWS Shield Advanced to block all SQL injection attempts automatically.

D ��Set up Amazon Inspector to block all SQL injection attempts automatically.

�𰸣�A

������ Correct Answer: A Detailed Explanation: SQL injection is a common web attack where malicious code is injected into a database query. To block this, you need a tool that inspects  incoming web traffic and blocks suspicious  patterns. - Option A (Correct): AWS WAF (Web Application Firewall) is specifically designed to protect web applications by filtering traffic based on rules (web ACLs). It can detect and block SQL injection patterns in HTTP requests. Placing it in front of the ALB ensures all traffic passes through the firewall before reaching your servers. - Option B (Incorrect): ALB listener rules route traffic but cannot inspect content for SQL injection. A fixed response would only send a  reply, not block the attack. - Option C (Incorrect): AWS Shield Advanced focuses on DDoS protection, not application-layer attacks like SQL injection. - Option D (Incorrect): Amazon Inspector scans     for      vulnerabilities      but     doesn     ��  t      block      attacks      in     real-time.      Reference      Links:      -      [AWS      WAF Documentation](https://docs.aws.amazon.com/waf/latest/developer guide/waf-chapter.html)  -   [SQL  Injection  Protection  with AWS WAF](https://aws.amazon.com/waf/features/#SQL_Injection)

298.A  company  has  an  Amazon  S3  data  lake  that  is  governed   by  AWS   Lake  Formation.  The  company  wants  to  create  a visualization in Amazon Quick sight by joining the data in the data lake with operational data that is stored in an Amazon Aurora MySQL database. The company wants to enforce column-level authorization so that the company ��s marketing team can access only a subset of columns in the database. Which solution will meet these requirements with the LEAST operational overhead?

A �� Use Amazon EMR to ingest the data directly from the database to the Quick sight SPICE engine. Include only the required columns.

B ��Use AWS Glue Studio to ingest the data from the database to the S3 data lake. Attach an IAM policy to the Quick sight users to enforce column-level access control. Use Amazon S3 as the data source in Quick sight.

C��Use AWS Glue Elastic Views to create a materialized view for the database in Amazon S3. Create an S3 bucket policy to enforce column-level access control for the Quick sight users. Use Amazon S3 as the data source in Quick sight.

D �� Use a Lake Formation blueprint to ingest the data from the database to the S3 data lake. Use Lake Formation to enforce column-level access control for the Quick sight users. Use Amazon Athena as the data source in Quick sight.

�𰸣�D

�� �� �� Correct Answer:  D  Detailed  Explanation:  The  correct  solution  is  D  because  it  uses  AWS  Lake  Formation  to  handle column-level permissions and integrates seamlessly with Amazon Athena, minimizing operational overhead. Here's why: 1. Lake Formation Blueprints: These automate data ingestion from Aurora MySQL to the S3 data lake. Blueprints simplify ETL (Extract, Transform, Load) workflows, reducing manual effort. 2. Column-Level Access Control: Lake Formation allows administrators to define granular permissions (e.g., restricting access to specific columns). This ensures the marketing team only sees authorized columns without complex code or policies. 3. Amazon Athena Integration: By querying data via Athena, Quick sight inherits Lake Formation ��s permissions. Athena acts as a federated query layer, combining data from the S3 data lake and Aurora MySQL while enforcing security rules. Why Other Options Fail: - A: Using EMR requires managing clusters and writing custom code, increasing operational complexity. - B: IAM policies can ��t enforce column-level restrictions. S3 bucket policies only control access to objects, not columns. - C: S3 bucket policies don ��t support column-level controls, and Elastic Views focuses on materialized views, not access               governance.                Reference                Links:               -                [AWS                Lake                Formation               Access Control](https://docs.aws.amazon.com/lake-formation/latest/dg/access-control.html)       -        [Amazon       Athena        Federated Query](https://aws.amazon.com/blogs/aws/amazon-athena-federated-query/)

299.A transaction processing company has weekly scripted batch jobs that run on Amazon EC2 instances. The EC2 instances are in an Auto Scaling group. The number of transactions can vary, but the baseline CPU utilization that is noted on each run is at least 60%. The company needs to provision the capacity 30 minutes before the jobs run. Currently, engineers complete this task

by manually modifying the Auto Scaling group parameters. The company does not have the resources to analyze the required capacity trends for the Auto Scaling group counts. The company needs an automated way to modify the Auto Scaling group ��s desired capacity. Which solution will meet these requirements with the LEAST operational overhead?

A ��Create a dynamic scaling policy for the Auto Scaling group. Configure the policy to scale based on the CPU utilization metric. Set the target value for the metric to 60%.

B ��Create a scheduled scaling policy for the Auto Scaling group. Set the appropriate desired capacity, minimum capacity, and maximum capacity. Set the recurrence to weekly. Set the start time to 30 minutes before the batch jobs run.

C ��Create a predictive scaling policy for the Auto Scaling group. Configure the policy to scale based on forecast. Set the scaling metric to CPU utilization. Set the target value for the metric to 60%. In the policy, set the instances to pre-launch 30 minutes before the jobs run.

D ��Create an Amazon Event bridge event to invoke an AWS Lambda function when the CPU utilization metric value for the Auto Scaling group reaches 60%. Configure the Lambda function to increase the Auto Scaling group ��s desired capacity and maximum capacity by 20%.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The company needs to automatically provision EC2 capacity 30 minutes before weekly batch jobs run, without manual intervention. Here's why option C is the best:  1.  Predictive Scaling (Option C): - AWS Predictive Scaling  uses  machine  learning  to  analyze  historical  usage  patterns  and  forecast  future demand. This  matches  the requirement for no resources to analyze trends. - It automatically scales capacity 30 minutes in advance (via the pre - launch setting), ensuring instances are ready when jobs start. - Uses CPU utilization (set to 60%) as the metric, aligning with the baseline requirement. 2. Why other options fail: - A (Dynamic Scaling): Reacts to real - time CPU usage, not future needs. Scaling happens during the job,  not 30  minutes  before. -  B  (Scheduled  Scaling):  Requires  manual  capacity  settings.  Since transaction volume varies, fixed schedules can't adapt to changes. - D (Event bridge/Lambda): Triggers scaling after CPU  hits 60%, causing delays. Instances           won't            be            ready           in            time.            Reference           Link:            [AWS            Predictive            Scaling Documentation](https://docs.aws.amazon.com/auto scaling/ec2/user guide/ec2-auto - scaling - predictive - scaling.html)

300.A solutions architect is designing a company ��s disaster recovery (DR) architecture. The company has a MySQL database that runs on an Amazon  EC2  instance  in a  private  subnet with  scheduled  backup. The  DR  design  needs  to  include  multiple  AWS Regions. Which solution will meet these requirements with the LEAST operational overhead?

A �� Migrate  the  MySQL  database  to  multiple  EC2  instances.  Configure  a  standby  EC2  instance  in  the  DR  Region.  Turn  on replication.

B �� Migrate  the  MySQL  database to Amazon  RDS.  Use  a  Multi-AZ  deployment.  Turn  on  read  replication  for the  primary  DB instance in the different Availability Zones.

C ��Migrate the MySQL database to an Amazon Aurora global database. Host the primary DB cluster in the primary Region. Host the secondary DB cluster in the DR Region.

D��Store the scheduled backup of the MySQL database in an Amazon S3 bucket that is configured for S3 Cross-Region Replication (CRR). Use the data backup to restore the database in the DR Region.

�𰸣�C

������ Correct Answer C �� Migrate the MySQL database to an Amazon Aurora global database. Host the primary DB cluster in      the primary Region. Host the secondary DB cluster in the DR Region. Detailed Explanation Option C is the best choice because     Amazon  Aurora  Global  Database   is  specifically  designed  for  multi-Region  disaster  recovery  (DR)  with  minimal  operational     overhead. Aurora Global Database automatically replicates data to a secondary Region with low latency, provides fast failover      (typically   ��1 minute), and keeps the secondary Region as a read-only copy until a disaster occurs. This fully managed solution     eliminates the need to manually configure replication or backups across Regions, reducing operational complexity. - Why not A?     Running MySQL on EC2 instances across Regions requires manual setup of replication, monitoring, and failover, which increases     operational effort. - Why not B? RDS Multi-AZ and read replicas only work within a single Region (not cross-Region), so this doesn �� t meet the multi-Region DR requirement. - Why not D? Restoring from S3 backups in a DR Region involves downtime and manual     steps, leading to higher recovery time objectives (RTO) and operational work during a disaster. Reference Link [Amazon Aurora    Global Database](https://aws.amazon.com/rds/aurora/global-database/)

301.A company has a Java application that uses Amazon Simple Queue Service (Amazon SQS) to parse messages. The application cannot parse messages that are larger than 256 KB in size. The company wants to implement a solution to give the application the ability to parse messages as large as 50 MB. Which solution will meet these requirements with the FEWEST changes to the code?

A ��Use the Amazon SQS Extended Client Library for Java to host messages that are larger than 256 KB in Amazon S3.

B ��Use Amazon Event bridge to post large messages from the application instead of Amazon SQS.

C ��Change the limit in Amazon SQS to handle messages that are larger than 256 KB.

D ��Store messages that are larger than 256 KB in Amazon Elastic File System (Amazon EFS). Configure Amazon SQS to reference this location in the messages.

�𰸣�A

������ Correct Answer A Detailed Explanation The correct solution is A because the Amazon SQS Extended Client Library for Java is specifically designed to handle messages larger than 256 KB. Here ��s how it works in simple terms: 1. **Problem** : SQS has a hard limit of 256 KB per message. Larger messages (like 50 MB) can ��t be sent directly. 2. **Solution** : The Extended Client Library automatically stores large messages in Amazon S3 (which supports huge files) and leaves a small pointer (a reference to the S3 object) in the SQS message. 3. **Code Changes** : You only need to integrate the library into your Java app�� no major rewrites. The library handles the behind-the-scenes work of storing/retrieving messages from S3. Other options fail because: - B: Event bridge isn ��t a queue service and has its own message size limits (up to 256 KB for most cases). - C: SQS ��s 256 KB limit is fixed and cannot be changed. -  D:  EFS isn ��t integrated with SQS, so you ��d  have to write extra code to  manage files and references               manually.               Reference                Links               -               [Amazon                SQS               Extended               Client Library](https://docs.aws.amazon.com/Aws simple queue service/latest/Sqs developer guide/sqs-s3-messages.html)       -       [SQS Message                                                                                                                                                                                                                           Size
Limits](https://docs.aws.amazon.com/Aws simple queue service/latest/Sqs developer guide/quotas-messages.html)

302.A company wants to restrict access to the content of one of its main web applications and to protect the content by using authorization techniques available on AWS. The company wants to implement a serverless architecture and an authentication solution for fewer than 100 users. The solution needs to integrate with the main web application and serve web content globally. The solution must also scale as the company's user base grows while providing the lowest login latency possible. Which solution will meet these requirements MOST cost-effectively?

A �� Use Amazon Cognito for authentication.  Use  Lambda@Edge for authorization.  Use Amazon Cloud front to serve the web application globally.

B �� Use AWS  Directory Service for  Microsoft Active  Directory for authentication.  Use AWS  Lambda for authorization.  Use  an Application Load Balancer to serve the web application globally.

C��Use Amazon Cognito for authentication. Use AWS Lambda for authorization. Use Amazon S3 Transfer Acceleration to serve the web application globally.

D ��Use AWS Directory Service for Microsoft Active Directory for authentication. Use Lambda@Edge for authorization. Use AWS Elastic Beanstalk to serve the web application globally.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution combines Amazon Cognito, Lambda@Edge, and Cloud front. Here's  why:  1.  Amazon  Cognito  is  serverless,  scales  automatically,  and  handles  authentication  for  small-to-large  user  bases cost-effectively.  2.  Lambda@Edge  runs  authorization  checks  at  Cloud front's  edge  locations,  minimizing  login  latency  by processing requests closer to users globally. 3. Cloud front serves web content globally via AWS's CDN, ensuring fast delivery and seamless scalability. Other options fail because: - B/D use AWS Directory Service (designed for complex Microsoft AD integrations, overkill for 100 users) and non-serverless components (e.g., Elastic Beanstalk). - C uses S3 Transfer Acceleration (optimized for uploads,      not      content       delivery)      and       lacks      edge-optimized       authorization.      Reference       Links:      -       [Amazon Cognito](https://aws.amazon.com/cognito/)               -                [Lambda@Edge](https://aws.amazon.com/lambda/edge/)               - [Cloud front](https://aws.amazon.com/cloud front/)

303.A company has an aging network-attached storage (NAS) array in its data center. The NAS array presents SMB shares and NFS shares to client workstations. The company does not want to purchase a new NAS array. The company also does not want to incur the cost of renewing the NAS array ��s support contract. Some of the data is accessed frequently, but much of the data is inactive. A solutions architect needs to implement a solution that migrates the data to Amazon S3, uses S3 Lifecycle policies, and maintains the same look and feel for the client workstations. The solutions architect has identified AWS Storage Gateway as part of the solution. Which type of storage gateway should the solutions architect provision to meet these requirements?

A ��Volume Gateway

B ��Tape Gateway

C ��Amazon FSx File Gateway D ��Amazon S3 File Gateway
�𰸣�D

������ Correct Answer: D. Amazon S3 File Gateway Detailed Explanation: The Amazon S3 File Gateway is the right choice here because it directly addresses the requirements. Let ��s break it down: 1. Migrate to Amazon S3: The S3 File Gateway acts as a bridge  between the existing  NAS  and Amazon S3.  It  stores files  as objects  in S3  buckets,  seamlessly  migrating  data  without changing how clients access it. 2. S3 Lifecycle Policies: Since inactive data can be transitioned to cost - effective storage tiers (like S3 Glacier), the S3 File Gateway integrates with S3 Lifecycle policies. This aligns with the requirement to manage frequently vs. infrequently  accessed  data.  3.  Same  Look    ��  Feel:  The  S3  File  Gateway  supports  SMB  (for  Windows)  and  NFS  (for  Linux) protocols, maintaining the existing file - sharing experience for client workstations. Users won��t notice a difference��it ��s like the old NAS but backed by S3. Why not other options? - A. Volume Gateway: This provides block storage (iSCSI) volumes, not file shares (SMB/NFS). It ��s for scenarios like replacing SANs, not NAS. - B. Tape Gateway: Designed for backups to virtual tapes in S3/Glacier, not live file sharing. - C. Amazon FSx File Gateway: This integrates with Amazon FSx (e.g., FSx for Windows File Server),

not     S3.       It      doesn     ��  t      use      S3       Lifecycle      policies      directly.      Reference      Link:      [AWS      Storage      Gateway Types](https://docs.aws.amazon.com/storage gateway/latest/user guide/storage gateway-types.html)

304.A company has an application that is running on Amazon EC2 instances. A solutions architect has standardized the company on a particular instance family and various instance sizes based on the current needs of the company. The company wants to maximize cost savings for the application over the next 3 years. The company needs to be able to change the instance family and sizes  in the  next  6  months  based  on  application  popularity  and  usage.  Which  solution  will  meet  these  requirements  MOST cost-effectively?

A ��Compute Savings Plan

B ��EC2 Instance Savings Plan

C ��Zonal Reserved Instances

D ��Standard Reserved Instances

�𰸣�A

���� �� Correct Answer: A. Compute Savings Plan Detailed Explanation: The Compute Savings Plan is the most cost - effective solution here because it offers flexibility to change both instance families and sizes while maintaining significant cost savings over 3 years. Unlike EC2  Instance Savings Plans (which lock you to a specific instance family) or Reserved Instances (which require fixed instance types/regions), the Compute Savings Plan automatically applies discounts to ANY EC2 instance type or size within the same AWS account. This aligns perfectly with the requirement to potentially switch instance families/sizes within 6 months while still maximizing 3 - year savings. The other options either restrict instance flexibility (B/D) or provide weaker discounts (C). Reference  Links:  -  AWS  Savings  Plans:   https://aws.amazon.com/savings plans/  -  Compute  vs  EC2   Instance  Savings   Plans: https://docs.aws.amazon.com/savings plans/latest/user guide/what - is - savings - plans.html#sp - types

305.A company collects data from a large number of participants who use wearable devices. The company stores the data in an Amazon Dynamo db table and uses applications to analyze the data. The data workload is constant and predictable. The company wants  to  stay  at  or  below  its  forecasted   budget  for   Dynamo db.  Which  solution  will   meet  these   requirements  MOST cost-effectively?

A �� Use  provisioned  mode  and  Dynamo db  Standard-Infrequent  Access  (Dynamo db  Standard-IA).  Reserve  capacity  for  the forecasted workload.

B ��Use provisioned mode. Specify the read capacity units (RCUs) and write capacity units (WCUs).

C �� Use on-demand mode. Set the read capacity units (RCUs) and write capacity units (WCUs) high enough to accommodate changes in the workload.

D ��Use on-demand mode. Specify the read capacity units (RCUs) and write capacity units (WCUs) with reserved capacity.

�𰸣�B

������ Correct Answer B Explanation The company has a constant and predictable workload, which is the key scenario for using Dynamo db Provisioned Mode. Here's why: - Provisioned Mode allows you to specify fixed Read Capacity Units (RCUs) and Write Capacity Units (WCUs) upfront. Since the workload is predictable, you can accurately set these values to match the workload, avoiding over provisioning (which wastes  money) or under provisioning (which causes throttling). - On-demand mode (Options

C/D) is better for unpredictable or spiky workloads, but it costs more for steady workloads. - Dynamo db Standard-IA (Option A) reduces storage costs for infrequently accessed data, but the question doesn ��t mention access patterns. If the data is actively analyzed,  Standard-IA  would  be  inefficient  (higher  RCU/WCU  costs).  -  Reserved  Capacity  (Option  A/D)  is  unnecessary  here. Reserved Capacity applies to pre-paying for  Provisioned  Mode at a discount, but Option B achieves the same cost savings  by simply  setting   RCUs/WCUs  correctly  for  a  steady  workload.   In  short:   For   predictable  workloads,   Provisioned  Mode  with well-defined       RCUs/WCUs        (Option       B)        is       the        most       cost-effective.        Reference       Links       -        [Dynamo db

Pricing](https://aws.amazon.com/dynamo db/pricing/)



-


[Capacity

Modes](https://docs.aws.amazon.com/amazon dynamo db/latest/developer guide/How it works.Readwrite capacity mode.html)

306.A company stores confidential data in an Amazon Aurora Postgresql database in the ap-southeast-3 Region. The database is encrypted with an AWS Key Management Service (AWS KMS) customer managed key. The company was recently acquired and must securely share a backup of the database with the acquiring company ��s AWS account in ap-southeast-3. What should a solutions architect do to meet these requirements?

A ��Create a database snapshot. Copy the snapshot to a new unencrypted snapshot. Share the new snapshot with the acquiring company ��s AWS account.

B ��Create a database snapshot. Add the acquiring company ��s AWS account to the KMS key policy. Share the snapshot with the acquiring company ��s AWS account.

C ��Create a database snapshot that uses a different AWS managed KMS key. Add the acquiring company ��s AWS account to the KMS key alias. Share the snapshot with the acquiring company's AWS account.

D �� Create a  database snapshot.  Download the database snapshot.  Upload the database snapshot to an Amazon S3  bucket. Update the S3 bucket policy to allow access from the acquiring company ��s AWS account.

�𰸣�B

������ Correct Answer B Detailed Explanation To securely share an encrypted Aurora snapshot with another AWS account, both the snapshot and the KMS key used for encryption must be accessible to the target account. Here ��s why option B works: 1. Snapshot Encryption: The original snapshot is encrypted with a customer-managed KMS key. For the acquiring company to use it, they need permission to decrypt this key. 2. KMS Key Policy: Adding the acquiring company ��s AWS account to the KMS key policy grants them permission to use the key for decryption. 3. Snapshot Sharing: Sharing the snapshot directly (via AWS RDS) allows the  acquiring  account  to  restore  the  database  without  exposing  unencrypted  data  or  using  insecure  methods  like  manual downloads. Other options fail because: - A: Removing encryption compromises security. - C: AWS-managed keys can ��t be shared across accounts via aliases. - D: Downloading/uploading encrypted snapshots via S3 is unnecessary and less secure. Reference Links - [Sharing Encrypted Snapshots](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_Share snapshot.html) - [Key Policies for Cross-Account Access](https://docs.aws.amazon.com/kms/latest/developer guide/key-policies.html)

307.A  company  is  moving  its  data  management  application  to  AWS.  The  company  wants  to  transition  to  an  event-driven architecture. The  architecture  needs  to  be  more  distributed  and  to  use  serverless  concepts  while  performing  the  different aspects  of  the   workflow.  The   company   also  wants  to  minimize  operational  overhead.  Which  solution  will  meet  these requirements?

A ��Build out the workflow in AWS Glue. Use AWS Glue to invoke AWS Lambda functions to process the workflow steps.

B ��Build out the workflow in AWS Step Functions. Deploy the application on Amazon EC2 instances. Use Step Functions to invoke the workflow steps on the EC2 instances.

C��Build out the workflow in Amazon Event bridge. Use Event bridge to invoke AWS Lambda functions on a schedule to process the workflow steps.

D ��Build out the workflow in AWS Step Functions. Use Step Functions to create a state machine. Use the state machine to invoke AWS Lambda functions to process the workflow steps.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The best solution is to use AWS Step Functions with Lambda functions. Here's why:  1.  Serverless    ��  Low  Operational  Overhead:  Step  Functions  is  a  fully  managed  serverless  service  that  coordinates workflows using state machines. Lambda is also serverless. Together, they eliminate infrastructure management. 2. Event-Driven Architecture: Step Functions natively handles event-driven workflows. It can trigger Lambda functions when specific events occur, chain multiple steps, handle errors/retries, and work with other AWS services. 3. Distributed Workflow: Step Functions allows you to visually design complex workflows with parallel processing, conditional branching, and service integrations - perfect for distributed systems. Other options are less ideal: - A (Glue): Designed for ETL/data processing, not general workflows. - B (EC2): Requires  server  management,  contradicting  serverless.  -  C  (Event bridge):  Good  for  event  routing  but  lacks  native  workflow orchestration  capabilities.   Reference   Links:  -   [AWS  Step   Functions](https://aws.amazon.com/step-functions/)  -   [Serverless Architectures](https://aws.amazon.com/serverless/)

308.A company is designing the network for an online multi-player game. The game uses the UDP networking protocol and will be deployed  in eight AWS  Regions. The  network architecture  needs to  minimize  latency and  packet  loss to give end  users a high-quality gaming experience. Which solution will meet these requirements?

A ��Setup a transit gateway in each Region. Create inter-Region peering attachments between each transit gateway.

B ��Set up AWS Global Accelerator with UDP listeners and endpoint groups in each Region.

C ��Set up Amazon Cloud front with UDP turned on. Configure an origin in each Region.

D ��Set up a VPC peering mesh between each Region. Turn on UDP for each VPC.

�𰸣�B

�� �� �� Correct Answer:  B  Detailed  Explanation:  AWS  Global  Accelerator  is  designed  to  optimize  traffic  routing  for  better performance  and  availability.  It  uses  the AWS  global  network  to  route  user  traffic  to the  nearest  healthy  endpoint  via  UDP listeners, reducing latency and packet loss. Here ��s why it ��s the best choice: 1. UDP Support: Global Accelerator supports UDP, which is critical for real - time gaming traffic where low latency is prioritized over reliability. 2. Edge Routing: Traffic enters AWS ��s global  network at the  nearest edge  location,  minimizing  hops and reducing  latency. 3.  Endpoint Groups:  Deploying  endpoint groups in all eight regions ensures traffic is routed to the closest healthy region, improving redundancy and performance. Other options fall short: - A (Transit Gateway): Focuses on inter - region VPC connectivity but doesn ��t optimize global user - to - AWS routing. - C (Cloud front): Designed for HTTP(S) content delivery, not real - time UDP traffic. - D (VPC Peering): Creates complex cross      -       region       mesh       connections       without       optimized       routing.      Reference:        [AWS      Global       Accelerator Documentation](https://docs.aws.amazon.com/global-accelerator/)

309.A company hosts a three-tier web application on Amazon EC2 instances in a single Availability Zone. The web application uses a self-managed MySQL database that is hosted on an EC2 instance to store data in an Amazon Elastic Block Store (Amazon EBS) volume. The MySQL database currently uses a 1 TB Provisioned IOPS SSD (io2) EBS volume. The company expects traffic of
1,000 IOPS for both reads and writes at peak traffic. The company wants to minimize any disruptions, stabilize performance, and reduce costs while retaining the capacity for double t he IOPS. The company wants to move the database tier to a fully managed

solution that is highly available and fault tolerant. Which solution will meet these requirements MOST cost-effectively?

A ��Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with an io2 Block Express EBS volume.

B ��Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose SSD (gp2) EBS volume. C ��Use Amazon S3 Intelligent-Tiering access tiers.

D ��Use two large EC2 instances to host the database in active-passive mode.

�𰸣�B

������ Correct Answer B Detailed Explanation The company wants to migrate to a fully managed, highly available solution while minimizing costs.  Here's why Option B is the best choice:  1.  Multi-AZ Amazon  RDS for  MySQL: - A  Multi-AZ  RDS deployment automatically provisions a primary database in one Availability Zone (AZ) and a standby replica in another AZ. This ensures high availability and fault tolerance, as AWS handles failover seamlessly during outages. 2. General Purpose SSD (gp2) Volume: - The current workload requires 2,000 IOPS (1,000 reads + 1,000 writes). A 1 TB gp2 volume provides a baseline of 3,000 IOPS (3 IOPS per GB   �� 1,000 GB). This exceeds the current need and allows room to double t he IOPS (up to 4,000) by increasing the volume size or using burstable credits. - Cost savings: gp2 is cheaper than io2/io2 Block Express, which is overkill for this workload. io2 is designed for extreme performance (up to 256,000 IOPS), which isn ��t needed here. 3. Why Other Options Fail: - A: io2 Block Express is expensive and unnecessary for 2,000 -4,000 IOPS. - C: S3 Intelligent-Tiering is for object storage, not databases. - D: Self-managed  EC2  instances  in  active-passive  mode  add  operational  complexity  and  don ��t  fully  automate  HA/failover.  By choosing  Multi-AZ  RDS with gp2, the company  reduces  costs, ensures  high availability, and  retains scalability for future  IOPS growth.  Reference  Links  -   [Amazon  RDS  Multi-AZ](https://aws.amazon.com/rds/features/multi-az/)  -   [Amazon   EBS  Volume Types](https://aws.amazon.com/ebs/volume-types/)

310.A company hosts a serverless application on AWS. The application uses Amazon API Gateway, AWS Lambda, and an Amazon RDS  for  Postgresql  database.  The  company  notices  an  increase  in  application  errors  that  result  from  database  connection timeouts during times of peak traffic or unpredictable traffic. The company needs a solution that reduces the application failures with the least amount of change to the code. What should a solutions architect do to meet these requirements?

A ��Reduce the Lambda concurrency rate.

B ��Enable RDS Proxy on t he RDS DB instance.

C ��Resize t he RDS DB instance class to accept more connections.

D ��Migrate the database to Amazon Dynamo db with on-demand scaling.

�𰸣�B

������ Correct Answer B �� Enable RDS Proxy on t he RDS DB instance. Detailed Explanation When Lambda functions scale up during peak traffic, each function instance might create a new database connection to Amazon RDS. This can overwhelm the database, leading to connection limits  being reached and causing timeouts.  RDS  Proxy acts as a connection pool manager: it maintains a  reusable  pool of database connections, allowing  multiple  Lambda instances to share connections efficiently. This reduces the risk of overwhelming the database with too many connections. - Why not A? Reducing Lambda concurrency would limit scalability and hurt performance. - Why not C? Resizing t he RDS instance might help temporarily, but it ��s not cost-effective for  unpredictable  traffic  and  doesn �� t  address  connection  pooling.  -  Why  not  D?  Migrating  to  Dynamo db  would  require significant code changes (e.g., switching from SQL to NoSQL), which violates the least code change requirement. Reference Link

[AWS RDS Proxy Documentation](https://docs.aws.amazon.com/Amazon rds/latest/User guide/rds-proxy.html)

311.A company is migrating an old application to AWS. The application runs a batch job every hour and is CPU intensive. The batch job takes 15 minutes on average with an on-premises server. The server has 64 virtual CPU (vCPU) and 512 Gi B of memory. Which solution will run the batch job within 15 minutes with the LEAST operational overhead?

A ��Use AWS Lambda with functional scaling.

B ��Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate.

C ��Use Amazon Light sail with AWS Auto Scaling.

D ��Use AWS Batch on Amazon EC2.

�𰸣�D

������ Correct Answer D �� Use AWS Batch on Amazon EC2. Detailed Explanation The batch job is CPU-intensive, requires 64 vCPUs and 512 Gi B of memory, and must finish within 15 minutes. Here ��s why AWS Batch on EC2 is the best choice: 1. Resource Requirements: - The job  needs a  high-CPU  instance (64 vCPUs) and large  memory  (512 Gi B).  EC2 offers instance types  (e.g., `c5.18xlarge` with 72 vCPUs) that meet this demand. Other options like Lambda or Fargate cannot scale to this level for a single task. 2. Operational Overhead: - AWS Batch automates resource provisioning, job scheduling, and scaling. You define the job and resource requirements, and Batch handles the rest. No need to manually manage servers or clusters. 3. Cost Efficiency: - EC2 instances launch only when the job runs (hourly), then terminate afterward, minimizing costs. AWS  Batch optimizes instance selection and reduces idle time. 4. Runtime Compatibility: - Lambda has a 15-minute execution limit, risking timeouts. Fargate has  lower vCPU/memory  limits  per  task,  and  Light sail  lacks  sufficiently  powerful  instances.  AWS  Batch  on  EC2  avoids  these limitations.          Reference          Links          -          [AWS          Batch](https://aws.amazon.com/batch/)          -           [EC2          Instance Types](https://aws.amazon.com/ec2/instance-types/)

312.A company stores its data objects in Amazon S3 Standard storage. A solutions architect has found that 75% of the data is rarely accessed after 30 days. The company needs all the data to remain immediately accessible with the same high availability and resiliency, but the company wants to minimize storage costs. Which storage solution will meet these requirements?

A ��Move the data objects to S3 Glacier Deep Archive after 30 days.

B ��Move the data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.

C ��Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.

D ��Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately.

�𰸣�B

���� �� Correct Answer:  B Detailed Explanation: The best solution is to move the data to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days. Here ��s why: 1. Immediate Accessibility: S3 Standard-IA provides millisecond access to data, just like S3  Standard.  This  meets  the  requirement  that  data  must  remain  immediately  accessible.  2.  High  Availability/Resiliency:  S3 Standard-IA,  like  S3  Standard,  stores  data  across  multiple  Availability  Zones  (AZs).  This  ensures  the  same  level  of  durability (99.999999999%)  and  availability  (99.9%)  as  the  original  storage  class.  3.  Cost  Savings:  S3  Standard-IA  is  cheaper  than  S3 Standard for storage (about 40 - 50% lower cost) but has a small retrieval fee. Since 75% of data is rarely accessed after 30 days, this  balances  cost  and  accessibility.  Why  other  options  fail:  -  A  (Glacier  Deep  Archive):  Data  is  not  immediately  accessible

(retrieval takes  hours/days). - C/D  (One Zone-IA): Stores data in only one AZ,  reducing  resiliency. This violates the same  high availability             and              resiliency              requirement.              Reference              Link:             [Amazon              S3              Storage Classes](https://aws.amazon.com/s3/storage-classes/)

313.A social media company runs its application on Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB is the origin for an Amazon Cloud front distribution. The application has more than a billion images stored in an Amazon S3 bucket and processes thousands of images each second. The company wants to resize the images dynamically and serve appropriate formats to clients. Which solution will meet these requirements with the LEAST operational overhead?

A ��Install an external image management library on an EC2 instance. Use the image management library to process the images.

B ��Create a Cloud front origin request policy. Use the policy to automatically resize images and to serve the appropriate format based on the User-Agent HTTP header in the request.

C ��Use a Lambda@Edge function with an external image management library. Associate the Lambda@Edge function with the Cloud front behaviors that serve the images.

D �� Create a Cloud front response headers policy. Use the policy to automatically resize images and to serve the appropriate format based on the User-Agent HTTP header in the request.

�𰸣�C

�� �� ��  Correct  Answer:   C.   Use  a  Lambda@Edge  function  with   an   external  image  management  library.  Associate  the Lambda@Edge  function  with  the  Cloud front  behaviors  that  serve  the  images.  Detailed  Explanation:  The  best  solution  is  C because Lambda@Edge allows you to run code at Cloud front edge locations (closer to users) without managing servers. Here ��s why: 1. Dynamic Image Resizing: Lambda@Edge can process images on-the-fly using an external image library (like Sharp). When a  user  requests  an  image,  the  Lambda@Edge function  resizes  it  based  on  parameters  (e.g.,  device  type  in  the  `User-Agent` header) and returns the optimized version. This avoids storing multiple image sizes in S3, saving storage costs. 2. Low Operational Overhead: Lambda@Edge scales automatically with traffic (supports thousands of requests/sec), so you don ��t need to manage EC2 instances or scaling policies. This  reduces operational work compared to options like A (EC2-based) or manual setups. 3. Cloud front Integration: By attaching the Lambda@Edge function to Cloud front behaviors, you ensure it triggers only for image requests,  keeping   processing  efficient.  Cloud front  caches  resized  images  at  edge  locations,   reducing  latency  for   repeated requests. Why other options fail: - A: Running libraries on EC2 adds operational work (scaling, patching servers) and may not handle sudden traffic spikes efficiently. - B/D: Cloud front policies (origin request/response headers) only modify headers, not process      images.      They       lack      the      ability      to       resize      images      or       run      custom      code.       Reference:      [AWS Lambda@Edge](https://aws.amazon.com/lambda/edge/)                                [Resize                                 Images                                with
Lambda@Edge](https://aws.amazon.com/blogs/networking-and-content-delivery/resize-images-on-the-fly-with-amazon-cloudfr ont-aws-lambda edge/)

314.A hospital needs to store patient records in an Amazon S3 bucket. The hospital ��s compliance team must ensure that all protected health information (PHI) is encrypted in transit and at rest. The compliance team must administer the encryption key for data at rest. Which solution will meet these requirements?

A ��Create a public SSL/TLS certificate in AWS Certificate Manager (ACM). Associate the certificate with Amazon S3. Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.

B��Use the aws:Securetransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default  encryption  for  each  S3  bucket  to  use  server-side  encryption  with  S3  managed  encryption  keys  (SSE-S3).  Assign  the

compliance team to manage t he SSE-S3 keys.

C��Use the aws:Securetransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.

D ��Use the aws:Securetransport condition on S3 bucket policies to allow only encrypted connections over  HTTPS (TLS).  Use Amazon Macie to protect the sensitive data that is stored in Amazon S3. Assign the compliance team to manage Macie.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The correct answer is C because it meets both requirements: encrypting data in transit and at rest while allowing the compliance team to manage the encryption keys. Here ��s why: 1. Encryption in Transit: The `aws:Securetransport` condition in the S3 bucket policy ensures that data is only transferred over HTTPS (TLS), which enforces encryption  during  transmission.  This  satisfies  the  encrypted  in  transit  requirement.  2.   Encryption  at   Rest:  Using  SSE-KMS (Server-Side Encryption with AWS KMS keys) allows the compliance team to manage the encryption keys directly in AWS Key Management Service (KMS). Unlike SSE-S3 (where AWS fully manages the keys), KMS gives the team control over key policies, rotations, and access audits, meeting the administer the encryption key requirement. 3. Why Other Options Fail: - A: Using ACM certificates for S3 encryption in transit is unnecessary because S3 natively supports HTTPS. Also, ACM certificates are typically used for custom domain  names,  not general TLS enforcement. -  B: SSE-S3  uses AWS-managed  keys, so the compliance team cannot administer the keys, violating the requirement. - D: Amazon Macie helps discover sensitive data but doesn ��t enforce encryption           or            key            management.            Reference           Links:            -            [AWS            S3           Encryption            in Transit](https://docs.aws.amazon.com/AmazonS3/latest/user guide/using-https.html)             -              [AWS              KMS              Key Management](https://docs.aws.amazon.com/kms/latest/developer guide/overview.html)

315.A company uses Amazon API Gateway to run a private gateway with two REST APIs in the same VPC. The Buy stock RESTful web service calls the Check funds RESTful web service to ensure that enough funds are available before a stock can be purchased. The company has noticed in the VPC flow logs that the Buy stock RESTful web service calls the Check funds RESTful web service over the internet instead of through the VPC. A solutions architect must implement a solution so that the APIs communicate through the VPC. Which solution will meet these requirements with the FEWEST changes to the code?

A��Add an X-API-Key header in the HTTP header for authorization.

B ��Use an interface endpoint.

C ��Use a gateway endpoint.

D ��Add an Amazon Simple Queue Service (Amazon SQS) queue between the two REST APIs.

�𰸣�B

������ Correct Answer: B. Use an interface endpoint. Detailed Explanation: When two APIs in the same VPC communicate via public  internet  (as  seen  in  VPC flow  logs),  it  means  they're  using  API  Gateway's  public  endpoint  URL.  To  force VPC-internal communication, you should  use  an  interface VPC  endpoint  (AWS  Private link).  Here's  why  this  works:  1.  Interface  Endpoints create   a    private    connection    to   API    Gateway    within   your    VPC    2.   They    provide    a    private    DNS    hostname    (e.g., vpce-xxx.execute-api.us-east-1.vpce.amazon aws.com) 3. You only need to modify the API invocation URL in your code to use this private endpoint 4.  No internet gateway/NAT required, keeping traffic entirely within AWS network 5. Requires  minimal code changes - just update the API endpoint URL Other options: - C. Gateway endpoints only work for S3/Dynamo db - D. SQS would require    significant    architectural    changes    -     A.    API    keys    handle     authentication,    not     networking    Reference    Link:

https://docs.aws.amazon.com/api gateway/latest/developer guide/api gateway-private-apis.html

316.A  company   hosts  a   multiplayer  gaming   application  on  AWS.  The   company  wants  the   application  to   read  data  with sub-millisecond latency and run one-time queries on historical data. Which solution will meet these requirements with the LEAST operational overhead?

A��Use Amazon RDS for data that is frequently accessed. Run a periodic custom script to export the data to an Amazon S3 bucket.

B ��Store the data directly in an Amazon S3 bucket. Implement an S3  Lifecycle  policy to move older data to S3 Glacier  Deep Archive for long-term storage. Run one-time queries on the data in Amazon S3 by using Amazon Athena.

C��Use Amazon Dynamo db with Dynamo db Accelerator (DAX) for data that is frequently accessed. Export the data to an Amazon S3 bucket by using Dynamo db table export. Run one-time queries on the data in Amazon S3 by using Amazon Athena.

D ��Use Amazon Dynamo db for data that is frequently accessed. Turn on streaming to Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to read the data from Kinesis Data Streams. Store the records in an Amazon S3 bucket.

�𰸣�C

�� �� �� Correct Answer:  C  Detailed  Explanation: The  question  asks for  a solution that  provides  sub-millisecond  latency for frequently accessed data, supports one-time queries on historical data, and minimizes operational overhead. Here's why Option
C is the best choice: 1. Sub-millisecond latency: - Amazon Dynamo db (a NoSQL database) offers single-digit millisecond latency    for   reads/writes.   -   Dynamo db  Accelerator   (DAX)   adds   an   in-memory   caching   layer,   reducing   latency   to   microseconds    (sub-millisecond), which is critical for real-time gaming applications. 2. One-time  historical queries: - Dynamo db Table Export   automatically exports data to Amazon S3 with no manual scripting (low operational effort). - Amazon Athena allows serverless   SQL queries directly on S3 data, ideal for analyzing historical datasets. 3. Operational simplicity: - Dynamo db and DAX are fully  managed services (no server maintenance). - Built-in Dynamo db export to S3 eliminates custom scripts (unlike Option A/D). -  Athena requires  no infrastructure setup (unlike Options B/D, which might need extra steps for data organization). Why other   options fail: - Option A: RDS lacks sub-millisecond latency and requires manual scripting. - Option B: Storing all data directly in S3    results in high latency for real-time access. - Option D: Kinesis pipelines add complexity, and DAX isn ��t used here, so latency isn �� t optimized. Reference Links: - [Dynamo db Accelerator (DAX)](https://aws.amazon.com/dynamo db/dax/) - [Dynamo db Export to   S3](https://docs.aws.amazon.com/amazon dynamo db/latest/developer guide/Data export.html)                      -                       [Amazon   Athena](https://aws.amazon.com/athena/)

317.A  company  is  building  a  game  system  that  needs  to  send  unique  events  to  separate  leaderboard,  matchmaking,  and authentication services concurrently. The company needs an AWS event-driven system that guarantees the order of the events. Which solution will meet these requirements?

A ��Amazon Event bridge event bus

B ��Amazon Simple Notification Service (Amazon SNS) FI FO topics

C ��Amazon Simple Notification Service (Amazon SNS) standard topics D ��Amazon Simple Queue Service (Amazon SQS) FI FO queues

�𰸣�B

���� �� Correct Answer: B. Amazon Simple Notification Service (Amazon SNS) FI FO topics Detailed Explanation: The scenario

requires an event-driven system that guarantees both event ordering and concurrent delivery to multiple services. Here's why SNS FI FO topics are the best choice: 1. Ordering: SNS FI FO topics ensure messages are delivered in the exact order they were published, critical for systems  like leaderboards or matchmaking where sequence matters (e.g., scoring events). 2. Fan-out to Multiple Services: SNS  FI FO topics can send the same event to multiple destinations (e.g., SQS  F IFO queues for leaderboard, authentication,  and  matchmaking  services)  simultaneously,  avoiding  redundant  event  publishing.  3.  De duplication:  SNS  FI FO topics  prevent duplicate events  using  de duplication  IDs,  ensuring  each  unique  event  is  processed  only  once. Why  not  other options?  -  A  (Event bridge):  While   Event bridge  can  route  events,  it  doesn  �� t  natively  guarantee  strict  ordering   like  FI FO queues/topics. - C (SNS Standard Topics): Standard topics don ��t guarantee ordering or de duplication. - D (SQS F IFO Queues): SQS FI FO queues ensure ordering  but  lack  native fan-out capability. You ��d  need  to  publish the same  event to  multiple queues manually,               increasing               complexity.               Reference               Links:               -                [Amazon               SNS               F IFO Topics](https://docs.aws.amazon.com/sns/latest/dg/fifo-message-ordering.html)   -   [Fan-out   to   SQS   FIFO   Queues   with   SNS FIFO](https://aws.amazon.com/blogs/aws/introducing-amazon-sns-fifo-topics-with-strict-ordering-and-de duplication/)

318.A  company  runs  a  web  application  that  is  backed  by  Amazon  RDS.  A  new  database  administrator  caused  data  loss  by accidentally editing information in a database table. To help recover from this type of incident, the company wants the ability to restore the database to its state from 5 minutes before any change within the last 30 days. Which feature should the solutions architect include in the design to meet this requirement?

A ��Read replicas

B ��Manual snapshots

C ��Automated backups  D ��Multi-AZ deployments

�𰸣�C

������ Correct Answer: C. Automated backups Detailed Explanation: Automated backups in Amazon RDS automatically perform daily full backups and capture transaction logs. This allows Point-in-Time Recovery (PITR), which lets you restore the database to any second within the retention period (up to 35 days). This matches the requirement to recover to a state 5 minutes before any change within the last 30 days. - Option A (Read replicas): These are used for scaling read operations, not for restoring accidental data  changes.  -  Option   B  (Manual  snapshots):  These   require  manual  creation  and  won't  automatically   provide  5-minute granularity. - Option D (Multi-AZ): This provides high availability during outages but doesn't protect against accidental data edits. Reference: [AWS RDS Backup Documentation](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_PIT.html)

319.A company ��s web application consists of an Amazon API Gateway API in front of an AWS Lambda function and an Amazon Dynamo db database. The Lambda function handles the business logic, and the Dynamo db table hosts the data. The application uses Amazon Cognito user  pools to identify the individual users of the application. A solutions architect needs to update the application so that only users who have a subscription can access premium content. Which solution will meet this requirement with the LEAST operational overhead?

A ��Enable API caching and throttling on the API Gateway API.

B ��Set up AWS WAF on the API Gateway API. Create a rule to filter users who have a subscription.

C ��Apply fine-grained IAM permissions to the premium content in the Dynamo db table.

D ��Implement API usage plans and API keys to limit the access of users who do not have a subscription.

�𰸣�D

������ Correct Answer: D. Implement API usage plans and API keys to limit the access of users who do not have a subscription. Detailed Explanation: The goal is to restrict premium content access to subscribed users with minimal operational effort. Here ��s
why Option D works best: 1. API Gateway Usage Plans   �� API Keys: - API Gateway natively supports usage plans and API keys to manage access tiers (e.g., free vs.  premium users). - Subscribed users can be assigned an API key linked to a usage plan that grants access to premium endpoints. - Unsubscribed users (without a valid API key) are automatically blocked by API Gateway before  reaching the  Lambda  function,  reducing  unnecessary  Lambda  invocations  and  code  changes.  2.  Minimal  Operational Overhead: - No code changes to the Lambda function or Dynamo db are required. - Configuration is done directly in API Gateway,  leveraging built - in features instead of custom logic. 3. Why Other Options Are Less Efficient: - A (Caching/Throttling): Optimizes performance but doesn ��t enforce subscription - based access. - B (AWS WAF): WAF blocks threats (e.g., SQL injection) but can �� t validate user subscriptions stored in Cognito. - C (IAM Permissions): Dynamo db IAM policies control data access but can ��t dynamically  check  user  subscriptions  per  request  without  complex  Lambda  logic.  Reference  Link:  [AWS  API  Gateway  Usage Plans](https://docs.aws.amazon.com/api gateway/latest/developer guide/api - gateway - api - usage - plans.html)

320.A company is using Amazon Route 53 latency-based routing to route requests to its UDP-based application for users around the world. The application is hosted on redundant servers in the company's on-premises data centers in the United States, Asia, and Europe. The company ��s compliance requirements state that the application must be hosted on premises. The company wants to  improve  the  performance  and  availability  of  the  application.  What  should  a  solutions  architect  do  to  meet  these requirements?

A ��Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-premises endpoints. Create an accelerator by using AWS Global Accelerator, and register the NLBs as its endpoints. Provide access to the application by using a CNAME that points to the accelerator DNS.

B ��Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-premises endpoints. Create an accelerator by using AWS Global Accelerator, and register the ALBs as its endpoints. Provide access to the application by using a CNAME that points to the accelerator DNS.

C ��Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-premises endpoints. In Route 53, create a  latency-based  record that  points  to the three  NLBs,  and  use  it  as  an origin for an Amazon Cloud front distribution. Provide access to the application by using a CNAME that points to the Cloud front DNS.

D ��Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-premises endpoints. In Route 53, create a latency-based record that points to the three ALBs, and use it as an origin for an Amazon Cloud front distribution. Provide access to the application by using a CNAME that points to the Cloud front DNS.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The company needs to improve performance and availability of their UDP-based application hosted on-premises in three regions. Here's why Option A is correct: 1. UDP Support: - Network Load Balancers (NLBs) operate at Layer 4 (TCP/UDP), making them suitable for UDP traffic. Application Load Balancers (ALBs) only support HTTP/HTTPS (Layer 7), so Options B/D are invalid. 2. AWS Global Accelerator: - Global Accelerator uses AWS ��s global network to route traffic to the nearest AWS endpoint (NLB in this case), reducing  latency vs. standard internet routing.  It also  provides static  IPs and automatic  failover,  improving  availability.  -  Cloud front  (Options  C/D)  is  a  CDN  for  HTTP/HTTPS  and  caching,  not  UDP.  This eliminates C/D.  3.  On-Premises  Compliance: -  NLBs  can forward traffic to on-premises  servers via AWS VPN/Direct  Connect. Global Accelerator then directs users to the closest NLB, which routes to the nearest on-premises data center. Why not other options: - B/D: ALBs don��t support UDP. - C/D: Cloud front doesn��t support UDP; it��s designed for HTTP/HTTPS. Reference Links:

-             [AWS             Global             Accelerator](https://aws.amazon.com/global-accelerator/)              -             [Network              Load Balancer](https://aws.amazon.com/elastic load balancing/network-load-balancer/)

321.A solutions architect wants all new users to have specific complexity requirements and mandatory rotation periods for IAM user passwords. What should the solutions architect do to accomplish this?

A ��Set an overall password policy for the entire AWS account.

B ��Set a password policy for each IAM user in the AWS account.

C ��Use third-party vendor software to set password requirements.

D ��Attach an Amazon Cloud watch rule to the Create_newuser event to set the password with the appropriate requirements.

�𰸣�A

���� �� Correct Answer: A Detailed Explanation: The correct approach is to set an overall password policy for the entire AWS account.  AWS   allows  you  to   define   a  centralized  password  policy  at  the   account  level  using  IAM  (Identity  and  Access Management). This  policy applies to all  IAM users in the account, including new users created afterward.  By configuring this policy, you can enforce requirements such as: - Minimum password length. - Use of uppercase letters, lowercase letters, numbers, and symbols. - Mandatory password rotation period (e.g., passwords expire every 90 days). - Prevention of password reuse. Why other options are incorrect: - B: IAM does not allow setting individual password policies per user. Policies are applied account - wide.  -  C:  AWS   natively  supports   password   policies,  so  third  -   party  tools  are   unnecessary  for   basic  requirements.  -   D: Cloud watch rules cannot directly enforce password policies during user creation. This would be an overly complex and non - standard solution.  Beginner -  Friendly  Explanation: Think of the AWS account password policy  like a rulebook for passwords. When you set it up once for your entire AWS account, every new user automatically follows these  rules.  For example, if the rulebook says, Passwords must be at least 12 characters and change every 3 months, all new users will have to comply. This is simpler and more efficient than manually configuring rules for each user (Option B) or using external tools (Option C). Reference Link:  [AWS  IAM  Password  Policies](https://docs.aws.amazon.com/IAM/latest/User guide/id_credentials_passwords_account  - policy.html)

322.A company has migrated an application to Amazon EC2 Linux instances. One of these EC2 instances runs several 1-hour tasks on  a  schedule. These tasks were written  by  different  teams  and  have  no  common  programming  language.  The  company  is concerned  about  performance  and  scalability  while  these  tasks   run  on  a  single   instance.  A  solutions  architect   needs  to implement  a  solution  to  resolve  these  concerns.  Which  solution  will  meet  these  requirements  with  the  LEAST  operational overhead?

A ��Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon Event bridge (Amazon Cloud watch Events).

B ��Convert the EC2 instance to a container. Use AWS App Runner to create the container on demand to run the tasks as jobs.

C �� Copy  the  tasks  into  AWS   Lambda  functions.  Schedule  the   Lambda  functions   by  using  Amazon   Event bridge  (Amazon Cloud watch Events).

D��Create an Amazon Machine Image (AMI) of the EC2 instance that runs the tasks. Create an Auto Scaling group with the AMI to run multiple copies of the instance.

�𰸣�A

���� �� Correct Answer: A Detailed Explanation: The best solution here is AWS Batch with Event bridge scheduling because it handles both scalability and operational simplicity. Here's why: - AWS Batch automatically provisions the right amount of EC2 instances  (or  uses  Fargate  serverless  compute)  to  run  your  jobs.  Since  the  tasks  are  1-hour  long  and  written  in  different languages,  Batch can run them as isolated jobs without needing to rewrite the code or manage infrastructure. -  Event bridge Scheduler  can trigger these jobs at specific times  (like cron jobs),  replacing the  need for  manual scheduling on a single  EC2 instance. - No code changes: AWS Batch runs existing tasks as-is (no need to container ize them or rewrite as Lambda functions). - Auto-scaling: Batch scales up/down resources automatically, so you don ��t have to worry about overloading a single instance. Why other options are worse: - C (Lambda) fails because Lambda functions have a 15-minute runtime limit (these tasks run for 1 hour). -  D (Auto Scaling) forces you to manage scaling  logic and task distribution manually (high operational effort). - B (App Runner)   is    designed   for    web   apps,    not    batch   jobs,    and    requires   containerization    (extra   work).    Reference:    [AWS Batch](https://aws.amazon.com/batch/) [Event bridge Scheduler](https://aws.amazon.com/event bridge/)

323.A company runs a public three-tier web application in a VPC. The application runs on Amazon EC2 instances across multiple Availability Zones. The EC2 instances that run in private subnets need to communicate with a license server over the internet. The company needs a managed solution that minimizes operational maintenance. Which solution meets these requirements?

A ��Provision a NAT instance in a public subnet. Modify each private subnet's route table with a default route that points to the NAT instance.

B ��Provision a NAT instance in a private subnet. Modify each private subnet's route table with a default route that points to the NAT instance.

C ��Provision a NAT gateway in a public subnet. Modify each private subnet's route table with a default route that points to the NAT gateway.

D ��Provision a NAT gateway in a private subnet. Modify each private subnet's route table with a default route that points to the NAT gateway.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The correct solution is to use a NAT Gateway in a public subnet and update the private  subnets'  route  tables  to  direct  internet-bound  traffic  to  it.  Here's  why:  1.  Managed  Service:  NAT  Gateway  is  a  fully managed  AWS   service,  unlike  NAT  instances  (which  are  manually  configured  EC2  instances).  This   minimizes  operational maintenance, as AWS handles updates, scaling, and availability. 2. Placement in Public Subnet: A NAT Gateway must be placed in a  public  subnet  because  it  needs  a  public  IP  address  to  communicate  with  the  internet.  Private  subnets  cannot  host  NAT Gateways. 3. Route Table Configuration: Private subnets need a route table entry (a default route for `0.0.0.0/0`) pointing to the NAT Gateway. This allows EC2 instances in private subnets to send outbound traffic to the internet (e.g., to connect to the license server) while blocking unsolicited inbound traffic for security. Why Other Options Are Wrong: - A/B: NAT instances are not fully managed, requiring manual maintenance. - D: NAT Gateways cannot be placed in private subnets (they require public IPs and
public                subnet                 resources).                 Reference                Links:                 -                 [AWS                 NAT                Gateway Documentation](https://docs.aws.amazon.com/vpc/latest/user guide/vpc-nat-gateway.html)     -       [NAT      Gateway     vs.       NAT Instance](https://aws.amazon.com/compare/the-difference-between-nat-gateway-and-nat-instance/)

324.A company wants to migrate an Oracle database to AWS. The database consists of a single table that contains millions of geographic information systems (GIS) images that are high resolution and are identified by a geographic code. When a natural disaster occurs, tens of thousands of images get updated every few minutes. Each geographic code has a single image or row that is associated with it. The company wants a solution that is highly available and scalable during such events. Which solution meets these requirements MOST cost-effectively?

A ��Store the images and geographic codes in a database table. Use Oracle running on an Amazon RDS Multi-AZ DB instance.

B��Store the images in Amazon S3 buckets. Use Amazon Dynamo db with the geographic code as the key and the image S3 URL as the value.

C ��Store the images and geographic codes in an Amazon Dynamo db table. Configure Dynamo db Accelerator (DAX) during times of high load.

D��Store the images in Amazon S3 buckets. Store geographic codes and image S3 URLs in a database table. Use Oracle running on an Amazon RDS Multi-AZ DB instance.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution is to store the images in Amazon S3 and use Dynamo db for metadata (geographic codes and S3 URLs). Here's why: 1. Amazon S3 is ideal for storing large, high-resolution GIS images due to its unlimited scalability, durability, and cost-effectiveness compared to database storage. 2. Amazon Dynamo db (NoSQL database) handles high-velocity updates (tens of thousands per minute) with automatic scaling. Using the geographic code as the primary key enables fast lookups, and storing S3 URLs instead of the actual images in Dynamo db keeps the database lightweight. 3. Cost Optimization:  Storing  images  in  S3  is  cheaper  than  storing  them  in  a  database.  Dynamo db's  pay-per-request  pricing  scales automatically  during  disasters  without   over-provisioning.   Other   options  fail   because:  -  A/C  store   images   in  databases (RDS/Dynamo db), which is expensive and inefficient for large files. - D uses RDS (relational database), which struggles with high write scalability  compared to  Dynamo db. - C  uses  Dynamo db  for  images,  but  Dynamo db  has  a  400KB  item  limit,  making  it unsuitable     for     large      GIS     images.      Reference      Links:     -      [Amazon     S3](https://aws.amazon.com/s3/)      -     [Amazon Dynamo db](https://aws.amazon.com/dynamo db/)

325.A  company  has  an  application  that  collects  data from  IoT  sensors  on  automobiles.  The  data  is  streamed  and  stored  in Amazon  S3  through  Amazon  Kinesis  Data  Firehose.  The  data  produces  trillions  of  S3  objects  each  year.  Each  morning,  the company uses the data from the previous 30 days to retrain a suite of machine learning (ML) models. Four times each year, the company uses the data from the previous 12 months to perform analysis and train other ML models. The data must be available with minimal delay for up to 1 year. After 1 year, the data must be retained for archival purposes. Which storage solution meets these requirements MOST cost-effectively?

A��Use the S3 Intelligent-Tiering storage class. Create an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 1 year.

B ��Use the S3 Intelligent-Tiering storage class. Configure S3 Intelligent-Tiering to automatically move objects to S3 Glacier Deep Archive after 1 year.

C ��Use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Create an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 1 year.

D �� Use the S3 Standard storage class. Create an S3 Lifecycle policy to transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days, and then to S3 Glacier Deep Archive after 1 year.

�𰸣�D

������ Correct Answer: D Detailed Explanation The company's data has a clear access pattern: - First 30 days: Data is accessed daily for ML retraining (frequent access). - 30 days to 1 year: Data is accessed quarterly (infrequent access). - After 1 year: Data is archived (rarely accessed). Option D (Correct): 1. S3 Standard is ideal for the first 30 days because it ��s designed for frequent

access (low storage cost + no retrieval fees). 2. After 30 days, transitioning to S3 Standard-IA saves costs for infrequent quarterly access (lower storage cost than Standard, but retrieval fees apply). 3. After 1 year, moving to S3 Glacier Deep Archive minimizes costs for archival (cheapest storage, but retrieval takes hours). Why not other options: - A/B (Intelligent-Tiering): Adds extra costs for monitoring access  patterns (unnecessary here, since access  patterns are  predictable). - C (Standard-IA from day 1):  Using Standard-IA upfront would incur higher retrieval fees for daily access in the first 30 days. Key Take away: Use lifecycle policies to match storage classes to predictable access patterns. Avoid paying for automated tiering (Intelligent-Tiering) when patterns are fixed.    Reference    Links:    -     [Amazon    S3    Storage    Classes](https://aws.amazon.com/s3/storage-classes/)    -     [S3    Lifecycle Policies](https://docs.aws.amazon.com/AmazonS3/latest/user guide/object-lifecycle-mgmt.html)

326.A company is  running several  business applications  in three separate VPCs within the us-east-1  Region. The applications must be able to communicate between VPCs. The applications also must be able to consistently send hundreds of gigabytes of data each day to a  latency-sensitive application that  runs in a single on-premises data center. A solutions architect  needs to design a network connectivity solution that maximizes cost-effectiveness. Which solution meets these requirements?

A��Configure three AWS Site-to-Site VPN connections from the data center to AWS. Establish connectivity by configuring one VPN connection for each VPC.

B ��Launch a third-party virtual network appliance in each VPC. Establish an IPsec VPN tunnel between the data center and each virtual appliance.

C �� Set  up  three AWS  Direct  Connect  connections  from the data  center to a  Direct  Connect  gateway  in  us-east-1 .  Establish connectivity by configuring each VPC to use one of the Direct Connect connections.

D��Set up one AWS Direct Connect connection from the data center to AWS. Create a transit gateway, and attach each VPC to the transit gateway. Establish connectivity between the Direct Connect connection and the transit gateway.

�𰸣�D

�� �� ��  Correct Answer  D  Detailed  Explanation  The  best  solution  here  is  D  because  it  uses  a  single  AWS  Direct  Connect connection  combined  with  a  Transit  Gateway.  Here's  why  this  works  best:  1.  Direct  Connect  provides  a  dedicated,  high  - bandwidth  connection  between  the  on  -  premises  data  center  and  AWS.  This  minimizes  latency  and  supports  transferring hundreds  of  gigabytes  daily,  which  is  critical  for  the  latency  -  sensitive  application.  2.  Transit  Gateway  simplifies  network architecture by connecting all three VPCs. Instead of managing separate connections for each VPC (as in options A, B, or C), the Transit Gateway acts as a central hub, allowing all VPCs to communicate with each other and share the single Direct Connect connection.  This   reduces  costs  compared  to   setting  up   multiple  Direct  Connect  connections  or  VPN   tunnels.  3.  Cost  - effectiveness: Using one Direct Connect connection (instead of three) and a Transit Gateway (paying only for data transfer and hourly fees) is far cheaper than maintaining multiple redundant connections. Other options are less optimal: - A (Site - to - Site VPN) relies on public internet, which introduces latency and isn ��t ideal for large data transfers. - B (third - party VPN appliances) adds  complexity  and  still   uses  internet  -  based  VPNs.  -  C  (three   Direct  Connect  connections)  is   unnecessarily  expensive. Reference         Links        -         [AWS          Direct        Connect](https://aws.amazon.com/directconnect/)         -         [AWS         Transit Gateway](https://aws.amazon.com/transit - gateway/)

327.An ecommerce company is building a distributed application that involves several serverless functions and AWS services to complete order-processing tasks. These tasks require manual approvals as part of the workflow. A solutions architect needs to design  an  architecture  for  the  order-processing  application.  The  solution  must  be  able  to  combine  multiple  AWS  Lambda functions into responsive serverless applications. The solution also must orchestrate data and services that run on Amazon EC2 instances,  containers,  or  on-premises  servers.  Which  solution  will  meet  these   requirements  with  the   LEAST   operational overhead?

A ��Use AWS Step Functions to build the application.

B ��Integrate all the application components in an AWS Glue job.

C ��Use Amazon Simple Queue Service (Amazon SQS) to build the application.

D ��Use AWS Lambda functions and Amazon Event bridge events to build the application.

�𰸣�A

���� �� Correct Answer A �� Use AWS Step  Functions to  build the application.  Detailed  Explanation AWS Step  Functions is designed to orchestrate multi-step workflows involving AWS services (like Lambda), on-premises systems, and human approvals. Here ��s why it fits best: 1. Serverless Coordination: Step Functions natively integrates with Lambda, EC2, containers, and external APIs. It manages task sequencing, retries, error handling, and state tracking automatically, reducing code complexity. 2. Human Approvals:  It  supports  manual  approval  steps  (e.g.,   using  Amazon  SNS  or  API  Gateway)  without  custom  code.  3.   Hybrid
Workflows:   Step   Functions   can  orchestrate  tasks   across   serverless  functions   (Lambda),   EC2   instances,   containers,   and on-premises systems using AWS SDK integrations or HTTPS endpoints. 4. Low Operational Overhead: As a fully managed service, Step  Functions  handles scaling, availability, and  logging, eliminating  infrastructure  management. Other options fall short: -  B (AWS Glue):  Built for ETL/data processing, not workflow orchestration. - C (SQS):  Requires  manual coding for workflow  logic, retries, and approvals (high overhead). - D (Lambda + Event bridge): Event bridge triggers events but doesn ��t manage state or complex workflows natively. Reference Link [AWS Step Functions](https://aws.amazon.com/step-functions/)

328.A company  has  launched an Amazon  RDS for  MySQL  DB  instance.  Most  of  the connections to the database  come from serverless applications. Application traffic to the database changes significantly at random intervals. At times of high demand, users report that their applications experience database connection rejection errors. Which solution will resolve this issue with the LEAST operational overhead?

A ��Create a proxy in RDS Proxy. Configure the users �� applications to use the DB instance through RDS Proxy.

B ��Deploy Amazon Elastic ache for Memcached between the users �� applications and the DB instance.

C��Migrate the DB instance to a different instance class that has higher I/O capacity. Configure the users �� applications to use the new DB instance.

D ��Configure Multi-AZ for the DB instance. Configure the users �� applications to switch between the DB instances.

�𰸣�A

���� �� Correct Answer A Detailed Explanation The issue here is database connection rejection during traffic spikes, common with serverless apps that scale rapidly. RDS  Proxy (Option A) directly addresses this by managing a connection pool, allowing apps  to  reuse  existing  connections  instead  of  creating  new  ones  each  time.  This  prevents  exceeding  the  database's  max connection limit. Why not B? Elastic ache is for caching, not connection management. It doesn't solve connection limits. Why not C? Upgrading the instance might help temporarily, but it doesn't dynamically manage connections and requires manual scaling. Why not D? Multi-AZ improves availability but doesn't handle connection limits. RDS Proxy requires minimal setup (just a proxy configuration)  and  automatically  handles  connection  pooling,  making  it  the  most  efficient  solution.  Reference  Links  -  [RDS Proxy](https://aws.amazon.com/rds/proxy/)                                      -                                        [Serverless                                       Database
Challenges](https://aws.amazon.com/blogs/database/managing-database-connections-for-serverless-apps/)

329.A company recently deployed a new auditing system to centralize information about operating system versions, patching,

and installed software for Amazon EC2 instances. A solutions architect must ensure all instances provisioned through EC2 Auto Scaling groups successfully send reports to the auditing system as soon as they are launched and terminated. Which solution achieves these goals MOST efficiently?

A ��Use a scheduled AWS Lambda function and run a script remotely on all EC2 instances to send data to the audit system.

B ��Use EC2 Auto Scaling lifecycle hooks to run a custom script to send data to the audit system when instances are launched and terminated.

C ��Use an EC2 Auto Scaling launch configuration to run a custom script through user data to send data to the audit system when instances are launched and terminated.

D ��Run a custom script on the instance operating system to send data to the audit system. Configure the script to be invoked by the EC2 Auto Scaling group when the instance starts and is terminated.

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: The best solution is B because EC2 Auto Scaling Lifecycle Hooks allow you to trigger custom actions (like running a script) during instance launch or termination. When an instance enters a lifecycle state (e.g., launching or terminating), the hook pauses the process, letting you run scripts to report data to the auditing system before the instance becomes fully active or is deleted. This ensures immediate reporting without relying on schedules (A) or OS-level scripts that may fail if an instance terminates abruptly (D). Option C only handles launch events (via user data), not termination. Option
D is unreliable because Auto Scaling can ��t directly invoke OS scripts during termination. Lifecycle Hooks (B) are purpose-built for this                     scenario.                     Reference                      Links:                     -                      [AWS                      Lifecycle                     Hooks Documentation](https://docs.aws.amazon.com/auto scaling/ec2/user guide/lifecycle-hooks.html)   -   [Using   Lifecycle   Hooks   for Reporting](https://aws.amazon.com/blogs/compute/tracking-instance-launch-and-termination-using-amazon-ec2-auto-scaling-li fecycle-hooks/)

330.A company is developing a real-time multiplayer game that uses UDP for communications between the client and servers in an Auto Scaling group. Spikes in demand are anticipated during the day, so the game server platform must adapt accordingly. Developers want to store gamer scores and other non-relational data in a database solution that will scale without intervention. Which solution should a solutions architect recommend?

A ��Use Amazon Route 53 for traffic distribution and Amazon Aurora Serverless for data storage.

B ��Use a Network Load Balancer for traffic distribution and Amazon Dynamo db on-demand for data storage.

C ��Use a Network Load Balancer for traffic distribution and Amazon Aurora Global Database for data storage.

D ��Use an Application Load Balancer for traffic distribution and Amazon Dynamo db global tables for data storage.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The question involves two main requirements: handling UDP traffic for a real - time  game  and  choosing  a  non  -  relational  database  that  scales  automatically.  Here's  why  option  B  is  correct:  1.  **Traffic Distribution (Network Load Balancer - NLB):** - The game uses UDP, a connectionless protocol ideal for real - time applications. AWS ��s  Network  Load  Balancer  (NLB)  operates at Layer 4 (TCP/UDP) and is optimized for  high -  performance,  low -  latency scenarios like gaming. ALB (Application Load  Balancer) works at Layer 7 (HTTP/HTTPS) and doesn ��t support  UDP, ruling out options A and D. 2. **Database (Dynamo db On - Demand):** - The requirement is for a non - relational database that scales

without manual intervention.  Dynamo db On -  Demand automatically adjusts capacity to  handle spikes in demand,  making it perfect for unpredictable workloads like gaming traffic. Aurora (options A/C) is relational and requires more management, while Dynamo db Global Tables (option D) focus on multi - region replication, which isn ��t explicitly needed here. **Why not other options?** - **A/C: ** Aurora is relational and doesn ��t fit the non - relational requirement. - **D:** ALB doesn ��t support UDP, and           Global            Tables           add            unnecessary            complexity.           Reference            Links:            -           [NLB            vs ALB](https://aws.amazon.com/elastic load balancing/features/)                      -                        [Dynamo db                       On                       -
Demand](https://aws.amazon.com/dynamo db/pricing/on - demand/)

331.A company hosts a front end application that uses an Amazon API Gateway API backend that is integrated with AWS Lambda. When the API receives requests, the Lambda function loads many libraries. Then the Lambda function connects to an Amazon RDS database, processes the data, and returns the data to the front end application. The company wants to ensure that response latency is as low as possible for all its users with the fewest number of changes to the company's operations. Which solution will meet these requirements?

A ��Establish a connection between the front end application and the database to make queries faster by bypassing the API.

B ��Configure provisioned concurrency for the Lambda function that handles the requests.

C ��Cache the results of the queries in Amazon S3 for faster retrieval of similar datasets.

D ��Increase the size of the database to increase the number of connections Lambda can establish at one time.

�𰸣�B

���� �� Correct Answer:  B Detailed Explanation: The correct answer is B. Configure provisioned concurrency for the Lambda function that handles the requests. When a Lambda function is invoked for the first time (or after scaling), it experiences a cold start, which  includes  initializing the  runtime,  loading  libraries, and setting up connections. Since the  Lambda function  in this scenario  loads  many  libraries  and  connects  to  RDS,  cold  starts  will  significantly  increase  response  latency.  Provisioned Concurrency  keeps  a  specified  number  of  Lambda  instances  warm  and  ready  to  respond  instantly.  This  reduces  cold  starts, ensuring faster execution and lower latency for users. It requires minimal operational changes��only adjusting the concurrency configuration��without modifying the application code, database, or architecture. Other options are less effective: - A: Bypassing the API Gateway and Lambda to connect directly to the database introduces security risks,  breaks the serverless design, and requires major architectural changes. - C: Caching in S3 adds complexity (code changes for cache logic) and S3 is not optimized for  low -  latency data  retrieval compared to in -  memory solutions  like Amazon  Elastic ache. -  D: Scaling the database  might improve  connection  capacity  but  doesn ��t  address  the  root  cause  (Lambda  cold  starts)  and  increases  costs  unnecessarily. Reference   Links:   -   [AWS   Lambda    Provisioned   Concurrency](https://docs.aws.amazon.com/lambda/latest/dg/provisioned   - concurrency.html)  -  [Understanding  AWS  Lambda  Cold  Starts](https://aws.amazon.com/blogs/compute/operating  -  lambda  - performance - optimization - part - 1/)

332.A  company  is  migrating  its  on-premises  workload  to  the  AWS  Cloud.  The  company  already  uses  several  Amazon  EC2 instances and Amazon RDS DB instances. The company wants a solution that automatically starts and stops the EC2 instances and DB instances outside of business hours. The solution must minimize cost and infrastructure maintenance. Which solution will meet these requirements?

A ��Scale the EC2 instances by using elastic resize. Scale the DB instances to zero outside of business hours.

B ��Explore AWS Marketplace for partner solutions that will automatically start and stop the EC2 instances and DB instances on a schedule.

C �� Launch  another  EC2  instance.  Configure  a crontab schedule to  run shell  scripts that will start and stop the existing  EC2 instances and DB instances on a schedule.

D��Create an AWS Lambda function that will start and stop the EC2 instances and DB instances. Configure Amazon Event bridge to invoke the Lambda function on a schedule.

�𰸣�D

������ Correct Answer D Detailed Explanation The best solution is D because: 1. AWS Lambda is a serverless service that runs code  without  requiring  dedicated  infrastructure.  By  creating  a   Lambda  function  to  start/stop   EC2  and  RDS  instances,  the company avoids the maintenance overhead of managing servers (unlike Option C, which adds another EC2 instance). 2. Amazon Event bridge (formerly Cloud watch Events) can trigger the Lambda function on a predefined schedule (e.g., start instances at 8 AM, stop them at 6 PM). This automation ensures the instances run only during business hours, minimizing costs. 3. Cost and Maintenance: Lambda and Event bridge are fully managed AWS services, so there ��s no infrastructure to maintain. This aligns with the requirement to reduce both cost and operational effort. Why the other options are incorrect: - A: EC2 instances cannot be scaled to zero with elastic resize (elastic resize applies to Amazon Redshift, not EC2). While RDS can be stopped, scaling to zero is not a valid operation. - B: While AWS Marketplace solutions might work, they often incur additional costs and require third - party tool management, which conflicts with minimizing infrastructure maintenance. - C: Adding another EC2 instance to manage  start/stop  scripts  introduces  unnecessary  infrastructure  (and  costs),  defeating  the  goal  of  minimizing  maintenance. Reference           Links           -           [AWS            Lambda](https://aws.amazon.com/lambda/)           -           [Amazon           Event bridge Scheduler](https://aws.amazon.com/event bridge/)                   -                   [Stopping                   and                   Starting                    RDS
Instances](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_Stop instance.html)

333.A company hosts a three-tier web application that includes a Postgresql database. The database stores the metadata from documents. The company searches the metadata for key terms to retrieve documents that the company reviews in a report each month. The documents are stored in Amazon S3. The documents are usually written only once, but they are updated frequently. The  reporting  process  takes  a  few  hours  with  the  use  of  relational  queries.  The  reporting  process  must  not  prevent  any document modifications or the addition of new documents. A solutions architect needs to implement a solution to speed up the reporting process. Which solution will meet these requirements with the LEAST amount of change to the application code?

A��Set up a new Amazon Document db (with MongoDB compatibility) cluster that includes a read replica. Scale the read replica to generate the reports.

B ��Set up a new Amazon Aurora Postgresql DB cluster that includes an Aurora Replica. Issue queries to the Aurora Replica to generate the reports.

C ��Set up a new Amazon RDS for Postgresql Multi-AZ DB instance. Configure the reporting module to query the secondary RDS node so that the reporting module does not affect the primary node.

D ��Set up a new Amazon Dynamo db table to store the documents. Use a fixed write capacity to support new document entries. Automatically scale the read capacity to support the reports.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution is to set up an Amazon Aurora Postgresql DB cluster with an Aurora Replica. Here's why: 1. Minimal Code Changes: The company already uses Postgresql, so Aurora Postgresql (which is fully  compatible)  requires the  least  code  changes.  Other  options  like  Document db  (A)  or  Dynamo db  (D)  would  force  major rewrites. 2. Read Scaling for Reporting: Aurora Replicas are designed for read-heavy workloads. By directing report queries to the replica,  the  primary  database  stays  available  for  document  updates/new  entries.  This  avoids  performance  conflicts.  3.  RDS

Multi-AZ (C) is not ideal: RDS Multi-AZ's standby node isn't directly accessible for queries (unlike Aurora Replicas). It ��s meant for failover,  not  scaling. 4.  Dynamo db  (D)  adds  complexity:  Switching  to  Dynamo db  would  require  redesigning  the  data  model (relational   ��  NoSQL)  and  risks  over complicating  a  system  already  working  with  Postgresql.  In  short,  Aurora  Postgresql replicas let the company reuse existing code, offload reporting to a dedicated read node, and keep the primary database focused
on                    handling                     updates.                     Reference                     Links:                    -                     [Amazon                    Aurora
Replicas](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/Aurora.Replication.html)  -  [Aurora  vs  RDS  Read
Replicas](https://aws.amazon.com/rds/aurora/faqs/#Replication)

334.A company has a three-tier application on AWS that ingests sensor data from its users �� devices. The traffic flows through a Network Load Balancer (NLB), then to Amazon EC2 instances for the web tier, and finally to EC2 instances for the application tier. The application tier makes calls to a database. What should a solutions architect do to improve the security of the data in transit?

A ��Configure a TLS listener. Deploy the server certificate on the NLB.

B ��Configure AWS Shield Advanced. Enable AWS WAF on the NLB.

C ��Change the load balancer to an Application Load Balancer (ALB). Enable AWS WAF on the ALB.

D ��Encrypt the Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instances by using AWS Key Management Service (AWS KMS).

�𰸣�A

������ Correct Answer: A Detailed Explanation: To secure data in transit between clients and the Network Load Balancer (NLB), the  best  approach  is  to  configure  a  TLS  listener  on  the  NLB  and  deploy  a  server  certificate.  Here's  why  this  works:  1.  TLS Encryption: A TLS  listener  (HTTPS)  encrypts  data  between  clients  and  the  NLB,  protecting  sensitive  sensor  data  from  being intercepted. 2.  NLB Capability: While  NLB  operates at  Layer 4  (TCP/UDP), it still supports TLS termination. This  means  it can handle encryption/decryption even though it doesn't process Layer 7 traffic like an ALB. 3. Scope of Protection: This secures the first leg of communication (client to NLB). For full end-to-end encryption, you'd also need HTTPS between NLB and EC2 instances, but the question focuses on improving security (not necessarily full end-to-end). Why other options are less appropriate: - B: AWS WAF doesn't work with NLB (only ALB/Cloud front). Shield Advanced protects against DDoS attacks but doesn't encrypt data.
- C: While ALB + WAF improves security, it changes infrastructure unnecessarily. The question doesn't  mention  needing WAF rules  or  Layer  7  features.  -  D:   EBS  encryption  protects  data  at  rest,  not  data  in  transit.   Reference  Links:  -   [NLB  TLS Termination](https://docs.aws.amazon.com/elastic load balancing/latest/network/create-tls-listener.html)    -    [Comparing     Load Balancers](https://aws.amazon.com/elastic load balancing/features/#compare)

335.A company  is  planning to  migrate  a  commercial  off-the-shelf application from  its  on-premises  data  center to AWS. The software  has  a  software  licensing  model  using  sockets  and  cores  with  predictable  capacity  and  uptime  requirements.  The company wants to  use  its existing  licenses, which were  purchased earlier this year. Which Amazon  EC2  pricing option  is the MOST cost-effective?

A ��Dedicated Reserved Hosts

B ��Dedicated On-Demand Hosts

C ��Dedicated Reserved Instances

D ��Dedicated On-Demand Instances

�𰸣�A

���� �� Correct Answer: A. Dedicated Reserved Hosts Detailed Explanation: The best choice here is Dedicated Reserved Hosts because the company  needs to  use existing  licenses tied to  physical sockets/cores.  Here's  why:  1.  Licensing  Requirements:  - Commercial software with socket/core licensing often requires visibility into physical hardware. - Dedicated Hosts provide access to the actual physical server, letting you track socket/core usage for licensing compliance. 2. Cost Optimization: - Reserved Hosts offer ~70% cost savings vs. On-Demand  Hosts when committed to  1/3-year terms. - The company has predictable workloads (predictable capacity), making Reserved pricing ideal for long-term savings. 3. Key Difference: - Dedicated Instances (options C/D) run on isolated hardware but don't give host-level control. - Dedicated Hosts (options A/B) provide full visibility/control of the physical       server,       which       is       critical       for        socket/core       licenses.       Reference        Links:       -       [AWS        Dedicated Hosts](https://aws.amazon.com/ec2/dedicated-hosts/)                      -                       [Reserved                      vs                       On-Demand
Pricing](https://aws.amazon.com/ec2/pricing/reserved-instances/)

336.A company runs an application on Amazon EC2 Linux instances across multiple Availability Zones. The application needs a storage layer that is highly available and Portable Operating System Interface (POSIX)-compliant. The storage layer must provide maximum  data  durability  and  must  be  shareable  across  the  EC2  instances.  The  data  in  the  storage  layer  will  be  accessed frequently for the first 30 days and will be accessed infrequently after that time. Which solution will meet these requirements MOST cost-effectively?

A ��Use the Amazon S3 Standard storage class. Create an S3 Lifecycle policy to move infrequently accessed data to S3 Glacier.

B �� Use  the  Amazon  S3  Standard  storage  class.  Create  an  S3   Lifecycle  policy  to   move  infrequently  accessed  data  to  S3 Standard-Infrequent Access (S3 Standard-IA).

C �� Use the Amazon  Elastic  File System  (Amazon  EFS) Standard storage class.  Create a  lifecycle  management  policy to move infrequently accessed data to EFS Standard-Infrequent Access (EFS Standard-IA).

D �� Use the Amazon Elastic File System (Amazon EFS) One Zone storage class. Create a lifecycle management policy to move infrequently accessed data to EFS One Zone-Infrequent Access (EFS One Zone-IA).

�𰸣�C

������ Correct Answer C Explanation The question requires a POSIX-compliant, highly available storage layer shared across EC2 instances. Here ��s why C is correct: 1. POSIX Compliance   �� Shared Access: Amazon EFS is a fully managed POSIX-compliant file system. Unlike S3 (object storage), EFS natively supports file system operations, making it ideal for EC2 instances needing shared file storage. 2. High Availability   �� Durability: The EFS Standard storage class stores data redundantly across multiple Availability Zones (AZs), ensuring maximum durability and availability. Options using the One Zone class (D) fail here, as they lack multi-AZ redundancy. 3. Cost-Effective Lifecycle Management:  EFS allows lifecycle policies to automatically move infrequently accessed files to the EFS Standard-IA class after 30 days. This reduces costs while maintaining multi-AZ redundancy. S3-based options (A/B) are  cheaper  but  not  POSIX-compliant  for   direct  file  system   mounting.  4.  Eliminating  Other  Options:  -  A/B:  S3  isn  �� t POSIX-compliant  for  direct  file  sharing.  -  D:  EFS  One  Zone  lacks  multi-AZ  redundancy,  risking  availability.  Reference  Links  - [Amazon      EFS       Storage      Classes](https://docs.aws.amazon.com/efs/latest/ug/storage-classes.html)       -       [EFS       Lifecycle Management](https://docs.aws.amazon.com/efs/latest/ug/lifecycle-management-efs.html)

337.A solutions architect is creating a new VPC design. There are two public subnets for the load balancer, two private subnets for web servers, and two private subnets for MySQL. The web servers use only HTTPS. The solutions architect has already created a security group for the load balancer allowing port 443 from 0.0.0.0/0. Company policy requires that each resource has the least access required to still be able to perform its tasks. Which additional configuration strategy should the solutions architect use to meet these requirements?

A ��Create a security group for the web servers and allow port 443 from 0.0.0.0/0. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.

B ��Create a network ACL for the web servers and allow port 443 from 0.0.0.0/0 . Create a network ACL for the MySQL servers and allow port 3306 from the web servers security group.

C ��Create a security group for the web servers and allow port 443 from the load balancer. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.

D ��Create a network ACL for the web servers and allow port 443 from the load balancer. Create a network ACL for the MySQL servers and allow port 3306 from the web servers security group.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The correct answer is C because it uses security groups (stateful) to enforce least privilege by referencing other security groups instead of allowing broad IP ranges. Here's why: 1. Web Servers Security Group: - Allows HTTPS (port 443) only from the Load Balancer's security group, not from the entire internet (0.0.0.0/0). This ensures web servers are accessible exclusively via the load balancer, not directly from the internet. 2. MySQL Security Group: - Allows MySQL (port 3306) only from the Web Servers' security group, ensuring only web servers can connect to the database. Why Not Other Options: - A: Allowing 0.0.0.0/0 on web servers violates least privilege (web servers are private and should only accept traffic from the load balancer). - B   �� D: Network ACLs (stateless) are subnet-level and less precise than security groups. They require explicit allow/deny rules for both inbound/outbound traffic, making them harder to manage and less secure for this scenario. Key Concepts: - Security Groups (instance-level firewall) are ideal for granular control using security group references. - Network ACLs (subnet-level firewall) are stateless and better suited for broad subnet-level rules, not specific application-tier restrictions. Reference   Links:   -    [AWS   Security   Groups](https://docs.aws.amazon.com/vpc/latest/user guide/VPC_Security groups.html)   - [Network                                                               ACLs                                                               vs.                                                               Security
Groups](https://docs.aws.amazon.com/vpc/latest/user guide/VPC_Security.html#VPC_Security_Comparison)

338.An ecommerce company is running a multi-tier application on AWS. The front-end and backend tiers both run on Amazon EC2,  and  the  database  runs  on  Amazon  RDS  for  MySQL.  The  backend  tier  communicates  with  t he  RDS  instance.  There  are frequent calls to return identical datasets from the database that are causing performance slowdowns. Which action should be taken to improve the performance of the backend?

A ��Implement Amazon SNS to store the database calls.

B ��Implement Amazon Elastic ache to cache the large datasets.

C ��Implement an RDS for MySQL read replica to cache database calls.

D ��Implement Amazon Kinesis Data Firehose to stream the calls to the database.

�𰸣�B

������ Correct Answer B �� Implement Amazon Elastic ache to cache the large datasets. Detailed Explanation The performance slow down occurs because the backend tier repeatedly fetches identical datasets from the database. Caching these frequently accessed datasets in memory (using Amazon Elastic ache) allows the backend to retrieve the data directly from the cache instead of querying the database every time. This reduces the load on t he RDS instance, speeds up response times, and improves overall performance. - Why not A/C/D? - A (SNS): Amazon SNS is a pub/sub messaging service, not a storage/caching solution. - C (RDS

Read Replica): Read replicas distribute read traffic but don ��t cache data. Repeated identical queries would still hit the replica, causing delays. - D (Kinesis Data Firehose): This service streams data to storage/analytics tools and doesn ��t address repeated database queries. Reference Link [Amazon Elastic ache Overview](https://aws.amazon.com/elastic ache/)

339.A company is deploying a two-tier web application in a VPC. The web tier is using an Amazon EC2 Auto Scaling group with public subnets that span  multiple Availability Zones. The database tier consists of an Amazon  RDS for  MySQL  DB instance in separate private subnets. The web tier requires access to the database to retrieve product information. The web application is not working as intended. The web application reports that it cannot connect to the database. The database is confirmed to be up and  running. All configurations for the  network ACLs, security groups, and  route tables are still in their default states. What should a solutions architect recommend to fix the application?

A ��Add an explicit rule to the private subnet ��s network ACL to allow traffic from the web tier ��s EC2 instances.

B ��Add a route in the VPC route table to allow traffic between the web tier ��s EC2 instances and the database tier.

C��Deploy the web tier's EC2 instances and the database tier ��s RDS instance into two separate VPCs, and configure VPC peering.

D ��Add an inbound rule to the security group of the database tier ��s RDS instance to allow traffic from the web tiers security group.

�𰸣�D

������ Correct Answer D Detailed Explanation The issue is caused by the database's security group blocking inbound traffic from the web tier. Here ��s why: 1. Security Groups: By default, security groups block all inbound traffic. The RDS database ��s security group needs an inbound rule explicitly allowing traffic from the web tier ��s EC2 instances. Since the web tier uses an Auto Scaling group, using the web tier ��s security group as the source (instead of static IPs) ensures all EC2 instances in the web tier can connect. 2. Network ACLs (Option A): Default network ACLs allow all inbound/outbound traffic, so no extra rules are needed here.
3. Route Tables (Option B): Subnets in the same VPC can communicate internally by default. No route table changes are required.
4. VPC Peering (Option C): Both tiers are already in the same VPC, so peering is unnecessary. Why D is Correct: Security groups act as virtual firewalls. Without an inbound rule in t he RDS security group allowing traffic from the web tier ��s security group, the database       rejects        connection        requests       from       the        web       tier.        Reference        Links       -        [AWS        Security Groups](https://docs.aws.amazon.com/vpc/latest/user guide/VPC_Security groups.html)      -      [RDS       Security      Group      Best Practices](https://aws.amazon.com/blogs/database/best-practices-for-configuring-network-settings-for-amazon-rds/)

340.A company has a large dataset for its online advertising business stored in an Amazon RDS for MySQL DB instance in a single Availability  Zone.  The   company  wants   business   reporting  queries  to   run  without   impacting  the  write  operations  to  the production DB instance. Which solution meets these requirements?

A ��Deploy RDS read replicas to process the business reporting queries.

B ��Scale out the DB instance horizontally by placing it behind an Elastic Load Balancer.

C ��Scale up the DB instance to a larger instance type to handle write operations and queries.

D ��Deploy the DB instance in multiple Availability Zones to process the business reporting queries.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution is to use RDS read replicas. Here's why: - Read replicas create a

copy of your main database (asynchronous replication) that is read-only. Business reporting queries (read operations) can run on the replica, while the main database (primary instance) handles all write operations (like inserting/updating ads data). This way, reporting queries won't compete with write operations for resources like CPU or memory, ensuring consistent performance for both. - Why other options are wrong: - B: RDS instances can't be scaled horizontally with a load balancer. Load balancers work for distributing traffic across application servers, not databases. - C: Scaling up (bigger instance) increases capacity, but read/write operations still happen on the same instance, so heavy reporting queries could still slow down writes. - D: Multi-AZ deployment creates a standby replica for high availability (automatic failover), but the standby replica isn't accessible for queries. It doesn't help         with         separating         read/write         traffic.         Reference         Links:         -         [Using         Amazon          RDS         Read Replicas](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_Read repl.html)        -        [Multi-AZ        vs.         Read
Replicas](https://aws.amazon.com/rds/features/multi-az/)

341.A company needs a backup strategy for its three-tier stateless web application. The web application runs on Amazon EC2 instances in an Auto Scaling group with a dynamic scaling policy that is configured to respond to scaling events. The database tier runs on Amazon RDS for Postgresql. The web application does not require temporary local storage on the EC2 instances. The company ��s recovery  point objective (RPO) is 2 hours. The backup strategy must maximize scalability and optimize resource utilization for this environment. Which solution will meet these requirements?

A��Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of t he EC2 instances and database every 2 hours to meet the RPO.

B ��Configure a snapshot lifecycle policy to take Amazon Elastic Block Store (Amazon EBS) snapshots. Enable automated backups in Amazon RDS to meet the RPO.

C ��Retain the latest Amazon Machine Images (AMIs) of the web and application tiers. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.

D ��Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of t he EC2 instances every 2 hours. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.

�𰸣�C

������ Correct Answer C Explanation The best solution is C because it effectively addresses both the stateless EC2 instances and t he RDS database while meeting the 2-hour RPO. Here ��s why: 1. For the EC2 Web Tier (Stateless): - Retaining the latest Amazon Machine  Images  (AMIs)  ensures  that  new  EC2  instances  launched  by  the  Auto  Scaling  group  (ASG)  always  use  the  most up-to-date configuration. Since the application is stateless (no persistent data on EC2), frequent EBS snapshots (as in options A, B, D) are unnecessary and wasteful. AMIs are more scalable and resource-efficient here. 2. For t he RDS Database Tier: - Enabling automated  RDS  backups  (daily  snapshots)  combined with  point-in-time  recovery  (PITR)  allows  restoring the  database to any specific second within  a  35-day  retention window. This easily  meets  the  2-hour  RPO  requirement.  PITR  is  more  precise  and flexible than manually scheduling snapshots every 2 hours (as in options A, D). Why other options are less optimal: - A   �� D: Taking EBS snapshots every 2 hours for EC2 is redundant for stateless instances. - B: Using EBS snapshot lifecycle policies adds unnecessary overhead for EC2 tiers that don ��t require persistent storage. - C avoids over-engineering and aligns with AWS best practices          for           stateless           architectures.           Reference           Links          -           [AWS           Auto           Scaling           with AMIs](https://docs.aws.amazon.com/auto scaling/ec2/user guide/launch-templates.html)          -           [RDS          Backups           and PITR](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_PIT.html)

342.A company wants to deploy a  new  public web application on AWS. The application includes a web server tier that uses Amazon  EC2  instances. The  application  also  includes  a  database tier that  uses  an Amazon  RDS  for  MySQL  DB  instance.  The application must be secure and accessible for global customers that have dynamic IP addresses. How should a solutions architect configure the security groups to meet these requirements?

A ��Configure the security group for the web servers to allow inbound traffic on port 443 from 0.0.0.0/0. Configure the security group for the DB instance to allow inbound traffic on port 3306 from the security group of the web servers.

B ��Configure the security group for the web servers to allow inbound traffic on port 443 from the IP addresses of the customers. Configure the security group for the  DB  instance to  allow  inbound traffic on  port  3306 from the security group of the web servers.

C ��Configure the security group for the web servers to allow inbound traffic on port 443 from the IP addresses of the customers. Configure the security group for the DB instance to allow inbound traffic on port 3306 from the IP addresses of the customers.

D ��Configure the security group for the web servers to allow inbound traffic on port 443 from 0.0.0.0/0. Configure the security group for the DB instance to allow inbound traffic on port 3306 from 0.0.0.0/0.

�𰸣�A

������ Correct Answer A �� Configure the security group for the web servers to allow inbound traffic on port 443 from 0.0.0.0/0. Configure the security group for the DB instance to allow inbound traffic on port 3306 from the security group of the web servers. Explanation - Web Server Security Group (Port 443 from 0.0.0.0/0): The web server needs to accept public HTTPS traffic (port 443) from global users. Since customers have dynamic IP addresses (which change frequently), using 0.0.0.0/0 (all IPs) ensures accessibility without requiring constant IP updates. - Database Security Group (Port 3306 from Web Server ��s Security Group): The database should NEVER be publicly exposed. By allowing traffic only from the web server ��s security group (not specific IPs), you ensure only the web servers can communicate with the database. This uses AWS ��s security group referencing for tighter security. Why Other Options Are Wrong: - B/C: Requiring customer IPs for the web server (B/C) would fail because dynamic IPs change,  blocking  users.  Letting  customer  IPs  access  the  database  (C)  is  a  major  security  risk.  -  D:  Opening  the  database  to 0.0.0.0/0   (public   internet)   is   extremely    unsafe   and   violates   best    practices.   Reference    Links   -   [AWS   Security   Groups Basics](https://docs.aws.amazon.com/AWSEC2/latest/User guide/ec2-security-groups.html)     -       [Best      Practices     for       RDS Security](https://aws.amazon.com/blogs/database/best-practices-for-securing-amazon-rds/)

343.A payment processing company records all voice communication with its customers and stores the audio files in an Amazon S3 bucket. The company needs to capture the text from the audio files. The company must remove from the text any personally identifiable information (P II) that belongs to customers. What should a solutions architect do to meet these requirements?

A ��Process the audio files by using Amazon Kinesis Video Streams. Use an AWS Lambda function to scan for known P II patterns.

B ��When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start an Amazon T extract task to analyze the call recordings.

C ��Configure an Amazon Transcribe transcription job with P II redaction turned on. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start the transcription job. Store the output in a separate S3 bucket.

D ��Create an Amazon Connect contact flow that ingests the audio files with transcription turned on. Embed an AWS Lambda function to scan for known P II patterns. Use Amazon Event bridge to start the contact flow when an audio file is uploaded to the S3 bucket.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The correct solution uses Amazon Transcribe, which is specifically designed for converting speech to text. Transcribe  has  a  built-in  P II  redaction  feature  that  automatically  identifies  and  removes  sensitive

information like names, addresses, or credit card numbers. Here's why this works: 1. Event-Driven Workflow: When an audio file is  uploaded  to  S3,  a   Lambda  function  triggers  the   process   automatically.  2.  Specialized  Service:  Amazon  Transcribe   (not T extract/Kinesis) is the right tool for audio-to-text conversion. 3. Built-In P II Handling: Transcribe's redaction feature eliminates the need for manual pattern-matching via Lambda. 4. Secure Storage: Output is saved separately to avoid mixing raw audio and processed text. Other options are incorrect because: - A:  Kinesis Video Streams is for video, not batch audio processing. - B: T extract  analyzes  documents/images,   not  audio.  -   D:  Amazon  Connect  is  a  contact  center  service  unrelated  to  batch  file processing.                            Reference:                            [Amazon                            Transcribe                            P II                             Redaction
Documentation](https://docs.aws.amazon.com/transcribe/latest/dg/pii-redaction.html)

344.A company  is  running  a  multi-tier  ecommerce  web  application  in the AWS  Cloud. The  application  runs  on Amazon  EC2 instances with an Amazon RDS for MySQL Multi-AZ DB instance. Amazon RDS is configured with the latest generation DB instance with  2,000  GB  of  storage  in  a  General  Purpose  SSD  (gp3)  Amazon  Elastic  Block  Store  (Amazon  EBS)  volume.  The  database performance  affects  the  application  during  periods  of  high  demand.  A  database  administrator  analyzes  the  logs  in  Amazon Cloud watch Logs and discovers that the application performance always degrades when the number of read and write IOPS is higher than 20,000. What should a solutions architect do to improve the application performance?

A ��Replace the volume with a magnetic volume.

B ��Increase the number of IOPS on the gp3 volume.

C ��Replace the volume with a Provisioned IOPS SSD (io2) volume.

D ��Replace the 2,000 GB gp3 volume with two 1,000 GB gp3 volumes.

�𰸣�C

������ Correct Answer: C. Replace the volume with a Provisioned IOPS SSD (io2) volume. Detailed Explanation: The current issue occurs when the database exceeds 20,000 IOPS, causing performance degradation. - GP3 volumes have a maximum configurable IOPS of 16,000 (even though the default baseline is lower). This means GP3 can't handle the workload's 20,000+ IOPS demand. - IO2  volumes  (Provisioned   IOPS  SSD)  are  designed  for   high-performance  databases  and  support   up  to  256,000   IOPS  with consistent low latency. By switching to io2, the team can provision IOPS above 20,000 to match the workload's requirements. Why other options fail: - A: Magnetic volumes are outdated and have very low IOPS (hundreds), making performance worse. - B: GP3's max IOPS (16,000) is still below the required 20,000. - D: Splitting the volume won't help because RDS uses a single storage volume    per     database     instance;     you     can't    combine     IOPS     from     multiple     volumes.     Reference:     [AWS    Storage Types](https://docs.aws.amazon.com/Amazon rds/latest/User guide/CHAP_Storage.html)

345.An  IAM  user  made  several  configuration  changes  to  AWS  resources   in  their  company's  account  during  a   production deployment last week. A solutions architect learned that a couple of security group rules are  not configured as desired. The solutions architect wants to confirm which IAM user was responsible for making changes. Which service should the solutions architect use to find the desired information?

A ��Amazon Guard duty

B ��Amazon Inspector C ��AWS Cloud trail

D ��AWS Config

�𰸣�C

������ Correct Answer: C. AWS Cloud trail Detailed Explanation: AWS Cloud trail logs all API activity in an AWS account, including changes to security group rules. It records details like who (IAM user/role), what action (API call), when, and which resource was modified.  In  this  scenario,  the  solutions  architect  can  filter  Cloud trail  events  for  security  group-related  API  calls  (e.g., `Authorize security group ingress`)  during the deployment time frame to  identify the  responsible  IAM  user.  Other  options: - A. Amazon Guard duty: Detects threats (e.g., unauthorized access), not for auditing configuration changes. - B. Amazon Inspector: Scans  for  vulnerabilities  in  EC2  instances/applications,  unrelated  to  tracking  user  activity.  -  D.  AWS  Config:  Tracks  resource configuration  history  but  focuses  on  what  changed,  not  who  made  the  change.  Reference   Link:  [AWS  Cloud trail  User Guide](https://docs.aws.amazon.com/aws cloud trail/latest/user guide/cloud trail-user-guide.html)

346.A company has implemented a self-managed DNS service on AWS. The solution consists of the following:   ? Amazon EC2 instances in different AWS Regions   ? Endpoints  of a standard accelerator  in AWS Global Accelerator The company wants to protect the solution against DDoS attacks. What should a solutions architect do to meet this requirement?

A ��Subscribe to AWS Shield Advanced. Add the accelerator as a resource to protect.

B ��Subscribe to AWS Shield Advanced. Add the EC2 instances as resources to protect.

C ��Create an AWS WAF web ACL that includes a rate-based rule. Associate the web ACL with the accelerator.

D ��Create an AWS WAF web ACL that includes a rate-based rule. Associate the web ACL with the EC2 instances.

�𰸣�A

������ Correct Answer: A Detailed Explanation: To protect against DDoS attacks for a solution using AWS Global Accelerator and EC2  instances,  AWS  Shield  Advanced  is  the  appropriate  service.  Shield  Advanced  provides  enhanced   DDoS  protection  for network-layer (Layer 3/4) resources like Global Accelerator. By subscribing to Shield Advanced and adding the accelerator as a protected resource, all traffic routed through the accelerator (including traffic to backend EC2 instances across regions) gains automatic DDoS mitigation. Why not other options? - B: Protecting individual EC2 instances with Shield Advanced is less efficient, especially when they are distributed across regions. Shield Advanced for Global Accelerator simplifies protection by covering all traffic at the entry point. - C/D: AWS WAF is designed for application-layer (Layer 7) protections (e.g., HTTP/HTTPS). Since Global Accelerator  operates  at  Layer  3/4  and  DNS  typically  uses  UDP/TCP  (Layer  4),  WAF  cannot  be  directly  associated  with  the accelerator         or         DNS         endpoints.          Reference         Links:         -          [AWS         Shield         Advanced         for         Global Accelerator](https://docs.aws.amazon.com/global-accelerator/latest/dg/ddos-protection.html)   -    [AWS   Shield    Advanced   vs. WAF](https://aws.amazon.com/shield/faq/)

347.An ecommerce company needs to run a scheduled daily job to aggregate and filter sales records for analytics. The company stores the sales records in an Amazon S3 bucket. Each object can be up to 10 GB in size. Based on the number of sales events, the job can take up to an hour to complete. The CPU and memory usage of the job are constant and are known in advance. A solutions architect needs to minimize the amount of operational effort that is needed for the job to run. Which solution meets these requirements?

A ��Create an AWS Lambda function that has an Amazon Event bridge notification. Schedule the Event bridge event to run once a day.

B��Create an AWS Lambda function. Create an Amazon API Gateway HTTP API, and integrate the API with the function. Create an Amazon Event bridge scheduled event that calls the API and invokes the function.

C �� Create  an Amazon  Elastic  Container  Service  (Amazon  ECS)  cluster  with  an  AWS  Fargate  launch  type.  Create  an  Amazon Event bridge scheduled event that launches an ECS task on the cluster to run the job.

D ��Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch type and an Auto Scaling group with at least one EC2 instance. Create an Amazon Event bridge scheduled event that launches an ECS task on the cluster to run the job.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The correct answer is C because AWS Fargate is a serverless compute engine for containers, which eliminates the need to manage EC2 instances. Here's why it's the best choice: 1. Lambda Limitations: Options A and B use AWS Lambda, which has a 15 - minute maximum execution time. Since the job can take up to an hour, Lambda is unsuitable. 2. No Server Management with Fargate: Option C uses Amazon ECS with Fargate, a fully managed service. You only define  the  task  (CPU/memory   requirements),  and  AWS   handles   provisioning,  scaling,  and  maintenance.  This   minimizes operational effort. 3. Avoiding EC2 Overhead: Option D requires managing an EC2 - based ECS cluster and an Auto Scaling group, which  adds operational complexity  (e.g.,  patching  instances,  scaling  policies).  Fargate  removes  this  overhead. 4.  Event bridge Integration:  Event bridge can directly trigger  ECS  Fargate  tasks on a schedule, fitting the daily job  requirement without extra components  like  API  Gateway  (Option   B).  Reference  Links:  -  [AWS  Fargate  Overview](https://aws.amazon.com/fargate/)  - [Scheduling                                                                ECS                                                                Tasks                                                               with
Event bridge](https://docs.aws.amazon.com/Amazon ecs/latest/developer guide/scheduled_tasks.html)

348.A company needs to transfer 600 TB of data from its on-premises network-attached storage (NAS) system to the AWS Cloud. The data transfer must be complete within 2 weeks. The data is sensitive and must be encrypted in transit. The company ��s internet   connection   can   support   an    upload   speed   of    100   Mbps.   Which   solution    meets   these    requirements   MOST cost-effectively?

A��Use Amazon S3 multi-part upload functionality to transfer the files over HTTPS.

B ��Create a VPN connection between the on-premises NAS system and the nearest AWS Region. Transfer the data over the VPN connection.

C ��Use the AWS Snow Family console to order several AWS Snowball Edge Storage Optimized devices. Use the devices to transfer the data to Amazon S3.

D ��Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS Region. Transfer the data over a VPN connection into the Region to store the data in Amazon S3.

�𰸣�C

���� �� Correct Answer: C Detailed Explanation: The company needs to transfer 600 TB of data in 2 weeks with encryption in transit. Their 100 Mbps internet connection is too slow for this task. Calculating the upload time: - 600 TB = 600,000 GB - At 100 Mbps (~12.5 MB/s): (600,000 GB   �� 1,000 MB/GB)   �� 12.5 MB/s   �� 55,555,555 seconds   �� 643 days Options A and B would fail  due  to  this  bandwidth  limitation.  Option   D's  10  Gb ps  Direct  Connect   requires  expensive  infrastructure  upgrades  and installation  time  (usually  weeks/months),  making  it  impractical.  Why  C  works  best:  AWS  Snowball  Edge  Storage  Optimized devices (option C) physically transfer data offline: 1. Each device holds 80 TB   �� ~8 devices needed 2. AWS handles shipping and encryption  3.  Total  time    ��  1  week   (shipping  +  data  copy)  4.  Avoids   internet  transfer  costs/time  5.   Meets  encryption requirements  automatically  This   is  the   most  cost-effective  solution  for   large  offline  transfers  without   network   upgrades. Reference Link: [AWS Snowball Edge documentation](https://aws.amazon.com/snowball/edge/)

349.A financial company hosts a web application on AWS. The application uses an Amazon API Gateway Regional API endpoint to give users the ability to retrieve current stock prices. The company ��s security team has noticed an increase in the number of API requests. The security team is concerned that HTTP flood attacks might take the application offline. A solutions architect must design a solution to protect the application from this type of attack. Which solution meets these requirements with the LEAST operational overhead?

A��Create an Amazon Cloud front distribution in front of the API Gateway Regional API endpoint with a maximum TTL of 24 hours.

B ��Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web ACL with the API Gateway stage.

C �� Use  Amazon Cloud watch  metrics to  monitor the  Count  metric  and  alert the security team when the  predefined  rate  is reached.

D ��Create an Amazon Cloud front distribution with Lambda@Edge in front of the API Gateway Regional API endpoint. Create an AWS Lambda function to block requests from IP addresses that exceed the predefined rate.

�𰸣�B

������ Correct Answer B Detailed Explanation The best solution is B because AWS WAF rate-based rules are specifically designed to  mitigate  HTTP  flood  attacks.  Here �� s  why:  -  Rate-based  rules  automatically  track  and  block  IP  addresses  that  exceed  a predefined  request  rate  (e.g.,  1,000  requests  in  5  minutes).  This   directly  addresses  HTTP  flood   attacks  without   manual intervention. - Minimal operational overhead: AWS WAF integrates natively with API Gateway. You only need to configure the rate-based rule and associate the web ACL with the API Gateway stage �� no code or infrastructure management is required. - Alternatives: - A (Cloud front with TTL) focuses on caching, which does not block attacks. - C (Cloud watch alerts) only monitors traffic but doesn ��t block attacks. - D (Cloud front + Lambda@Edge) works but requires writing and maintaining custom Lambda code, increasing operational complexity compared to the built-in WAF rate-based rule. Reference Links - [AWS WAF Rate-Based Rules](https://docs.aws.amazon.com/waf/latest/developer guide/waf-rate-based-rules.html)  -  [Protecting  APIs  with  AWS  WAF and API Gateway](https://aws.amazon.com/blogs/security/protect-your-apis-using-amazon-api-gateway-and-aws-waf/)

350.A meteorological startup company has a custom web application to sell weather data to its users online. The company uses Amazon Dynamo db to store its data and wants to build a new service that sends an alert to the managers of four internal teams every time a new weather event is recorded. The company does  not want this new service to affect the performance of the current application. What should a solutions architect do to meet these  requirements with the LEAST amount of operational overhead?

A ��Use Dynamo db transactions to write new event data to the table. Configure the transactions to notify internal teams.

B ��Have the current application publish a message to four Amazon Simple Notification Service (Amazon SNS) topics. Have each team subscribe to one topic.

C��Enable Amazon Dynamo db Streams on the table. Use triggers to write to a single Amazon Simple Notification Service (Amazon SNS) topic to which the teams can subscribe.

D ��Add a custom attribute to each record to flag new items. Write a cron job that scans the table every minute for items that are new and notifies an Amazon Simple Queue Service (Amazon SQS) queue to which the teams can subscribe.

�𰸣�C

������ Correct Answer C Detailed Explanation The best solution is to use Dynamo db Streams and Amazon SNS. Here ��s why: 1.

Dynamo db Streams captures changes to the table (e.g., new weather events) in real time without impacting the performance of the  existing  application.  This  avoids  adding  extra  load  to  the  application  itself.  2.  A  trigger  (like  an  AWS  Lambda  function) processes the  stream  data  and  sends  a  notification to  a  single  SNS  topic.  SNS  can then fan  out the  alert to  all four teams simultaneously. 3. This approach  is fully serverless,  requires  no code changes to the existing app, and  minimizes operational overhead. Other options are worse because: - A and B modify the existing application or use inefficient transactions. - D uses inefficient          table          scans           (slow,          costly,           and          delayed).           Reference           Links          -           [Dynamo db Streams](https://docs.aws.amazon.com/amazon dynamo db/latest/developer guide/Streams.html)           -            [SNS             Fanout Pattern](https://docs.aws.amazon.com/sns/latest/dg/sns-common-scenarios.html)

351.A company wants to use the AWS Cloud to make an existing application highly available and resilient. The current version of the application  resides  in the  company's data center. The application  recently experienced data  loss  after  a database server crashed because of an unexpected power outage. The company needs a solution that avoids any single points of failure. The solution must give the application the ability to scale to meet user demand. Which solution will meet these requirements?

A ��Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration.

B��Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group in a single Availability Zone. Deploy the database on an EC2 instance. Enable EC2 Auto Recovery.

C ��Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance with a read replica in a single Availability Zone. Promote the read replica to replace the primary DB instance if the primary DB instance fails.

D �� Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Deploy the  primary and secondary database servers on  EC2  instances  across  multiple Availability Zones.  Use Amazon  Elastic Block Store (Amazon EBS) Multi-Attach to create shared storage between the instances.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The company needs a solution that eliminates single points of failure (SPOF) and scales with demand. Here ��s why Option A works best: 1. Application Servers (EC2 + Auto Scaling): - Auto Scaling ensures the application automatically adjusts the number of EC2 instances based on traffic, enabling scalability. - Deploying across multiple Availability Zones (AZs) ensures redundancy. If one AZ fails, the application remains available in other AZs. 2. Database (RDS Multi - AZ): - Amazon RDS Multi - AZ creates a primary database instance in one AZ and a standby replica in another AZ. - If the primary instance fails (e.g., power outage), RDS automatically fails over to the standby, minimizing downtime and data loss. This avoids SPOF for the database. Why other  options fail: -  Option  B:  Uses  a  single  AZ  for  app servers  (SPOF)  and  relies  on  EC2  Auto Recovery for the database. This is less reliable than RDS Multi - AZ ��s automatic failover. - Option C: A read replica is for scaling read operations, not high availability. Promoting it manually is slower and riskier than RDS Multi - AZ ��s automatic failover. - Option D: Self - managed databases on EC2 with EBS Multi - Attach are complex to configure and lack the seamless failover of RDS               Multi               -               AZ.                Reference               Links:               -                [Amazon               RDS               Multi                - AZ](https://docs.aws.amazon.com/Amazon rds/latest/User guide/Concepts.MultiAZ.html)          -          [EC2          Auto          Scaling Groups](https://docs.aws.amazon.com/auto scaling/ec2/user guide/Auto scaling group.html)

352.A company needs to ingest and handle large amounts of streaming data that its application generates. The application runs on Amazon EC2 instances and sends data to Amazon Kinesis Data Streams, which is configured with default settings. Every other day, the application consumes the data and writes the data to an Amazon S3 bucket for business intelligence (BI) processing. The company observes that Amazon S3 is not receiving all the data that the application sends to Kinesis Data Streams. What should a solutions architect do to resolve this issue?

A ��Update the Kinesis Data Streams default settings by modifying the data retention period.

B ��Update the application to use the Kinesis Producer Library (KPL) to send the data to Kinesis Data Streams.

C ��Update the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data Streams.

D ��Turn on S3 Versioning within the S3 bucket to preserve every version of every object that is ingested in the S3 bucket.

�𰸣�A

�� �� �� Correct Answer: C  Detailed  Explanation: The default  retention  period for  Kinesis  Data Streams is  24  hours,  but the company consumes  data every 48  hours. This  means  data  expires  before  being  processed.  However,  the  root  cause  is  likely insufficient shards,  not just  retention.  Kinesis shards define throughput capacity.  If the data  ingress exceeds the shard  limits (1MB/sec or 1,000 records/sec per shard), throttling occurs, and data is lost. While increasing retention (Option A) helps with expiration, it doesn ��t  resolve throughput bottlenecks. Why Option C is better: - Kinesis automatically deletes data after the retention period, but if shards are overloaded, data is lost immediately due to throttling, not just after 24 hours. - Increasing shards ensures the stream can  handle the  incoming data  rate,  preventing throttling and data  loss.  Retention  (Option A)  is  a secondary adjustment. For a beginner:  Imagine Kinesis as a pipeline.  Each shard is a pipe with a fixed width. If data pours in faster than the pipes can handle, it overflows (data loss). Adding more pipes (shards) prevents overflow. Retention period is like how long water stays in the pipes �� useful, but irrelevant if the pipes are already overflowing. Reference Links: - [Kinesis Data Streams               Limits](https://docs.aws.amazon.com/streams/latest/dev/service-sizes-and-limits.html)               -                [Scaling Shards](https://docs.aws.amazon.com/streams/latest/dev/scaling.html)

353.A developer has an application that uses an AWS  Lambda function to upload files to Amazon S3 and needs the required permissions to perform the task. The developer already has an  IAM user with valid  IAM credentials  required for Amazon S3. What should a solutions architect do to grant the permissions?

A ��Add required IAM permissions in the resource policy of the Lambda function.

B ��Create a signed request using the existing IAM credentials in the Lambda function.

C ��Create a new IAM user and use the existing IAM credentials in the Lambda function.

D ��Create an IAM execution role with the required permissions and attach the IAM role to the Lambda function.

�𰸣�D

������ Correct Answer: D Detailed Explanation: When an AWS Lambda function needs to interact with other AWS services (like Amazon  S3),  it  requires  permissions  to  do  so.  Instead  of  using  long-term  IAM  user  credentials  (which  is  insecure  and  not recommended), AWS recommends using an IAM execution role. Here ��s why: - An IAM execution role is attached directly to the Lambda function. - When the Lambda function runs, AWS automatically provides temporary credentials from this role, which are more secure (no hardcoded secrets) and automatically rotated. - The existing IAM user credentials mentioned in the question are irrelevant here because Lambda functions should never rely on IAM user credentials. Roles are the standard and secure way to grant permissions. Options A, B, and C are incorrect because: - A: Lambda resource policies control who can invoke the function, not what the function can do. - B: Signed requests with IAM user credentials would expose long-term secrets, violating security best  practices.  -  C:  Creating  a  new  IAM  user  still  uses  long-term  credentials,  which  are  unnecessary  and  risky  for  Lambda. Reference:                                    [AWS                                     Documentation:                                     Lambda                                     Execution
Roles](https://docs.aws.amazon.com/lambda/latest/dg/lambda-permissions.html)

354.A company has deployed a serverless application that invokes an AWS Lambda function when new documents are uploaded to  an  Amazon  S3  bucket.  The  application  uses  the  Lambda  function  to  process  the  documents.  After  a  recent  marketing campaign, the company noticed that the application did not process many of the documents. What should a solutions architect do to improve the architecture of this application?

A ��Set the Lambda function's runtime timeout value to 15 minutes.

B ��Configure an S3 bucket replication policy. Stage the documents in the S3 bucket for later processing.

C ��Deploy an additional Lambda function. Load balance the processing of the documents across the two Lambda functions.

D ��Create an Amazon Simple Queue Service (Amazon SQS) queue. Send the requests to the queue. Configure the queue as an event source for Lambda.

�𰸣�D

������ Correct Answer: D Detailed Explanation: When an S3 bucket directly triggers a Lambda function, there's a risk of missed events if  Lambda throttles due to concurrency  limits or errors. SQS acts as a  buffer: S3 events go  into the queue first, then Lambda  processes them  at  its own  pace. This  ensures  no  documents are  lost  even  during traffic spikes. Other  options: - A: Timeout extension doesn't fix event loss -  B:  Replication adds complexity without solving  processing  issues - C:  Manual  load balancing  is  unnecessary  since  Lambda+SQS  auto-scales  better  Reference  Link:  AWS  documentation  on  using  SQS  as  event source for Lambda: https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html

355.A  company  is  implementing  a  shared  storage  solution  for  a  gaming  application  that  is  hosted  in  the  AWS  Cloud.  The company needs the ability to use Lustre clients to access data. The solution must be fully managed. Which solution meets these requirements?

A ��Create an AWS Data sync task that shares the data as a mountable file system. Mount the file system to the application server.

B �� Create  an  AWS  Storage  Gateway  file  gateway.  Create  a  file  share  that  uses  the  required  client  protocol.  Connect  the application server to the file share.

C��Create an Amazon Elastic File System (Amazon EFS) file system, and configure it to support Lustre. Attach the file system to the origin server. Connect the application server to the file system.

D ��Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system.

�𰸣�D

������ Correct Answer D �� Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system.  Detailed Explanation The question asks for a fully managed shared storage solution that supports Lustre clients in AWS. Here's why Amazon FSx for Lustre (Option D) is the correct choice:  1.  Lustre Support:  FSx for Lustre is explicitly designed for the Lustre file system protocol, which is required for applications using Lustre clients (common in HPC, gaming, or machine learning workloads). 2. Fully Managed: AWS handles maintenance, updates, and scaling, meeting the fully managed requirement. 3. Performance: FSx for Lustre is optimized for high-throughput, low-latency workloads (critical for gaming  applications).  Why  other  options  are   incorrect:  -  A   (Data sync):   Data sync  is  a  data  transfer/migration  tool,   not  a mountable file system for Lustre clients. - B (Storage Gateway): File Gateway supports SMB/ NFS protocols, not Lustre. - C (EFS):

Amazon   EFS   uses   the    NFS   protocol   and   does    not   natively   support    Lustre   clients.   Reference    Link   [Amazon    FSx   for Lustre](https://aws.amazon.com/fsx/lustre/)

356.A company runs an application that receives data from thousands of geographically dispersed remote devices that use UDP. The application processes the data immediately and sends a message back to the device if necessary. No data is stored. The company needs a solution that minimizes latency for the data transmission from the devices. The solution also must provide rapid failover to another AWS Region. Which solution will meet these requirements?

A ��Configure an Amazon Route 53 failover routing policy. Create a Network Load Balancer (NLB) in each of the two Regions. Configure the NLB to invoke an AWS Lambda function to process the data.

B �� Use AWS Global Accelerator. Create a Network Load Balancer (NLB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the Nl process the data in Amazon ECS.

C ��Use AWS Global Accelerator. Create an Application Load Balancer (ALB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the ALB. Process the data in Amazon ECS.

D ��Configure an Amazon Route 53 failover routing policy. Create an Application Load Balancer (ALB) in each of the two Regions. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the ALB. Process the data in Amazon ECS.

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: The question requires a solution that minimizes latency for UDP-based data transmission  and   provides  rapid  failover  between  AWS   Regions.  Here's  why  option   B  is  the   best  choice:   1.  AWS  Global Accelerator: Optimizes traffic routing using AWS ��s global network, reducing latency by directing traffic to the nearest healthy endpoint.  It  also  enables  rapid  regional  failover  (within  seconds)  using  health  checks,  unlike  Route  53  (DNS-based),  which depends on DNS TTL delays. 2. Network Load Balancer (NLB): Supports UDP traffic (required by the devices), unlike Application Load Balancers (ALB), which only handle HTTP/HTTPS. 3. Amazon ECS with Fargate: Provides a serverless container platform to process data in real time. Containers are ideal for persistent, low-latency workloads compared to AWS Lambda (used in option A), which introduces cold-start delays for sporadic UDP traffic. Options C/D use ALBs (which don ��t support UDP), and option A relies on   Route  53   (slower  failover)  and   Lambda   (higher   latency  for   intermittent   requests).   Reference   Links:  -   [AWS   Global Accelerator](https://aws.amazon.com/global-accelerator/)                     -                      [Network                     Load                      Balancer
Features](https://aws.amazon.com/elastic load balancing/features/#Product_comparisons)           -           [ECS            Fargate           vs Lambda](https://aws.amazon.com/blogs/containers/when-to-use-aws-fargate-vs-aws-lambda/)

357.A solutions architect must migrate a Windows Internet Information Services (IIS) web application to AWS. The application currently relies on a file share  hosted  in the  user's on-premises  network-attached storage (NAS). The solutions architect  has proposed migrating the IIS web servers to Amazon EC2 instances in multiple Availability Zones that are connected to the storage solution, and configuring an Elastic Load Balancer attached to the instances. Which replacement to the on-premises file share is MOST resilient and durable?

A ��Migrate the file share to Amazon RDS.

B ��Migrate the file share to AWS Storage Gateway.

C ��Migrate the file share to Amazon FSx for Windows File Server.

D ��Migrate the file share to Amazon Elastic File System (Amazon EFS).

�𰸣�C

���� �� Correct Answer: C. Migrate the file share to Amazon FSx for Windows File Server. Detailed Explanation: - Option C is correct  because Amazon  FSx for Windows File Server  is a fully  managed service designed specifically for Windows-based file storage.  It  natively  supports  the  SMB   protocol  used  by  Windows/IIS  applications,  ensuring  compatibility.   It  automatically provides Multi-AZ redundancy (high availability) and replicates data across AZs, making it the most resilient and durable option for Windows workloads. It also integrates seamlessly with Active Directory. Why other options are incorrect: - A. Amazon RDS: This is a database service (for SQL Server, MySQL, etc.), not a file storage solution. It cannot replace a file share. - B. Storage Gateway: This is a hybrid storage tool for connecting on-premises apps to AWS storage. It ��s not a fully cloud-native replacement and adds complexity. - D. Amazon EFS: This is a Linux-focused NFS-based file system. While Windows can connect to EFS via a client, it ��s not natively optimized for Windows/IIS workloads and lacks built-in Multi-AZ redundancy for Windows environments. Reference Link: [Amazon FSx for Windows File Server](https://aws.amazon.com/fsx/windows/)

358.A company is deploying a new application on Amazon EC2 instances. The application writes data to Amazon Elastic Block Store (Amazon EBS) volumes. The company needs to ensure that all data that is written to t he EBS volumes is encrypted at rest. Which solution will meet this requirement?

A ��Create an IAM role that specifies EBS encryption. Attach the role to the EC2 instances.

B ��Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.

C ��Create an EC2 instance tag that has a key of Encrypt and a value of True. Tag all instances that require encryption at the EBS level.

D ��Create an AWS Key Management Service (AWS KMS) key policy that enforces EBS encryption in the account. Ensure that the key policy is active.

�𰸣�B

������ Correct Answer: B Detailed Explanation: To ensure data written to Amazon EBS volumes is encrypted at rest, you must create the EBS volumes as encrypted volumes and attach them to the EC2 instances. Here's why: - Option B is correct because EBS encryption is enabled at the volume level. When you create an EBS volume with encryption enabled, all data stored on it (including snapshots) is automatically encrypted using an AWS Key Management Service (KMS) key. This applies to both new data and  existing  data  copied  to the volume.  -  Why  other  options  are  incorrect:  -  Option  A:  IAM  roles  control  permissions,  not encryption settings. While roles might grant access to KMS keys, they don't enforce EBS encryption. - Option C: EC2 instance tags are for metadata (e.g., naming, categorization), not for enabling encryption features. - Option D: KMS key policies govern who can use a key, but they don ��t automatically encrypt EBS volumes. AWS account-level default encryption settings (separate from KMS policies) would need to be enabled to force encryption, which this option doesn ��t mention. Reference Link: [Amazon EBS Encryption](https://docs.aws.amazon.com/AWSEC2/latest/User guide/Ebs encryption.html)

359.A company  has  a web  application with  sporadic  usage  patterns.  There  is  heavy  usage  at  the  beginning  of  each  month, moderate usage at the start of each week, and unpredictable usage during the week. The application consists of a web server and a MySQL database server running inside the data center. The company would like to move the application to the AWS Cloud, and needs to select a cost-effective database platform that will not require database modifications. Which solution will meet these requirements?

A ��Amazon Dynamo db

B ��Amazon RDS for MySQL

C ��MySQL-compatible Amazon Aurora Serverless

D ��MySQL deployed on Amazon EC2 in an Auto Scaling group

�𰸣�C

�� �� ��  Correct  Answer  C  ��  MySQL-compatible  Amazon  Aurora  Serverless  Detailed  Explanation  The  company  needs  a cost-effective,  fully   managed  database  solution  that  automatically  scales  with   unpredictable  workloads  without   requiring code/database  modifications.  -  Option  C  (Aurora  Serverless)  is  ideal  because:  1.  Serverless  scaling:  It  automatically  adjusts database capacity based on real-time demand. During low usage (e.g., mid-week), costs drop as resources scale down. During spikes (e.g., month-start), it scales up instantly. 2. MySQL compatibility: Works with existing MySQL-based applications without code changes. 3. Fully managed: AWS handles maintenance, backups, and scaling, reducing operational overhead. - Why other options are  less suitable: - A (Dynamo db): A  NoSQL database  requiring significant code/database  redesign. -  B (RDS  MySQL): Requires manually scaling instance sizes, which is inefficient for sporadic usage. - D (EC2 + Auto Scaling): Managing MySQL on EC2 is complex (e.g., scaling storage, replication). Auto Scaling focuses on compute, not database performance. Reference Link [AWS Aurora Serverless Documentation](https://aws.amazon.com/rds/aurora/serverless/)

360.An image-hosting company stores its objects in Amazon S3 buckets. The company wants to avoid accidental exposure of the objects in the S3 buckets to the public. All S3 objects in the entire AWS account need to remain private. Which solution will meet these requirements?

A ��Use Amazon Guard duty to monitor S3 bucket policies. Create an automatic remediation action rule that uses an AWS Lambda function to remediate any change that makes the objects public.

B��Use AWS Trusted Advisor to find publicly accessible S3 buckets. Configure email notifications in Trusted Advisor when a change is detected. Manually change the S3 bucket policy if it allows public access.

C ��Use AWS Resource Access Manager to find publicly accessible S3 buckets. Use Amazon Simple Notification Service (Amazon SNS)  to  invoke  an  AWS   Lambda  function  when   a  change   is   detected.  Deploy  a  Lambda  function  that   programmatically re mediates the change.

D ��Use the S3 Block Public Access feature on the account level. Use AWS Organizations to create a service control policy (SCP) that prevents IAM users from changing the setting. Apply the SCP to the account.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The best solution is to use S3 Block Public Access at the account level combined with an SCP (Service Control Policy). Here's why this works: 1. S3 Block Public Access acts like a master switch that overrides any bucket policies or object permissions. When enabled at the account level, it automatically blocks all public access to every S3 bucket in the account, even if someone accidentally sets a bucket policy to public. 2. AWS Organizations SCP adds an extra layer of protection. The SCP acts as guardrails that prevent even IAM users (including admins) from modifying or disabling the S3 Block Public Access settings. This ensures nobody can accidentally or intentionally turn off this protection. Think of it like: - S3 Block Public Access = A padlock on all your buckets - SCP = A rule that prevents anyone from removing the padlock This combination provides automatic protection across all current and future buckets, and prevents human errors from compromising security. Other  options  are  less  effective:  -  Options  A/C   rely  on  detecting  changes  after  they  happen  -  Option  B  requires  manual

intervention  -  Only  Option  D  proactively  prevents  public  access  at  the  root  level.  Reference  Links:  -  [AWS  S3  Block  Public Access](https://docs.aws.amazon.com/AmazonS3/latest/user guide/access-control-block-public-access.html)             -              [AWS Organizations SCPs](https://docs.aws.amazon.com/organizations/latest/user guide/orgs_manage_policies_scps.html)

361.An ecommerce company  is experiencing an  increase  in  user traffic. The  company ��s  store  is  deployed  on Amazon  EC2 instances as a two-tier web application consisting of a web tier and a separate database tier. As traffic increases, the company notices that the architecture is causing significant delays in sending timely marketing and order confirmation email to users. The company wants to reduce the time it spends resolving complex email delivery issues and minimize operational overhead. What should a solutions architect do to meet these requirements?

A ��Create a separate application tier using EC2 instances dedicated to email processing.

B ��Configure the web instance to send email through Amazon Simple Email Service (Amazon SES).

C ��Configure the web instance to send email through Amazon Simple Notification Service (Amazon SNS).

D ��Create a separate application tier using EC2 instances dedicated to email processing. Place the instances in an Auto Scaling group.

�𰸣�B

�� �� �� Correct Answer:  B. Configure the web  instance to send email through Amazon Simple  Email Service  (Amazon SES). Detailed Explanation: The company is facing delays in sending emails due to the current EC2-based setup.  Using Amazon SES (Simple Email Service) is the best solution because SES is a dedicated email-sending service designed to handle large volumes of emails efficiently. By integrating SES, the web tier EC2 instances can offload email-sending tasks to SES, eliminating the need to manage email servers or troubleshoot email delivery issues manually. This reduces delays, minimizes operational overhead, and ensures reliable email delivery without scaling EC2 instances (as in Options A and D). Why not other options? - A/D: Creating a separate EC2 tier (with or without Auto Scaling) adds operational complexity and costs, as you still have to manage servers and scaling. - C: Amazon SNS (Simple Notification Service) is for sending notifications (e.g., SMS, mobile push) and isn ��t optimized for bulk transactional emails like order confirmations. Reference Link: [Amazon SES](https://aws.amazon.com/ses/)

362.A company has a business system that generates hundreds of reports each day. The business system saves the reports to a network share in CSV format. The company needs to store this data  in the AWS Cloud  in  near-real time for analysis. Which solution will meet these requirements with the LEAST administrative overhead?

A ��Use AWS Data sync to transfer the files to Amazon S3. Create a scheduled task that runs at the end of each day.

B ��Create an Amazon S3 File Gateway. Update the business system to use a new network share from the S3 File Gateway.

C �� Use AWS Data sync to transfer the files to Amazon S3. Create an application that uses the Data sync API in the automation workflow.

D��Deploy an AWS Transfer for SFTP endpoint. Create a script that checks for new files on the network share and uploads the new files by using SFTP.

�𰸣�B

���� �� Correct Answer B Detailed Explanation The best solution is B because Amazon S3 File Gateway allows the company to keep using its existing network share setup while automatically saving files to Amazon S3 in near-real time. Here ��s why: - S3 File

Gateway acts like a local network file share but backs up data directly to Amazon S3. The company only needs to update the business  system  to   use  the   new  S3   File  Gateway  share �� no   code  changes,  scripts,   or  scheduled  tasks  are   required.  - Near-real-time  storage:  Files  are  uploaded  to  Amazon  S3  as  soon  as  they  are  saved  to  the  network  share,  meeting  the requirement for timely data availability. - Least administrative overhead: No need to build automation workflows (like in Option C), manage scripts (like in Option D), or rely on delayed daily transfers (like in Option A). Other options involve extra steps (like scripting, API integration, or manual scheduling), which increase complexity and administrative effort. Reference Links - [Amazon S3            File             Gateway](https://aws.amazon.com/storage gateway/file/s3/)            -             [AWS            Storage             Gateway Overview](https://docs.aws.amazon.com/storage gateway/latest/user guide/What is storage gateway.html)

363.A company is storing petabytes of data in Amazon S3 Standard. The data is stored in multiple S3 buckets and is accessed with varying frequency. The company does not know access patterns for all the data. The company needs to implement a solution for each  S3  bucket  to  optimize the  cost  of S3  usage.  Which  solution  will  meet  these  requirements  with  the  MOST  operational efficiency?

A ��Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Intelligent-Tiering.

B ��Use the S3 storage class analysis tool to determine the correct tier for each object in the S3 bucket. Move each object to the identified storage tier.

C ��Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Glacier Instant Retrieval.

D��Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 One Zone-Infrequent Access (S3 One Zone-IA).

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best choice here is S3 Intelligent-Tiering (option A). Here's why: - Unknown  access patterns: The company doesn ��t  know how often data is accessed. S3  Intelligent-Tiering automatically moves objects  between four tiers (Frequent, Infrequent, Archive Instant, Deep Archive) based on their access patterns. No manual analysis or   guesswork is needed. - Cost-optimized: It ensures objects are stored in the cheapest tier possible for their usage. If an object isn �� t accessed for 30 days, it moves to the Infrequent Access tier, saving costs. - Zero operational overhead: Unlike options B, C, or D,    Intelligent-Tiering  requires  no  manual analysis  (B),  no  risk  of  mis classifying  rarely  accessed data (C/D), and  no downtime for   retrieval (C/D). Other options fall short: - Option B (Storage Class Analysis + manual moves) is time-consuming for petabytes of  data. - Option C/D (Glacier/One Zone-IA) force all data into a single tier, which might be wrong for some objects (e.g., frequently   accessed      data      moved      to       Glacier      would      cost       more      in       retrieval      fees).      Reference       Link:      [AWS      S3   Intelligent-Tiering](https://docs.aws.amazon.com/AmazonS3/latest/user guide/intelligent-tiering.html)

364.A company uses Amazon EC2 instances and AWS Lambda functions to run its application. The company has VPCs with public subnets and  private subnets  in  its AWS account. The  EC2  instances  run  in a  private subnet in one of the VPCs. The  Lambda functions need direct network access to the EC2 instances for the application to work. The application will run for at least 1 year. The company expects the  number of  Lambda functions that the application uses to increase during that time. The company wants to maximize its savings on all application resources and to keep network latency between the services low. Which solution will meet these requirements?

A ��Purchase an EC2 Instance Savings Plan Optimize the Lambda functions �� duration and  memory usage and the number of invocations. Connect the Lambda functions to the private subnet that contains the EC2 instances.

B �� Purchase  an  EC2  Instance  Savings  Plan  Optimize  the  Lambda  functions'  duration  and  memory  usage,  the  number  of invocations, and the amount of data that is transferred. Connect the Lambda functions to a public subnet in the same VPC where

the EC2 instances run.

C ��Purchase a Compute Savings Plan. Optimize the Lambda functions �� duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to the private subnet that contains the EC2 instances.

D��Purchase a Compute Savings Plan. Optimize the Lambda functions �� duration and memory usage, the number of invocations, and the amount of data that is transferred. Keep the Lambda functions in the Lambda service VPC.

�𰸣�C

���� �� Correct Answer C Detailed Explanation The company needs to connect Lambda functions to EC2 instances in a private subnet while  minimizing costs and latency. Here ��s why Option C  is the best choice:  1. Compute Savings  Plan: - A Compute Savings Plan provides flexibility by covering both EC2 instances and Lambda usage, unlike the EC2 Instance Savings Plan (which only applies to EC2). This maximizes savings as Lambda usage scales over time. - The EC2 Instance Savings Plan (Options A/B) locks discounts to specific  EC2  instance families,  but  Lambda  usage wouldn ��t  benefit  from this.  2.  Lambda  Optimization: - Optimizing duration,  memory, invocations, and data transfer  reduces  Lambda  costs.  Data transfer costs  matter  here  because Lambda - to - EC2 communication incurs charges if data crosses Availability Zones or regions. Including this optimization (as in C) saves more money than excluding it (as in A). 3. Network Setup: - Lambda functions must be attached to the same VPC ��s private subnet as the EC2 instances for direct, low - latency access. - Public subnets (Option B) are unnecessary and less secure. The Lambda service VPC (Option D) is unrelated to the company ��s VPC, so it wouldn ��t allow access to the  EC2 instances. Key Takeaways - Use Compute Savings Plans for mixed EC2 + Lambda workloads. - Always attach Lambda to the private subnet in the same VPC  as  backend  resources.  -  Optimize  all  cost  drivers  (duration,  memory,  invocations,  and  data  transfer)  for  Lambda. Reference   Links  -   [AWS  Compute  Savings   Plans](https://aws.amazon.com/savings plans/compute  -   pricing/)  -   [Configuring Lambda in a VPC](https://docs.aws.amazon.com/lambda/latest/dg/configuration - vpc.html)

365.A  solutions  architect  needs  to  allow  team  members  to  access  Amazon  S3  buckets  in  two  different  AWS  accounts:  a development account and a production account. The team currently has access to S3 buckets in the development account by using  unique  IAM  users  that  are  assigned  to  an  IAM  group  that  has  appropriate  permissions  in  the  account.  The  solutions architect  has created an  IAM  role in the  production account. The  role  has a  policy that grants access to an S3  bucket in the production account. Which solution will meet these requirements while complying with the principle of least privilege?

A ��Attach the Administrator Access policy to the development account users.

B ��Add the development account as a principal in the trust policy of the role in the production account. C ��Turn off the S3 Block Public Access feature on the S3 bucket in the production account.

D ��Create a user in the production account with unique credentials for each team member.

�𰸣�B

������Correct Answer B Detailed Explanation Option B is correct because it allows cross-account access securely using IAM roles. By  adding  the  development  account  as  a  principal  in  the  trust  policy  of  the  production  account's  IAM  role,  users  in  the development account can assume this role temporarily. This approach: 1. Follows least privilege: The role only grants S3 bucket access permissions (no excessive rights). 2. Avoids credential duplication: Team members don��t need separate credentials for the production account. 3. Centralizes control: Permissions are managed via the role ��s policy, not individual users. Other options fail because: - A: Administrator Access grants unlimited permissions (violates least privilege). - C: Disabling S3 Block Public Access exposes the bucket to public risks. -  D: Creating users in the  production account adds management overhead and duplicates credentials. How it works: 1. The production account ��s IAM role has a trust policy stating the development account ��s users can

assume this role. 2. Users in the development account request temporary credentials via Assume role to access the production account ��s  S3  bucket.  3. The  role ��s  permissions  policy  defines  exactly what  S3 actions are allowed.  Reference  Link  [AWS Cross-Account Access with Roles](https://docs.aws.amazon.com/IAM/latest/User guide/tutorial_cross-account-with-roles.html)

366.A company wants to use an Amazon RDS for Postgresql DB cluster to simplify time-consuming database administrative tasks for production database workloads. The company wants to ensure that its database is highly available and will provide automatic failover support in most scenarios in less than 40 seconds. The company wants to offload reads off of the primary instance and keep costs as low as possible. Which solution will meet these requirements?

A �� Use an Amazon RDS  Multi-AZ  DB instance deployment. Create one read replica and point the read workload to the read replica.

B �� Use  an Amazon  RDS  Multi-AZ  DB  duster  deployment  Create two  read  replicas  and  point the  read  workload to the  read replicas.

C ��Use an Amazon RDS Multi-AZ DB instance deployment. Point the read workload to the secondary instances in the Multi-AZ pair.

D ��Use an Amazon RDS Multi-AZ DB cluster deployment Point the read workload to the reader endpoint.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The best solution is to use an Amazon RDS Multi-AZ DB cluster deployment and route read workloads to the reader endpoint. Here ��s why: 1. High Availability   �� Fast Failover (��40 seconds): - A Multi-AZ DB cluster  maintains two  readable  standby  instances  across  different  Availability  Zones.  If  the  primary  instance  fails,  automatic failover to a standby typically completes in under 35 seconds, meeting the 40-second requirement. 2. Offloading Read Workloads:
- The cluster ��s reader endpoint automatically distributes read traffic across the two standby instances. This offloads reads from the primary instance without needing separate read replicas, reducing costs. 3. Cost Efficiency: - Unlike solutions that require creating separate read replicas (Options A/B), the Multi-AZ DB cluster ��s built-in standby instances handle reads at no extra cost beyond the cluster itself. Why Other Options Fail: - Options A/B: Adding read replicas increases costs and complexity. Multi-AZ DB instances (non-cluster) also have slower failover (~60-120 seconds). - Option C: Secondary instances in a traditional Multi-AZ DB instance (non-cluster) cannot serve read traffic��they ��re standby-only. Reference Link: [Amazon RDS Multi-AZ DB Cluster Deployments](https://docs.aws.amazon.com/Amazon rds/latest/User guide/multi-az-db-clusters-concepts.html)

367.A company runs a highly available SFTP service. The SFTP service uses two Amazon EC2 Linux instances that run with elastic IP  addresses to  accept traffic from trusted  IP  sources  on  the  internet. The  SFTP service  is  backed  by  shared  storage  that  is attached to the instances.  User accounts are created and managed as  Linux users in the SFTP servers. The company wants a serverless option that provides  high IOPS performance and highly configurable security. The company also wants to maintain control over user permissions. Which solution will meet these requirements?

A ��Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume. Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses. Attach the EBS volume to the SFTP service endpoint. Grant users access to the SFTP service.

B��Create an encrypted Amazon Elastic File System (Amazon EFS) volume. Create an AWS Transfer Family SFTP service with elastic IP addresses and a VPC endpoint that has internet-facing access. Attach a security group to the endpoint that allows only trusted IP addresses. Attach the EFS volume to the SFTP service endpoint. Grant users access to the SFTP service.

C �� Create an Amazon S3  bucket with default encryption enabled. Create an AWS Transfer  Family SFTP service with a  public

endpoint that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service.

D �� Create an Amazon  S3  bucket  with  default  encryption  enabled.  Create  an AWS Transfer  Family  SFTP  service  with  a  VPC endpoint that has internal access in a private subnet. Attach a security group that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service.

�𰸣�B

������ Correct Answer: B Explanation: The company needs a serverless SFTP solution with high IOPS, configurable security, and control over user permissions. Here's why Option B is the best choice: 1. Amazon EFS (Elastic File System): - EFS provides shared, scalable storage with high IOPS (input/output operations per second) performance. It automatically scales throughput based on file  system  size  and  can  handle  concurrent  access  from  multiple  servers,  making  it  ideal  for  shared  storage.  -  EFS  supports encryption at rest and integrates with AWS Transfer  Family, which  is fully managed  (serverless). 2. AWS Transfer  Family SFTP Service: - AWS Transfer Family is a serverless SFTP service that eliminates the need to manage EC2 instances. It supports VPC endpoints, which allow secure, private connectivity within the VPC while still  being accessible over the internet via elastic  IP addresses.  -  The   security  group  attached  to  the  VPC   endpoint  restricts  access  to  trusted   IP  addresses,   ensuring  secure internet-facing access. 3. User Permissions: - EFS uses POSIX-compliant file permissions (like Linux user/group permissions). This allows  the  company  to   maintain  control   over   user  access  at  the  file/directory   level,   similar  to  their   current   Linux  user management. Why Other Options Fail: - Option A: EBS volumes cannot be shared across multiple servers simultaneously, making them unsuitable for shared storage. - Options C   �� D: Amazon S3 is not optimized for high IOPS (it ��s better for large objects). While S3 can be used with Transfer Family, it requires mapping users to S3 paths via IAM policies, which is less granular than POSIX  permissions  (used  in  EFS).  S3  also  lacks  native  Linux  user/group  permission  controls.  Reference  Links:  -  [AWS  Transfer Family        +        EFS        Integration](https://docs.aws.amazon.com/transfer/latest/user guide/efs.html)         -        [Amazon         EFS Performance](https://aws.amazon.com/efs/performance/)

368.A company is developing a new machine learning (ML) model solution on AWS. The models are developed as independent micro services that fetch approximately 1 GB of model data from Amazon S3 at startup and load the data into memory. Users access the models through an asynchronous API. Users can send a request or a batch of requests and specify where the results should  be sent. The company  provides  models to  hundreds of  users. The  usage  patterns for the  models  are  irregular. Some models could be unused for days or weeks. Other models could receive batches of thousands of requests at a time. Which design should a solutions architect recommend to meet these requirements?

A ��Direct the requests from the API to a Network Load Balancer (NLB). Deploy the models as AWS Lambda functions that are invoked by the NLB.

B �� Direct the  requests from the API to an Application  Load  Balancer  (ALB).  Deploy the  models as Amazon  Elastic Container Service (Amazon ECS) services that read from an Amazon Simple Queue Service (Amazon SQS) queue. Use AWS App Mesh to scale the instances of t he ECS cluster based on the SQS queue size.

C �� Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as AWS Lambda functions that  are  invoked  by SQS  events.  Use AWS Auto Scaling to  increase the  number  of vCPUs for the  Lambda functions based on the SQS queue size.

D ��Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue. Enable AWS Auto Scaling on Amazon ECS for both the cluster and copies of the service based on the queue size.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The scenario requires handling irregular, bursty traffic with models that load 1GB of data at startup. Here ��s why Option D works best: 1. Asynchronous Processing: Using Amazon SQS decouples the API from the models, allowing requests to queue during sudden spikes. 2. Efficient Scaling: ECS services can autoscale based on the SQS queue size. When  requests  pile  up, AWS Auto  Scaling adds  more  ECS tasks  (containers),  each  loading the  1GB  model  data  once  at startup and processing multiple queued requests. This avoids repeated cold starts. 3. Cost Optimization: ECS tasks scale to zero when idle, saving costs during inactive periods. Lambda (Options A/C) would reload the 1GB data on every cold start, increasing latency and costs. 4. Resource Control: ECS allows allocating sufficient memory/CPU for the 1GB model data, unlike Lambda ��s fixed memory tiers. Why not other options: - A/C (Lambda): Frequent cold starts with 1GB data reloads hurt performance and cost. - B (App Mesh): App Mesh isn ��t  used for scaling; ECS auto scaling based on SQS is simpler and direct.  Reference:  [ECS Auto scaling   with   SQS](https://docs.aws.amazon.com/Amazon ecs/latest/developer guide/service-auto scaling.html)   [SQS   as   a Scaling Trigger](https://aws.amazon.com/blogs/compute/auto scaling-amazon-ecs-services-based-on-sqs-queue-length/)

369.A company is running a custom application on Amazon EC2 On-Demand Instances. The application has front end nodes that need to run 24 hours a day, 7 days a week and backend nodes that need to run only for a short time based on workload. The number of backend nodes varies during the day. The company needs to scale out and scale in more instances based on workload. Which solution will meet these requirements MOST cost-effectively?

A ��Use Reserved Instances for the front end nodes. Use AWS Fargate for the backend nodes.

B ��Use Reserved Instances for the front end nodes. Use Spot Instances for the backend nodes.

C ��Use Spot Instances for the front end nodes. Use Reserved Instances for the backend nodes.

D ��Use Spot Instances for the front end nodes. Use AWS Fargate for the backend nodes.

�𰸣�B

������ Correct Answer B Detailed Explanation For the front end nodes running 24/7, Reserved Instances (RIs) are ideal. RIs offer significant cost savings (up to 70% vs. On-Demand) for predictable, long-term workloads because you commit to a 1- or 3-year term. For the backend nodes (short-term, variable workload), Spot Instances are the most cost-effective. Spot Instances let you use unused AWS capacity at up to 90% off On-Demand prices. While they can be interrupted, this is acceptable for temporary workloads that can tolerate interruptions. Why not other options? - A/D (Fargate): Fargate is serverless but typically costs more than Spot Instances for compute-heavy tasks. - C/D (Spot for front end): Spot is risky for 24/7 front end nodes due to potential interruptions.  -  C  (RIs  for  backend):  RIs  are  wasteful  for  short-term,  variable  workloads.  Reference  Links  -  [AWS  Reserved Instances](https://aws.amazon.com/ec2/pricing/reserved-instances/)                             -                              [AWS                              Spot
Instances](https://aws.amazon.com/ec2/spot/)

370.A company uses high block storage capacity to runs its workloads on premises. The company's daily peak input and output transactions per second are not more than 15,000 IOPS. The company wants to migrate the workloads to Amazon EC2 and to provision disk performance independent of storage capacity. Which Amazon Elastic Block Store (Amazon EBS) volume type will meet these requirements MOST cost-effectively?

A ��GP2 volume type

B ��io2 volume type C ��GP3 volume type

D ��io1 volume type

�𰸣�C

������ Correct Answer: C. GP3 volume type Detailed Explanation: The company needs an EBS volume type where performance (IOPS/throughput) is independent of storage capacity to avoid over provisioning storage just for higher performance. Here ��s why GP3 is the best choice: 1. GP3 (General Purpose SSD v3): - Allows independent scaling of IOPS and throughput without increasing storage size. - Baseline performance: 3,000 IOPS and 125 MiB/s throughput. - Can provision up to 16,000 IOPS and 1,000 MiB/s (for an additional cost), which easily covers the 15,000 IOPS requirement. - Cost-effective compared to io1/io2, as you pay only for the extra IOPS/throughput needed. 2. Why not GP2/io1/io2? - GP2: IOPS is tied to storage size (3 IOPS/GB). To get 15,000 IOPS, you��d need 5,000 GB of storage (15,000   �� 3 = 5,000), which is wasteful if the workload doesn��t need that much capacity.
- io1/io2: Designed for ultra-high IOPS (up to 256,000+). Overkill for 15,000 IOPS and more expensive than GP3. GP3 provides the required    performance   flexibility    at   the    lowest    cost   for   this    scenario.    Reference    Links:    -    [Amazon    EBS    Volume Types](https://docs.aws.amazon.com/AWSEC2/latest/User guide/ebs-volume-types.html)                   -                   [GP3                   vs. GP2](https://aws.amazon.com/ebs/general-purpose/)

371.A company needs to store data from its healthcare application. The application ��s data frequently changes. A new regulation requires audit access at all levels of the stored data. The company hosts the application on an on-premises infrastructure that is running out of storage capacity. A solutions architect must securely migrate the existing data to AWS while satisfying the new regulation. Which solution will meet these requirements?

A ��Use AWS Data sync to move the existing data to Amazon S3. Use AWS Cloud trail to log data events.

B ��Use AWS Snow cone to move the existing data to Amazon S3. Use AWS Cloud trail to log management events.

C ��Use Amazon S3 Transfer Acceleration to move the existing data to Amazon S3. Use AWS Cloud trail to log data events.

D ��Use AWS Storage Gateway to move the existing data to Amazon S3. Use AWS Cloud trail to log management events.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The correct solution is A because: 1. AWS Data sync is optimized for frequently changing data, as it provides incremental transfer capabilities. This makes it ideal for migrating and continuously syncing data from on-premises to Amazon S3, especially when storage capacity is limited locally. 2. AWS Cloud trail Data Events are required to audit object-level operations in Amazon S3 (e.g., Put object, Deleteobject). The regulation mandates auditing at all levels, which includes both management events (account activity) and data events (object operations). Why other options are incorrect: - B (Snow cone  +   Management   Events):   Snow cone   is  for  offline   bulk  transfers,   not  suitable  for  frequently  changing   data. Management events alone don't audit S3 object operations. - C (Transfer Acceleration): While Transfer Acceleration speeds up transfers,  it  lacks  Data sync's  incremental  sync  capability for  ongoing changes. -  D  (Storage  Gateway +  Management  Events): Storage Gateway creates a hybrid storage solution but isn't optimized for full migration. Management events miss object-level auditing.        Reference        Links:         -        [AWS         Data sync](https://aws.amazon.com/data sync/)        -        [Cloud trail         Data Events](https://docs.aws.amazon.com/aws cloud trail/latest/user guide/logging-data-events.html)

372.A solutions  architect  is  implementing  a  complex Java  application with  a  MySQL  database. The Java  application  must  be deployed on Apache Tomcat and must be highly available. What should the solutions architect do to meet these requirements?

A ��Deploy the application in AWS Lambda. Configure an Amazon API Gateway API to connect with the Lambda functions.

B �� Deploy the application by using AWS  Elastic  Beanstalk. Configure a load-balanced environment and a rolling deployment

policy.

C ��Migrate the database to Amazon Elastic ache. Configure the Elastic ache security group to allow access from the application.

D ��Launch an Amazon EC2 instance. Install a MySQL server on the EC2 instance. Configure the application on the server. Create an AMI. Use the AMI to create a launch template with an Auto Scaling group.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The correct answer is B because AWS Elastic Beanstalk simplifies deploying and scaling Java/Tomcat applications while ensuring high availability. Here ��s why: 1. High Availability: Elastic Beanstalk automatically configures a load balancer and distributes traffic across multiple instances in different Availability Zones (AZs). If one instance fails, traffic  reroutes to  healthy  instances.  2.  Rolling  Deployments:  A  rolling  deployment  policy  updates  instances  in  batches, ensuring the application stays available during updates. 3. Managed Infrastructure: Elastic Beanstalk handles server provisioning,
scaling, and monitoring, reducing operational overhead. Why other options are wrong: - A: Lambda is serverless and unsuitable for Tomcat-based Java applications. - C:  Elastic ache  is a caching service  (Redis/Memcached),  not a  MySQL  replacement. -  D: Running  MySQL on  EC2  lacks  automated  HA features  (use Amazon  RDS  instead).  Manual scaling  is  error-prone compared to Elastic  Beanstalk  �� s  automation.  Reference  Links:  [AWS  Elastic  Beanstalk](https://aws.amazon.com/elastic beanstalk/)  [High Availability in Elastic Beanstalk](https://docs.aws.amazon.com/elastic beanstalk/latest/dg/using-features.managing.ha.html)

373.A serverless  application  uses Amazon API  Gateway, AWS  Lambda,  and Amazon  Dynamo db.  The  Lambda function  needs permissions to read and write to the Dynamo db table. Which solution will give the Lambda function access to the Dynamo db table MOST securely?

A ��Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user that allows read and write access to the Dynamo db table. Store the access_key_id and secret_access_key parameters as part of the Lambda environment variables. Ensure that other AWS users do not have read and write access to the Lambda function configuration.

B ��Create an IAM role that includes Lambda as a trusted service. Attach a policy to the role that allows read and write access to the Dynamo db table. Update the configuration of the Lambda function to use the new role as the execution role.

C ��Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user that allows read and write access to the Dynamo db table. Store the access_key_id and secret_access_key parameters in AWS Systems Manager Parameter Store as secure string parameters. Update the Lambda function code to retrieve the secure string parameters before connecting to the Dynamo db table.

D ��Create an IAM role that includes Dynamo db as a trusted service. Attach a policy to the role that allows read and write access from the Lambda function. Update the code of the Lambda function to attach to the new role as an execution role.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best and most secure way to grant a Lambda function access to Dynamo db is by using an IAM execution role. Here's why: 1. IAM Roles vs. Users: - IAM roles provide temporary credentials automatically rotated by AWS, while IAM users require long - term access keys (Option A and C). Hardcoding credentials (even in environment variables  or   Parameter  Store)  creates  security   risks.   2.  Trust   Relationship:   -  The   role   must  trust  the   Lambda   service (lambda.amazon aws.com) as a trusted entity (Option B). Option D incorrectly lists Dynamo db as the trusted service, which is not required since  Dynamo db  is the  resource  being accessed,  not the service assuming the  role. 3.  Least  Privilege: - Attaching a policy with only Dynamo db read/write permissions (no extra privileges) follows security best practices. 4. Automatic Credential Management: -  Lambda  automatically  assumes  the  execution  role's  permissions  without  manual  credential  handling  (unlike

Options     A/C      where     credentials      must      be     retrieved/stored).      Reference      Links:      -      [AWS     Lambda      Execution Role](https://docs.aws.amazon.com/lambda/latest/dg/lambda      -      intro      -      execution      -       role.html)      -      [IAM       Best Practices](https://docs.aws.amazon.com/IAM/latest/User guide/best - practices.html)

374.The following IAM policy is attached to an IAM group. This is the only policy applied to the group.     What are the effective IAM permissions of t his policy for group members?

A��Group members are permitted any Amazon EC2 action within the us-east-1 Region. Statements after the Allow permission are not applied.

B ��Group members are denied any Amazon EC2 permissions in the us-east-1 Region unless they are logged in with multi-factor authentication (MFA).

C ��Group members are allowed the ec2:Stop instances and ec2:Terminate instances permissions for all Regions when logged in with multi-factor authentication (MFA). Group members are permitted any other Amazon EC2 action.

D ��Group members are allowed the ec2:Stop instances and ec2:Terminate instances permissions for the us-east-1 Region only when logged in with multi-factor authentication (MFA). Group members are permitted any other Amazon EC2 action within the us-east-1 Region.

�𰸣�D

�� �� ��   Correct   Answer   D    Detailed   Explanation   The   IAM   policy    has   two   statements:   1.    Deny   Statement:   Blocks `ec2:Stop instances` and `ec2:Terminate instances` in the `us-east-1` Region unless the user is authenticated with MFA. 2. Allow Statement: Permits all EC2 actions (`ec2:*`) in the `us-east-1` Region. Key Points: - IAM policies use an implicit deny by default, but explicit  Deny always overrides Allow. -  For `Stop instances` and `Terminate instances`  in `us-east-1`, the  Deny applies only when MFA is not used. If MFA is used, the Deny is skipped, and the Allow grants permission. - All other EC2 actions in `us-east-1` are permitted regardless of MFA (due to the Allow statement). - The policy applies only to `us-east-1` (explicitly specified in both statements). Why Other Options Are Incorrect: - A: Incorrect. The Allow is not negated; MFA is required only for Stop/Terminate.
-  B:  Incorrect.  Only  Stop/Terminate  are  restricted  by  MFA,  not  all  EC2  actions.  -  C:  Incorrect.  The  policy  restricts  actions  to `us-east-1`,              not              all              Regions.              Reference              Links              -              [IAM               Policy              Evaluation Logic](https://docs.aws.amazon.com/IAM/latest/User guide/reference_policies_evaluation-logic.html)  -  [Using  MFA  with   IAM Policies](https://docs.aws.amazon.com/IAM/latest/User guide/id_credentials_mfa_configure-api-require.html)

375.A company has developed a new video game as a web application. The application is in a three-tier architecture in a VPC with Amazon RDS for MySQL in the database layer. Several players will compete concurrently online. The game ��s developers want to display a top-10 scoreboard in near-real time and offer the ability to stop and restore the game while preserving the current scores. What should a solutions architect do to meet these requirements?

A ��Set up an Amazon Elastic ache for Memcached cluster to cache the scores for the web application to display.

B ��Set up an Amazon Elastic ache for Redis cluster to compute and cache the scores for the web application to display.

C �� Place  an  Amazon  Cloud front  distribution  in  front  of  the  web  application  to  cache  the  scoreboard  in  a  section  of  the application.

D ��Create a read replica on Amazon RDS for MySQL to run queries to compute the scoreboard and serve the read traffic to the web application.

�𰸣�B

������ Correct Answer: B Detailed Explanation: To meet the requirements of displaying a near-real-time top-10 scoreboard and preserving scores during pauses, Amazon Elastic ache for Redis is the best choice. Here's why: 1. Real-Time Leaderboards: Redis supports sorted sets  (a data structure) that can automatically  rank scores  in  real time. This  makes  it far  more  efficient than querying a relational database (like RDS) for frequent score updates and sorting. 2. Data Persistence:  Redis offers persistence options (snapshot ting or AOF), ensuring scores aren ��t lost if the game is paused or the cache restarts. Memcached (Option A) lacks built-in persistence. 3. High Concurrency: Redis handles high read/write traffic from multiple players better than repeatedly querying an  RDS database  (Option  D), which would  introduce  latency and strain the database. 4. Caching vs. Static Content: Cloud front (Option C) caches static content (e.g., images),  not dynamic data  like a constantly  updating scoreboard. Why not other options? - A (Memcached): No sorted sets or persistence. - C (Cloud front): Unsuitable for dynamic data. - D (RDS Read Replica): Relational databases are slower for real-time ranking and frequent updates. Reference Links: - [Amazon Elastic ache for Redis            Use             Cases](https://aws.amazon.com/elastic ache/redis/)             -             [Redis            Sorted             Sets             for Leaderboards](https://redis.io/docs/data-types/sorted-sets/)

376.An ecommerce company wants to use machine learning (ML) algorithms to build and train models. The company will use the models to visualize complex scenarios and to detect trends in customer data. The architecture team wants to integrate its ML models with a reporting platform to analyze the augmented data and use the data directly in its business intelligence dashboards. Which solution will meet these requirements with the LEAST operational overhead?

A ��Use AWS Glue to create an ML transform to build and train models. Use Amazon Open search Service to visualize the data.

B ��Use Amazon Sage maker to build and train models. Use Amazon Quick sight to visualize the data.

C �� Use  a  pre-built  ML  Amazon  Machine  Image  (AMI)  from  the  AWS  Marketplace  to  build  and  train  models.  Use  Amazon Open search Service to visualize the data.

D ��Use Amazon Quick sight to build and train models by using calculated fields. Use Amazon Quick sight to visualize the data.

�𰸣�B

������ Correct Answer B �� Use Amazon Sage maker to build and train models. Use Amazon Quick sight to visualize the data. Detailed Explanation The best solution is B because it uses fully managed AWS services with minimal operational overhead: 1. Amazon Sage maker is a fully managed ML service. It handles model building, training, and deployment automatically, eliminating the need to manage servers or infrastructure. 2. Amazon Quick sight is a serverless BI tool that integrates directly with Sage maker. It can analyze augmented data and create dashboards without requiring manual data transfers or infrastructure setup. Other options are less optimal: - A/C require manual ML setup (AWS Glue ML transforms are limited, and pre-built AMIs need server management). - D uses Quick sight calculated fields, which are too basic for complex ML tasks. - A/C/ D also use Open search or custom setups, which add operational complexity compared to the serverless Sage maker+Quick sight combo. Reference Links - [Amazon Sage maker](https://aws.amazon.com/sage maker/) - [Amazon Quick sight](https://aws.amazon.com/quick sight/)

377.A company is running its production and nonproduction environment workloads in multiple AWS accounts. The accounts are in an organization in AWS Organizations. The company needs to design a solution that will prevent the modification of cost usage tags. Which solution will meet these requirements?

A ��Create a custom AWS Config rule to prevent tag modification except by authorized principals.

B ��Create a custom trail in AWS Cloud trail to prevent tag modification.

C ��Create a service control policy (SCP) to prevent tag modification except by authorized principals.

D ��Create custom Amazon Cloud watch logs to prevent tag modification.

�𰸣�C

������ Correct Answer C Detailed Explanation The correct solution is to use a Service Control Policy (SCP). SCPs are part of AWS Organizations and allow you to centrally control permissions across all accounts in your organization. They act as a guardrail to restrict what actions users and roles can perform, even if their  IAM policies allow it. By creating an SCP that explicitly denies modifications  to  cost  usage  tags  (e.g.,  blocking  the  `Tag resources`  or  `Un tag resources`  API  actions)  except  for  authorized principals, you ensure that no one��accidentally or maliciously��can alter these tags, regardless of their IAM permissions. Other options are incorrect because: - A (AWS Config Rule): Config rules only detect non-compliant resources; they cannot prevent tag modifications. - B (Cloud trail Trail): Cloud trail logs API activity but does not block actions. - D (Cloud watch Logs): Cloud watch is for  monitoring  and  logging,  not  enforcement.  SCPs  are  the  only  way  to  enforce  organization-wide  restrictions  proactively. Reference                          Links                           -                           [AWS                           Service                          Control                           Policies
(SCPs)](https://docs.aws.amazon.com/organizations/latest/user guide/orgs_manage_policies_scps.html)       -       [Tagging       AWS Resources](https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html)

378.A company hosts its application in the AWS Cloud. The application runs on Amazon EC2 instances behind an Elastic Load Balancer in an Auto Scaling group and with an Amazon Dynamo db table. The company wants to ensure the application can be made available in another aws Region with minimal downtime. What should a solutions architect do to meet these requirements with the LEAST amount of downtime?

A ��Create an Auto Scaling group and a load balancer in the disaster recovery Region. Configure the Dynamo db table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.

B ��Create an AWS Cloud formation template to create EC2 instances, load balancers, and Dynamo db tables to be launched when needed Configure DNS failover to point to the new disaster recovery Region's load balancer.

C��Create an AWS Cloud formation template to create EC2 instances and a load balancer to be launched when needed. Configure the Dynamo db table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.

D ��Create an Auto Scaling group and load balancer in the disaster recovery Region. Configure the Dynamo db table as a global table. Create an Amazon Cloud watch alarm to trigger an AWS Lambda function that updates Amazon Route 53 pointing to the disaster recovery load balancer.

�𰸣�A

������ Correct Answer: A Detailed Explanation: To minimize downtime during a regional failover, the best approach is to have the disaster  recovery  (DR)  infrastructure  pre-provisioned  and the  database  replicated  in  real-time.  1.  Auto  Scaling   ��  Load Balancer in DR Region: By creating an Auto Scaling group and load balancer in the DR region upfront, EC2 instances can scale automatically  during a failover, avoiding  delays from  manual  resource  provisioning.  2.  Dynamo db  Global  Tables:  Configuring Dynamo db as a global table ensures real-time, cross-region replication of data. This means the DR region ��s application can immediately access  up-to-date  data without  manual  synchronization.  3.  DNS  Failover  with  Route  53:  Using  DNS  failover  (via Amazon Route 53) allows traffic to be redirected quickly to the DR region ��s load balancer. Route 53 health checks can detect outages  and  automate  the  switch,  minimizing  downtime.  Why  other  options  are   less   ideal:  -  Option  B/C:   Relying  on Cloud formation to create resources during a failure adds downtime (e.g., waiting for EC2/Dynamo db provisioning). Dynamo db global tables must exist beforehand for real-time replication. - Option D: Using a Cloud watch alarm + Lambda adds complexity and latency compared to  Route 53 ��s  native  DNS failover, which  is faster and simpler. Reference  Links: -  [Dynamo db Global

Tables](https://docs.aws.amazon.com/amazon dynamo db/latest/developer guide/Global tables.html)      -        [Route      53        DNS Failover](https://docs.aws.amazon.com/Route53/latest/Developer guide/dns-failover.html)

379.A company needs to migrate a MySQL database from its on-premises data center to AWS within 2 weeks. The database is 20 TB in size. The company wants to complete the migration with  minimal downtime. Which solution will  migrate the database MOST cost-effectively?

A ��Order an AWS Snowball Edge Storage Optimized device. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes. Send the Snowball Edge device to AWS to finish the migration and continue the ongoing replication.

B��Order an AWS Snowmobile vehicle. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to  migrate the database with  ongoing changes. Send the Snowmobile vehicle  back to AWS to finish the  migration  and continue the ongoing replication.

C ��Order an AWS Snowball Edge Compute Optimized with GPU device. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with ongoing changes. Send the Snowball device to AWS to finish the migration and continue the ongoing replication

D ��Order a 1 GB dedicated AWS Direct Connect connection to establish a connection with the data center. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes.

�𰸣�A

�� �� ��  Correct  Answer:  A  Detailed  Explanation:  1.  Why  A  is  Correct:  -  Snowball  Edge  Storage  Optimized  is  designed  for large-scale data transfers (like 20 TB) and can handle up to 80 TB per device. Shipping physical storage is faster and cheaper than transferring 20 TB over the internet, even with a 1 Gb ps Direct Connect (which could take days/weeks and incur high costs). - AWS DMS + SCT handles schema conversion (if needed) and replicates ongoing changes during migration, minimizing downtime. The Snowball device transfers the  bulk data, while  DMS  syncs  incremental changes  until cutover.  2. Why Other  Options Are Wrong: - B (Snowmobile): Snowmobile is for exabyte-scale data (e.g., 100+ PB). For 20 TB, Snowball is far cheaper and faster. - C (Compute Optimized with GPU): This Snowball type is for compute-heavy tasks (e.g., ML), not storage. The Storage Optimized variant  is  cheaper  and   better  suited  for  data   migration.  -   D  (Direct  Connect):  A   1  Gb ps  connection  would  take  ~2  days (theoretically) for 20 TB, but real-world factors (latency, network congestion) make this  risky for a 2-week timeline. Snowball ensures      predictable      transfer      time       and      avoids       bandwidth      costs.      Reference       Links:      -       [AWS      Snowball Edge](https://aws.amazon.com/snowball/edge/)  -  [AWS  DMS](https://aws.amazon.com/dms/)  -  [Choosing  Between  Snowball and Direct Connect](https://aws.amazon.com/answers/networking/aws-snowball-vs-aws-direct-connect/)

380.A  company  moved  its  on-premises  Postgresql  database  to  an  Amazon  RDS  for  Postgresql  DB  instance.  The  company successfully launched a new product. The workload on the database has increased. The company wants to accommodate the larger workload without adding infrastructure. Which solution will meet these requirements MOST cost-effectively?

A ��Buy reserved DB instances for the total workload. Make the Amazon RDS for Postgresql DB instance larger.

B ��Make the Amazon RDS for Postgresql DB instance a Multi-AZ DB instance.

C ��Buy reserved DB instances for the total workload. Add another Amazon RDS for Postgresql DB instance.

D ��Make the Amazon RDS for Postgresql DB instance an on-demand DB instance.

�𰸣�A

������ Correct Answer: A. Buy reserved DB instances for the total workload. Make the Amazon RDS for Postgresql DB instance larger.  Detailed  Explanation:  The  most  cost-effective  solution  is  to  use  Reserved  Instances  (which  offer  significant  discounts compared to  On-Demand  pricing)  while  vertically  scaling the  DB  instance  (e.g.,  upgrading  from  a  smaller  instance  type  like db.t3.medium to a larger one like db.t3.large). This approach avoids adding new infrastructure (like extra instances in Option C) and directly increases the database's capacity to handle higher workloads. - Reserved Instances reduce long-term costs by up to 70% compared to On-Demand pricing. - Vertical scaling (making the instance larger) improves CPU, memory, and I/O capacity without adding complexity or new infrastructure. - Options B (Multi-AZ) and D (On-Demand) don ��t address performance scaling. Option      C      adds      new      infrastructure,      which      violates      the      requirement.      Reference      Link:       [AWS      Reserved Instances](https://aws.amazon.com/rds/reserved-instances/)                                                     [AWS                                                      RDS
Scaling](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_Modify instance.html)

381.A company operates an ecommerce website on Amazon EC2 instances behind an Application Load Balancer (ALB) in an Auto Scaling group. The site is experiencing performance issues related to a high request rate from illegitimate external systems with changing IP addresses. The security team is worried about potential DDoS attacks against the website. The company must block the illegitimate incoming requests in a way that has a minimal impact on legitimate users. What should a solutions architect recommend?

A ��Deploy Amazon Inspector and associate it with the ALB.

B ��Deploy AWS WAF, associate it with the ALB, and configure a rate-limiting rule.

C ��Deploy rules to the network ACLs associated with the ALB to block the incoming traffic.

D ��Deploy Amazon Guard duty and enable rate-limiting protection when configuring Guard duty.

�𰸣�B

������ Correct Answer: B Detailed Explanation: AWS WAF (Web Application Firewall) is designed to protect web applications by monitoring and controlling incoming traffic. By associating AWS WAF with the Application Load Balancer (ALB), the company can create rate-limiting rules to restrict the number of requests from a single IP address within a specific time window. This directly addresses the issue of high request rates from illegitimate systems (e.g., bots or attackers) while allowing legitimate users to access the site normally. - Why not other options? - A. Amazon Inspector focuses on vulnerability assessments and security best practices for EC2 instances, not real-time traffic filtering. - C. Network ACLs operate at the subnet level and are stateless, making them unsuitable for blocking dynamic IP addresses or application-layer attacks (like HTTP floods). - D. Amazon Guard duty detects threats using machine learning but does not block traffic directly. It would require integration with other services (like AWS WAF) to enforce protections. AWS WAF ��s rate-limiting rules are granular, easy to configure, and minimize false positives for legitimate users.                                Reference                                 Link:                                 [AWS                                WAF                                 Rate-Based
Rules](https://docs.aws.amazon.com/waf/latest/developer guide/waf-rate-based-rules.html)

382.A company wants to share accounting data with an external auditor. The data is stored in an Amazon RDS DB instance that resides in a private subnet. The auditor has its own AWS account and requires its own copy of the database. What is the MOST secure way for the company to share the database with the auditor?

A ��Create a read replica of the database. Configure IAM standard database authentication to grant the auditor access.

B��Export the database contents to text files. Store the files in an Amazon S3 bucket. Create a new IAM user for the auditor. Grant

the user access to the S3 bucket.

C ��Copy a snapshot of the database to an Amazon S3 bucket. Create an IAM user. Share the user's keys with the auditor to grant access to the object in the S3 bucket.

D �� Create  an  encrypted  snapshot  of  the  database.  Share  the  snapshot  with  the  auditor.  Allow  access  to  the  AWS   Key Management Service (AWS KMS) encryption key.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The most secure method is to create an encrypted snapshot of t he RDS database and share it with the auditor's AWS account. This ensures: 1. Encryption: The snapshot is encrypted using AWS KMS, protecting data at rest. 2. Cross-Account Sharing: The auditor can restore the snapshot in their own AWS account without direct access to the company ��s VPC or RDS instance. 3. KMS Key Access Control: The company grants the auditor ��s account permission to use the KMS key, ensuring only authorized parties can decrypt the snapshot. Other options are less secure: - A exposes the database directly to external access (riskier network setup). - B/C involve sharing raw data or credentials (e.g., IAM user keys), which is less secure     than      encrypted      snapshots      with      KMS-based      access      control.       Reference:      [Sharing      Encrypted      RDS Snapshots](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_Share snapshot.html)   [Cross-Account   KMS   Key Access](https://docs.aws.amazon.com/kms/latest/developer guide/key-policy-modifying-external-accounts.html)

383.A solutions architect configured a VPC t hat has a small range of IP addresses. The number of Amazon EC2 instances that are in the VPC is increasing, and there is an insufficient number of IP addresses for future workloads. Which solution resolves this issue with the LEAST operational overhead?

A ��Add an additional IPv4 CIDR block to increase the number of IP addresses and create additional subnets in the VPC. Create new resources in the new subnets by using the new CIDR.

B ��Create a second VPC with additional subnets. Use a peering connection to connect the second VPC with the first VPC Update the routes and create new resources in the subnets of t he second VPC.

C ��Use AWS Transit Gateway to add a transit gateway and connect a second VPC with the first Vp update the routes of the transit gateway and VPCs. Create new resources in the subnets of t he second VPC.

D ��Create a second VPC. Create a Site-to-Site VPN connection between the first VPC and the second VPC by using a VPN-hosted solution on Amazon EC2 and a virtual private gateway. Update the route between VPCs to the traffic through the VPN. Create new resources in the subnets of t he second VPC.

�𰸣�A

�� �� �� Correct Answer: A  Detailed  Explanation: When  a VPC  runs out  of  IP addresses, the simplest solution with  minimal operational overhead is to add an additional IPv4 CIDR block to the existing VPC. This allows you to expand the IP address range without creating new VPCs or managing complex networking setups like peering connections, VPNs, or Transit Gateways (which are required in options B, C, and D). By adding a new CIDR block, you can create new subnets with the expanded IP range and deploy resources there immediately. This approach avoids the need to reconfigure routing tables, set up inter - VPC connections, or manage additional components, making it the most straightforward and least operationally intensive solution. Reference Link: [Amazon VPC CIDR Blocks](https://docs.aws.amazon.com/vpc/latest/user guide/VPC_Subnets.html#add-cidr-block)

384.A company hosts a multi-tier web application on Amazon Linux Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company observes that the Auto Scaling group

launches more On-Demand Instances when the application's end users access high volumes of static web content. The company wants to optimize cost. What should a solutions architect do to redesign the application MOST cost-effectively?

A ��Update the Auto Scaling group to use Reserved Instances instead of On-Demand Instances.

B ��Update the Auto Scaling group to scale by launching Spot Instances instead of On-Demand Instances.

C ��Create an Amazon Cloud front distribution to host the static web contents from an Amazon S3 bucket.

D ��Create an AWS Lambda function behind an Amazon API Gateway API to host the static website contents.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The key issue here is high traffic to static content causing costly On-Demand EC2 scaling. Option C is the most cost-effective because: 1. S3 + Cloud front separates static content from EC2 instances 2. Cloud front CDN caches content at edge locations, reducing origin server load 3. Cost savings come from: - Paying less for S3 storage vs EC2 compute  -  Reduced  data  transfer  costs  through  CDN  caching  -  Fewer  EC2  instances  needed  overall  4.  Automatic  scaling  - Cloud front/S3 handle traffic spikes natively without scaling events Other options have limitations: - A (Reserved Instances): Still uses EC2 for static content - B (Spot Instances): Risk of interruptions for web servers - D (Lambda/API Gateway): Higher cost per request for static content Reference Links: [Amazon Cloud front pricing](https://aws.amazon.com/cloud front/pricing/) [S3 Static Website    Hosting](https://docs.aws.amazon.com/AmazonS3/latest/dev/Website hosting.html)    [AWS     Well-Architected    static content best practices](https://aws.amazon.com/architecture/well-architected/)

385.A  company  stores  several  petabytes  of  data  across  multiple  AWS  accounts.  The  company  uses  AWS  Lake  Formation  to manage  its  data  lake.  The  company's  data  science  team  wants  to  securely  share  selective  data  from  its  accounts  with  the company's engineering team for analytical purposes. Which solution will meet these requirements with the LEAST operational overhead?

A �� Copy the  required  data  to a common  account. Create an  IAM  access  role  in  that account. Grant access  by  specifying a permission policy that includes users from the engineering team accounts as trusted entities.

B �� Use  the  Lake  Formation  permissions  Grant  command  in  each  account  where  the  data  is  stored  to  allow  the  required engineering team users to access the data.

C ��Use AWS Data Exchange to privately publish the required data to the required engineering team accounts.

D ��Use Lake Formation tag-based access control to authorize and grant cross-account permissions for the required data to the engineering team accounts.

�𰸣�D

������Correct Answer: D Detailed Explanation: Lake Formation tag-based access control (TBAC) allows you to define permissions based  on  resource  tags,  enabling  centralized  cross-account  data  sharing  without   manual  per-account  configurations.  This minimizes operational overhead compared to options requiring data duplication (A), repeated manual grants (B), or third-party services (C). With TBAC, you tag datasets once, and engineering teams automatically get access via policies linked to those tags, even        across        accounts.        Reference         Links:        -        AWS        Lake         Formation        Tag-Based        Access        Control: https://docs.aws.amazon.com/lake-formation/latest/dg/tag-based-access-control.html  -  Cross-account  Data  Sharing  with  Lake Formation:
https://aws.amazon.com/blogs/big-data/share-data-across-accounts-using-aws-lake-formation-tag-based-access-control/

386.A  company wants to  host  a scalable  web  application  on AWS. The  application will  be  accessed  by  users  from  different geographic regions of the world. Application users will be able to download and upload unique data up to gigabytes in size. The development team wants a cost-effective solution to minimize upload and download latency and maximize performance. What should a solutions architect do to accomplish this?

A ��Use Amazon S3 with Transfer Acceleration to host the application.

B ��Use Amazon S3 with Cache control headers to host the application.

C ��Use Amazon EC2 with Auto Scaling and Amazon Cloud front to host the application.

D ��Use Amazon EC2 with Auto Scaling and Amazon Elastic ache to host the application.

�𰸣�A

������ Correct Answer: A. Use Amazon S3 with Transfer Acceleration to host the application. Detailed Explanation: The scenario involves a globally distributed user base uploading/downloading large files (gigabytes in size) with a focus on minimizing latency and  cost.  Here �� s  why  Amazon  S3  Transfer  Acceleration  is  the  best  choice:  1.  Low  Latency  for  Global  Users:  S3  Transfer Acceleration uses Amazon Cloud front ��s globally distributed edge locations to optimize the network path between users and S3. This reduces latency by routing uploads/downloads through the nearest AWS edge location, even if the S3 bucket is in a different region. 2. Cost-Effective for Large Files: Transfer Acceleration is designed for large file transfers. While there ��s a small cost for using edge locations, it avoids the need for provisioning and managing EC2 instances or caching layers (as in options C/D), which can be more expensive for static content. 3. Scalability: S3 inherently scales to handle massive traffic and storage needs without manual intervention, unlike EC2-based solutions that require Auto Scaling configurations. Why Other Options Are Less Suitable: - B (Cache control Headers): Caching helps repeated requests for the same files but doesn ��t optimize initial uploads/downloads of unique large files. - C/D (EC2 + Cloud front/Elastic ache): EC2 requires managing servers and scaling, which adds complexity and cost. Cloud front accelerates content delivery but isn ��t optimized for uploading large files directly. Elastic ache is for database caching, not file storage. Reference Links: - [Amazon S3 Transfer Acceleration](https://aws.amazon.com/s3/transfer-acceleration/)
-                                                [Cloud front                                                vs.                                                S3                                                Transfer Acceleration](https://aws.amazon.com/blogs/aws/new-accelerate-data-transfers-to-amazon-s3-with-transfer-acceleration/)

387.A company has hired a solutions architect to design a reliable architecture for its application. The application consists of one Amazon  RDS  DB  instance  and two  manually  provisioned Amazon  EC2  instances that  run  web servers. The  EC2  instances  are located in a single Availability Zone. An employee recently deleted the DB instance, and the application was unavailable for 24 hours as a result. The company is concerned with the overall reliability of its environment. What should the solutions architect do to maximize reliability of the application's infrastructure?

A��Delete one EC2 instance and enable termination protection on the other EC2 instance. Update the DB instance to be Multi-AZ, and enable deletion protection.

B ��Update the DB instance to be Multi-AZ, and enable deletion protection. Place the EC2 instances behind an Application Load Balancer, and run them in an EC2 Auto Scaling group across multiple Availability Zones.

C ��Create an additional DB instance along with an Amazon API Gateway and an AWS Lambda function. Configure the application to invoke the Lambda function through API Gateway. Have the Lambda function write the data to the two DB instances.

D��Place the EC2 instances in an EC2 Auto Scaling group that has multiple subnets located in multiple Availability Zones. Use Spot Instances instead of On-Demand Instances. Set up Amazon Cloud watch alarms to monitor the health of the instances Update the

DB instance to be Multi-AZ, and enable deletion protection.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The main reliability issues in the current architecture are: 1. Single-point failure in  RDS: The  RDS  instance  isn't  Multi-AZ  (high  availability) and  lacks deletion  protection,  making  it vulnerable to outages and accidental deletion. 2. EC2 fragility: Manually managed EC2 instances in one Availability Zone (AZ) create web server vulnerability to AZ failures. Solution B addresses both layers: - Database layer: - Multi-AZ RDS automatically fails over to a standby replica in another AZ during outages. - Deletion protection prevents accidental DB deletion. - Web server layer: - Auto Scaling Group (ASG) automatically replaces failed instances and distributes them across multiple AZs. - Application Load Balancer (ALB) routes traffic to healthy instances across AZs. Why other options are less ideal: - A:  Keeping  manual  EC2 instances (even with termination protection) doesn't solve AZ redundancy. - C: Introducing Lambda/API Gateway unnecessarily complicates the existing EC2-based architecture. - D: Spot Instances (while cost-effective) can be interrupted, reducing reliability for critical web servers. Reference Links:        -        [Amazon         RDS        Multi-AZ](https://aws.amazon.com/rds/details/multi-az/)        -         [EC2        Auto        Scaling Groups](https://docs.aws.amazon.com/auto scaling/ec2/user guide/Auto scaling group.html)            -            [Application             Load Balancer](https://aws.amazon.com/elastic load balancing/application-load-balancer/)

388.A company is storing 700 terabytes of data on a large network-attached storage (NAS) system in its corporate data center. The company has a hybrid environment with a 10 Gbps AWS Direct Connect connection. After an audit from a regulator, the company has 90 days to move t he data to the cloud. The company needs to move the data efficiently and without disruption. The company still needs to be able to access and update the data during the transfer window. Which solution will meet these requirements?

A ��Create an AWS Data sync agent in the corporate data center. Create a data transfer task Start the transfer to an Amazon S3 bucket.

B ��Back up the data to AWS Snowball Edge Storage Optimized devices. Ship the devices to an AWS data center. Mount a target Amazon S3 bucket on the on-premises file system.

C ��Use rsync to copy the data directly from local storage to a designated Amazon S3 bucket over the Direct Connect connection.

D ��Back up the data on tapes. Ship the tapes to an AWS data center. Mount a target Amazon S3 bucket on the on-premises file system.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution here is A. Use AWS Data sync. Here's why this is the best choice for  a  beginner  to  understand:  1.  Purpose-Built  for  Large  Transfers:  AWS  Data sync  is  designed  specifically  for  moving  large datasets (like 700TB) between on-premises storage and AWS. It automates data transfers and handles retries if errors occur. 2. Efficient with 10 Gb ps Direct Connect: With a fast 10 Gb ps connection, Data sync can fully utilize this bandwidth. Transferring 700TB over 10 Gb ps would take ~7 days (theoretical maximum), leaving plenty of time for incremental updates during the 90-day window. 3. Incremental Transfers: Data sync only transfers changed files after the initial transfer. This means: - The company can keep updating files locally during the migration - Final sync before cutover would be quick - No service disruption 4. Simpler than Physical Devices (Snowball): While Snowball (Option B) could physically ship data, this is unnecessary with a fast Direct Connect connection.   Data sync  avoids  shipping   hardware  delays.  5.   Better  than   rsync   (Option  C):   Data sync   handles  encryption, compression, and integrity checks automatically, unlike manual rsync scripts. 6. Access During Migration: Data sync doesn ��t lock files during transfer, so the company can keep using the data throughout the process. Why Other Options Fail: - B (Snowball): Physical shipping adds complexity for a network that ��s already fast enough. - C (rsync): Not optimized for cloud transfers, no automatic error handling. - D (Tapes): Tape backups can ��t support ongoing updates during  migration. Reference Link:  [AWS

Data sync Documentation](https://docs.aws.amazon.com/data sync/latest/user guide/what-is-data sync.html)

389.A company stores data in PDF format in an Amazon S3 bucket. The company must follow a legal requirement to retain all new  and  existing  data  in  Amazon  S3  for  7  years.  Which  solution  will  meet  these  requirements  with  the  LEAST  operational overhead?

A �� Turn  on the S3 Versioning feature for the S3  bucket.  Configure  S3  Lifecycle  to  delete the  data  after  7  years.  Configure multi-factor authentication (MFA) delete for all S3 objects.

B ��Turn on S3 Object Lock with governance retention mode for the S3 bucket. Set the retention period to expire after 7 years. Recopy all existing objects to bring the existing data into compliance.

C ��Turn on S3 Object Lock with compliance retention mode for the S3 bucket. Set the retention period to expire after 7 years. Recopy all existing objects to bring the existing data into compliance.

D��Turn on S3 Object Lock with compliance retention mode for the S3 bucket. Set the retention period to expire after 7 years. Use S3 Batch Operations to bring the existing data into compliance.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The correct answer is D because it uses S3 Object Lock in compliance mode and S3  Batch Operations to meet the legal requirement with minimal effort.  Here's why: 1. S3 Object  Lock (Compliance Mode): - Compliance retention mode prevents anyone (including root/admin users) from deleting or modifying objects until the retention period expires (7 years in this case). This satisfies the legal requirement. - Governance mode (Option B/C) allows privileged users to  override  retention  settings,  which  doesn't  meet  strict  legal  requirements.  2.  S3  Batch  Operations:  -  Instead  of  manually re copying all existing objects (Option C), S3  Batch Operations automatically applies Object Lock settings to existing objects in bulk. This reduces operational overhead significantly. Other options fail because: - Option A: Versioning + Lifecycle can't prevent accidental/malicious  deletion  of  current  versions.   MFA  Delete  only  adds  authentication  for  deletions   but  doesn't  enforce retention. - Option B/C: Governance mode isn't strict enough for legal compliance, and manual re copying (C) is labor-intensive. Reference                                     Links:                                    -                                      [S3                                    Object                                      Lock
Overview](https://docs.aws.amazon.com/AmazonS3/latest/user guide/object-lock-overview.html)             -              [S3              Batch Operations](https://docs.aws.amazon.com/AmazonS3/latest/user guide/batch-ops.html)

390.A company has a stateless web application that runs on AWS Lambda functions that are invoked by Amazon API Gateway. The company wants to deploy the application across multiple AWS Regions to provide Regional failover capabilities. What should a solutions architect do to route traffic to multiple Regions?

A ��Create Amazon Route 53 health checks for each Region. Use an active-active failover configuration.

B ��Create an Amazon Cloud front distribution with an origin for each Region. Use Cloud front health checks to route traffic.

C �� Create  a transit  gateway. Attach the transit gateway to the API  Gateway endpoint  in  each  Region.  Configure the transit gateway to route requests.
D �� Create  an Application  Load  Balancer  in  the  primary  Region.  Set  the target group to  point to the API  Gateway endpoint hostnames in each Region.

�𰸣�A

������ Correct Answer A �� Create Amazon Route 53 health checks for each Region. Use an active-active failover configuration. Detailed Explanation To route traffic across  multiple AWS Regions for regional failover, Amazon Route 53 is the best solution. Here ��s why: 1. Route 53 Health Checks: You can configure health checks for your API Gateway endpoints in each Region. These checks monitor the availability of your application. 2. Active-Active Failover: In an active-active setup, traffic is distributed to all healthy Regions. If a health check detects a failure in one Region, Route 53 automatically stops routing traffic to the unhealthy Region, ensuring users are directed only to the healthy Regions. 3. Global DNS-Level Routing: Route 53 operates at the DNS layer, making it ideal for cross-Region failover. Users are redirected to healthy Regions seamlessly without application changes. Why other options are incorrect: - B. Cloud front: While Cloud front can route traffic to multiple origins, it ��s designed for caching and accelerating content,  not regional failover.  Its  health checks are less flexible for full application failover. - C. Transit Gateway: Transit Gateway connects VPCs/networks,  but API  Gateway  is  a  managed service with  no  direct  integration for  cross-Region routing via Transit Gateway. - D. Application Load Balancer (ALB): ALBs are Region-specific. They can��t natively route traffic to API Gateway      endpoints      in      other      Regions.       Reference      Link      [Amazon       Route      53      Health      Checks      and       DNS Failover](https://docs.aws.amazon.com/Route53/latest/Developer guide/dns-failover.html)

391.A  company  has  two  VPCs  named  Management  and  Production.  The  Management  VPC  uses  VPNs  through  a  customer gateway to connect to a single device in the data center. The Production VPC uses a virtual private gateway with two attached AWS  Direct Connect connections. The  Management and  Production VPCs  both use a single VPC  peering connection to allow communication between the applications. What should a solutions architect do to  mitigate any single  point of failure in this architecture?

A ��Add a set of VPNs between the Management and Production VPCs.

B ��Add a second virtual private gateway and attach it to the Management VPC.

C ��Add a second set of VPNs to the Management VPC from a second customer gateway device.

D ��Add a second VPC peering connection between the Management VPC and the Production VPC.

�𰸣�C

������Correct Answer: C Detailed Explanation: The Management VPC currently has a single point of failure in its VPN connection to the on-premises data center because it relies on one customer gateway device. If this device fails, the VPN connection breaks. Option C adds redundancy by creating a second VPN connection using a second customer gateway device, eliminating this single point of failure. Other options are incorrect: - A: Adding VPNs between VPCs is unnecessary since they already communicate via VPC  peering.  -  B:  Virtual  private  gateways are for  Direct  Connect/VPNAWS,  but  the  Management  VPC  uses  VPN  (not  Direct Connect). - D: VPC peering connections are inherently redundant within an AWS Region; adding a second peering connection doesn               ��       t                improve                availability.                 Reference:                 [AWS                Site-to-Site                VPN Redundancy](https://docs.aws.amazon.com/vpn/latest/s2svpn/redundant-vpns.html)                         [AWS                          Customer
Gateway](https://docs.aws.amazon.com/vpn/latest/s2svpn/your-cgw.html)

392.A company runs its application on an Oracle database. The company plans to quickly migrate to AWS because of limited resources for the  database,  backup  administration,  and  data  center  maintenance. The  application  uses  third-party  database features   that   require   privileged   access.   Which   solution   will   help   the   company   migrate   the   database   to   AWS   MOST cost-effectively?

A ��Migrate the database to Amazon RDS for Oracle. Replace third-party features with cloud services.

B ��Migrate the database to Amazon RDS Custom for Oracle. Customize the database settings to support third-party features.

C��Migrate the database to an Amazon EC2 Amazon Machine Image (AMI) for Oracle. Customize the database settings to support third-party features.

D �� Migrate the database to Amazon RDS for Postgresql by rewriting the application code to remove dependency on Oracle APEX.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The company needs to migrate an Oracle database that uses third-party features requiring privileged access. Amazon RDS Custom for Oracle is designed for databases needing OS-level customization while still providing managed database services (backups, maintenance, etc.). This avoids the operational burden of self-managing an EC2 instance  (Option  C)  and  preserves  the  existing  third-party  features  without  costly  rewrites  (Options  A  and  D).  RDS  Custom balances flexibility with AWS-managed administration, making it the most cost-effective choice. Reference Links: - [Amazon RDS Custom](https://aws.amazon.com/rds/custom/)                             -                               [RDS                              Custom                             vs.
RDS](https://docs.aws.amazon.com/Amazon rds/latest/User guide/custom.html)

393.A company runs a Java-based job on an Amazon EC2 instance. The job runs every hour and takes 10 seconds to run. The job runs on a scheduled interval and consumes 1 GB of memory. The CPU utilization of the instance is low except for short surges during which the job uses the maximum CPU available. The company wants to optimize the costs to run the job. Which solution will meet these requirements?

A ��Use AWS App2Container (A2C) to container ize the job. Run the job as an Amazon Elastic Container Service (Amazon ECS) task on AWS Fargate with 0.5 virtual CPU (vCPU) and 1 GB of memory.

B ��Copy the code into an AWS Lambda function that has 1 GB of memory. Create an Amazon Event bridge scheduled rule to run the code each hour.

C �� Use AWS App2Container (A2C) to container ize the job. Install the container in the existing Amazon Machine Image (AMI). Ensure that the schedule stops the container when the task finishes.

D ��Configure the existing schedule to stop the EC2 instance at the completion of the job and restart the EC2 instance when the next job starts.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The job runs for only 10 seconds every hour, making it highly intermittent. AWS Lambda is ideal here because it charges only for the actual execution time (rounded to the nearest 1ms) and allocated memory. By  using  Lambda  with  1GB  memory  and  triggering  it  hourly  via  Amazon  Event bridge,  the  company  avoids  paying  for  idle resources  (like  a  constantly  running  EC2  instance  or  Fargate  container).  While  Java  has  a  slower  cold  start  in  Lambda,  the 10-second runtime dwarfs this minor delay. Options A (Fargate) and D (EC2 stop/start) still incur higher costs due to per-minute billing (Fargate) or hourly EC2 billing even when stopped. Option C doesn ��t reduce costs since the EC2 instance remains running. Lambda ��s pay-per-use model makes it the most cost-effective choice for short, infrequent tasks. Reference Link: [AWS Lambda Pricing](https://aws.amazon.com/lambda/pricing/)

394.A  company  wants to  implement  a  backup  strategy  for  Amazon  EC2  data  and  multiple  Amazon  S3  buckets.  Because  of regulatory requirements, the company must retain backup files for a specific time period. The company must not alter the files for the duration of the retention period. Which solution will meet these requirements?

A ��Use AWS Backup to create a backup vault that has a vault lock in governance mode. Create the required backup plan.

B ��Use Amazon Data Lifecycle Manager to create the required automated snapshot policy.

C ��Use Amazon S3 File Gateway to create the backup. Configure the appropriate S3 Lifecycle management.

D ��Use AWS Backup to create a backup vault that has a vault lock in compliance mode. Create the required backup plan.

�𰸣�D

���� �� Correct Answer: D Detailed Explanation: The question requires a backup solution that ensures data immutability (no alterations) during a retention period for both EC2 and S3 data. Here ��s why Option D is correct: - AWS Backup centrally manages backups for EC2, S3, and other AWS services. - A backup vault with vault lock in compliance mode enforces immutability. In this mode: - No one (including AWS account administrators) can delete or modify backups before the retention period expires. - This
aligns with strict regulatory requirements (e.g., legal holds, financial audits). Other options fail because: - A (Governance Mode): Allows admins to override retention policies, violating immutability. - B (Data Lifecycle Manager): Manages EBS snapshots but lacks S3 integration and enforced immutability. - C (S3 File Gateway + Lifecycle): Lifecycle rules manage storage tiers/expiry but don ��t guarantee immutability unless S3 Object Lock is explicitly enabled (not mentioned here). Reference: [AWS Backup Vault Lock](https://docs.aws.amazon.com/aws-backup/latest/dev guide/vault-lock.html)              [AWS             Backup              Compliance Mode](https://docs.aws.amazon.com/aws-backup/latest/dev guide/backup-vault-lock-policies.html)

395.A company has resources across multiple AWS Regions and accounts. A newly hired solutions architect discovers a previous employee did not provide details about the resources inventory. The solutions architect needs to build and map the relationship details of the various workloads across all accounts. Which solution will meet these  requirements in the MOST operationally efficient way?

A ��Use AWS Systems Manager Inventory to generate a map view from the detailed view report.

B ��Use AWS Step Functions to collect workload details. Build architecture diagrams of t he workloads manually. C ��Use Workload Discovery on AWS to generate architecture diagrams of t he workloads.

D ��Use AWS X-Ray to view the workload details. Build architecture diagrams with relationships.

�𰸣�C

������ Correct Answer C Detailed Explanation The correct answer is C. Use Workload Discovery on AWS to generate architecture diagrams of t he workloads. Here ��s why: - Workload Discovery on AWS is specifically designed to automate the discovery of resources across  multiple AWS accounts and  regions.  It  uses  AWS  Resource Groups, AWS Config,  and other services to  map relationships  between  workloads  and  generate  visual  architecture  diagrams.  This  is  the  most  operationally  efficient  option because  it  automates  the  entire  process  without  manual  effort.  -  Option  A  (AWS  Systems  Manager  Inventory)  focuses  on collecting   inventory   data   (e.g.,   software,   patches)   for   EC2   instances,   not   mapping   cross-account/cross-region   resource relationships.  -  Option  B  (AWS  Step  Functions  +  manual  diagrams)  requires  manual  effort  to  build  diagrams,  which  is time-consuming  and  error-prone.  -  Option  D  (AWS  X-Ray)  is  for  tracing  application  performance  (e.g.,  micro services),  not inventory or architecture mapping. Workload Discovery simplifies cross-account visibility and aligns with AWS best practices for resource                       governance.                        Reference                         Links                       -                         [AWS                       Workload
Discovery](https://aws.amazon.com/solutions/implementations/workload-discovery-on-aws/)           -             [AWS            Resource Groups](https://docs.aws.amazon.com/ARG/latest/user guide/welcome.html)

396.A company runs applications on Amazon EC2 instances in one AWS Region. The company wants to back up the EC2 instances

to a second Region. The company also wants to provision EC2 resources in the second Region and manage the EC2 instances centrally from one AWS account. Which solution will meet these requirements MOST cost-effectively?

A �� Create  a  disaster  recovery  (DR)  plan  t hat  has  a  similar  number  of  EC2  instances  in  the  second  Region.  Configure  data replication.

B ��Create point-in-time Amazon Elastic Block Store (Amazon EBS) snapshots of the EC2 instances. Copy the snapshots to the second Region periodically.

C ��Create a backup plan by using AWS Backup. Configure cross-Region backup to the second Region for the EC2 instances.

D �� Deploy a similar number of EC2 instances in the second Region.  Use AWS Data sync to transfer the data from the source Region to the second Region.

�𰸣�C

�� �� ��  Correct  Answer:  C   Detailed  Explanation:  The  most  cost-effective  solution   is  C   because  AWS   Backup  automates cross-Region backups for EC2 instances (including EBS volumes) without requiring manual snapshot management. This service centralizes backup policies and reduces costs by storing only backups (not running duplicate instances). Options A and D involve running redundant  EC2  instances  in  both  Regions, which  is expensive. Option  B  requires  manual snapshot copying and  lacks centralized management. AWS Backup ��s automated, managed approach aligns best with the requirements for cost efficiency and               centralized               control.                Reference               Link:                -               [AWS                Backup               Cross-Region Backup](https://docs.aws.amazon.com/aws-backup/latest/dev guide/cross-region-backup.html)        -        [AWS        Backup        for EC2](https://aws.amazon.com/backup/features/)

397.A company that uses AWS is building an application to transfer data to a product manufacturer. The company has its own identity  provider  (IdP). The  company wants the  IdP to  authenticate  application  users while the  users  use  the  application to transfer data. The company must use Applicability Statement 2 (AS2) protocol. Which solution will meet these requirements?

A ��Use AWS Data sync to transfer the data. Create an AWS Lambda function for IdP authentication.

B �� Use Amazon AppFlow flows to transfer the  data. Create an Amazon  Elastic Container  Service  (Amazon  ECS) task for  IdP authentication.

C ��Use AWS Transfer Family to transfer the data. Create an AWS Lambda function for IdP authentication.

D ��Use AWS Storage Gateway to transfer the data. Create an Amazon Cognito identity pool for IdP authentication.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The correct solution is C: Use AWS Transfer Family to transfer the data. Create an AWS Lambda function for IdP authentication. Here ��s why: 1. AS2 Protocol Support: AWS Transfer Family added native support for the AS2 protocol in 2023. This makes it the only AWS service explicitly mentioned in the options that directly supports AS2, which is a hard requirement in the question. 2. Custom Authentication with Lambda: AWS Transfer Family allows integration with a Lambda function for authentication. Since the company has its own Identity Provider (IdP), the Lambda function can validate user credentials by connecting to the existing IdP system (e.g., using SAML or OAuth). This ensures users are authenticated via the company ��s IdP when transferring data. 3. Why Other Options Fail: - A (Data sync + Lambda): AWS Data sync is for bulk data migration (e.g., S3, EFS) and does not support AS2. - B (AppFlow + ECS): Amazon AppFlow integrates SaaS apps (e.g., Sales force) with AWS services but does not support AS2. ECS tasks are unrelated to AS2 or IdP authentication here. - D (Storage Gateway +

Cognito): Storage  Gateway  is for  hybrid  cloud storage  (e.g.,  S3/file gateway)  and does  not  support AS2.  Cognito  is AWS ��s managed IdP, but the question requires using the company ��s existing IdP, not Cognito. Reference Link: [AWS Transfer Family AS2 Documentation](https://docs.aws.amazon.com/transfer/latest/user guide/as2-connector.html)

398.A company uses AWS Organizations to run workloads within multiple AWS accounts. A tagging policy adds department tags to  AWS   resources  when  the   company  creates  tags.  An   accounting  team   needs  to  determine   spending   on  Amazon   EC2 consumption. The accounting team must determine which departments are responsible for the costs regardless ofAWS account. The accounting team  has access to AWS Cost  Explorer for  all AWS accounts within the organization and  needs to access all reports from Cost Explorer. Which solution meets these requirements in the MOST operationally efficient way?

A ��From the Organizations management account billing console, activate a user-defined cost allocation tag named department. Create one cost report in Cost Explorer grouping by tag name, and filter by EC2.

B ��From the Organizations management account billing console, activate an AWS-defined cost allocation tag named department. Create one cost report in Cost Explorer grouping by tag name, and filter by EC2.

C��From the Organizations member account billing console, activate a user-defined cost allocation tag named department. Create one cost report in Cost Explorer grouping by the tag name, and filter by EC2.

D �� From the Organizations member account billing console, activate an AWS-defined cost allocation tag named department. Create one cost report in Cost Explorer grouping by tag name, and filter by EC2.

�𰸣�A

���� �� Correct Answer: A Detailed Explanation: The company uses AWS Organizations and has a tagging policy that enforces department tags. To track costs by department across all accounts, they need to use user-defined cost allocation tags (since the department  tag   is  custom,  not  AWS-generated   like  aws:created by).   Here's  why   option  A  works   best:   1.  Activate   in  the management account: Cost allocation tags must be activated in the Organizations management account to apply to all linked accounts.  Activating   in  member  accounts  (options  C/D)  would   only  affect  that   single  account.  2.  User-defined  tag:  The department tag is created by the company, so it ��s a user-defined tag (not AWS-defined, which are system-generated tags like aws:service).  This  rules  out  options  B/D.  3.  Single  report:  Once  activated,  Cost  Explorer  in  the  management  account  can aggregate costs across all accounts by filtering EC2 and grouping by the department tag. Reference Link: [AWS Cost Allocation Tags Documentation](https://docs.aws.amazon.com/aws account billing/latest/aboutv2/cost-alloc-tags.html)

399.A company wants to securely exchange data between its software as a service (SaaS) application Sales force account and Amazon S3. The company must encrypt the data at rest by using AWS Key Management Service (AWS KMS) customer managed keys (CMKs). The company must also encrypt the data in transit. The company has enabled API access for the Sales force account.

A ��Create AWS Lambda functions to transfer the data securely from Sales force to Amazon S3.

B ��Create an AWS Step Functions workflow. Define the task to transfer the data securely from Sales force to Amazon S3. C ��Create Amazon AppFlow flows to transfer the data securely from Sales force to Amazon S3.

D ��Create a custom connector for Sales force to transfer the data securely from Sales force to Amazon S3.

�𰸣�C

�� �� �� Correct Answer: C  Detailed  Explanation: Amazon AppFlow  is  a fully  managed AWS  service  designed specifically for

securely transferring data between SaaS applications (like Sales force) and AWS services (like Amazon S3). Here ��s why it ��s the best choice: 1. Encryption at Rest: AppFlow automatically encrypts data stored in Amazon S3 using AWS KMS customer managed keys  (CMKs)  if configured. You can specify the encryption  settings when  defining the S3  destination  in the AppFlow flow. 2. Encryption in Transit: AppFlow uses TLS (Transport Layer Security) to encrypt data during transfer between Sales force and AWS, ensuring security for data in motion. 3. No Custom Code Required: Unlike Lambda (Option A) or custom connectors (Option D), AppFlow provides pre-built connectors for Sales force and S3. This eliminates the need to write code or manage infrastructure, reducing complexity and errors. 4. API Access: Since the company already enabled Sales force API access, AppFlow can leverage this  to   authenticate  and  transfer   data  seamlessly.  Options  A   and  B  require  manual  coding  and  lack  built-in  encryption management. Option  D involves unnecessary effort since AppFlow already offers a managed Sales force connector. Reference Links:  -  [Amazon  AppFlow   Documentation](https://docs.aws.amazon.com/appflow/)  -   [Encrypting   Data  with  AWS  KMS  in AppFlow](https://docs.aws.amazon.com/appflow/latest/user guide/security-kms.html)

400.A company is developing a mobile gaming app in a single AWS Region. The app runs on multiple Amazon EC2 instances in an Auto Scaling group. The company stores the app data in Amazon Dynamo db. The app communicates by using TCP traffic and UDP traffic between the users and the servers. The application will be used globally. The company wants to ensure the lowest possible latency for all users. Which solution will meet these requirements?

A �� Use AWS  Global Accelerator to  create  an  accelerator.  Create  an Application  Load  Balancer  (ALB)  behind  an  accelerator endpoint that uses Global Accelerator integration and listening on the TCP and  UDP ports.  Update the Auto Scaling group to register instances on the ALB.

B ��Use AWS Global Accelerator to create an accelerator. Create a Network Load Balancer (NLB) behind an accelerator endpoint that  uses  Global Accelerator  integration and  listening on the TCP  and  UDP  ports.  Update  the Auto Scaling group to  register instances on the NLB.

C ��Create an Amazon Cloud front content delivery network (CDN) endpoint. Create a Network Load Balancer (NLB) behind the endpoint and  listening  on the TCP  and  UDP  ports.  Update  the  Auto Scaling group to  register  instances  on the  NLB.  Update Cloud front to use the NLB as the origin.

D ��Create an Amazon Cloud front content delivery network (CDN) endpoint. Create an Application Load Balancer (ALB) behind the endpoint and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the ALB. Update Cloud front to use the ALB as the origin.

�𰸣�B

������ Correct Answer B Detailed Explanation The correct solution is B because: 1. Global Accelerator optimizes global routing using AWS ��s  private  backbone  network,  reducing  latency for  users worldwide. This  is critical for  real-time  applications  like gaming. 2. Network Load Balancer (NLB) operates at Layer 4 (TCP/UDP), which is required here since the app uses both TCP and UDP traffic. ALB (Layer 7/HTTP-only) and Cloud front (HTTP/S-focused CDN) don��t support UDP. 3. Cloud front (Options C/D) is designed for caching static content (e.g., images, videos) over  HTTP/S, not real-time gaming traffic.  It also doesn ��t  natively support UDP. Why not other options? - A/D: ALB doesn ��t support UDP. - C/D: Cloud front isn ��t designed for real-time UDP/TCP traffic. Reference Links - [AWS Global Accelerator vs. Cloud front](https://aws.amazon.com/global-accelerator/faqs/) - [Network Load Balancer Features](https://aws.amazon.com/elastic load balancing/features/#Details_for_Elastic_Load_Balancing_products)

401.A  company  has  an  application  that  processes  customer  orders.  The  company  hosts  the  application  on  an  Amazon  EC2 instance that saves the orders to an Amazon Aurora database. Occasionally when traffic is high the workload does not process orders fast enough. What should a solutions architect do to write the orders reliably to the database as quickly as possible?

A �� Increase the  instance  size  of the  EC2  instance  when  traffic  is  high.  Write  orders to Amazon  Simple  Notification  Service

(Amazon SNS). Subscribe the database endpoint to the SNS topic.

B ��Write orders to an Amazon Simple Queue Service (Amazon SQS) queue. Use EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SQS queue and process orders into the database.

C��Write orders to Amazon Simple Notification Service (Amazon SNS). Subscribe the database endpoint to the SNS topic. Use EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SNS topic.

D ��Write orders to an Amazon Simple Queue Service (Amazon SQS) queue when the EC2 instance reaches CPU threshold limits. Use scheduled scaling of EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SQS queue and process orders into the database.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution is to use Amazon SQS (Simple Queue Service) to decouple the order processing from the EC2 instance. Here's why: 1. Problem: During traffic spikes, a single EC2 instance can't handle orders fast enough, risking delays or data loss. 2. SQS as a Buffer: Writing orders to an SQS queue acts as a shock absorber. Even if orders flood in, they're safely stored in the queue (first-in-first-out) without overloading the system. 3. Auto Scaling Group (ASG): Using multiple EC2 instances in an ASG allows automatic scaling. If the queue grows (more orders), ASG adds more EC2 instances to process orders in parallel. When traffic drops, it scales down to save costs. 4. Reliability: SQS guarantees at-least-once delivery, so no orders are lost even if an EC2 instance fails. Failed messages can be retried. 5. Load Balancer (ALB): The ALB distributes traffic to  healthy  EC2 instances, improving availability. Why other options are worse: - A/C: SNS (Simple  Notification Service) sends messages to all subscribers immediately but doesn't queue them. If the database is busy, messages get lost. - D: Scaling based on CPU thresholds or schedules  is slower and less  responsive than scaling  based on SQS queue  length (which directly reflects        workload).         Reference         Links:         -         [AWS         SQS](https://aws.amazon.com/sqs/)        -         [AWS         Auto Scaling](https://aws.amazon.com/auto scaling/)                                       -                                       [Decoupling                                       with
SQS](https://docs.aws.amazon.com/white papers/latest/serverless-multi-tier-architectures-api-gateway/sqs-queues.html)

402.An IoT company is releasing a mattress that has sensors to collect data about a user ��s sleep. The sensors will send data to an Amazon S3 bucket. The sensors collect approximately 2 MB of data every night for each mattress. The company must process and summarize the data for each mattress. The results need to be available as soon as possible. Data processing will require 1 GB of memory and will finish within 30 seconds. Which solution will meet these requirements MOST cost-effectively?

A ��Use AWS Glue with a Scala job

B ��Use Amazon EMR with an Apache Spark script C ��Use AWS Lambda with a Python script

D ��Use AWS Glue with a PySpark job

�𰸣�C

������ Correct Answer C �� Use AWS Lambda with a Python script Detailed Explanation AWS Lambda is the most cost - effective solution because: 1. Small Data Size: Each mattress generates only 2 MB of data per night, which is very small. Lambda can easily process this within  its  memory  (1 GB) and time  limits  (30  seconds).  2. Serverless   ��  Event  -  Driven:  Lambda  automatically triggers when new data arrives in Amazon S3, ensuring results are available immediately after processing. No servers to manage!
3. Pay - Per - Use Pricing: Lambda charges you only for the milliseconds your code runs. Since processing takes 30 seconds once per day per mattress, costs are extremely low. Other options are less optimal: - AWS Glue/EMR: These are designed for large -

scale data processing (terabytes/petabytes). They have higher costs for setup, runtime, and infrastructure management, which is overkill for tiny 2  MB files. - Scala/PySpark: These tools add  unnecessary complexity for simple data summarization tasks. A lightweight      Python       script       in      Lambda       is       simpler      and       cheaper.       Reference       Links      -       [AWS       Lambda
Pricing](https://aws.amazon.com/lambda/pricing/)                                                              -                                                              [Lambda
Limits](https://docs.aws.amazon.com/lambda/latest/dg/getting started-limits.html)              -              [S3               Triggers              for Lambda](https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html)

403.A  company  hosts  an  online  shopping  application  that  stores  all  orders  in  an  Amazon  RDS  for  Postgresql  Single-AZ  DB instance. Management wants to eliminate single points of failure and has asked a solutions architect to recommend an approach to   minimize  database   downtime  without   requiring  any  changes  to  the  application  code.  Which  solution   meets  these requirements?

A �� Convert the existing database instance to a  Multi-AZ deployment by modifying the database instance and specifying the Multi-AZ option.

B �� Create  a  new  RDS  Multi-AZ  deployment.  Take  a  snapshot  of  the  current  RDS  instance  and  restore  the  new  Multi-AZ deployment with the snapshot.

C��Create a read-only replica of the Postgresql database in another Availability Zone. Use Amazon Route 53 weighted record sets to distribute requests across the databases.

D ��Place t he RDS for Postgresql database in an Amazon EC2 Auto Scaling group with a minimum group size of two. Use Amazon Route 53 weighted record sets to distribute requests across instances.

�𰸣�A

������ Correct Answer A �� Convert the existing database instance to a Multi-AZ deployment by modifying the database instance and specifying the  Multi-AZ  option.  Detailed  Explanation The  best  solution  is A  because  Amazon  RDS  Multi-AZ  deployments automatically create a standby replica of the database in a different Availability Zone (AZ). This setup eliminates single points of failure  by  ensuring  that  if  the  primary  database  fails,  RDS  will  automatically  fail  over  to  the  standby  replica  with  minimal downtime.  Importantly, this requires  no changes to the application code��the application continues to connect to the same database endpoint, and RDS handles the failover transparently. Other options are less suitable: - B involves manually restoring a snapshot to a new Multi-AZ deployment, which would require downtime and application reconfiguration (to point to the new endpoint). - C uses read replicas, but read replicas are for scaling read operations, not high availability. Promoting a read replica to a primary during a failure requires manual intervention, leading to downtime. - D incorrectly assumes RDS databases can be placed in EC2 Auto Scaling groups, which is not possible��RDS is a managed service separate from EC2. Reference Link [Amazon RDS Multi-AZ Deployments](https://docs.aws.amazon.com/Amazon rds/latest/User guide/Concepts.MultiAZ.html)

404.A company is developing an application to support customer demands. The company wants to deploy the application on multiple Amazon EC2 Nitro-based instances within the same Availability Zone. The company also wants to give the application the ability to write to multiple block storage volumes  in multiple  EC2  Nitro-based instances simultaneously to achieve higher application availability. Which solution will meet these requirements?

A ��Use General Purpose SSD (gp3) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach

B ��Use Throughput Optimized HDD (st1) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach C ��Use Provisioned IOPS SSD (io2) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach

D ��Use General Purpose SSD (gp2) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach

�𰸣�C

���� �� Correct Answer: C Detailed  Explanation: To allow  multiple  EC2 instances to simultaneously write to the same  block storage volume, you need Amazon EBS Multi-Attach, which is only supported by Provisioned IOPS SSD (io2) volumes. Other EBS types like gp3, st1, or gp2 do not support Multi-Attach. Using io2 volumes ensures high availability and concurrent write access across           instances           in           the           same           Availability           Zone.            Reference           Link:           [Amazon           EBS Multi-Attach](https://docs.aws.amazon.com/AWSEC2/latest/User guide/ebs-volumes-multi.html)

405.A company designed a stateless two-tier application that uses Amazon EC2 in a single Availability Zone and an Amazon RDS Multi-AZ DB instance. New company management wants to ensure the application is highly available. What should a solutions architect do to meet this requirement?

A ��Configure the application to use Multi-AZ EC2 Auto Scaling and create an Application Load Balancer

B ��Configure the application to take snapshots of the EC2 instances and send them to a different AWS Region

C ��Configure the application to use Amazon Route 53 latency-based routing to feed requests to the application

D ��Configure Amazon Route 53 rules to handle incoming requests and create a Multi-AZ Application Load Balancer

�𰸣�A

������ Correct Answer: A. Configure the application to use Multi-AZ EC2 Auto Scaling and create an Application Load Balancer Detailed Explanation: The application is stateless, meaning it doesn ��t rely on local instance data, so it can easily scale horizontally. To ensure high availability: 1. Multi-AZ EC2 Auto Scaling: This automatically launches EC2 instances across multiple Availability Zones (AZs). If one AZ fails, Auto Scaling replaces instances in another AZ. 2. Application Load Balancer (ALB): Distributes traffic evenly across EC2 instances in all AZs and reroutes traffic away from unhealthy instances. The existing RDS Multi-AZ DB already handles database redundancy, so the focus is on fixing the EC2 layer (originally in a single AZ). Options B (snapshots) and C/D (Route  53) don ��t  address the critical  issue of  EC2  redundancy  within the  same AWS  Region.  Reference  Links:  -  [EC2  Auto Scaling](https://docs.aws.amazon.com/auto scaling/ec2/user guide/auto-scaling-benefits.html)          -           [Application           Load Balancer](https://docs.aws.amazon.com/elastic load balancing/latest/application/introduction.html)

406.A company uses AWS Organizations. A member account has purchased a Compute Savings Plan. Because of changes in the workloads inside the member account, the account no longer receives the full benefit of the Compute Savings Plan commitment. The company uses less than 50% of its purchased compute power.

A ��Turn on discount sharing from the Billing Preferences section of the account console in the member account that purchased the Compute Savings Plan.

B �� Turn  on  discount  sharing  from  the  Billing  Preferences  section  of  the  account  console  in  the  company's  Organizations management account.
C ��Migrate additional compute workloads from another AWS account to the account that has the Compute Savings Plan.

D ��Sell the excess Savings Plan commitment in the Reserved Instance Marketplace.

�𰸣�B

���� �� Correct Answer B Detailed Explanation When using AWS Organizations, Savings Plans discounts can be shared across member  accounts  to  maximize  utilization.  The   key  points  are:   1.   Discount  sharing  must  be  enabled  at  the  Organizations management account level (Option B). This allows the Savings Plan commitment from one account to apply to eligible usage in other member accounts. 2. The member account that purchased the Savings Plan (Option A) cannot enable sharing alone - this setting  is  controlled  by  the  management  account.  3.  Migrating  workloads  (Option  C)  is  impractical  and  doesn't  address  the fundamental  issue  of  commitment  utilization.  4.  Savings  Plans  cannot  be  resold  (Option  D)  -  this  option  refers  to  Reserved Instances which  have  different  rules.  By  enabling  discount sharing  in  the  management account's  Billing  Preferences,  unused Compute Savings Plan commitment from the purchasing account will automatically apply to other accounts' compute usage in the    organization,    reducing    wasted    commitment.    Reference    Link    [AWS    Savings    Plans    documentation    on    discount sharing](https://docs.aws.amazon.com/savings-plans/latest/user guide/sp-organizations.html)

407.A company is developing a micro services application that will provide a search catalog for customers. The company must use REST APIs to present the front end of the application to users. The REST APIs must access the backend services that the company hosts in containers in private VPC subnets. Which solution will meet these requirements?

A��Design a Web socket API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a private VPC link for API Gateway to access Amazon ECS.

B ��Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a private VPC link for API Gateway to access Amazon ECS.

C��Design a Web socket API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a security group for API Gateway to access Amazon ECS.

D ��Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a security group for API Gateway to access Amazon ECS.

�𰸣�B

���� �� Correct Answer B Detailed Explanation The question requires REST APIs (eliminating Web socket options A and C). To connect Amazon API Gateway to backend services in a private VPC subnet, you must use a VPC Link. Here's why: - VPC Link (AWS Private link) allows API Gateway to securely access resources in a private subnet without exposing them to the public internet. This is the standard method for integrating API Gateway with private backend services. - Security groups (option D) alone cannot solve this because API Gateway is a managed service outside the VPC. Security groups control traffic at the instance level, but they    don   �� t    enable    connectivity    to    private    subnets    from    outside    the   VPC.    Reference    Link    [AWS    VPC    Link Documentation](https://docs.aws.amazon.com/api gateway/latest/developer guide/getting-started-with-private-integration.html)

408.A company stores raw collected data in an Amazon S3 bucket. The data is used for several types of analytics on behalf of the company's customers. The type of analytics requested determines the access pattern on the S3 objects. The company cannot predict or control the access pattern. The company wants to reduce its S3 costs. Which solution will meet these requirements?

A ��Use S3 replication to transition infrequently accessed objects to S3 Standard-Infrequent Access (S3 Standard-IA)

B ��Use S3 Lifecycle rules to transition objects from S3 Standard to Standard-Infrequent Access (S3 Standard-IA) C ��Use S3 Lifecycle rules to transition objects from S3 Standard to S3 Intelligent-Tiering

D ��Use S3 Inventory to identify and transition objects that have not been accessed from S3 Standard to S3 Intelligent-Tiering

�𰸣�C

������ Correct Answer: C Detailed Explanation: The company needs to reduce S3 costs without predicting access patterns. Here �� s why C  (S3  Intelligent-Tiering)  is  the  best  choice:  1.  S3  Intelligent-Tiering  automatically  moves  objects  between  two  tiers:  -	 Frequent  Access  Tier:  For  objects  accessed  often.  -  Infrequent  Access  Tier:  For  objects  rarely  accessed.  It  monitors  access	 patterns  and  adjusts  tiers  automatically,  avoiding  retrieval  fees   (unlike  S3  Standard-IA).  2.  Why   not  other  options:  -  A	 (Replication): Copies  data to another  bucket/region  but  doesn ��t optimize costs  based  on access  patterns.  -  B  (Lifecycle to	 Standard-IA): Forces objects into a cheaper tier but risks high retrieval costs if access patterns change. - D (S3 Inventory): Relies	 on  manual  analysis,  which  is  slow  and  impractical  for  unpredictable  access.  Intelligent-Tiering  ensures  cost  savings  without	 needing   to    predict   usage,   making   it    ideal    for   unpredictable    analytics    workloads.   Reference   Links:   -    [Amazon   S3	 Intelligent-Tiering](https://aws.amazon.com/s3/storage-classes/intelligent-tiering/)                     -                     [S3                     Storage
Classes](https://aws.amazon.com/s3/storage-classes/)

409.A  company  has  applications  hosted  on  Amazon   EC2  instances  with   IPv6  addresses.  The  applications  must  initiate communications with other external applications using the internet. However the company ��s security policy states that any external service cannot initiate a connection to the EC2 instances. What should a solutions architect recommend to resolve this issue?

A ��Create a NAT gateway and make it the destination of the subnet's route table

B ��Create an internet gateway and make it the destination of the subnet's route table

C ��Create a virtual private gateway and make it the destination of the subnet's route table

D ��Create an egress-only internet gateway and make it the destination of the subnet's route table

�𰸣�D

������ Correct Answer: D Detailed Explanation: The egress-only internet gateway is designed for IPv6 to allow outbound traffic (EC2  initiating connections to the  internet) while  blocking  inbound traffic  initiated  from external services.  Unlike  a  standard internet gateway (which allows bidirectional traffic for public IPv6 addresses), the egress-only gateway acts like a one-way door for IPv6, enforcing the security policy. A NAT gateway (A) only works for IPv4, and a virtual private gateway (C) is for VPNs, not general           internet            access.            Reference            Link:            [Egress-only            Internet            Gateways            -           AWS Documentation](https://docs.aws.amazon.com/vpc/latest/user guide/egress-only-internet-gateway.html)

410.A  company  is  creating  an  application  that  runs  on  containers  in  a  VPC.  The  application  stores  and  accesses  data  in  an Amazon S3 bucket. During the development phase, the application will store and access 1 TB of data in Amazon S3 each day. The company wants to minimize costs and wants to prevent traffic from traversing the internet whenever possible. Which solution will meet these requirements?

A ��Enable S3 Intelligent-Tiering for the S3 bucket

B ��Enable S3 Transfer Acceleration for the S3 bucket

C ��Create a gateway VPC endpoint for Amazon S3. Associate this endpoint with all route tables in the VPC

D ��Create an interface endpoint for Amazon S3 in the VPC. Associate this endpoint with all route tables in the VPC

�𰸣�C

������ Correct Answer: C Detailed Explanation: To minimize costs and avoid internet traffic, the best solution is to use a Gateway VPC Endpoint for Amazon S3. Here ��s why: 1. Gateway VPC Endpoint: - Allows direct, private connectivity between your VPC and Amazon S3 without using the internet. - Traffic stays within the AWS network, improving security and avoiding data transfer costs over  the  internet.  -  No  additional  cost �� you  only  pay  for  the  S3  operations  and  data  storage  (no  hourly  charges  or  data processing fees). 2. Why not other options: - A (S3 Intelligent - Tiering): Reduces storage costs but doesn ��t address network traffic. - B (S3 Transfer Acceleration): Uses Cloud front ��s network to speed up transfers but still routes traffic over the internet (increasing costs). - D (Interface Endpoint): Uses AWS Private link but charges hourly for the endpoint and data processing, which is unnecessary since S3 supports the cheaper Gateway Endpoint. By creating a Gateway VPC Endpoint and associating it with all route tables  in  the  VPC,  traffic  to  S3  stays  within  AWS �� s  private  network,  meeting  both  cost  and  security  requirements. Reference Links: - [AWS VPC Endpoints for S3](https://docs.aws.amazon.com/AmazonS3/latest/user guide/private link - interface - endpoints.html) - [Gateway vs. Interface Endpoints](https://aws.amazon.com/vpc/pricing/)

411.A company has a mobile chat application with a data store based in Amazon Dynamo db. Users would like new messages to be  read  with  as  little  latency  as  possible.  A  solutions  architect  needs  to  design  an  optimal  solution  that  requires  minimal application changes. Which method should the solutions architect select?

A ��Configure Amazon Dynamo db Accelerator (DAX) for the new messages table. Update the code to use the DAX endpoint.

B ��Add Dynamo db read replicas to handle the increased read load. Update the application to point to the read endpoint for the read replicas.

C ��Double the number of read capacity units for the new messages table in Dynamo db. Continue to use the existing Dynamo db endpoint.

D ��Add an Amazon  Elastic ache for  Redis cache to the application stack.  Update the application to point to the  Redis cache endpoint instead of Dynamo db.

�𰸣�A

������ Correct Answer: A Detailed Explanation: To minimize latency when reading new messages from Dynamo db while keeping application changes minimal, the best solution is Amazon Dynamo db Accelerator (DAX). Here's why: 1. What is DAX? DAX is an in -  memory  caching  layer  designed  specifically  for  Dynamo db.  It  reduces  read  latency  from  milliseconds  to  microseconds  by caching frequently accessed data. This is ideal for real - time applications like chat apps where users expect instant updates. 2. Minimal  Code  Changes  With  DAX,  you  only  need  to  update  the  application  to  point  to  the  DAX  endpoint  instead  of  the Dynamo db  endpoint.  No  major  code  rewrites  or  caching  logic  (like  cache  invalidation)  are  required.  DAX  handles  caching automatically while  maintaining consistency with the  underlying  Dynamo db table. 3. Why  Not the Other Options? -  B  (Read Replicas): Dynamo db does not natively support read replicas. This option is invalid. - C (Increase Read Capacity): Adding more read capacity units (RCUs) increases throughput but does not reduce latency. It ��s a scaling fix, not a latency optimization. - D (Elastic ache):  While  Redis  can  cache  data,  it  requires  significant  application  changes  (e.g.,  rewriting  queries  to  use  Redis, handling cache logic). This adds complexity and deviates from minimal changes. Reference Link: [Amazon Dynamo db Accelerator (DAX)](https://aws.amazon.com/dynamo db/dax/)

412.A company hosts a website on Amazon EC2 instances behind an Application Load Balancer (ALB). The website serves static content. Website traffic is increasing, and the company is concerned about a potential increase in cost.

A ��Create an Amazon Cloud front distribution to cache state files at edge locations

B ��Create an Amazon Elastic ache cluster. Connect the ALB to the Elastic ache cluster to serve cached files

C ��Create an AWS WAF web ACL and associate it with the ALB. Add a rule to the web ACL to cache static files

D ��Create a second ALB in an alternative AWS Region. Route user traffic to the closest Region to minimize data transfer costs

�𰸣�A

������Correct Answer: A Detailed Explanation: The best solution is to use Amazon Cloud front, a Content Delivery Network (CDN) that caches static content (like images, CSS, JS files) at edge locations globally. This reduces requests to the EC2 instances and lowers  data transfer  costs  because  users  download  cached  content  from the  nearest  edge  location  instead  of  the ALB/EC2. Option B is incorrect: ALB cannot directly connect to Elastic ache (used for database caching, not static files). Option C is incorrect: AWS WAF is for security (blocking malicious requests), not caching. Option D is incorrect: Adding a second ALB in another Region adds   complexity   and   doesn   �� t    efficiently    cache    content    like    Cloud front.    Reference    Link:    [Amazon    Cloud front Overview](https://aws.amazon.com/cloud front/)

413.A company has multiple VPCs across AWS Regions to support and run workloads that are isolated from workloads in other Regions. Because of a recent application launch requirement, the company ��s VPCs must communicate with all other VPCs across all Regions. Which solution will meet these requirements with the LEAST amount of administrative effort?

A �� Use  VPC  peering  to  manage  VPC  communication  in  a  single  Region.  Use  VPC  peering  across  Regions  to  manage  VPC communications.

B ��Use AWS Direct Connect gateways across all Regions to connect VPCs across regions and manage VPC communications.

C ��Use AWS Transit Gateway to manage VPC communication in a single Region and Transit Gateway peering across Regions to manage VPC communications.

D ��Use AWS Private link across all Regions to connect VPCs across Regions and manage VPC communications

�𰸣�C

�� �� ��  Correct  Answer:   C  Detailed   Explanation:  The   best  solution  is  C  because  AWS  Transit   Gateway   simplifies  VPC communication within a single Region by acting as a central hub. Instead of creating individual VPC peering connections (which becomes  complex  as  the  number  of  VPCs  grows),  you  attach  all  VPCs  in  a  Region  to  a  Transit  Gateway.  For  cross-Region communication, you use Transit Gateway peering, which connects Transit Gateways in different Regions. This setup minimizes administrative effort because it avoids the n-squared complexity of VPC peering and provides a scalable, centralized architecture. Other  options  are  less  efficient:  -  A  (VPC  peering)  requires  manually  managing  many  point-to-point  connections,  which  is time-consuming.  -   B   (Direct  Connect)   is  designed  for   hybrid  cloud   (connecting  on-premises  to  AWS),   not  VPC-to-VPC communication. -  D  (Private link)  is for  accessing  specific services  privately,  not full VPC-to-VPC  connectivity.  Reference  Link: [AWS            Transit             Gateway](https://aws.amazon.com/transit-gateway/)              [Transit             Gateway             Inter-Region Peering](https://docs.aws.amazon.com/vpc/latest/tgw/tgw-transit-gateways.html)

414.A  company  is  designing  a  containerized  application  that  will  use  Amazon  Elastic  Container  Service  (Amazon  ECS).  The application  needs to access  a shared file  system that  is  highly  durable  and can  recover  data to another AWS  Region  with  a recovery point objective (RPO) of 8 hours. The file system needs to provide a mount target m each Availability Zone within a Region. A solutions architect wants to use AWS Backup to manage the replication to another Region. Which solution will meet these requirements?

A ��Amazon FSx for Windows File Server with a Multi-AZ deployment

B ��Amazon FSx for NetApp ONTAP with a Multi-AZ deployment

C ��Amazon Elastic File System (Amazon EFS) with the Standard storage class D ��Amazon FSx for OpenZFS
�𰸣�C

�� �� �� Correct Answer:  C  Detailed  Explanation  The  correct  solution  is  Amazon  Elastic  File  System  (Amazon  EFS)  with  the Standard storage class. Here ��s why: 1. Shared File System for Containers: Amazon EFS provides a fully managed, shared file system compatible with Amazon ECS. Containers can simultaneously access the same file system, making it ideal for distributed applications. 2. Multi-AZ Mount Targets: EFS automatically creates mount targets in every Availability Zone (AZ) within a region, ensuring high availability and redundancy. This meets the requirement for mount targets in each AZ. 3. Cross-Region Recovery with AWS Backup: While EFS does not natively replicate data across regions, AWS Backup can create scheduled backups of EFS and copy them to another region. By setting a backup schedule every 8 hours, the Recovery Point Objective (RPO) of 8 hours is achieved. 4. Durability: EFS stores data redundantly across multiple AZs, ensuring high durability. The Standard storage class is designed for frequently accessed files, aligning with typical container workloads. Why Other Options Are Incorrect: - A (FSx for Windows) and D (FSx for OpenZFS): These are not natively integrated with AWS Backup for cross-region replication. They also lack automatic multi-AZ mount targets. - B (FSx for NetApp ONTAP): While it supports cross-region replication via NetApp ��s Snap mirror, the question specifies using AWS Backup, which is not the primary tool for this service. Reference Links: - [Amazon EFS   Multi-AZ   Architecture](https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html#how-it-works-availability)   -    [AWS Backup                                                              for                                                               EFS                                                               Cross-Region
Replication](https://aws.amazon.com/blogs/storage/automate-cross-region-backup-of-amazon-efs-file-systems-using-aws-backu p/)

415.A company is expecting rapid growth in the near future. A solutions architect needs to configure existing users and grant permissions to new users on AWS. The solutions architect has decided to create IAM groups. The solutions architect will add the new users to IAM groups based on department. Which additional action is the MOST secure way to grant permissions to the new users?

A ��Apply service control policies (SCPs) to manage access permissions

B ��Create IAM roles that have least privilege permission. Attach the roles to the IAM groups

C ��Create an IAM policy that grants least privilege permission. Attach the policy to the IAM groups

D ��Create IAM roles. Associate the roles with a permissions boundary that defines the maximum permissions

�𰸣�C

������ Correct Answer: C Detailed Explanation: The most secure way to grant permissions in this scenario is C. Create an IAM policy with least privilege permissions and attach it to the IAM groups. Here's why: 1. Groups + Policies = Efficient Permission Management IAM groups are specifically designed to manage permissions for multiple users. By attaching policies directly to groups:   -   All   users   in   the   group   automatically   inherit   the    permissions   -   It's   easier   to   maintain   consistency   across department-based permissions - Reduces duplication of policies for individual users 2. Why Not Roles (Options B/D)? IAM roles are meant for temporary access (e.g., EC2 instances, cross-account access), not permanent user permissions. Users would need to manually assume roles through the AWS CLI/API, which is impractical for daily access. 3. Why Not SCPs (Option A)? Service

Control  Policies  (SCPs)  are  organization-level  guardrails  in  AWS  Organizations,  not  for  granular  user  permissions.  They  set maximum  permissions  boundaries  but  don't  grant  specific  access.  4.  Least  Privilege  Achieved  Creating  custom  IAM  policies ensures you grant only the necessary permissions (least privilege), which is a core AWS security best practice. Reference Links: - [AWS      IAM      Best      Practices](https://docs.aws.amazon.com/IAM/latest/User guide/best-practices.html)      -       [Using      IAM Groups](https://docs.aws.amazon.com/IAM/latest/User guide/id_groups.html)

416.A group requires permissions to list an Amazon S3 bucket and delete objects from that bucket. An administrator has created the following IAM policy to provide access to the bucket and applied that policy to the group. The group is not able to delete objects in the bucket. The company follows least-privilege access rules.     Which statement should a solutions architect  add to the policy to correct bucket access?

A��

B��

C��

D��

�𰸣�D

������ Correct Answer D Detailed Explanation To delete objects in an S3 bucket, an IAM policy must grant the `s3:Deleteobject` permission on the bucket's objects. The correct resource format for objects in a bucket is `arn:aws:s3:::bucket-name/*`. Option D specifies the correct action (`s3:Deleteobject`) and resource scope (`/*` for all objects in the bucket), adhering to least - privilege access. Other options either target the bucket itself (incorrect for object operations) or use overly broad permissions like `s3:*`, which     violates     least      -     privilege      principles.      Reference     Link      [AWS     S3      Actions,     Resources,      and     Condition Keys](https://docs.aws.amazon.com/service - authorization/latest/reference/list_amazons3.html)

417.A  law firm  needs to share information with the  public. The  information includes  hundreds of files that  must  be  publicly readable. Modifications or deletions of t he files by anyone before a designated future date are prohibited. Which solution will meet these requirements in the MOST secure way?

A ��Upload all files to an Amazon S3 bucket that is configured for static website hosting. Grant read-only IAM permissions to any AWS principals that access the S3 bucket until the designated date.

B ��Create a new Amazon S3 bucket with S3 Versioning enabled. Use S3 Object Lock with a retention period in accordance with the designated date. Configure the S3 bucket for static website hosting. Set an S3 bucket policy to allow read-only access to the objects.

C ��Create a new Amazon S3 bucket with S3 Versioning enabled. Configure an event trigger to run an AWS Lambda function in case of object modification or deletion. Configure the Lambda function to replace the objects with the original versions from a private S3 bucket.

D ��Upload all files to an Amazon S3 bucket that is configured for static website hosting. Select the folder that contains the files. Use S3 Object Lock with a retention period in accordance with the designated date. Grant read-only IAM permissions to any AWS principals that access the S3 bucket.

�𰸣�B

���� �� Correct Answer  B  Detailed  Explanation The correct solution uses S3 Object  Lock with a retention period to prevent modifications or deletions until the designated date. Here ��s why: 1. S3 Object Lock: This feature enforces a retention period, making  objects   immutable  (unchangeable  and  un deletable)   until  the   retention   period  expires.  This  directly   meets  the requirement to block modifications/deletions until the specified date. 2. S3 Versioning: Enabling versioning ensures that even if someone tries to overwrite an object, the original version is  preserved. Combined with Object  Lock, it adds an extra layer of protection.  3.  Bucket  Policy  for  Read-Only  Access: A  bucket  policy  can  grant  public  read  access  to  objects  (via  s3:Getobject permissions) while keeping the bucket secure. This ensures files are publicly readable without allowing edits. 4. Static Website Hosting: While optional for sharing files, this is included in the question ��s context. Other options fail because: - A and D rely on IAM permissions, which don ��t apply to anonymous public users (like the general public). They also lack Object Lock. - C uses a reactive approach (Lambda to restore objects) instead of proactively blocking changes. This creates a risk of temporary data loss or complexity. -  D incorrectly  implies applying Object  Lock to a folder,  but Object  Lock works at the  bucket/object  level,  not folders.  Reference   Links  -  [S3  Object   Lock](https://docs.aws.amazon.com/AmazonS3/latest/user guide/object-lock.html)  -   [S3 Bucket Policies](https://docs.aws.amazon.com/AmazonS3/latest/user guide/bucket-policies.html)

418.A  company  is   making  a   prototype  of  the   infrastructure  for   its  new  website   by  manually  provisioning  the  necessary infrastructure. This infrastructure includes an Auto Scaling group, an Application Load Balancer and an Amazon RDS database. After  the  configuration   has   been  thoroughly  validated,  the  company  wants  the  capability  to   immediately   deploy  the infrastructure for development and production use in two Availability Zones in an automated fashion. What should a solutions architect recommend to meet these requirements?

A ��Use AWS Systems Manager to replicate and provision the prototype infrastructure in two Availability Zones

B ��Define the infrastructure as a template by using the prototype infrastructure as a guide. Deploy the infrastructure with AWS Cloud formation.

C ��Use AWS Config to record the inventory of resources that are used in the prototype infrastructure. Use AWS Config to deploy the prototype infrastructure into two Availability Zones.

D ��Use AWS Elastic Beanstalk and configure it to use an automated reference to the prototype infrastructure to automatically deploy new environments in two Availability Zones.

�𰸣�B

������ Correct Answer B Detailed Explanation The best solution is to use AWS Cloud formation to define the infrastructure as code (IaC). Here ��s why: 1. Infrastructure as Code (IaC): Cloud formation allows you to create reusable templates that describe the entire infrastructure (Auto Scaling, Load Balancer, RDS, etc.). This ensures consistency and repeatability across environments (development/production).  2. Automated  Deployment:  Once  the  template  is validated,  you  can  deploy the  infrastructure  in multiple  Availability  Zones  with  a   single  command,  eliminating   manual   processes.  3.  Scalability:  Cloud formation   handles dependencies between resources automatically, ensuring the infrastructure is deployed correctly every time. Why other options are  incorrect:  -  A:  AWS  Systems   Manager   is  for   managing  existing  resources  (e.g.,   patching,  monitoring),  not  deploying infrastructure. - C: AWS Config audits and records resource configurations but cannot deploy infrastructure. - D: Elastic Beanstalk focuses on deploying applications (e.g., code, web servers) but doesn ��t natively automate RDS setup or complex multi - resource templates. Reference Link [AWS Cloud formation](https://aws.amazon.com/cloud formation/)

419.A business application is hosted on Amazon EC2 and uses Amazon S3 for encrypted object storage. The chief information security officer  has  directed that  no  application traffic  between the two  services  should traverse the  public  internet. Which capability should the solutions architect use to meet the compliance requirements?

A ��AWS Key Management Service (AWS KMS)

B ��VPC endpoint C ��Private subnet
D ��Virtual private gateway

�𰸣�B

���� �� Correct Answer: B. VPC endpoint Detailed Explanation: To ensure traffic between Amazon EC2 and Amazon S3 stays within the AWS  network  (avoiding the  public  internet),  use  a VPC endpoint.  Here's why:  *  Problem  with  public  internet:  By default, EC2 instances in a VPC need internet access to communicate with S3. Even in a private subnet, this would require a NAT gateway (which uses the public internet). * VPC endpoint solution: Creates a private tunnel between your VPC and S3. All traffic stays within AWS's internal network, meeting the security requirement. * Other options: * A. AWS KMS manages encryption keys but doesn't control network paths. * C. Private subnets alone don't enable private S3 access (needs VPC endpoint or NAT). * D. Virtual  private  gateways  connect  your  VPC  to  on-premises  networks,  not  AWS  services  like  S3.  Reference  Link:  [AWS  VPC Endpoints Documentation](https://docs.aws.amazon.com/vpc/latest/private link/vpc-endpoints-s3.html)

420.A  company  hosts  a  three-tier  web  application  in  the  AWS  Cloud.  A  Multi-Az amazon  RDS  for  MySQL  server  forms  the database layer Amazon Elastic ache forms the cache layer. The company wants a caching strategy that adds or updates data in the cache when a customer adds an item to the database. The data in the cache must always match the data in the database. Which solution will meet these requirements?

A ��Implement the lazy loading caching strategy

B ��Implement the write-through caching strategy C ��Implement the adding TTL caching strategy

D ��Implement the AWS Appconfig caching strategy

�𰸣�B

�� �� �� Correct Answer:  B.  Implement  the write-through  caching  strategy  Detailed  Explanation:  The  write-through  caching strategy  ensures  that  whenever  data  is  written  to  the  database,  it  is  simultaneously  updated  or  added  to  the  cache.  This guarantees the cache and database stay in sync, meeting the requirement for consistent data. Here's why other options don't work: - A. Lazy Loading: Only updates the cache when data is read (not during writes), risking stale cache data after database updates. - C. TTL (Time-To-Live): Automatically removes old cache entries after a set time but doesn ��t sync data during writes. -
D.   AWS   Appconfig:   Manages   application   configurations,    not   cache-database   synchronization.   Write-through    is   like   a double-check system: when a customer adds an item, it ��s saved to both the database and cache at the same time, preventing mismatches.                                     Reference:                                      [Amazon                                      Elastic ache                                      Best Practices](https://docs.aws.amazon.com/Amazon elastic ache/latest/red-ug/Best practices.html)

421.A company wants to migrate 100 GB of historical data from an on-premises location to an Amazon S3 bucket. The company has a 100 megabits per second (Mbps) internet connection on premises. The company needs to encrypt the data in transit to the S3 bucket. The company will store new data directly in Amazon S3. Which solution will meet these requirements with the LEAST operational overhead?

A ��Use the s3 sync command in the AWS CLI to move the data directly to an S3 bucket

B ��Use AWS Data sync to migrate the data from the on-premises location to an S3 bucket C ��Use AWS Snowball to move the data to an S3 bucket
D ��Set up an IPsec VPN from the on-premises location to AWS. Use the s3 cp command in the AWS CLI to move the data directly to an S3 bucket

�𰸣�B

������ Correct Answer B �� Use AWS Data sync to migrate the data from the on - premises location to an S3 bucket Detailed Explanation The best solution is AWS Data sync because it is a managed service designed specifically for automated, secure, and efficient data transfers between on - premises systems and AWS. Here ��s why: 1. Encryption in Transit: Data sync automatically uses TLS (Transport Layer Security) to encrypt data during transfer, meeting the security requirement without extra configuration.
2.  Operational  Overhead:  While  setting  up  Data sync  requires  installing  an  agent  on  -  premises,  it  simplifies  the  process  by handling  retries,  error  recovery,  and  tracking  progress  automatically.  This  reduces  manual  effort  compared  to  scripting  and monitoring CLI commands (Options A/D). 3. Efficiency: For 100 GB of data, a 100 Mbps connection could take ~2.3 hours, but Data sync optimizes transfer speed and handles large numbers of files  more reliably than manual CLI commands. 4.  Managed Service:  Data sync  is  purpose  -  built  for  migrations,  ensuring  minimal  long  -  term  operational  effort,  even  if  initial  setup  is required. CLI - based solutions (A/D) might need custom scripting for error handling, while Snowball (C) adds physical logistics for a   relatively   small   dataset.   Reference   Links   -   [AWS   Data sync   Overview](https://aws.amazon.com/data sync/)   -   [Data sync Encryption](https://docs.aws.amazon.com/data sync/latest/user guide/encryption    -     in     -     transit.html)    -     [AWS     CLI     vs. Data sync](https://aws.amazon.com/blogs/storage/migrating - data - to - aws - choosing - the - right - tool/)

422.A company containerized a Windows job that runs on .NET 6 Framework under a Windows container. The company wants to run this job in the AWS Cloud. The job runs every 10 minutes. The job ��s runtime varies between 1 minute and 3 minutes. Which solution will meet these requirements MOST cost-effectively?

A �� Create an AWS  Lambda function  based  on the container image of the job. Configure Amazon  Event bridge to invoke the function every 10 minutes.

B ��Use AWS Batch to create a job that uses AWS Fargate resources. Configure the job scheduling to run every 10 minutes.

C �� Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate to run the job. Create a scheduled task based on the container image of the job to run every 10 minutes.

D ��Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate to run the job. Create a standalone task based on the container image of the job. Use Windows task scheduler to run the job every 10 minutes.

�𰸣�C

���� �� Correct Answer: C Detailed Explanation: The most cost-effective solution is to use Amazon ECS on AWS Fargate with a scheduled task. Here's why: 1. Serverless   �� Pay-per-use: Fargate charges only for the time the container runs (1-3 minutes per job). Since the job runs every  10  minutes, there's  no cost when  idle.  2.  Native Scheduling:  ECS scheduled tasks  use Amazon Event bridge for cron-like triggers, eliminating the need for always-running resources (unlike Option D's Windows task scheduler in a running container). 3. Windows Compatibility: Fargate supports Windows containers, unlike AWS Lambda (Option A) which only supports  Linux containers. 4. Simpler than AWS  Batch: AWS  Batch  (Option  B)  is designed for complex  batch workloads, making it overkill for this simple periodic task. Option D would be more expensive as it requires a continuously running container for the Windows scheduler. Option A is invalid because Lambda doesn't support Windows containers. Reference Link: [Amazon

ECS Scheduled Tasks](https://docs.aws.amazon.com/Amazon ecs/latest/developer guide/scheduled_tasks.html)

423.A company is looking for a solution that can store video archives in AWS from old news footage. The company needs to minimize costs and will rarely need to restore these files. When the files are needed, they must be available in a maximum of five minutes. What is the MOST cost-effective solution?

A ��Store the video archives in Amazon S3 Glacier and use Expedited retrievals.

B ��Store the video archives in Amazon S3 Glacier and use Standard retrievals.

C ��Store the video archives in Amazon S3 Standard-Infrequent Access (S3 Standard-IA).

D ��Store the video archives in Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA).

�𰸣�A

������ Correct Answer: B. Store the video archives in Amazon S3 Glacier and use Standard retrievals. Detailed Explanation: The question focuses on minimizing costs for rarely accessed video archives while ensuring files are retrievable within 5 minutes. Here's why Option B is better than the original answer (A): 1. Cost Efficiency: - S3 Glacier is the cheapest storage class for long - term archives, far cheaper than S3 Standard - IA or One Zone - IA (Options C/D). - Standard retrievals in Glacier are much cheaper than Expedited retrievals (Option A). Since the company rarely needs restores, paying extra for Expedited retrievals (A) isn't cost - effective. 2. Retrieval Time: - AWS guarantees Standard retrievals in Glacier complete within 3 -5 hours, but in practice, they often finish in minutes (especially for small files).  For rarely accessed archives, this unpredictability is acceptable. - Expedited retrievals (Option A) guarantee 1 -5 minutes but cost 10x more than Standard retrievals. Since the requirement is maximum 5 minutes, Standard retrievals (B) are sufficient and cheaper. 3. Why Not S3 Standard - IA/One Zone - IA (C/D)? - These are for frequently  accessed  infrequent  data.  Their  storage  costs  are  3  - 4x  higher  than  Glacier,  making  them  unsuitable  for  rarely accessed  archives.   Reference:   [AWS  S3  Storage  Classes](https://aws.amazon.com/s3/storage  -  classes/)   [Glacier   Retrieval Options](https://docs.aws.amazon.com/AmazonS3/latest/user guide/restoring - objects.html)

424.A company is building a three-tier application on AWS. The presentation tier will serve a static website The logic tier is a containerized application. This application will store data in a relational database. The company wants to simplify deployment and to reduce operational costs. Which solution will meet these requirements?

A �� Use Amazon S3 to host static content. Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute power. Use a managed Amazon RDS cluster for the database.

B �� Use Amazon Cloud front to host static content. Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for compute power. Use a managed Amazon RDS cluster for the database.

C ��Use Amazon S3 to host static content. Use Amazon Elastic Ku bernet es Service (Amazon EKS) with AWS Fargate for compute power. Use a managed Amazon RDS cluster for the database.

D �� Use Amazon  EC2  Reserved  Instances  to  host  static  content.  Use  Amazon  Elastic  Ku bernet es  Service  (Amazon  EKS)  with Amazon EC2 for compute power. Use a managed Amazon RDS cluster for the database.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The question asks for a cost-effective and simplified deployment of a 3-tier AWS application. Here's why option A is best: 1. Presentation Tier (Static Website): - Amazon S3 is perfect for hosting static websites.

It's  serverless,  requires  no  maintenance,  automatically  scales,  and  is  cheaper  than   running  EC2  instances.  2.  Logic  Tier (Containerized  App):  -  Amazon   ECS  with  AWS   Fargate  removes  the   need  to  manage  EC2  servers.   Fargate  handles  server provisioning, scaling, and patching automatically (serverless containers), reducing operational work and costs. 3. Database Tier: - Amazon  RDS  is a  managed  relational  database service.  It automates  backups,  updates,  and scaling,  eliminating the  need for manual database administration. Why other options are worse: - Option B: Using Cloud front (a CDN) to host static content is incorrect��Cloud front accelerates content delivery but doesn��t host files. Using EC2 for ECS adds server management overhead.
- Option C: Amazon EKS (Ku bernet es) is more complex than ECS for basic container needs, increasing operational effort. - Option
D:  Hosting  static  content  on  EC2  is  expensive  and  requires  server  management,  defeating  the  goal  of  simplicity  and  cost reduction. Key AWS Concepts: - Serverless = Less Management + Lower Costs: Services like S3, Fargate, and RDS let you focus on code, not servers. - Managed Services: AWS handles infrastructure maintenance, reducing your operational burden. Reference Links: -  [Amazon S3 Static Website  Hosting](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Website hosting.html) - [AWS Fargate Overview](https://aws.amazon.com/fargate/) - [Amazon RDS Features](https://aws.amazon.com/rds/)

425.A company seeks a storage solution for its application. The solution must be highly available and scalable. The solution also must function as a file system be mountable by multiple Linux instances in AWS and on premises through native protocols, and have no minimum size requirements. The company has set up a Site-to-Site VPN for access from its on-premises network to its VPC. Which storage solution meets these requirements?

A ��Amazon FSx Multi-AZ deployments

B ��Amazon Elastic Block Store (Amazon EBS) Multi-Attach volumes

C ��Amazon Elastic File System (Amazon EFS) with multiple mount targets

D ��Amazon Elastic File System (Amazon EFS) with a single mount target and multiple access points

�𰸣�C

������ Correct Answer: C Detailed Explanation: Amazon Elastic File System (EFS) is a fully managed, scalable network file system that supports the NFS protocol. It meets all the requirements: - High Availability   �� Scalability: EFS automatically scales storage and is designed for 99.999999999% (11 9s) durability. Multi-AZ deployments ensure high availability. - Mountable by Multiple Linux Instances: EFS can be mounted concurrently by EC2 instances (in AWS) and on-premises servers (via VPN) using native NFS protocols. - No Minimum Size: EFS charges based on usage with no minimum capacity. - Multiple Mount Targets: Creating mount targets in multiple Availability Zones (AZs) ensures redundancy and accessibility across regions. Option D (single mount target) would reduce availability, while Options A (FSx) and B (EBS) are either protocol-specific (e.g., SMB for FSx) or lack cross-premises support      (EBS).      Reference      Links:      [Amazon      EFS      Features](https://aws.amazon.com/efs/features/)      [EFS      Multi-AZ Resilience](https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html#concepts-availability)

426.A 4-year-old media company is using the AWS Organizations all features feature set to organize its AWS accounts. According to the company's finance team, the billing information on the member accounts must not be accessible to anyone, including the root user of the member accounts. Which solution will meet these requirements?

A ��Add all finance team users to an IAM group. Attach an AWS managed policy named Billing to the group.

B ��Attach an identity-based policy to deny access to the billing information to all users, including the root user.

C ��Create a service control policy (SCP) to deny access to the billing information. Attach the SCP to the root organizational unit (OU).

D ��Convert from the Organizations all features feature set to the Organizations consolidated billing feature set.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The correct solution is to create a Service Control Policy (SCP) and attach it to the root Organizational Unit (OU). Here's why: 1. SCPs in AWS Organizations: - SCPs are used to centrally control permissions for all accounts in an AWS Organization, including restrictions on the root user of member accounts. - Unlike IAM policies (which apply to individual users/roles), SCPs act as guardrails for entire accounts. 2. Why Other Options Fail: - Option A: The `Billing` policy grants billing access, but it can't restrict the root user. - Option B: Identity-based policies can ��t restrict the root user, as root has unrestricted access by default. - Option D: The consolidated billing feature set lacks SCPs, so it can ��t enforce billing restrictions. 3. How SCPs Work: - An SCP with a `Deny` effect for billing-related actions (e.g., `aws-portal:*Billing`) attached to the root OU will block  access  to   billing   information  for  all   users/roles   (including  root)  in  member  accounts.  Reference  Links:  -   [AWS  SCP Documentation](https://docs.aws.amazon.com/organizations/latest/user guide/orgs_manage_policies_scps.html)   -   [Restricting Billing Access with SCPs](https://docs.aws.amazon.com/aws account billing/latest/aboutv2/control-access-billing.html)

427.An ecommerce company runs an application in the AWS Cloud that is integrated with an on-premises warehouse solution. The company uses Amazon Simple Notification Service (Amazon SNS) to send order messages to an on-premises HTTPS endpoint so the warehouse application can process the orders. The local data center team has detected that some of the order messages were not received. A solutions architect needs to retain messages that are not delivered and analyze the messages for up to 14 days. Which solution will meet these requirements with the LEAST development effort?

A ��Configure an Amazon SNS dead letter queue that has an Amazon Kinesis Data Stream target with a retention period of 14 days.

B ��Add an Amazon Simple Queue Service (Amazon SQS) queue with a retention period of 14 days between the application and Amazon SNS.

C ��Configure an Amazon SNS dead letter queue that has an Amazon Simple Queue Service (Amazon SQS) target with a retention period of 14 days.

D ��Configure an Amazon SNS dead letter queue that has an Amazon Dynamo db target with a TTL attribute set for a retention period of 14 days.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The problem involves undelivered Amazon SNS messages needing retention for 14 days.  Here's why Option C is the  best choice:  1. Amazon SNS  Dead-Letter  Queues  (DLQs): When an SNS  message fails to deliver to a subscribed  endpoint  (e.g.,  an  HTTPS  endpoint), SNS can automatically  route the failed  messages to  a  DLQ after retries.  This  requires  minimal  configuration  and  avoids  custom  code.  2.  Amazon  SQS  as  the  DLQ  Target:  Amazon  SQS  is  a managed    queue    service    that     natively    supports     message    retention    for     up    to    14     days    (configurable    via     the `Message retention period` attribute). By configuring an SQS queue as the DLQ, undelivered messages are stored directly in the queue, retained for the required duration, and can be analyzed later. 3. Least Development Effort: This solution only requires: - Creating an SQS queue with a  14-day retention period. - Configuring the SNS subscription to  use this queue as the  DLQ.  No application  code  changes  or  complex  integrations  are  needed.  Why  Other  Options  Are  Less  Ideal:  -  Option  A  (Kinesis  Data Stream):  Kinesis  requires additional setup (e.g., shards, consumers) and costs more for short-term retention. - Option B (SQS between App and SNS): Adding an SQS queue before SNS changes the architecture (app��SQS��SNS��HTTPS), which introduces unnecessary  complexity.  -  Option  D  (Dynamo db  with  TTL):  Dynamo db  TTL  deletes  items  automatically,  which  risks  losing messages  before  14 days.  It also  requires formatting  messages for  Dynamo db, adding development work.  Reference  Links:  - [Amazon  SNS  Dead-Letter  Queues](https://docs.aws.amazon.com/sns/latest/dg/sns-dead-letter-queues.html)  -   [Amazon  SQS

Message
Retention](https://docs.aws.amazon.com/Aws simple queue service/latest/Sqs developer guide/sqs-message-retention.html)

428.A  gaming  company  uses  Amazon  Dynamo db  to  store  user  information  such  as  geographic  location,  player  data,  and leaderboards. The company needs to configure continuous backups to an Amazon S3 bucket with a minimal amount of coding. The backups must not affect availability of the application and must not affect the read capacity units (RCUs) that are defined for the table. Which solution meets these requirements?

A ��Use an Amazon EMR cluster. Create an Apache Hive job to back up the data to Amazon S3.

B��Export the data directly from Dynamo db to Amazon S3 with continuous backups. Turn on point-in-time recovery for the table.

C ��Configure Amazon Dynamo db Streams. Create an AWS Lambda function to consume the stream and export the data to an Amazon S3 bucket.

D �� Create an AWS  Lambda function to export the data from the database tables to Amazon S3 on a  regular  basis. Turn on point-in-time recovery for the table.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The correct solution is to enable Dynamo db Point-in-Time Recovery (PITR) and use the  Export to S3 feature.  Here ��s  why:  1.  Point-in-Time  Recovery  (PITR):  Enabling  PITR  provides  continuous  backups  by default, allowing you to restore the table to any point in time within the last 35 days. These backups are managed automatically by AWS and do not consume read capacity units (RCUs) or affect the application ��s availability. 2. Export to S3: Dynamo db ��s native  export  feature  lets  you  directly  export  table  data  to  an  S3  bucket  without  writing  custom  code.  This  process  runs asynchronously in the background, so it doesn ��t impact the table ��s performance or RCUs. Why other options are incorrect: - A (EMR + Hive): Requires manual scripting and cluster management, which involves significant coding and operational overhead. - C (Dynamo db Streams + Lambda): While this works for real-time data processing, it requires writing a Lambda function (coding) and might consume RCUs if the Lambda reads directly from the table. - D (Lambda + PITR): Using a Lambda function to export data would require custom code and could consume RCUs if the function performs scans or queries on the table. Key Take away:
Dynamo db ��s built-in PITR and Export to S3 features meet all requirements with zero coding, no RCU impact, and no downtime. Reference                              Links:                               -                               [AWS                               Dynamo db                               Point-in-Time
Recovery](https://docs.aws.amazon.com/amazon dynamo db/latest/developer guide/Point in time recovery.html)        -          [Export Dynamo db Data to S3](https://docs.aws.amazon.com/amazon dynamo db/latest/developer guide/Data export.html)

429.A solutions architect is designing an asynchronous application to process credit card data validation requests for a bank. The application must be secure and be able to process each  request at least once. Which solution will  meet these  requirements MOST cost-effectively?

A �� Use AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) standard queues as the event source. Use AWS Key Management Service (SSE-KMS) for encryption. Add the kms:Decrypt permission for the Lambda execution role.

B ��Use AWS Lambda event source mapping. Use Amazon Simple Queue Service (Amazon SQS) FI FO queues as the event source. Use  SQS  managed  encryption  keys  (SSE-SQS)  for  encryption.  Add  the  encryption  key  invocation  permission  for  the  Lambda function.

C �� Use the AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS)  FI FO queues as the event source. Use AWS KMS keys (SSE-KMS). Add the kms:Decrypt permission for the Lambda execution role.

D ��Use the AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) standard queues as the event source. Use AWS KMS keys (SSE-KMS) for encryption. Add the encryption key invocation permission for the Lambda function.

�𰸣�A

������ Correct Answer A Detailed Explanation The question asks for a secure, cost-effective solution that processes each request at  least  once.  Here's  the  breakdown:  1.  SQS  Standard  Queues  vs.  FI FO  Queues:  -  Standard  Queues guarantee  at  least  once message delivery (required) and offer higher throughput/lower cost. FI FO Queues ensure exactly once processing but are more expensive and unnecessary here since the requirement is at least once. - Eliminates Options B/C (both use FI FO queues, which are costlier). 2. Encryption: - SSE-KMS (AWS KMS encryption) is more secure than SSE-SQS (SQS-managed keys). Both options A and D use SSE-KMS, meeting the security requirement. - Option B uses SSE-SQS, which is less secure (eliminated). 3. Permissions:
- When using SSE-KMS, Lambda needs kms:Decrypt permission in its execution role to decrypt messages. - Option A explicitly adds  kms:Decrypt,  which  is  correct.  -  Option   D  mentions  encryption   key  invocation   permission,  which  is  ambiguous  and potentially incorrect (e.g., kms:Decrypt is the precise required permission). Why A is Best: It uses cost-effective Standard Queues, secure SSE-KMS encryption, and properly configures Lambda with kms:Decrypt. Option D ��s vague permission wording makes it unreliable.   FI FO  queues   (B/C)  add   unnecessary  cost  for  at   least   once   processing.   Reference   Link   -   [AWS  SQS   Queue Types](https://aws.amazon.com/sqs/faqs/)                             -                               [SQS                              Encryption                              with KMS](https://docs.aws.amazon.com/Aws simple queue service/latest/Sqs developer guide/sqs-server-side-encryption.html)        - [Lambda Permissions for KMS](https://docs.aws.amazon.com/lambda/latest/dg/configuration-encryption.html)

430.A company has multiple AWS accounts for development work. Some staff consistently use oversized Amazon EC2 instances, which causes the company to exceed the yearly budget for the development accounts. The company wants to centrally restrict the creation of AWS resources in these accounts. Which solution will  meet these  requirements with the  LEAST development effort?

A ��Develop AWS Systems Manager templates that use an approved EC2 creation process. Use the approved Systems Manager templates to provision EC2 instances.

B��Use AWS Organizations to organize the accounts into organizational units (OUs). Define and attach a service control policy (SCP) to control the usage of EC2 instance types.

C��Configure an Amazon Event bridge rule that invokes an AWS Lambda function when an EC2 instance is created. Stop disallowed EC2 instance types.

D ��Set up AWS Service Catalog products for the staff to create the allowed EC2 instance types. Ensure that staff can deploy EC2 instances only by using the Service Catalog products.

�𰸣�B

������ Correct Answer B Detailed Explanation The best solution is B because AWS Organizations Service Control Policies (SCPs) allow centralized governance across  multiple AWS accounts with  minimal effort.  Here ��s why:  1. Centralized  Restriction:  By organizing accounts into Organizational Units (OUs) in AWS Organizations, the company can attach an SCP to the OU. This SCP can explicitly deny the creation of oversized EC2 instance types (e.g., using a policy to block specific instance families or sizes). All accounts in the OU will automatically inherit these restrictions without needing per-account configuration. 2. Least Development Effort: SCPs require no code development, infrastructure deployment, or ongoing maintenance. Unlike options like Lambda (C) or Service Catalog (D), which require building and managing code/products, SCPs are policy-based and enforced at the organization level. 3. Immediate Enforcement: SCPs prevent users from even launching disallowed instances. Alternatives like Event bridge + Lambda (C) only react after an instance is created, leading to potential delays or temporary resource usage. Why Other Options

Are  Less  Ideal:  -  A  (Systems  Manager  Templates):  Requires  staff  to  adopt  a  specific  workflow.  Users  could  still  bypass  the templates  and  launch  instances  directly.  -  C  (Event bridge  +  Lambda):  Requires  coding,  testing,  and  maintaining  a  Lambda function. Also,  instances  might  run  briefly  before  being  stopped,  incurring  costs.  -  D  (Service  Catalog):  Demands  configuring products and permissions, and users must strictly use Service Catalog. If permissions aren ��t tightly locked down, users might still launch               EC2               instances               directly.                Reference               Link               [AWS               Organizations               and SCPs](https://docs.aws.amazon.com/organizations/latest/user guide/orgs_manage_policies_scps.html)

431.A company uses Amazon EC2 instances to host its internal systems. As part of a deployment operation, an administrator tries to use the AWS CLI to terminate an EC2 instance. However, the administrator receives a 403 (Access Denied) error message. The administrator is using an IAM role that has the following IAM policy attached:    What is the cause of the unsuccessful request?

A ��The EC2 instance has a resource-based policy with a Deny statement.

B ��The principal has not been specified in the policy statement.

C ��The `Action` field does not grant the actions that are required to terminate the EC2 instance.

D ��The request to terminate the EC2 instance does not originate from the CIDR blocks 192.0.2.0/24 or 203.0.113.0/24.

�𰸣�D

������ Correct Answer D Detailed Explanation The IAM policy allows `ec2:Start instances` and `ec2:Stop instances` actions, but it     includes a condition that restricts access to requests originating from specific CIDR blocks: 192.0.2.0/24 or 203.0.113.0/24. When     the administrator uses the AWS CLI to terminate an EC2 instance, the request is denied because: 1. The termination request does    not come from the allowed IP ranges: The CLI request likely originates from an IP address outside the specified CIDR blocks. 2.    The  policy  does  not  explicitly  allow  `ec2:Terminate instances` :   Even  if  the   IP  were  valid,  the   policy   lacks  permission  for    termination. However, the immediate cause of the 403 error here is the IP restriction (the request fails the `Ip address` condition).   For beginners: IAM policies can restrict access based on IP addresses. If your CLI/API calls come from an IP not listed in the policy �� s `aws:Source ip` condition, AWS blocks the request, even if the action is allowed. Always check both the actions and conditions     in      IAM       policies       when      troubleshooting       access       issues.      Reference       Link       [AWS       IAM       Policy      Condition    Keys](https://docs.aws.amazon.com/IAM/latest/User guide/reference_policies_condition-keys.html)

432.A company is conducting an internal audit. The company wants to ensure that the data in an Amazon S3  bucket that is associated with the company ��s AWS  Lake  Formation data  lake does  not contain sensitive customer or employee data. The company wants to discover  personally  identifiable  information  (P II) or financial  information, including  passport  numbers and credit card numbers. Which solution will meet these requirements?

A �� Configure  AWS Audit  Manager  on the  account. Select the  Payment  Card  Industry  Data  Security  Standards  (PCI  DSS)  for auditing.

B ��Configure Amazon S3 Inventory on the S3 bucket Configure Amazon Athena to query the inventory.

C ��Configure Amazon Macie to run a data discovery job that uses managed identifiers for the required data types.

D ��Use Amazon S3 Select to run a report across the S3 bucket.

�𰸣�C

������ Correct Answer C �� Configure Amazon Macie to run a data discovery job that uses managed identifiers for the required

data types.  Explanation  Amazon  Macie  is  specifically  designed  to  automatically  discover  and  classify  sensitive  data,  such  as personally identifiable information (P II), financial information (e.g., credit card numbers), and other sensitive data types stored in Amazon S3. It uses machine learning and managed identifiers (predefined patterns for sensitive data like passports, credit cards, etc.) to scan and detect such information. This makes it the best choice for auditing sensitive data in an S3 bucket tied to a data lake. Other options: - A (Audit Manager + PCI DSS): Focuses on compliance audits against standards like PCI DSS but does not actively scan for sensitive data. - B (S3 Inventory + Athena): S3 Inventory lists objects and metadata, but Athena queries alone cannot detect sensitive content within files. - D (S3 Select): Queries specific data within files but requires manual setup and does not      automate      sensitive       data       discovery.      Reference       Link       [Amazon      Macie         �C      Data       Security      and Privacy](https://aws.amazon.com/macie/)

433.A company has a service that reads and writes large amounts of data from an Amazon S3 bucket in the same AWS Region. The service is deployed on Amazon EC2 instances within the private subnet of a VPC. The service communicates with Amazon S3 over a NAT gateway in the public subnet. However, the company wants a solution that will reduce the data output costs. Which solution will meet these requirements MOST cost-effectively?

A��Provision a dedicated EC2 NAT instance in the public subnet. Configure the route table for the private subnet to use the elastic network interface of this instance as the destination for all S3 traffic.

B��Provision a dedicated EC2 NAT instance in the private subnet. Configure the route table for the public subnet to use the elastic network interface of this instance as the destination for all S3 traffic.

C ��Provision a VPC gateway endpoint. Configure the route table for the private subnet to use the gateway endpoint as the route for all S3 traffic.

D ��Provision a second NAT gateway. Configure the route table for the private subnet to use this NAT gateway as the destination for all S3 traffic.

�𰸣�C

������ Correct Answer: C Detailed Explanation: When EC2 instances in a private subnet communicate with Amazon S3 through a NAT gateway, data transfer costs are incurred because traffic passes through the NAT gateway (which charges for data processing and data transfer). To reduce costs, use a VPC Gateway Endpoint for S3. A Gateway Endpoint is a free, horizontally scaled entry point that allows private subnet instances to connect directly to S3 without using a NAT gateway, internet gateway, or public IP addresses. Traffic  stays within the AWS  network,  eliminating  NAT gateway  costs  and  reducing  data transfer fees.  -  Option  C correctly configures the  private subnet ��s  route table to send  S3 traffic through the Gateway  Endpoint,  bypassing the  NAT gateway entirely. - Other options (A/B/D) involve NAT instances/gateways, which still incur costs. Reference Link: [VPC Endpoints for Amazon S3](https://docs.aws.amazon.com/vpc/latest/private link/vpc-endpoints-s3.html)

434.A company uses Amazon S3 to store high-resolution pictures in an S3 bucket. To minimize application changes, the company stores the pictures as the latest version of an S3 object. The company needs to retain only the two most recent versions of the pictures. The company wants to reduce costs. The company has identified the S3 bucket as a large expense. Which solution will reduce the S3 costs with the LEAST operational overhead?

A ��Use S3 Lifecycle to delete expired object versions and retain the two most recent versions.

B ��Use an AWS Lambda function to check for older versions and delete all but the two most recent versions.   C ��Use S3 Batch Operations to delete noncurrent object versions and retain only the two most recent versions.

D ��Deactivate versioning on the S3 bucket and retain the two most recent versions.

�𰸣�A

�� �� �� Correct Answer: A  Detailed  Explanation: The  best solution  is A  because Amazon S3  Lifecycle  policies  automate the deletion  of older  object versions while  keeping the two  most  recent  ones.  This  requires  no  manual  intervention,  scripts,  or recurring tasks, minimizing operational overhead. - Option  B (Lambda) adds complexity since you need to write, deploy, and maintain code, which increases operational effort. - Option C (S3 Batch Operations) is designed for one - time bulk actions, not ongoing version management. - Option D (deactivating versioning) would stop new versions but doesn ��t delete old ones, failing to reduce costs. S3 Lifecycle policies natively support retaining a specific number of noncurrent versions. For example, setting a rule to delete noncurrent versions older than 1 day while keeping the 2 most recent versions ensures only the latest two are stored.  Reference  Links:  -  [S3  Lifecycle  for  Versioning](https://docs.aws.amazon.com/AmazonS3/latest/user guide/lifecycle   - configuration - examples. html)

435.A company needs to minimize the cost of its 1 Gbps AWS Direct Connect connection. The company's average connection utilization is less than 10%. A solutions architect must recommend a solution that will reduce the cost without compromising security. Which solution will meet these requirements?

A ��Set up a new 1 Gb ps Direct Connect connection. Share the connection with another AWS account.

B ��Set up a new 200 Mbps Direct Connect connection in the AWS Management Console.

C ��Contact an AWS Direct Connect Partner to order a 1 Gb ps connection. Share the connection with another AWS account.

D ��Contact an AWS Direct Connect Partner to order a 200 Mbps hosted connection for an existing AWS account.

�𰸣�D

�� �� �� Correct Answer  D  Detailed  Explanation  The  company's  current  1  Gb ps  Direct  Connect  connection  is  underutilized (average usage  ��10% = ~100 Mbps). To reduce costs, they can downsize to a smaller, more cost-effective connection. Why D is correct: - Hosted Connection via Partner: AWS Direct Connect hosted connections (offered through AWS Partners) allow flexible bandwidth options (50 Mbps to 10 Gb ps). A 200 Mbps hosted connection is cheaper than a 1 Gb ps dedicated connection while still exceeding the company ��s current needs (100 Mbps average usage). - Cost Savings: Direct Connect pricing is based on port speed and connection type.  Hosted connections  (via  Partners) are typically cheaper than dedicated connections (provisioned directly  by  AWS).  -   No  Security   Compromise:  Both  dedicated  and  hosted  connections  provide  the  same   private,  secure connectivity to AWS. Why other options are incorrect: - A   �� C: Sharing a 1 Gb ps connection with another account doesn ��t reduce the company ��s costs (they still pay for the full 1 Gbps port). - B: A 200 Mbps connection cannot be provisioned directly through the AWS Console �� only AWS  Partners  provide  hosted connections with flexible speeds.  Reference  Link  [AWS  Direct Connect Pricing](https://aws.amazon.com/directconnect/pricing/)

436.A company wants to  ingest  customer  payment  data  into the company's  data  lake  in Amazon S3. The  company  receives payment data every minute on average. The company wants to analyze the payment data in real time. Then the company wants to ingest the data into the data lake. Which solution will meet these requirements with the MOST operational efficiency?

A ��Use Amazon Kinesis Data Streams to ingest data. Use AWS Lambda to analyze the data in real time.

B ��Use AWS Glue to ingest data. Use Amazon Kinesis Data Analytics to analyze the data in real time.

C ��Use Amazon Kinesis Data Firehose to ingest data. Use Amazon Kinesis Data Analytics to analyze the data in real time.

D ��Use Amazon API Gateway to ingest data. Use AWS Lambda to analyze the data in real time.

�𰸣�C

���� �� Correct Answer: C Detailed Explanation: The best solution is Amazon Kinesis Data Firehose for ingestion and Amazon Kinesis  Data  Analytics  for  real-time  analysis.  Here's  why:  1.  Kinesis  Data  Firehose  automatically  ingests  streaming  data  (like payment data arriving every minute) and writes it directly to Amazon S3. It handles buffering, compression, and batching behind the scenes, requiring no server management. This makes it the most operationally efficient ingestion tool for this scenario. 2. Kinesis Data Analytics seamlessly processes streaming data in real time. It integrates directly with Firehose, allowing immediate analysis (e.g., fraud detection, transaction trends) without additional infrastructure setup. 3. Why not other options: - A (Data Streams + Lambda): While Data Streams is good for real-time ingestion, it requires manual shard management. Lambda would need extra code to read from Data Streams and write to S3, increasing operational work. - B (Glue + Analytics): AWS Glue is designed  for   batch  ETL  jobs,   not  real-time  ingestion.  -   D   (API  Gateway  +   Lambda):  API   Gateway  isn  �� t  optimized  for high-frequency  data  streaming.   Lambda  would  need  to  handle  both  processing  and  writing  to  S3,  which  is  less  efficient. Operational  Efficiency:  Firehose  and  Data  Analytics   are  fully   managed  services.  AWS   handles  scaling,  maintenance,  and infrastructure, letting the company focus on data analysis instead of backend management. Reference Links: - [Amazon Kinesis Data                Firehose](https://aws.amazon.com/kinesis/data-firehose/)                 -                [Amazon                 Kinesis                 Data Analytics](https://aws.amazon.com/kinesis/data-analytics/)

437.A company runs an infrastructure monitoring service. The company is building a new feature that will enable the service to monitor data  in customer AWS accounts. The  new feature will  call AWS APIs  in customer  accounts to describe Amazon  EC2 instances and read Amazon Cloud watch metrics. What should the company do to obtain access to customer accounts in the MOST secure way?

A ��Ensure that the customers create an IAM role in their account with read-only EC2 and Cloud watch permissions and a trust policy to the company ��s account.

B �� Create a serverless API that  implements a token vending  machine to  provide temporary AWS credentials for a  role with read-only EC2 and Cloud watch permissions.

C ��Ensure that the customers create an IAM user in their account with read-only EC2 and Cloud watch permissions. Encrypt and store customer access and secret keys in a secrets management system.

D �� Ensure that the  customers create an Amazon Cognito  user  in their  account to  use  an  IAM  role  with  read-only  EC2  and Cloud watch permissions. Encrypt and store the Amazon Cognito user and password in a secrets management system.

�𰸣�A

������ Correct Answer A Detailed Explanation The correct approach is for customers to create an IAM role in their AWS accounts with read-only permissions for EC2 and Cloud watch. This role must include a trust policy that explicitly allows the company's AWS account to assume it. Why this is secure: 1. Temporary credentials: The company uses AWS Security Token Service (STS) to assume  the  role,  which  provides  short-lived  credentials.  This  minimizes  the  risk  of  long-term  credential  exposure.  2.  Least privilege: The  role  grants  only  the  required  read-only  permissions.  3.  No  secrets  to  manage:  The  company  doesn ��t  store customer access keys or passwords, eliminating the risk of leaked credentials. Why other options are less secure: - B: A token vending machine adds unnecessary complexity and isn ��t required when AWS already provides STS for temporary credentials. - C: IAM users require long-term access keys, which are riskier than temporary credentials. - D: Amazon Cognito is designed for user authentication, not cross-account access, and storing passwords increases attack surface.  Reference  Link  [AWS Cross-Account Access Using IAM Roles](https://docs.aws.amazon.com/IAM/latest/User guide/tutorial_cross-account-with-roles.html)

438.A company needs to connect several VPCs  in the us-east-1  Region that span  hundreds of AWS accounts. The company's networking team has its own AWS account to manage the cloud network. What is the MOST operationally efficient solution to connect the VPCs?

A ��Set up VPC peering connections between each VPC. Update each associated subnet ��s route table

B ��Configure a NAT gateway and an internet gateway in each VPC to connect each VPC through the internet

C ��Create an AWS Transit Gateway in the networking team ��s AWS account. Configure static routes from each VPC.

D ��Deploy VPN gateways in each VPC. Create a transit VPC in the networking team ��s AWS account to connect to each VPC.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The most efficient solution is AWS Transit Gateway because it acts as a central hub to connect multiple VPCs across accounts and regions. Here ��s why: - Centralized Management: The networking team can create the Transit  Gateway  in  their  account  and  share  it  with  other  accounts  using  AWS  Resource  Access  Manager  (RAM), avoiding  complex  peering  setups.  -  Simplified   Routing:  VPCs  attach  to  the  Transit  Gateway,  and  routes  are  automatically propagated (or configured via static routes), eliminating the need for manual route-table updates in every VPC. - Scalability: It supports thousands of VPCs, making it ideal for environments with hundreds of accounts. Other options are less efficient: - A (VPC  Peering)  requires  managing  hundreds  of  point-to-point  connections,  which  is  impractical.  -  B  (NAT/Internet  Gateways) exposes traffic to the  public  internet,  which  is  insecure  and  costly.  -  D  (VPN  Gateways  +  Transit  VPC)  adds  complexity  and overhead           compared            to           Transit            Gateway.            Reference            Link:            [AWS           Transit            Gateway Documentation](https://docs.aws.amazon.com/transit-gateway/)

439.A company has Amazon EC2 instances that run nightly batch jobs to process data. The EC2 instances run in an Auto Scaling group that uses On-Demand billing. If a job fails on one instance, another instance will re process the job. The batch jobs run between 12:00 AM and 06:00 AM local time every day. Which solution will provide EC2 instances to meet these requirements MOST cost-effectively?

A ��Purchase a 1-year Savings Plan for Amazon EC2 that covers the instance family of the Auto Scaling group that the batch job uses.

B ��Purchase a 1-year Reserved Instance for the specific instance type and operating system of the instances in the Auto Scaling group that the batch job uses.

C ��Create a new launch template for the Auto Scaling group. Set the instances to Spot Instances. Set a policy to scale out based on CPU usage.

D ��Create a new launch template for the Auto Scaling group. Increase the instance size. Set a policy to scale out based on CPU usage.

�𰸣�C

������ Correct Answer: C Detailed Explanation: Spot Instances are significantly cheaper than On-Demand or Reserved Instances (up to 90% discount) and are ideal for fault-tolerant, interruptible workloads like nightly batch jobs. Since the batch jobs can restart on another instance if interrupted  (as stated  in the  problem), Spot  Instances are a  perfect fit. Auto Scaling with Spot Instances ensures capacity is  maintained cost-effectively during the 6-hour window. Options A and B (Savings  Plans/Reserved

Instances) require long-term commitments and are better suited for steady workloads, not intermittent nightly jobs. Option D (larger    instances)     increases     costs    without     addressing    the     billing     model.     Reference     Link:     [Amazon    EC2     Spot Instances](https://aws.amazon.com/ec2/spot/)

440.A social media company is building a feature for its website. The feature will give users the ability to upload photos. The company expects significant increases in demand during large events and must ensure that the website can handle the upload traffic from users. Which solution meets these requirements with the MOST scalability?

A ��Upload files from the user's browser to the application servers. Transfer the files to an Amazon S3 bucket.

B ��Provision an AWS Storage Gateway file gateway. Upload files directly from the user's browser to the file gateway.

C ��Generate Amazon S3 pre signed URLs in the application. Upload files directly from the user's browser into an S3 bucket.

D �� Provision an Amazon Elastic File System (Amazon EFS) file system. Upload files directly from the user's browser to the file system.

�𰸣�C

������ Correct Answer: C Detailed Explanation: When users upload photos directly to Amazon S3 using pre signed URLs (Option C), the application backend doesn ��t handle the actual file transfer. Instead, it simply generates a temporary, secure URL that allows the user ��s  browser to  upload directly to S3. This approach offloads the  upload traffic from the application servers, enabling automatic scalability  because Amazon S3 is designed to  handle  massive amounts of traffic and concurrent  requests without manual intervention. - Why not A? Uploading via application servers adds a bottleneck.  During traffic spikes, servers might overload, requiring constant scaling (which is less efficient and costlier). - Why not B? Storage Gateway is meant for hybrid cloud setups (e.g., connecting on - premises systems to AWS), not for direct browser uploads. It lacks S3 ��s scalability for public internet traffic. - Why not D? Amazon EFS is a network - attached file system for EC2 instances, not optimized for direct browser uploads.  Scaling  it  requires  managing  EC2  instances,  which  complicates  scalability.  Reference  Links:  -  [Amazon  S3  Pre signed URLs](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Share object pre signed url.html)                        -                        [S3
Scalability](https://aws.amazon.com/s3/features/#Performance)

441.A company has a web application for travel ticketing. The application is based on a database that runs in a single data center in North America. The company wants to expand the application to serve a global user base. The company needs to deploy the application to multiple AWS Regions. Average latency must be less than 1 second on updates to the reservation database. The company  wants  to  have  separate  deployments  of  its  web  platform  across  multiple  Regions.   However,  the  company  must maintain  a  single   primary  reservation  database  that  is  globally  consistent.  Which  solution  should  a  solutions  architect recommend to meet these requirements?

A �� Convert the  application to  use Amazon  Dynamo db.  Use  a global table for the  center  reservation table.  Use  the correct Regional endpoint in each Regional deployment.

B ��Migrate the database to an Amazon Aurora MySQL database. Deploy Aurora Read Replicas in each Region. Use the correct Regional endpoint in each Regional deployment for access to the database.

C ��Migrate the database to an Amazon RDS for MySQL database. Deploy MySQL read replicas in each Region. Use the correct Regional endpoint in each Regional deployment for access to the database.

D ��Migrate the application to an Amazon Aurora Serverless database. Deploy instances of t he database to each Region. Use the correct Regional endpoint in each  Regional deployment to access the database.  Use AWS Lambda functions to process event

streams in each Region to synchronize the databases.

�𰸣�B

������ Correct Answer B Detailed Explanation The company needs a globally consistent single primary database with low latency (under 1 second) for updates. Here ��s why Option B (Amazon Aurora MySQL with Read Replicas) is the best choice: 1. Single Primary Database: Aurora MySQL uses a single  primary instance for writes, ensuring global consistency. All updates go to the primary, avoiding conflicts from multi - region writes (e.g., Dynamo db ��s multi - active setup in Option A would risk eventual consistency).  2.  Low  Replication  Latency:  Aurora �� s  cross  -  region  Read  Replicas  replicate  data  asynchronously  but  typically achieve sub - second replication latency. This meets the 1 - second update latency requirement for replication. While user - to - primary write latency depends on distance, AWS ��s backbone network often keeps this manageable for average global users. 3. Regional Endpoints: Deploying Read Replicas in each Region lets applications read locally (reducing read latency) while directing writes to the primary. This balances performance and consistency. Why Other Options Fail: - A (Dynamo db Global Tables): Multi - active writes lead to eventual consistency, violating the globally consistent single primary requirement. - C (RDS Read Replicas): Cross - region replication for RDS is slower than Aurora, risking higher latency. - D (Aurora Serverless + Lambda Sync): Manual synchronization via Lambda adds complexity and potential delays, unlike Aurora ��s built - in replication. Reference Link [Amazon Aurora Global Database](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/aurora - global - database.html)

442.A company operates a two-tier application for image processing. The application uses two Availability Zones, each with one public subnet and one private subnet. An Application Load Balancer (ALB) for the web tier uses the public subnets. Amazon EC2 instances for the application tier use the private subnets. Users report that the application is running more slowly than expected. A security audit of the web server log files shows that the application is receiving millions of illegitimate requests from a small number  of  IP  addresses.  A  solutions  architect   needs  to  resolve  the  immediate  performance  problem  while  the  company investigates a more permanent solution. What should the solutions architect recommend to meet this requirement?

A ��Modify the inbound security group for the web tier. Add a deny rule for the IP addresses that are consuming resources.

B �� Modify  the  network ACL  for the web tier  subnets. Add  an  inbound  deny  rule  for the  IP  addresses  that  are  consuming resources.

C ��Modify the inbound security group for the application tier. Add a deny rule for the IP addresses that are consuming resources.

D ��Modify the network ACL for the application tier subnets. Add an inbound deny rule for the IP addresses that are consuming resources.

�𰸣�B

������ Correct Answer B Detailed Explanation The best immediate solution is to modify the Network ACL (NACL) for the web tier's  public subnets (where the ALB resides).  Here's why: 1. Security Groups vs.  NACLs: - Security Groups (Options A/C) only support allow rules, not explicit deny rules. You can ��t block specific IPs using security groups. - NACLs (Options B/D) act as a firewall at the subnet level and support explicit deny rules. 2. Traffic Flow: - The illegitimate requests first hit the ALB in the public subnets.  Blocking  them  at  the  subnet  level  (with  NACLs)  stops  the  traffic  before  it  reaches  the  ALB  or  backend  instances, reducing resource consumption. 3. Application Tier vs. Web Tier: - The attack targets the web tier (ALB), not the application tier (EC2 instances in private subnets). Modifying rules for the application tier (Options C/D) is irrelevant here. Why Not Option A? Security Groups can ��t  block specific  IPs��they only permit allowed traffic. Blocking requires NACLs.  Reference Links - [AWS Security   Groups   vs.    NACLs](https://docs.aws.amazon.com/vpc/latest/user guide/vpc-security-groups.html)   -    [Network   ACL Basics](https://docs.aws.amazon.com/vpc/latest/user guide/vpc-network-acls.html)

443.A global marketing company has applications that run in t he ap-southeast-2 Region and the eu-west-1 Region. Applications

that run in a VPC in eu-west-1 need to communicate securely with databases that run in a VPC in ap-southeast-2. Which network design will meet these requirements?

A ��Create a VPC peering connection between the eu-west-1 VPC and t he ap-southeast-2 VPC. Create an inbound rule in the eu-west-1 application security group that allows traffic from the database server  IP addresses  in t he ap-southeast-2 security group.

B��Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPC. Update the subnet route tables. Create an inbound rule in t he ap-southeast-2 database security group that references the security group ID of the application servers in eu-west-1.

C ��Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 Vp update the subnet route tables. Create an inbound rule in the ap-southeast-2 database security group that allows traffic from the eu-west-1 application server IP addresses.

D ��Create a transit gateway with a peering attachment between the eu-west-1 VPC and t he ap-southeast-2 VPC. After the transit gateways are properly peered and routing is configured, create an inbound rule in the database security group that references the security group ID of the application servers in eu-west-1.

�𰸣�C

������ Correct Answer: B Detailed Explanation: The correct answer is B. Here's why: 1) VPC Peering: You need to establish a VPC peering  connection  between  the  two  VPCs   in   different  regions  (ap-southeast-2  and  eu-west-1).  This   allows  cross-region communication. 2) Route Tables: After creating the peering connection, you must update route tables in both VPCs to ensure traffic between them is properly routed through the peering connection. 3) Security Groups: The critical point is that when VPCs are in different regions, you cannot reference security group IDs across regions.  However, answer  B is correct because: - The inbound rule is created in the database security group (in ap-southeast-2) - It references the security group ID of application servers in eu-west-1 (which is allowed within the same peering connection) - AWS documentation explicitly states that security group references work across peered VPCs even in different regions Common mistakes: - Answer C incorrectly uses IP addresses (which can change) - Answer  D over complicates with Transit Gateway (not needed for 2 VPCs) - Answer A  uses  IP addresses instead                         of                         security                         group                         references                         Reference                          link:
https://docs.aws.amazon.com/vpc/latest/peering/peering-security-groups.html

444.A  company  is  developing  software that  uses  a  Postgresql  database  schema.  The  company  needs  to  configure  multiple development environments and databases for the company's developers. On average, each development environment is used for half of the 8-hour workday. Which solution will meet these requirements MOST cost-effectively?

A ��Configure each development environment with its own Amazon Aurora Postgresql database

B ��Configure each development environment with its own Amazon RDS for Postgresql Single-AZ DB instances

C ��Configure each development environment with its own Amazon Aurora On-Demand Postgresql-Compatible database D ��Configure each development environment with its own Amazon S3 bucket by using Amazon S3 Object Select
�𰸣�C

�� �� ��   Correct   Answer:   C.   Configure   each   development   environment   with    its   own   Amazon   Aurora   On-Demand Postgresql-Compatible database Detailed Explanation: For development environments that are only used ~4 hours daily, cost

efficiency  depends  on  minimizing  charges  during  idle  periods.  Amazon  Aurora  On-Demand  (specifically  Aurora  Serverless) automatically scales down to zero when inactive, so you only pay for the actual usage time. In contrast, options A and B use always-running database instances (Aurora or RDS), which charge hourly even when idle. Option D (S3) is irrelevant for relational databases. Aurora Serverless eliminates manual start/stop management and ensures cost savings by pausing unused databases automatically.   Reference    Links:   -    [Amazon    Aurora   Serverless    Pricing](https://aws.amazon.com/rds/aurora/serverless/)    - [Compare Aurora vs RDS Pricing Models](https://aws.amazon.com/rds/aurora/pricing/)

445.A company uses AWS Organizations with resources tagged by account. The company also uses AWS Backup to back up its AWS infrastructure resources. The company needs to back up all AWS resources. Which solution will meet these requirements with the LEAST operational overhead?

A��Use AWS Config to identify all untagged resources. Tag the identified resources programmatically. Use tags in the backup plan.

B ��Use AWS Config to identify all resources that are not running. Add those resources to the backup vault.

C ��Require all AWS account owners to review their resources to identify the resources that need to be backed up.

D ��Use Amazon Inspector to identify all noncompliant resources.

�𰸣�A

������Correct Answer A ��Use AWS Config to identify all untagged resources. Tag the identified resources programmatically. Use tags in the backup plan. Detailed Explanation The goal is to back up all AWS resources with minimal effort. Here ��s why Option A works best: 1. AWS Config automatically scans resources across accounts in AWS Organizations and detects untagged resources (saving  manual  effort).  2.  Programmatically  tagging  (e.g.,  using  AWS  Lambda)  ensures  missing  tags  are  added  automatically, aligning with the company ��s existing tag-based strategy. 3. AWS  Backup can then  use these tags to include all resources in backup plans. This eliminates gaps in coverage and avoids relying on error-prone manual processes (like Option C). Other options fail because: - B focuses on not running resources (irrelevant to backups). - C requires manual work (high operational overhead).
- D uses Amazon Inspector (for security vulnerabilities, not backup compliance). By automating tag enforcement with AWS Config and  Lambda,  the  solution  scales  effortlessly  across  accounts  and  resources.   Reference  Links  -   [AWS  Backup  Tag-Based Backup](https://docs.aws.amazon.com/aws-backup/latest/dev guide/backup-plan-tags.html)     -      [AWS      Config      Compliance Monitoring](https://docs.aws.amazon.com/config/latest/developer guide/aggregate-data.html)

446.A social media company wants to allow its users to upload images in an application that is hosted in the AWS Cloud. The company needs a solution that automatically resizes the images so that the images can be displayed on multiple device types. The application experiences unpredictable traffic patterns throughout the day. The company is seeking a highly available solution that maximizes scalability. What should a solutions architect do to meet these requirements?

A ��Create a static website hosted in Amazon S3 that invokes AWS Lambda functions to resize the images and store the images in an Amazon S3 bucket.

B ��Create a static website hosted in Amazon Cloud front that invokes AWS Step Functions to resize the images and store the images in an Amazon RDS database.

C ��Create a dynamic website hosted on a web server that runs on an Amazon EC2 instance. Configure a process that runs on the EC2 instance to resize the images and store the images in an Amazon S3 bucket.

D ��Create a dynamic website hosted on an automatically scaling Amazon Elastic Container Service (Amazon ECS) cluster that creates a resize job in Amazon Simple Queue Service (Amazon SQS). Set up an image-resizing program that runs on an Amazon

EC2 instance to process the resize jobs.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution is option A. Here's why it works well for the requirements: 1. Amazon S3 for Storage: S3 is perfect for storing images as it's highly scalable, durable, and cost-effective. Static website hosting in S3 simplifies serving images. 2. AWS  Lambda for  Image Resizing: -  Lambda automatically scales with  unpredictable traffic (no capacity planning needed) - Serverless = no servers to manage - Pay-per-use cost model (charges only when resizing images) - Highly available by default across AWS Availability Zones 3. Automatic Workflow: When users upload images to S3   �� Triggers Lambda   �� Resizes images   �� Saves resized versions back to S3. This creates a fully automated pipeline. Why other options aren't ideal: - B: RDS isn't suitable for image storage (designed for structured data) - C: EC2 requires manual scaling and server management    -     D:    SQS+ECS     adds    unnecessary     complexity    compared    to     S3+Lambda     Reference    Links:     -    [AWS Lambda](https://aws.amazon.com/lambda/)          -            [Amazon           S3](https://aws.amazon.com/s3/)           -           [Serverless Architectures](https://aws.amazon.com/serverless/)

447.A company is running a micro services application on Amazon EC2 instances. The company wants to migrate the application to an Amazon  Elastic  Ku bernet es  Service  (Amazon  EKS)  cluster  for scalability. The  company  must configure the Amazon  EKS control plane with endpoint private access set to true and endpoint public access set to false to maintain security compliance. The company must also put the data plane in private subnets. However, the company has received error notifications because the node cannot join the cluster. Which solution will allow the node to join the cluster?

A ��Grant the required permission in AWS Identity and Access Management (IAM) to the Amazon eks node role IAM role.

B ��Create interface VPC endpoints to allow nodes to access the control plane.

C ��Recreate nodes in the public subnet. Restrict security groups for EC2 nodes.

D ��Allow outbound traffic in the security group of the nodes.

�𰸣�B

���� �� Correct Answer:  B. Create interface VPC endpoints to allow nodes to access the control plane. Detailed Explanation: When an Amazon EKS cluster is configured with private endpoint access (public access disabled), worker nodes in private subnets cannot  communicate  with  the  EKS  control  plane  (API  server)  over  the  public  internet.  Instead,  they  need  a  secure,  private connection   to   the   control    plane.    Here  �� s   why:   1.    Private   Endpoint    Requirement:    If   the   EKS    control   plane    has `endpoint public access=false`, it only allows connections via the private VPC endpoint. Worker nodes in private subnets (with no internet access)  need  a way to  reach this  private  endpoint.  2.  VPC  Endpoints:  Creating  interface VPC  endpoints for the  EKS service (`com.amazon aws. [region].eks`) allows nodes in private subnets to securely connect to the EKS control plane without internet access. These endpoints act as a private gateway within the VPC. 3. Why Other Options  Fail: - A (IAM  permissions): While the `Amazon eks node role` needs permissions, this error is specifically about nodes failing to join the cluster, which is a networking issue. - C (Public subnet nodes): Placing nodes in public subnets violates the requirement to keep the data plane in private subnets. -  D  (Outbound traffic):  Nodes  already  need outbound access  (e.g., via  NAT gateway) to download container images.  If  security  groups  block  outbound  traffic,  this  could  be  a  secondary  issue,  but  the  primary  fix  is  enabling  private control-plane       access       via       VPC       endpoints.       Reference        Link:       [Amazon        EKS       Cluster       Endpoint       Access
Control](https://docs.aws.amazon.com/eks/latest/user guide/cluster-endpoint.html)

448.A company provides an API interface to customers so the customers can retrieve their financial information.   �� he company expects a larger number of requests during peak usage times of the year. The company requires the API to respond consistently with low latency to ensure customer satisfaction. The company needs to provide a compute host for the API. Which solution will

meet these requirements with the LEAST operational overhead?

A ��Use an Application Load Balancer and Amazon Elastic Container Service (Amazon ECS).

B ��Use Amazon API Gateway and AWS Lambda functions with provisioned concurrency.

C ��Use an Application Load Balancer and an Amazon Elastic Ku bernet es Service (Amazon EKS) cluster.

D ��Use Amazon API Gateway and AWS Lambda functions with reserved concurrency.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best choice is B because it uses Amazon API Gateway and AWS Lambda with provisioned concurrency. Here ��s why: - Serverless architecture: API Gateway and Lambda are fully managed by AWS, meaning the company doesn ��t  need to manage servers, clusters, or scaling logic. This minimizes operational overhead. - Provisioned Concurrency:  Lambda   normally  has  a  cold  start  delay  when  scaling  up.   Provisioned  concurrency   keeps  Lambda  instances pre-initialized and ready to respond instantly, ensuring low latency even during sudden traffic spikes. - Automatic Scaling: API Gateway and  Lambda automatically handle traffic surges without manual intervention, which  is critical for peak usage times. Other options: - A   �� C (ECS/EKS + Load Balancer): Require managing containers, scaling clusters, and maintaining infrastructure, leading to higher operational effort. - D (Reserved Concurrency): Reserved concurrency only limits the max concurrent Lambda executions  but doesn ��t solve  cold starts.  Provisioned  concurrency  is  better for  consistent  latency.  Reference  Links:  -  [AWS Lambda Provisioned Concurrency](https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html) -  [Serverless Architectures with API Gateway and Lambda](https://aws.amazon.com/serverless/)

449.A company wants to send all AWS Systems Manager Session Manager logs to an Amazon S3 bucket for archival purposes. Which solution will meet this requirement with the MOST operational efficiency?

A ��Enable S3 logging in the Systems Manager console. Choose an S3 bucket to send the session data to.

B��Install the Amazon Cloud watch agent. Push all logs to a Cloud watch log group. Export the logs to an S3 bucket from the group for archival purposes.

C ��Create a Systems Manager document to upload all server logs to a central S3 bucket. Use Amazon Event bridge to run the Systems Manager document against all servers that are in the account daily.

D �� Install an Amazon Cloud watch agent. Push all logs to a Cloud watch log group. Create a Cloud watch logs subscription that pushes any incoming log events to an Amazon Kinesis Data Firehose delivery stream. Set Amazon S3 as the destination.

�𰸣�A

������ Correct Answer A Detailed Explanation To send AWS Systems Manager (SSM) Session Manager logs to an S3 bucket with the most operational efficiency, the simplest and most direct method is to enable S3 logging directly in t he SSM console. Option A is correct because Session Manager natively supports logging to S3 with minimal configuration. You only need to specify the S3 bucket in the Session Manager settings, and AWS automatically handles log delivery. This requires no additional agents, scripts, or integrations. Option B and D involve extra steps like installing the Cloud watch agent and configuring log subscriptions, which add  complexity  and  maintenance  overhead.  Option  C  requires  creating  custom  SSM  documents  and  scheduling  them  via Event bridge,  which  is  unnecessarily  complicated  for  this   use  case.   By  choosing  Option  A,  the  company  avoids  managing additional services or writing custom code, ensuring the solution is both simple and operationally efficient. Reference Link [AWS Session Manager Logging](https://docs.aws.amazon.com/systems-manager/latest/user guide/session-manager-logging.html)

450.An  application  uses  an Amazon  RDS  MySQL  DB  instance.  The  RDS  database  is  becoming  low  on  disk  space.  A  solutions architect wants to increase the disk space without downtime. Which solution meets these requirements with the LEAST amount of effort?

A ��Enable storage auto scaling in RDS

B ��Increase t he RDS database instance size

C ��Change t he RDS database instance storage type to Provisioned IOPS

D ��Back up t he RDS database, increase the storage capacity, restore the database, and stop the previous instance

�𰸣�A

������ Correct Answer: A Detailed Explanation: The easiest way to resolve low disk space without downtime is A. Enable storage auto scaling  in  RDS.  This  feature  automatically  scales  storage  capacity  when  usage  reaches  a  threshold,  requires  no  manual intervention,  and  avoids  downtime.  -  Option  B  (increase  instance  size)  changes  compute  resources  (CPU/RAM),  not  storage capacity, and might require downtime during resizing. - Option C (Provisioned IOPS) optimizes storage performance but doesn ��t increase storage size. - Option D (backup/restore) is time-consuming, risky, and causes downtime during the process. Storage auto scaling  (A)  is  fully  managed,  aligns  with  AWS  best  practices,  and  meets  the  least  effort  requirement.  Reference  Link: [Amazon                                                                                                    RDS                                                                                                    Storage
Auto scaling](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_PIOPS.Storage types.html#USER_PIOPS.Autosca ling)

451.A consulting company provides professional services to customers worldwide. The company provides solutions and tools for customers to expedite gathering and analyzing data on AWS. The company needs to centrally manage and deploy a common set of solutions and tools for customers to use for self-service purposes. Which solution will meet these requirements?

A ��Create AWS Cloud formation templates for the customers.

B ��Create AWS Service Catalog products for the customers.

C ��Create AWS Systems Manager templates for the customers.

D ��Create AWS Config items for the customers.

�𰸣�B

�� �� �� Correct Answer:  B  Detailed  Explanation: AWS  Service Catalog allows organizations to create and manage a curated catalog of approved AWS resources (like Cloud formation templates, EC2 instances, etc.) that users can self-service deploy. This aligns with the requirement to centrally manage tools/solutions and enable customer self-service. - Option A (Cloud formation templates)  requires customers to manually deploy templates,  lacking centralized management. - Option C  (Systems  Manager templates) focuses on operational automation (e.g., patching), not solution deployment. - Option D (AWS Config) tracks resource compliance,          unrelated          to           self-service          tool          deployment.           Reference          Link:           [AWS          Service Catalog](https://aws.amazon.com/service-catalog/)

452.A company is designing a  new web application that will  run on Amazon  EC2  Instances. The application will  use Amazon Dynamo db for backend data storage. The application traffic will  be unpredictable. The company expects that the application

read and write throughput to the database will be moderate to high. The company needs to scale in response to application traffic. Which Dynamo db table configuration will meet these requirements MOST cost-effectively?

A �� Configure Dynamo db with provisioned read and write by using the Dynamo db Standard table class. Set Dynamo db auto scaling to a maximum defined capacity.

B ��Configure Dynamo db in on-demand mode by using the Dynamo db Standard table class.

C �� Configure  Dynamo db  with  provisioned  read  and  write  by  using  the  Dynamo db  Standard  Infrequent  Access  (Dynamo db Standard-IA) table class. Set Dynamo db auto scaling to a maximum defined capacity.

D��Configure Dynamo db in on-demand mode by using the Dynamo db Standard Infrequent Access (Dynamo db Standard-IA) table class.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best choice is B (On-Demand mode with Dynamo db Standard table class) because: 1. Unpredictable Traffic: On-Demand mode automatically scales throughput capacity based on actual usage, making it ideal  for  unpredictable  workloads.  You  pay  per  request  without  needing  to  manually  adjust  capacity.  2.  Moderate-to-High Throughput:   On-Demand   handles   sudden   spikes   in   traffic   seamlessly,   avoiding  throttling   or   over provisioning   costs.   3. Cost-Effectiveness: While On-Demand has a higher per-request cost than Provisioned mode, it eliminates the risk of overpaying for unused capacity or under provisioning during traffic spikes. 4. Standard-IA Table Class (Options C/D): The Standard-IA class is cheaper  for  storage  but  charges  more  for  reads/writes.  Since  the  workload  has  moderate-to-high  throughput,  the  higher per-request costs of Standard-IA would outweigh storage savings. In contrast, Provisioned mode (Options A/C) requires guessing capacity limits and risks throttling (if under scaled) or wasted costs (if over scaled). On-Demand simplifies scaling and optimizes costs       for        unpredictable        workloads.        Reference        Links:       -        [Dynamo db        On-Demand        vs.        Provisioned Capacity](https://docs.aws.amazon.com/amazon dynamo db/latest/developer guide/How it works.Readwrite capacity mode.html) - [Dynamo db Table Classes](https://docs.aws.amazon.com/amazon dynamo db/latest/developer guide/Table classes.html)

453.A retail company has several businesses. The IT team for each business manages its own AWS account. Each team account is part of an organization in AWS Organizations. Each team monitors its product inventory levels in an Amazon Dynamo db table in the team's own AWS account. The company is deploying a central inventory reporting application into a shared AWS account. The application must be able to read items from all the teams' Dynamo db tables. Which authentication option will meet these requirements MOST securely?

A ��Integrate Dynamo db with AWS Secrets Manager in the inventory application account. Configure the application to use the correct secret from Secrets Manager to authenticate and read the Dynamo db table. Schedule secret rotation for every 30 days.

B ��In every business account, create an IAM user that has programmatic access. Configure the application to use the correct IAM user access key ID and secret access key to authenticate and read the Dynamo db table. Manually rotate IAM access keys every
30 days.

C ��In every business account, create an IAM role named BU_ROLE with a policy that gives the role access to the Dynamo db table and a trust policy to trust a specific  role in the inventory application account. In the inventory account, create a role named APP_ROLE that allows access to the STS Assume role API operation. Configure the application to use APP_ROLE and assume the cross account role BU_ROLE to read the Dynamo db table.

D �� Integrate  Dynamo db  with  AWS  Certificate  Manager  (ACM).  Generate  identity  certificates  to  authenticate  Dynamo db. Configure the application to use the correct certificate to authenticate and read the Dynamo db table.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The best and most secure approach is using IAM roles with cross-account trust (Option C). Here's why: 1. Temporary Credentials: The central application uses AWS STS (Security Token Service) to assume the `BU_ROLE`  in  each  business  account.  This   provides  short-term  credentials  that  automatically  expire,   reducing  the   risk  of long-term key exposure. 2.  Least  Privilege:  Each `BU_ROLE` only grants access to the specific  Dynamo db table in its account, following the principle of least privilege. 3. No Secret Storage: Unlike Options A/B (which require storing secrets/keys), IAM roles eliminate the  need to  manage sensitive  credentials  manually. 4.  Centralized Control: The `APP_ROLE`  in the  central account controls  which  identities  can  assume  the  cross-account  `BU_ROLE`,  simplifying  permissions  management.  Options  A/B  use long-term credentials  (secrets/keys) that are  riskier  if  leaked. Option  D  is  invalid  because ACM  certificates aren ��t  used for Dynamo db                   authentication.                    Reference                    Link:                    [AWS                    Cross-Account                    IAM
Roles](https://docs.aws.amazon.com/IAM/latest/User guide/tutorial_cross-account-with-roles.html)

454.A  company  runs  a  micro service-based  serverless  web  application.  The  application  must  be  able  to  retrieve  data  from multiple Amazon Dynamo db tables A solutions architect needs to give the application the ability to retrieve the data with no impact on the baseline performance of the application. Which solution will meet these requirements in the MOST operationally efficient way?

A ��AWS AppSync pipeline resolvers

B ��Amazon Cloud front with Lambda@Edge functions

C ��Edge-optimized Amazon API Gateway with AWS Lambda functions D ��Amazon Athena Federated Query with a Dynamo db connector
�𰸣�D

���� �� Correct Answer: D. Amazon Athena Federated Query with a Dynamo db connector Detailed Explanation: The question asks for the most operationally efficient way to retrieve data from multiple Dynamo db tables without impacting the baseline performance of a serverless application.  Here's why Option  D is the best choice:  1. Athena Federated Query allows querying multiple Dynamo db tables using standard SQL without writing custom code. This reduces operational complexity compared to building and maintaining Lambda functions or AppSync resolvers (Options A, B, C). 2. No direct load on Dynamo db: Athena uses a  Dynamo db  connector  to  pull  data.  This  connector  optimizes  queries  to  minimize  read  capacity  unit  (RCU)  consumption, preventing throttling and performance issues on the live tables. 3. Serverless and fully managed: Athena is serverless, so there ��s no infrastructure to manage. It scales automatically and charges per query, aligning with the serverless application ��s architecture.
4. Alternatives are less efficient: - Option A/C (AppSync/API Gateway + Lambda) require writing and maintaining code to query Dynamo db,  increasing  operational  overhead.  -   Option   B  (Lambda@Edge)  introduces  latency  for   non-cacheable  data   and complicates deployment. - These options also risk overloading Dynamo db if not carefully optimized. Tradeoff: Athena is designed for analytics, so it ��s not ideal for real-time queries. However, the question prioritizes baseline performance (avoiding load on Dynamo db)   over   sub-millisecond   latency,   making   Athena   the   best   fit.   Reference   Links:   -    [Amazon   Athena   Federated Query](https://docs.aws.amazon.com/athena/latest/ug/connect-to-a-data-source.html)      -        [Dynamo db       Connector       for Athena](https://docs.aws.amazon.com/athena/latest/ug/connectors-dynamo db.html)

455.A  company  wants  to  analyze  and  troubleshoot  Access  Denied  errors  and  Unauthorized  errors  that  are  related  to  IAM permissions. The company has AWS Cloud trail turned on. Which solution will meet these requirements with the LEAST effort?

A ��Use AWS Glue and write custom scripts to query Cloud trail logs for the errors.

B ��Use AWS Batch and write custom scripts to query Cloud trail logs for the errors.

C ��Search Cloud trail logs with Amazon Athena queries to identify the errors.

D ��Search Cloud trail logs with Amazon Quick sight. Create a dashboard to identify the errors.

�𰸣�C

������Correct Answer: C Detailed Explanation: The easiest solution is to use Amazon Athena to directly query the Cloud trail logs stored in Amazon S3. Here's why: - Athena is a serverless query service that allows you to analyze data in S3 using standard SQL. Cloud trail logs are already stored in S3, so you can query them without moving data or writing custom scripts. - You just need to define the table schema for Cloud trail logs in Athena and run SQL queries to filter `Access denied` or `Unauthorized` errors. - Options A and B (AWS Glue/AWS Batch + custom scripts) require extra effort to set up infrastructure and write code. - Option D (Quick sight)  adds  unnecessary  complexity  because  it  focuses  on  visualization/dashboards,  while  the  problem  only  asks  for troubleshooting (simple queries suffice). Athena provides the fastest, code-free way to analyze Cloud trail logs for IAM-related errors.              Reference               Link:               [AWS              Documentation               -               Querying              AWS               Cloud trail Logs](https://docs.aws.amazon.com/athena/latest/ug/cloud trail-logs.html)

456.A  company  wants  to  add  its  existing  AWS  usage  cost  to  its  operation  cost  dashboard.  A  solutions  architect  needs  to recommend a solution that will give the company access to its usage cost programmatically. The company must be able to access cost data for the current year and forecast costs for the next 12 months. Which solution will meet these requirements with the LEAST operational overhead?

A ��Access usage cost-related data by using the AWS Cost Explorer API with pagination.

B ��Access usage cost-related data by using downloadable AWS Cost Explorer report .csv files.

C ��Configure AWS Budgets actions to send usage cost data to the company through FTP.

D ��Create AWS Budgets reports for usage cost data. Send the data to the company through SMTP.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The correct solution is to use the AWS Cost Explorer API with pagination (Option A). Here's why this works best: 1. Programmatic Access: The Cost Explorer API provides direct programmatic access to cost data, allowing seamless integration with the company's dashboard without manual intervention. 2. Current + Forecast Data: AWS Cost Explorer  natively  supports  viewing  historical  costs  (up  to  13  months)  and  forecasting  future  costs  for  the  next  12  months, fulfilling both  requirements. 3.  Least Operational Overhead:  Using an API eliminates  manual steps like downloading CSV files (Option B) or configuring FTP/SMTP transfers (Options C/D). Once integrated, it runs automatically. 4. Pagination Support: The API's pagination feature ensures large datasets can be retrieved efficiently without data loss. Options C and D (AWS Budgets) are less suitable because: - AWS  Budgets focuses on budget alerts/notifications, not comprehensive cost forecasting. -  FTP/SMTP methods  add  complexity  (server  setup/email  parsing)  and  aren't  fully   automated.  Option  B  (CSV  files)  requires  manual downloads,   which    isn't   truly    programmatic    and   creates    maintenance   work.    Reference    Link:    [AWS    Cost    Explorer Features](https://aws.amazon.com/aws-cost-management/aws-cost-explorer/)                    [Cost                     Explorer                    API
Documentation](https://docs.aws.amazon.com/cost-explorer/latest/Api reference/Welcome.html)

457.A  solutions  architect  is  reviewing  the  resilience  of  an  application.  The  solutions  architect  notices  that  a  database administrator  recently failed  over the  application's Amazon Aurora  Postgresql  database  writer  instance  as  part  of  a  scaling

exercise. The failover resulted in 3 minutes of downtime for the application. Which solution will reduce the downtime for scaling exercises with the LEAST operational overhead?

A ��Create more Aurora Postgresql read replicas in the cluster to handle the load during failover.

B ��Set up a secondary Aurora Postgresql cluster in the same AWS Region. During failover, update the application to use the secondary cluster's writer endpoint.

C ��Create an Amazon Elastic ache for Memcached cluster to handle the load during failover.

D ��Set up an Amazon RDS proxy for the database. Update the application to use the proxy endpoint.

�𰸣�D

���� �� Correct Answer: D Detailed Explanation: Amazon RDS Proxy is a fully managed database proxy that helps applications manage database connections efficiently and handle failover s seamlessly. When using RDS Proxy, the application connects to the proxy endpoint instead of the database directly. During a failover (e.g., scaling exercises or maintenance), the proxy automatically redirects traffic to the new writer instance without requiring application reconfiguration or manual intervention. This reduces downtime  significantly  compared  to  a  direct  connection  to  the  database.  Other  options  involve  more  complex  setups  (like managing multiple clusters or read replicas) or don ��t directly address connection redirection during failover s. RDS Proxy requires minimal     operational     overhead      since      it    ��  s      a     managed      service.     Reference      Link:      [Amazon     RDS      Proxy Documentation](https://docs.aws.amazon.com/Amazon rds/latest/User guide/rds-proxy.html)

458.A company has a regional subscription-based streaming service that runs in a single AWS Region. The architecture consists of web servers and application servers on Amazon EC2 instances. The EC2 instances are in Auto Scaling groups behind Elastic Load Balancers. The architecture includes an Amazon Aurora global database cluster that extends across multiple Availability Zones. The company wants to expand globally and to ensure that its application has minimal downtime. Which solution will provide the MOST fault tolerance?

A ��Extend the Auto Scaling groups for the web tier and the application tier to deploy instances in Availability Zones in a second Region. Use an Aurora global database to deploy the database in the primary Region and the second Region. Use Amazon Route
53 health checks with a failover routing policy to the second Region.

B ��Deploy the web tier and the application tier to a second Region. Add an Aurora Postgresql cross-Region Aurora Replica in the second Region. Use Amazon Route 53 health checks with a failover routing policy to the second Region. Promote the secondary to primary as needed.

C ��Deploy the web tier and the application tier to a second Region. Create an Aurora Postgresql database in the second Region. Use AWS Database Migration Service (AWS DMS) to replicate the primary database to the second Region. Use Amazon Route 53 health checks with a failover routing policy to the second Region.

D �� Deploy the web tier  and the application tier to a second  Region.  Use  an Amazon Aurora global database to deploy the database in the primary Region and the second Region. Use Amazon Route 53 health checks with a failover routing policy to the second Region. Promote the secondary to primary as needed.

�𰸣�D

������ Correct Answer D Detailed Explanation The best solution is Option D because it uses an Aurora Global Database, which is specifically designed for global applications requiring low-latency reads and fast disaster recovery. Here's why: 1. Aurora Global

Database: - Maintains a primary database in one Region and a secondary in another Region with replication lag typically under 1 second.  -   If  the   primary  Region  fails,  you   can  promote  the   secondary  to   primary  in  seconds,  minimizing  downtime.  2. Multi-Region Deployment: - Deploying web/app tiers in both Regions ensures redundancy. - Route 53 ��s failover routing policy directs traffic to the secondary Region if the primary is unhealthy. 3. Why Other Options Are Worse: - Option A: Aurora Global Database already spans Regions, so extending Auto Scaling to a second Region is redundant. - Option B/C: Cross-Region Aurora Replicas or DMS replication have higher latency and slower failover compared to Aurora Global Database. For a beginner: Think of Aurora Global Database as a live backup in another Region that can take over almost instantly if the main Region fails. This setup           ensures           minimal            downtime           during           outages.            Reference           Link            [Aurora           Global Database](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/aurora-global-database.html)

459.A data analytics company wants to migrate its batch processing system to AWS. The company receives thousands of small data files  periodically during the day through FTP. An on-premises  batch job processes the data files overnight. However, the batch job takes hours to finish running. The company wants the AWS solution to process incoming data files as soon as possible with minimal changes to the FTP clients that send the files. The solution must delete the incoming data files after the files have been processed successfully. Processing for each file needs to take 3-8 minutes. Which solution will meet these requirements in the MOST operationally efficient way?

A ��Use an Amazon EC2 instance that runs an FTP server to store incoming files as objects in Amazon S3 Glacier Flexible Retrieval. Configure a job queue in AWS Batch. Use Amazon Event bridge rules to invoke the job to process the objects nightly from S3 Glacier Flexible Retrieval. Delete the objects after the job has processed the objects.

B ��Use an Amazon EC2 instance that runs an FTP server to store incoming files on an Amazon Elastic Block Store (Amazon EBS) volume. Configure a job queue in AWS Batch. Use Amazon Event bridge rules to invoke the job to process the files  nightly from the EBS volume. Delete the files after the job has processed the files.

C �� Use AWS Transfer Family to create an FTP server to store incoming files on an Amazon Elastic  Block Store (Amazon EBS) volume. Configure a job queue in AWS Batch. Use an Amazon S3 event notification when each file arrives to invoke the job in AWS Batch. Delete the files after the job has processed the files.

D �� Use AWS Transfer Family to create an FTP server to store incoming files in Amazon S3 Standard. Create an AWS  Lambda function to process the files and to delete the files after they are processed. Use an S3 event notification to invoke the Lambda function when the files arrive.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The best solution uses AWS Transfer Family to handle FTP uploads without client changes, stores files in Amazon S3 for immediate processing, and uses serverless AWS Lambda triggered by S3 events. Here's why: 1. FTP Handling: AWS Transfer Family provides managed FTP servers compatible with existing clients (no changes needed). Options A/B use EC2+FTP, requiring manual server management. 2. Real-Time Processing: S3 event notifications trigger Lambda instantly when files arrive. Options A/B/C use nightly batch processing (Event bridge/off-peak schedules), which delays processing.
3.  Runtime  Flexibility:  Lambda supports up to  15  minutes execution time, fitting the 3 - 8  minute  processing  requirement.  It auto-scales for parallel file processing. 4. Automatic Cleanup: Lambda can delete S3 files post-processing (simple code action). Batch-based solutions (A/B/C) require extra steps to track/delete files. 5. Operational Efficiency: Lambda/S3/Transfer Family are fully  managed  services  with  zero  infrastructure  maintenance.  EC2/EBS  (A/B/C)  add  operational  overhead  for  server/storage management. Option C incorrectly uses EBS with Transfer Family (not supported) and uses Batch, which is better for large jobs than per-file processing. Option D ��s serverless design minimizes costs/effort while meeting all requirements. Reference Links: - AWS         Transfer         Family:         https://aws.amazon.com/aws-transfer-family/         -          S3         +         Lambda         Integration: https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html

460.A  company  is  migrating  its  workloads  to  AWS.  The  company  has  transactional  and  sensitive  data  in  its  databases.  The company wants to  use AWS Cloud solutions to  increase security and  reduce operational overhead for the databases. Which solution will meet these requirements?

A ��Migrate the databases to Amazon EC2. Use an AWS Key Management Service (AWS KMS) AWS managed key for encryption.

B ��Migrate the databases to Amazon RDS Configure encryption at rest.

C ��Migrate the data to Amazon S3 Use Amazon Macie for data security and protection

D ��Migrate the database to Amazon RDS. Use Amazon Cloud watch Logs for data security and protection.

�𰸣�B

���� �� Correct Answer  B  Detailed  Explanation The  best solution is  B:  Migrate the databases to Amazon  RDS and configure encryption at rest. Here ��s why: 1. Amazon RDS is a managed database service, meaning AWS handles maintenance, backups, and  updates.  This  significantly  reduces  operational  overhead  compared  to  managing  databases  on  Amazon  EC2  (Option  A), where you must handle these tasks yourself. 2. Encryption at rest in Amazon RDS uses AWS Key Management Service (AWS KMS) to automatically encrypt stored data (e.g., databases, backups). This meets the requirement for securing sensitive transactional data. 3. Why other options are incorrect: - Option A: Using EC2 shifts operational work to the company (e.g., patching, scaling), increasing overhead. - Option C: Amazon S3 is for object storage, not transactional databases. Macie detects sensitive data but doesn ��t encrypt  it. - Option  D: Cloud watch Logs  monitor activity but don ��t encrypt data.  Reference  Links -  [Amazon RDS Encryption](https://docs.aws.amazon.com/Amazon rds/latest/User guide/Overview.Encryption.html)       -        [AWS        Managed Services](https://aws.amazon.com/rds/)

461.A  company  has an  online gaming application that  has TCP  and  UDP  multiplayer  gaming  capabilities.  The  company  uses Amazon  Route  53  to  point  the  application traffic  to  multiple  Network  Load  Balancers  (NLBs)  in  different  AWS  Regions.  The company needs to improve application performance and decrease latency for the online game in preparation for user growth. Which solution will meet these requirements?

A ��Add an Amazon Cloud front distribution in front of the NLBs. Increase the Cache-Control max-age parameter.

B ��Replace the NLBs with Application Load Balancers (ALBs). Configure Route 53 to use latency-based routing.

C ��Add AWS Global Accelerator in front of the NLBs. Configure a Global Accelerator endpoint to use the correct listener ports.

D��Add an Amazon API Gateway endpoint behind the NLBs. Enable API caching. Override method caching for the different stages.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The best solution is to use AWS Global Accelerator because it optimizes traffic
routing through AWS's global network infrastructure, reducing latency for real - time applications like online gaming. Here's why:
-  Supports TCP/UDP:  Global Accelerator works with  both  TCP  and  UDP  protocols,  which  are  critical  for  multiplayer  gaming. Options like Cloud front (A) or API Gateway (D) focus on HTTP(S) and aren't suitable for non - HTTP traffic. - Reduces Latency: Global Accelerator uses AWS's backbone network to route traffic to the nearest healthy endpoint (NLBs in this case), minimizing hops and improving performance. Route 53 latency routing (B) alone doesn ��t optimize the network path as effectively. - Static Anycast IPs: It provides fixed IP addresses that act as a single entry point, simplifying DNS management and ensuring consistent routing even during regional failover s. -  Preserves  NLB  Use:  Unlike option B, replacing  NLBs with ALBs would break TCP/UDP support, as ALBs only handle HTTP/HTTPS. Global Accelerator works seamlessly with existing NLBs. Why not other options? - A

(Cloud front): Caching is useless for real - time gaming data. Cloud front also doesn ��t support TCP/UDP. - B (ALBs + Route 53): ALBs can ��t handle TCP/UDP, and Route 53 alone doesn ��t optimize network paths. - D (API Gateway): Designed for REST APIs, not gaming protocols like TCP/UDP. Reference Link: [AWS Global Accelerator](https://aws.amazon.com/global - accelerator/)

462.A company needs to integrate with a third-party data feed. The data feed sends a webhook to notify an external service when  new  data  is  ready for consumption. A  developer wrote an AWS  Lambda function to  retrieve  data when the  company receives a webhook callback. The developer must make the Lambda function available for the third party to call. Which solution will meet these requirements with the MOST operational efficiency?

A ��Create a function URL for the Lambda function. Provide the Lambda function URL to the third party for the webhook.

B �� Deploy an Application Load Balancer (ALB) in front of the Lambda function. Provide the ALB URL to the third party for the webhook.

C��Create an Amazon Simple Notification Service (Amazon SNS) topic. Attach the topic to the Lambda function. Provide the public hostname of the SNS topic to the third party for the webhook.

D ��Create an Amazon Simple Queue Service (Amazon SQS) queue. Attach the queue to the Lambda function. Provide the public hostname of the SQS queue to the third party for the webhook.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution is to create a Lambda Function URL (Option A). Here's why: 1. Operational Simplicity: Lambda Function URLs  provide a built - in HTTPS endpoint for your Lambda function without needing extra services like API Gateway, load balancers, or message queues. This minimizes setup and maintenance. 2. Direct Webhook Integration: Third parties can send HTTP(S) requests directly to the Function URL, which automatically triggers the Lambda. This matches the webhook use case perfectly. 3. Cost - Effective: No additional charges for services like ALB, SNS, or SQS. You only pay for Lambda execution time. Why other options are less efficient: - Option B (ALB): Requires configuring a load balancer, target groups, and routing rules. Overkill for a single - function webhook. - Option C (SNS): Third parties would need AWS credentials or complex integrations to publish to SNS, which isn't typical for web hooks. - Option D (SQS): External services can ��t directly send messages to  SQS  without AWS  credentials,  making  it  impractical  for  third  -  party  web hooks.  Reference  Link:  [AWS  Lambda Function URLs Documentation](https://docs.aws.amazon.com/lambda/latest/dg/lambda-urls.html)

463.A company stores data in Amazon S3. According to regulations, the data must not contain personally identifiable information (P II). The company recently discovered that S3 buckets have some objects that contain P II. The company needs to automatically detect P II in S3 buckets and to notify the company ��s security team. Which solution will meet these requirements?

A��Use Amazon Macie. Create an Amazon Event bridge rule to filter the Sensitive data event type from Macie findings and to send an Amazon Simple Notification Service (Amazon SNS) notification to the security team.

B ��Use Amazon Guard duty. Create an Amazon Event bridge rule to filter the Critical event type from Guard duty findings and to send an Amazon Simple Notification Service (Amazon SNS) notification to the security team.

C ��Use Amazon Macie. Create an Amazon Event bridge rule to filter the Sensitive data:S3Object/Personal event type from Macie findings and to send an Amazon Simple Queue Service (Amazon SQS) notification to the security team.

D ��Use Amazon Guard duty. Create an Amazon Event bridge rule to filter the Critical event type from Guard duty findings and to send an Amazon Simple Queue Service (Amazon SQS) notification to the security team.

�𰸣�A

������ Correct Answer A �� Use Amazon Macie. Create an Amazon Event bridge rule to filter the Sensitive data event type from Macie findings and to send an Amazon Simple  Notification Service  (Amazon SNS)  notification to the  security team.  Detailed Explanation Amazon Macie is specifically designed to automatically discover and classify sensitive data (like P II) stored in Amazon S3 using machine learning. When Macie detects sensitive data, it generates findings categorized as Sensitive data. These findings are sent to AWS Event bridge, a service that routes events based on rules. Here ��s why Option A is correct: 1. Macie vs. Guard duty: Guard duty focuses on detecting security threats (e.g., unusual API activity), not scanning data content for P II. Macie is the right tool for P II detection. 2. Event bridge Rule: The rule filters for the Sensitive data event type (a broad category for all sensitive data findings), ensuring any P II detection triggers a notification. 3. SNS Notification: SNS sends real - time alerts (e.g., emails, SMS) to the  security team, which  is  ideal  for  urgent  notifications.  Options  C  and  D  use  SQS  (a  message  queue),  which  is  better  for decoupling systems but less direct for urgent alerts. Option B and D incorrectly use Guard duty, which doesn ��t detect P II in data content.    Reference    Link    [Amazon    Macie    Documentation](https://docs.aws.amazon.com/macie/latest/user/what    -    is    - macie.html)

464.A company wants to build a logging solution for its multiple AWS accounts. The company currently stores the logs from all accounts in a centralized account. The company has created an Amazon S3 bucket in the centralized account to store the VPC flow logs and AWS Cloud trail logs. All logs must be highly available for 30 days for frequent analysis, retained for an additional 60 days  for   backup   purposes,   and   deleted   90   days   after   creation.   Which   solution   will   meet   these   requirements   MOST cost-effectively?

A��Transition objects to the S3 Standard storage class 30 days after creation. Write an expiration action that directs Amazon S3 to delete objects after 90 days.

B ��Transition objects to the S3 Standard-Infrequent Access (S3 Standard-IA) storage class 30 days after creation. Move all objects to the S3 Glacier Flexible Retrieval storage class after 90 days. Write an expiration action that directs Amazon S3 to delete objects after 90 days.

C ��Transition objects to the S3 Glacier  Flexible Retrieval storage class 30 days after creation. Write an expiration action that directs Amazon S3 to delete objects after 90 days.

D ��Transition objects to the S3 One Zone-Infrequent Access  (S3 One Zone-IA) storage class 30 days after creation.  Move all objects to the S3 Glacier Flexible Retrieval storage class after 90 days. Write an expiration action that directs Amazon S3 to delete objects after 90 days.

�𰸣�C

������ Correct Answer C Detailed Explanation The requirements are: 1. High availability for the first 30 days (frequent analysis): - Start with the S3 Standard storage class  (default when  uploading  objects), which  provides  high  availability  and  durability. 2. Retention for an additional 60 days (backup): - After 30 days, transition logs to S3 Glacier Flexible Retrieval, a low-cost storage class for  archival/backup.  Glacier  is  much  cheaper  than  S3  Standard-IA  or  One Zone-IA,  making  this the  most  cost-effective choice for the 60-day backup phase. 3. Deletion after 90 days: - Configure a lifecycle expiration rule to delete objects 90 days after creation. Why other options are incorrect: - A: Transitioning to S3 Standard after 30 days doesn ��t reduce costs (S3 Standard is already the initial storage class). - B/D: Using S3 Standard-IA or One Zone-IA for the 60-day backup is more expensive than Glacier.    Glacier    is     designed    for     long-term    retention     at    lower     costs.    Reference     Links    -     [Amazon    S3    Storage Classes](https://aws.amazon.com/s3/storage-classes/)                                      -                                      [S3                                      Lifecycle
Configuration](https://docs.aws.amazon.com/AmazonS3/latest/user guide/object-lifecycle-mgmt.html)

465.A company is  building an Amazon  Elastic  Ku bernet es Service  (Amazon  EKS) cluster for its workloads. All secrets that are

stored in Amazon EKS must be encrypted in the Ku bernet es etcd key-value store. Which solution will meet these requirements?

A �� Create a new AWS Key Management Service (AWS KMS)  key. Use AWS Secrets Manager to manage, rotate, and store all secrets in Amazon EKS.

B ��Create a new AWS Key Management Service (AWS KMS) key. Enable Amazon EKS KMS secrets encryption on the Amazon EKS cluster.

C ��Create the Amazon EKS cluster with default options. Use the Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver as an add-on.

D ��Create a new AWS Key Management Service (AWS KMS) key with the alias/aws/ebs alias. Enable default Amazon Elastic Block Store (Amazon EBS) volume encryption for the account.

�𰸣�B

������ Correct Answer: B Detailed Explanation: To encrypt secrets stored in the Ku bernet es etcd key - value store for an Amazon EKS cluster, you must enable KMS secrets encryption during cluster creation. This encrypts secrets at rest in etcd using an AWS KMS key. Here ��s why other options are incorrect: - A (Secrets Manager): Stores secrets externally, not encrypting secrets already in  EKS/etcd.  -  C  (EBS  CSI  driver):  Manages  EBS  volume  encryption  for  pods,  not  etcd  secrets.  -  D  (EBS  volume  encryption): Encrypts      EBS       volumes       attached      to       worker       nodes,      not       etcd       data.       Reference       Link:      [EKS       Secret Encryption](https://docs.aws.amazon.com/eks/latest/user guide/encrypt-secrets.html)

466.A company wants to provide data scientists with near real-time read-only access to the company's production Amazon RDS for Postgresql database. The database is currently configured as a Single-AZ database. The data scientists use complex queries that will not affect the production database. The company needs a solution that is highly available. Which solution will  meet these requirements MOST cost-effectively?

A ��Scale the existing production database in a maintenance window to provide enough power for the data scientists.

B ��Change the setup from a Single-AZ to a Multi-AZ instance deployment with a larger secondary standby instance. Provide the data scientists access to the secondary instance.

C �� Change the setup from a Single-AZ to a  Multi-AZ  instance deployment. Provide two additional read  replicas for the data scientists.

D �� Change the setup from a Single-AZ to a  Multi-AZ cluster deployment with two  readable standby instances.  Provide  read endpoints to the data scientists.

�𰸣�D

������ Correct Answer D Detailed Explanation The best solution is D because it uses a Multi - AZ cluster deployment with two readable standby instances. Here ��s why: - High Availability: Multi - AZ cluster deployments (available for Postgresql on RDS) include a primary DB instance and two readable standby instances across different Availability Zones. If the primary fails, one standby becomes the new primary automatically, ensuring HA. - Near Real - Time Reads: The readable standby instances allow read - only queries (like the data scientists �� complex workloads) without impacting the primary database. This keeps production safe. - Cost - Effective: Unlike adding separate read replicas (Option C), the standby instances in a Multi - AZ cluster are part of the HA setup. This avoids paying for extra standalone replicas. Other options fail because: - A: Scaling the production database doesn ��t isolate the data scientists �� workload and risks impacting production. - B: Traditional Multi - AZ (non - cluster) has a non

- readable standby, so data scientists can ��t query it. - C: Adding read replicas works for read scaling, but replicas are standalone and   don  �� t   contribute   to    HA   (unlike   cluster   standby    instances).   Reference    Links   -   [Amazon    RDS   Multi   -   AZ    DB Cluster](https://docs.aws.amazon.com/Amazon rds/latest/User guide/multi  -  az  -  db  -  clusters  -  concepts.html)  -   [Readable Standby Instances in Multi - AZ Clusters](https://aws.amazon.com/blogs/aws/amazon - rds - multi - az - db - cluster/)

467.A company runs a three-tier web application in the AWS Cloud that operates across three Availability Zones. The application architecture has an Application Load Balancer, an Amazon EC2 web server that hosts user session states, and a MySQL database that runs on an EC2 instance. The company expects sudden increases in application traffic. The company wants to be able to scale to  meet future  application  capacity  demands  and to  ensure  high  availability  across  all  three Availability  Zones.  Which solution will meet these requirements?

A ��Migrate the MySQL database to Amazon RDS for MySQL with a Multi-AZ DB cluster deployment. Use Amazon Elastic ache for Redis with high availability to store session data and to cache reads. Migrate the web server to an Auto Scaling group that is in three Availability Zones.

B ��Migrate the MySQL database to Amazon RDS for MySQL with a Multi-AZ DB cluster deployment. Use Amazon Elastic ache for Memcached with high availability to store session data and to cache reads. Migrate the web server to an Auto Scaling group that is in three Availability Zones.

C ��Migrate the MySQL database to Amazon Dynamo db Use Dynamo db Accelerator (DAX) to cache reads. Store the session data in Dynamo db. Migrate the web server to an Auto Scaling group that is in three Availability Zones.

D ��Migrate the MySQL database to Amazon RDS for MySQL in a single Availability Zone. Use Amazon Elastic ache for Redis with high availability to store session  data  and to cache  reads.  Migrate the  web server to an Auto Scaling group that  is  in three Availability Zones.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution is A because it addresses all requirements for scalability and high availability. Here's why: 1. Amazon RDS Multi-AZ DB Cluster: - Migrating MySQL to RDS Multi-AZ ensures high availability for the database. If the primary DB fails, RDS automatically fails over to a standby replica in another Availability Zone (AZ). - Multi-AZ deployments span three AZs, aligning with the requirement for HA across three AZs. 2. Amazon Elastic ache for Redis (with HA): - Storing session states on EC2 instances is risky because EC2 instances can fail during scaling. Redis (with HA) provides a managed, in-memory cache that persists session data even if a node fails. - Redis supports replication across AZs, ensuring high availability for session data. Memcached (Option B) lacks native HA and persistence, making it unsuitable for critical session data. 3. Auto Scaling Group (ASG) in 3 AZs: - Migrating the web server to an ASG allows automatic scaling to handle traffic spikes. - Deploying instances across three AZs ensures redundancy and availability if one AZ fails. Why other options fail: - Option B: Memcached lacks HA and persistence, risking session data loss. - Option C: Dynamo db is a NoSQL database, which might not fit the existing relational MySQL setup. While Dynamo db scales well, migrating to it could require significant application changes. - Option D: RDS in a single AZ violates the HA requirement��a failure in that AZ would take down the database. Reference Links: - [Amazon RDS            Multi-AZ](https://aws.amazon.com/rds/details/multi-az/)            -            [Amazon            Elastic ache            Redis            vs. Memcached](https://aws.amazon.com/elastic ache/redis-vs-memcached/)                         -                         [Auto                          Scaling
Groups](https://docs.aws.amazon.com/auto scaling/ec2/user guide/auto-scaling-groups.html)

468.A global video streaming company uses Amazon Cloud front as a content distribution network (CDN). The company wants to roll out content in a phased manner across multiple countries. The company needs to ensure that viewers who are outside the countries  to  which  the  company  rolls  out  content  are  not  able  to  view  the  content.  Which  solution  will  meet  these requirements?

A ��Add geographic restrictions to the content in Cloud front by using an allow list. Set up a custom error message.

B ��Set up a new URL tor restricted content. Authorize access by using a signed URL and cookies. Set up a custom error message. C ��Encrypt the data for the content that the company distributes. Set up a custom error message.

D ��Create a new URL for restricted content. Set up a time-restricted access policy for signed URLs.

�𰸣�A

������ Correct Answer A �� Add geographic restrictions to the content in Cloud front by using an allow list. Set up a custom error message. Detailed Explanation The question asks how to restrict content access based on geography using Amazon Cloud front. Here ��s why Option A is correct: 1. Geographic Restrictions (Allow List): - Cloud front natively supports geographic restrictions (geo blocking). By creating an allow list of approved countries, Cloud front automatically blocks viewers outside those countries. - For example, if the company rolls out content in the US and Canada first, they can add these countries to the allow list. Users in other regions (e.g., Europe or Asia) will be blocked. 2. Custom Error Message: - When access is denied, Cloud front can display a custom error message (e.g., Content not available in your region). This improves user experience by explaining why access is restricted. Why Other Options Are Incorrect: - Option B (Signed URLs/Cookies): This controls access via authentication (e.g., paid subscribers),  not  geography.  It  doesn  �� t  solve  the  problem  of  blocking  users  based  on  location.  -  Option  C  (Encryption): Encrypting data protects content from unauthorized viewing but doesn ��t prevent users in restricted regions from accessing the encrypted files. - Option D (Time-Restricted URLs): This limits access by time (e.g., a 24-hour link), not geography. Reference Links:
-                                                             [AWS                                                              Cloud front                                                              Geographic Restrictions](https://docs.aws.amazon.com/Amazon cloud front/latest/Developer guide/geo restrictions.html)      -        [Cloud front Custom Error Pages](https://docs.aws.amazon.com/Amazon cloud front/latest/Developer guide/custom-error-pages.html)

469.A company wants to use the AWS Cloud to improve its on-premises disaster recovery (DR) configuration. The company's core production business application uses Microsoft SQL Server Standard, which runs on a virtual machine (VM). The application has a recovery point objective (RPO) of 30 seconds or fewer and a recovery time objective (RTO) of 60 minutes. The DR solution needs to minimize costs wherever possible. Which solution will meet these requirements?

A ��Configure a multi-site active/active setup between the on-premises server and AWS by using Microsoft SQL Server Enterprise with Always On availability groups.

B ��Configure a warm standby Amazon RDS for SQL Server database on AWS. Configure AWS Database Migration Service (AWS DMS) to use change data capture (CDC).

C ��Use AWS Elastic Disaster Recovery configured to replicate disk changes to AWS as a pilot light.

D ��Use third-party backup software to capture backups every night. Store a secondary set of backups in Amazon S3.

�𰸣�B

������Correct Answer: C. Use AWS Elastic Disaster Recovery configured to replicate disk changes to AWS as a pilot light. Detailed Explanation: The  question  requires  a  Disaster  Recovery  (DR)  solution  that  minimizes  costs  while  meeting  an  RPO  of    �� 30 seconds and RTO of  ��60 minutes. Here ��s why Option C is the best choice: 1. RPO Compliance: AWS Elastic Disaster Recovery (DRS)  replicates  disk  changes continuously  at the  block  level,  ensuring  an  RPO  of  seconds. This  meets the  30  - second  RPO requirement. 2.  RTO  Compliance:  With  a  pilot  light  setup,  DRS  keeps  a  minimal  environment  (e.g.,  replicated  data)  in  AWS. During  a  disaster,  it  automatically  provisions  full  -  scale  resources  (e.g.,  EC2  instances)  from  the  replicated  data.  Recovery typically takes  minutes,  easily fitting within the 60 -  minute  RTO. 3.  Cost  Optimization: The  pilot  light  model  avoids  running

expensive resources (like EC2/RDS instances) 24/7. You only pay for storage and replication, making it far cheaper than options like  running a warm  RDS  standby  (Option  B)  or  multi  -  site  clusters  (Option  A).  4. SQL Server  Compatibility:  Elastic  Disaster Recovery supports application - consistent replication for SQL Server (via VSS), ensuring database integrity. It also works with SQL Server Standard Edition, avoiding the need for costly Enterprise Edition (unlike Option A). Why Other Options Fail: - Option A: Requires SQL Server Enterprise Edition for Always On availability groups, which is expensive and incompatible with the existing Standard Edition. - Option B: A warm RDS standby runs 24/7, increasing costs. While DMS CDC meets RPO, manually promoting RDS and reconfiguring the application could delay RTO. - Option D: Nightly backups violate the 30 - second RPO. Reference: [AWS Elastic      Disaster      Recovery](https://aws.amazon.com/disaster      -      recovery/)      [Disaster      Recovery      Options      in      the Cloud](https://aws.amazon.com/blogs/aws/new - aws - elastic - disaster - recovery/)

470.A  company  has  an  on-premises  server  that  uses  an  Oracle  database  to  process  and  store  customer  information.  The company wants to  use an AWS  database service to achieve  higher  availability and to  improve application  performance. The company also wants to offload reporting from its primary database system. Which solution will meet these requirements in the MOST operationally efficient way?

A ��Use AWS Database Migration Service (AWS DMS) to create an Amazon RDS DB instance in multiple AWS Regions. Point the reporting functions toward a separate DB instance from the primary DB instance.

B �� Use Amazon RDS in a Single-AZ deployment to create an Oracle database. Create a read replica in the same zone as the primary DB instance. Direct the reporting functions to the read replica.

C ��Use Amazon RDS deployed in a Multi-AZ cluster deployment to create an Oracle database. Direct the reporting functions to use the reader instance in the cluster deployment.

D �� Use Amazon RDS deployed in a Multi-AZ instance deployment to create an Amazon Aurora database. Direct the reporting functions to the reader instances.

�𰸣�D

�� �� ��  Correct  Answer:  D  Detailed  Explanation:  The  company  needs  a  solution  that  provides  high  availability,  improved
performance, and the ability to offload reporting workloads from the primary database. Here ��s why Option D is the best choice:
1. High Availability: Amazon RDS Multi-AZ instance deployment ensures high availability by automatically maintaining a standby replica in a different Availability Zone (AZ). If the primary instance fails, AWS automatically fails over to the standby, minimizing downtime.  This  meets  the  requirement  for  higher  availability.  2.  Improved  Performance  with  Aurora:  Amazon  Aurora  is  a MySQL/Postgresql-compatible   database   optimized   for   AWS.   It   provides    up   to   5x   better   performance   than   standard MySQL/Postgresql  and   handles  high  concurrency   more  efficiently  than  traditional  Oracle  databases  on   RDS.  Aurora  �� s distributed, cloud-native architecture scales storage automatically and improves read/write performance, aligning with the goal of enhancing application performance. 3. Offloading Reporting Workloads: Aurora allows the creation of reader instances (read replicas) that  can  handle  read-heavy  workloads  like  reporting.  By  directing  reporting  queries  to  these  reader  instances,  the primary  database  (writer  instance)  is  freed  from  reporting  tasks,  improving  overall  efficiency.  Aurora   read   replicas  are automatically managed by AWS, reducing operational overhead. Why Other Options Are Less Efficient: - Option A: Using multiple AWS  Regions is unnecessary for basic  high availability and adds complexity. Managing cross-region databases is operationally heavy. - Option B: A Single-AZ deployment lacks high availability. Read replicas in the same AZ do not improve availability and still risk  downtime.  -  Option C:  Multi-AZ  Oracle  RDS  is  valid  for  high  availability,  but  Oracle  licensing  costs  are  high,  and  Aurora outperforms  Oracle  in  scalability  and  cost-efficiency.  Key  Take away:  Amazon  Aurora  (Option  D)  combines  high  availability, superior performance, and automated read scaling for reporting, making it the most operationally efficient choice.  Reference Links:        -        [Amazon         Aurora        Overview](https://aws.amazon.com/rds/aurora/)        -         [Amazon        RDS         Multi-AZ Deployments](https://aws.amazon.com/rds/features/multi-az/)

471.A media company uses an Amazon Cloud front distribution to deliver content over the internet. The company wants only premium customers to  have access to the  media streams and file content. The company stores all content in an Amazon S3 bucket. The company also  delivers  content on  demand to customers for  a specific  purpose,  such  as  movie  rentals  or  music downloads. Which solution will meet these requirements?

A ��Generate and provide S3 signed cookies to premium customers.

B ��Generate and provide Cloud front signed URLs to premium customers.

C ��Use origin access control (OAC) to limit the access of non-premium customers.

D ��Generate and activate field-level encryption to block non-premium customers.

�𰸣�B

���� �� Correct Answer:  B Detailed  Explanation: The  best solution is to use Cloud front signed  URLs  because they allow the company  to  securely  grant  time  -  limited  access  to  specific  content  (like  movie   rentals  or  music  downloads)  to  premium customers. Here's why this works: 1. Access Control: Signed URLs contain a unique signature and expiration time, ensuring only authorized users (premium customers) can access the content during the allowed period. 2. Integration with Cloud front: Since the content is delivered via Cloud front, using Cloud front signed URLs directly integrates with the distribution, blocking non - premium  users without valid  URLs.  3. On -  Demand Access:  For  scenarios  like  rentals or temporary  downloads, signed  URLs provide a flexible way to grant temporary access without exposing the S3 bucket to the public. Other options are less suitable: - A (S3 signed cookies): These control access at the S3 level, but if content is delivered via Cloud front, this bypasses Cloud front's security features and might not scale well for large audiences. - C (OAC): Origin Access Control only restricts direct access to the S3 bucket, ensuring users must go through Cloud front. However, it doesn ��t differentiate between premium and non - premium users. - D (Field - level encryption): This encrypts specific data fields, which is unrelated to restricting access based on user tiers. Reference                                           Links:                                            -                                            [Cloud front                                           Signed
URLs](https://docs.aws.amazon.com/Amazon cloud front/latest/Developer guide/private-content-signed-urls.html)  -  [Cloud front vs. S3 Signed URLs](https://aws.amazon.com/blogs/security/choosing-between-s3-pre signed-urls-and-cloud front-signed-urls/)

472.A retail company uses a regional Amazon API Gateway API for its public REST APIs. The API Gateway endpoint is a custom domain name that points to an Amazon Route 53 alias record. A solutions architect needs to create a solution that has minimal effects on customers and minimal data loss to release the new version of APIs. Which solution will meet these requirements?

A ��Create a canary release deployment stage for API Gateway. Deploy the latest API version. Point an appropriate percentage of traffic to the canary stage. After API verification, promote the canary stage to the production stage.

B ��Create a new API Gateway endpoint with a new version of the API in OpenAPI YAML file format. Use the import-to-update operation in merge mode into the API in API Gateway. Deploy the new version of the API to the production stage.

C ��Create a new API Gateway endpoint with a new version of the API in OpenAPI JSON file format. Use the import-to-update operation in overwrite mode into the API in API Gateway. Deploy the new version of the API to the production stage.

D ��Create a new API Gateway endpoint with new versions of the API definitions. Create a custom domain name for the new API Gateway API. Point the Route 53 alias record to the new API Gateway API custom domain name.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution is A because using a canary release deployment in Amazon API

Gateway  allows  gradual traffic  shifting  to the  new  API  version.  Here �� s  why:  -  Canary  Deployment  lets  you  route  a  small percentage  of  traffic  (e.g.,  10%)  to  the  new  API  version  while  keeping  most  traffic  on  the  existing  version.  This  minimizes customer impact. - If issues arise, you can roll back instantly by adjusting the traffic split. If the new version works, you promote it to 100% traffic with zero downtime. - No DNS changes (unlike Option D), avoiding Route 53 propagation delays. - Options B/C involve risky import-to-update operations that might overwrite/break existing configurations. - Option D requires creating a new API  and  updating  DNS,  which  introduces  complexity  and  potential  downtime.  Reference:  [Amazon  API   Gateway  Canary Deployments](https://docs.aws.amazon.com/api gateway/latest/developer guide/canary-release.html)

473.A company wants to direct  its  users to  a  backup static  error  page  if the  company's  primary  website  is  unavailable. The primary website's DNS records are hosted in Amazon Route 53. The domain is pointing to an Application Load Balancer (ALB). The   company   needs   a  solution  that   minimizes  changes   and   infrastructure  overhead.  Which   solution  will   meet  these requirements?

A ��Update the Route 53 records to use a latency routing policy. Add a static error page that is hosted in an Amazon S3 bucket to the records so that the traffic is sent to the most responsive endpoints.

B ��Set up a Route 53 active-passive failover configuration. Direct traffic to a static error page that is hosted in an Amazon S3 bucket when Route 53 health checks determine that the ALB endpoint is unhealthy.

C ��Set up a Route 53 active-active configuration with the ALB and an Amazon EC2 instance that hosts a static error page as endpoints. Configure Route 53 to send requests to the instance only if the health checks fail for the ALB.

D ��Update the Route 53 records to use a multivalue answer routing policy. Create a health check. Direct traffic to the website if the health check passes. Direct traffic to a static error page that is hosted in Amazon S3 if the health check does not pass.

�𰸣�B

������ Correct Answer B Detailed Explanation The best solution is Option B because it uses Route 53 ��s active-passive failover configuration. Here ��s why this works: 1. Active-Passive Failover: Route 53 automatically directs traffic to the primary endpoint (the ALB) under normal conditions. If Route 53 health checks detect that the ALB is unhealthy, traffic is rerouted to the passive endpoint (the static error page hosted in Amazon S3). 2. Minimal Changes/Overhead: - The static error page in S3 requires no servers or infrastructure to manage, reducing complexity. - Route 53 ��s built-in health checks and failover routing eliminate the need for additional code or services. 3. Other Options: - Option A (Latency Routing) doesn ��t guarantee failover; it prioritizes low-latency  endpoints,  which   may  not  work   if  the  ALB   is  down.  -   Option   C  (Active-Active  with   EC2)  adds  unnecessary infrastructure (an EC2 instance) and costs. - Option D (Multivalue Routing) isn ��t designed for static backup pages; it ��s better for distributing      traffic       across       multiple       healthy       endpoints.       Reference       Links       -       [Route       53       Active-Passive Failover](https://docs.aws.amazon.com/Route53/latest/Developer guide/dns-failover.html)   -    [Hosting    a   Static    Website    on S3](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Website hosting.html)

474.A recent analysis of a company's IT expenses highlights the need to reduce backup costs. The company's chief information officer wants to simplify the on-premises backup infrastructure and reduce costs by eliminating the use of physical backup tapes. The  company  must  preserve the  existing  investment  in  the on-premises  backup  applications  and workflows. What should  a solutions architect recommend?

A ��Set up AWS Storage Gateway to connect with the backup applications using the NFS interface.

B ��Set up an Amazon EFS file system that connects with the backup applications using the NFS interface. C ��Set up an Amazon EFS file system that connects with the backup applications using the iSCSI interface.

D ��Set up AWS Storage Gateway to connect with the backup applications using the iSCSI-virtual tape library (VTL) interface.

�𰸣�D

������ Correct Answer D �� Set up AWS Storage Gateway to connect with the backup applications using the iSCSI-virtual tape library (VTL) interface. Detailed Explanation The company wants to eliminate physical tapes while preserving its existing backup workflows. AWS Storage Gateway ��s Tape Gateway (VTL interface) is the best solution because: 1. Compatibility with Existing Workflows:  The  Tape  Gateway  emulates  a  physical  tape  library  using  the  industry-standard  iSCSI  interface.  Existing  backup applications (like Veeam, Commvault, etc.) can continue using their current processes, treating the virtual tapes as physical tapes. No changes to backup workflows are needed. 2. Cost Reduction: Virtual tapes are stored in Amazon S3 and Glacier, eliminating the need for physical tape infrastructure (hardware, maintenance, logistics). This reduces costs significantly. 3. Why Not Other Options:  -  A   (NFS):   NFS  is  for  file-based   storage,  not  tape   backups.   Backup  workflows   designed  for  tapes   would  need reconfiguration. -  B/C  (EFS): Amazon  EFS  is  a  cloud-native file  system and doesn ��t support tape emulation or  iSCSI.  It ��s unsuitable          for           tape-based           workflows.            Reference           Link           [AWS           Storage           Gateway           Tape Gateway](https://docs.aws.amazon.com/storage gateway/latest/user guide/Storage gateway tap ri.html)

475.A company has data collection sensors at different locations. The data collection sensors stream a high volume of data to the company. The company wants to design a platform on AWS to ingest and process high-volume streaming data. The solution must be scalable and support data collection in near real time. The company must store the data in Amazon S3 for future reporting. Which solution will meet these requirements with the LEAST operational overhead?

A ��Use Amazon Kinesis Data Firehose to deliver streaming data to Amazon S3.

B ��Use AWS Glue to deliver streaming data to Amazon S3.

C ��Use AWS Lambda to deliver streaming data and store the data to Amazon S3.

D ��Use AWS Database Migration Service (AWS DMS) to deliver streaming data to Amazon S3.

�𰸣�A

���� �� Correct Answer: A  Detailed Explanation: The  best solution is Amazon Kinesis  Data Firehose because it is specifically designed for near real-time streaming data ingestion and requires minimal operational overhead. Here ��s why: - Kinesis Data Firehose automatically scales to handle high data volumes, buffers and batches data for efficient delivery to Amazon S3, and requires no server management. It ��s fully managed, so you don ��t need to worry about infrastructure. - AWS Glue (Option B) is an ETL (Extract, Transform, Load) tool for batch processing, not real-time streaming. - AWS Lambda (Option C) could work but would  need  manual  scaling  and  error  handling,  increasing  operational  complexity.  -  AWS  DMS  (Option  D)  is  for  database migrations,          not           streaming          sensor           data.           Reference          Links:           -           [Amazon          Kinesis           Data Firehose](https://aws.amazon.com/kinesis/data-firehose/)               -                [Streaming                Data               Solutions                on AWS](https://aws.amazon.com/streaming-data/)

476.A company has separate AWS accounts for its finance, data analytics, and development departments. Because of costs and security concerns, the company wants to control which services each AWS account can use. Which solution will  meet these requirements with the LEAST operational overhead?

A ��Use AWS Systems Manager templates to control which AWS services each department can use.

B ��Create organization units (OUs) for each department in AWS Organizations. Attach service control policies (SCPs) to the OUs.

C ��Use AWS Cloud formation to automatically provision only the AWS services that each department can use.

D �� Set  up a  list of  products  in AWS Service Catalog in the AWS accounts to manage and control the  usage of specific AWS services.

�𰸣�B

������ Correct Answer: B Explanation: AWS Organizations allows you to group accounts into Organization Units (OUs) based on departments (e.g., finance, analytics). By attaching Service Control Policies (SCPs) to these OUs, you can centrally restrict or allow access to specific AWS services for all accounts within that OU. This is a centralized, account - level control that automatically applies to  all  existing  and  future  accounts  in  the  OU,  minimizing  operational  effort.  Other  options:  -  A  (Systems  Manager): Manages resources (like EC2 instances), not service permissions. - C (Cloud formation): Requires manual template deployment per account and can be bypassed. - D (Service Catalog): Controls user - level access to approved products, not account - wide service                        usage.                        Reference                         Link:                        [AWS                         Organizations                        and
SCPs](https://docs.aws.amazon.com/organizations/latest/user guide/orgs_manage_policies_scps.html)

477.A company has created a multi-tier application for its ecommerce website. The website uses an Application Load Balancer that resides in the public subnets, a web tier in the public subnets, and a MySQL cluster hosted on Amazon EC2 instances in the private subnets. The MySQL database needs to retrieve product catalog and pricing information that is hosted on the internet by a  third-party  provider.  A  solutions  architect  must  devise  a  strategy  that  maximizes  security  without  increasing  operational overhead. What should the solutions architect do to meet these requirements?

A ��Deploy a NAT instance in the VPC. Route all the internet-based traffic through the NAT instance.

B ��Deploy a NAT gateway in the public subnets. Modify the private subnet route table to direct all internet-bound traffic to the NAT gateway.

C ��Configure an internet gateway and attach it to the Vp modify the private subnet route table to direct internet-bound traffic to the internet gateway.

D ��Configure a virtual private gateway and attach it to the VPC. Modify the private subnet route table to direct internet-bound traffic to the virtual private gateway.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The MySQL database in the private subnet needs internet access to retrieve data from a third-party provider. However, instances in private subnets cannot directly communicate with the internet. A NAT gateway allows outbound internet traffic from private subnets while blocking inbound traffic, enhancing security. Why B is correct: - A NAT gateway (managed by AWS) is deployed in the public subnet and scales automatically, reducing operational overhead. - The private subnet ��s  route table  is  updated to send  internet-bound traffic (0.0.0.0/0) to the  NAT gateway. This  lets the MySQL instances securely access the internet without exposing them to incoming connections. Why other options are incorrect: - A: NAT instances require manual management (high operational overhead) and lack the scalability of a NAT gateway. - C: Directly routing private subnet traffic to an internet gateway would expose the MySQL instances to the public internet, compromising security. - D:  A  virtual  private  gateway  connects  to  on-premises  networks  via  VPN/DC,  not  the  public  internet.  Reference:  [AWS  NAT Gateway Documentation](https://docs.aws.amazon.com/vpc/latest/user guide/vpc-nat-gateway.html)

478.A company has a financial application that produces reports. The reports average 50 KB in size and are stored in Amazon S3. The reports are frequently accessed during the first week after production and must be stored for several years. The reports

must be retrievable within 6 hours. Which solution meets these requirements MOST cost-effectively?

A ��Use S3 Standard. Use an S3 Lifecycle rule to transition the reports to S3 Glacier after 7 days.

B ��Use S3 Standard. Use an S3 Lifecycle rule to transition the reports to S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days.

C �� Use  S3  Intelligent-Tiering.  Configure  S3  Intelligent-Tiering  to transition the  reports to S3  Standard-Infrequent Access  (S3 Standard-IA) and S3 Glacier.

D ��Use S3 Standard. Use an S3 Lifecycle rule to transition the reports to S3 Glacier Deep Archive after 7 days.

�𰸣�A

������ Correct Answer A �� Use S3 Standard. Use an S3 Lifecycle rule to transition the reports to S3 Glacier after 7 days. Detailed Explanation The reports are frequently accessed during the first week (ideal for S3 Standard ��s low-latency access). After 7 days, they ��re rarely accessed but must be stored for years. S3 Glacier offers low-cost archival storage with retrieval times within 3 �C5 hours  (meeting the  6  -  hour  requirement).  -  Option A  (Correct):  S3  Glacier  balances  cost  and  retrieval  time.  -  Option  B:  S3 Standard - IA is for infrequent - but - fast access, but it ��s more expensive than Glacier for long - term storage. - Option C: S3 Intelligent - Tiering adds monitoring costs and isn ��t needed since the access pattern is predictable (no unexpected access after 7 days). - Option  D: Glacier  Deep Archive  has  a  12 -  hour  retrieval time, violating the  6 -  hour  requirement.  Reference  Links  - [Amazon       S3        Storage       Classes](https://aws.amazon.com/s3/storage        -       classes/)        -        [S3       Glacier        Retrieval Options](https://docs.aws.amazon.com/AmazonS3/latest/user guide/restoring - objects.html)

479.A company needs to optimize the cost of its Amazon EC2 instances. The company also needs to change the type and family of its EC2 instances every 2-3 months. What should the company do to meet these requirements?

A ��Purchase Partial Upfront Reserved Instances for a 3-year term.

B ��Purchase a No Upfront Compute Savings Plan for a 1-year term.

C ��Purchase All Upfront Reserved Instances for a 1-year term.

D ��Purchase an All Upfront EC2 Instance Savings Plan for a 1-year term.

�𰸣�B

������ Correct Answer B �� Purchase a No Upfront Compute Savings Plan for a 1-year term. Detailed Explanation To optimize costs while maintaining flexibility to change EC2 instance types/families every 2-3 months: 1. Compute Savings Plans (Option B) allow  you  to  switch   between  instance  types,  families,   and  even  AWS  services   (EC2,   Fargate,  Lambda)  while   maintaining discounted pricing, as long as your overall usage matches the committed dollar amount. This matches the requirement to change instance types/families frequently. 2. Reserved Instances (Options A/C) lock you to specific instance types/families, making them unsuitable for frequent changes. 3. EC2 Instance Savings Plans (Option D) only allow flexibility within the same instance family (e.g.,  switching  from  m5.xlarge  to  m5.2xlarge),  not  across  different  families.  The  No  Upfront  option  (B)  provides  maximum flexibility with no initial payment, while the 1-year term aligns better with frequent changes than longer 3-year commitments. Reference Link [Savings Plans FAQ](https://aws.amazon.com/savings plans/faq/)

480.A solutions architect needs to review a company's Amazon S3 buckets to discover personally identifiable information (P II). The company stores the P II data in t he us-east-1 Region and us-west-2 Region. Which solution will meet these requirements with

the LEAST operational overhead?

A ��Configure Amazon Macie in each Region. Create a job to analyze the data that is in Amazon S3.

B ��Configure AWS Security Hub for all Regions. Create an AWS Config rule to analyze the data that is in Amazon S3. C ��Configure Amazon Inspector to analyze the data that is in Amazon S3.

D ��Configure Amazon Guard duty to analyze the data that is in Amazon S3.

�𰸣�A

���� �� Correct Answer: A. Configure Amazon Macie in each  Region. Create a job to analyze the data that is in Amazon S3. Detailed  Explanation:  Amazon  Macie  is  specifically  designed  to  automatically  discover  and  protect  sensitive  data  like  P II  in Amazon  S3.  It  uses  machine  learning  to  scan  stored  data  and  identify  high-risk  content  (e.g.,  names,  credit  card  numbers) without manual rules. While Macie must be enabled in both us-east-1 and us-west-2 Regions (since S3 data is region-scoped), this still involves minimal operational effort compared to alternatives. Other options are less suitable: - B. Security Hub + AWS Config: Security Hub aggregates findings but doesn't scan data content. AWS Config rules focus on resource configurations (e.g., bucket policies), not data content analysis. - C. Amazon  Inspector: Scans for vulnerabilities in  EC2/containers/Lambda, not S3 data content. - D. Guard duty: Detects threats via logs (e.g., unauthorized access), not data content inspection. Macie provides the   most   automated,    purpose-built   solution   for    P II   discovery   with    minimal   setup.    Reference   Link:    [Amazon    Macie Overview](https://aws.amazon.com/macie/)

481.A company's SAP application has a backend SQL Server database in an on-premises environment. The company wants to migrate  its  on-premises application and  database server to AWS. The  company  needs  an  instance type that  meets the  high demands of its SAP database. On-premises performance data shows that both the SAP application and the database have high memory utilization. Which solution will meet these requirements?

A ��Use the compute optimized instance family for the application. Use the memory optimized instance family for the database.

B ��Use the storage optimized instance family for both the application and the database.

C ��Use the memory optimized instance family for both the application and the database.

D��Use the high performance computing (HPC) optimized instance family for the application. Use the memory optimized instance family for the database.

�𰸣�C

�� �� �� Correct Answer: C.  Use the  memory optimized  instance family for  both the application and the database.  Detailed Explanation: The question states that both the SAP application and the SQL Server database have high memory utilization in the on-premises environment. AWS recommends memory optimized instances (e.g., R5, R6i, X1, X2idn) for workloads requiring high memory-to-CPU  ratios.  Here �� s  why:  -  SAP  Applications:  SAP  workloads  like  SAP  HANA  are  explicitly  designed  to  run  on memory-optimized instances because they rely heavily on in-memory processing. Even non-HANA SAP applications may require memory optimization if they show high memory usage. - SQL Server Databases: While SQL Server can run on general-purpose instances (e.g., M5), databases with high memory demands (e.g., large datasets cached in RAM) benefit from memory-optimized instances to avoid performance  bottlenecks. Other options fail because: - Compute optimized instances (A,  D):  Prioritize CPU over memory, which doesn ��t address the stated memory bottleneck. - Storage optimized instances (B): Focus on high disk I/O, not memory. - HPC instances (D): Target parallelized workloads (e.g., simulations), not generic SAP/database use cases. Reference

Links:   -    [AWS    SAP    on   AWS    Instance    Types](https://aws.amazon.com/sap/instance-types/)    -    [AWS    SQL   Server    Best Practices](https://docs.aws.amazon.com/prescriptive-guidance/latest/sql-server-best-practices/instance-selection.html)              - [Memory Optimized Instances](https://aws.amazon.com/ec2/instance-types/#Memory_Optimized)

482.A company runs an application in a VPC with public and private subnets. The VPC extends across multiple Availability Zones. The  application  runs  on  Amazon  EC2  instances  in  private  subnets.  The  application  uses  an  Amazon  Simple  Queue  Service (Amazon  SQS)  queue.  A  solutions  architect  needs  to  design  a  secure  solution  to  establish  a  connection  between  the  EC2 instances and the SQS queue. Which solution will meet these requirements?

A �� Implement  an  interface VPC  endpoint for Amazon  SQS.  Configure the  endpoint to  use  the  private  subnets. Add to the endpoint a security group that  has an  inbound access  rule that allows traffic from the  EC2  instances that are  in the  private subnets.

B �� Implement an  interface VPC endpoint for Amazon SQS.  Configure the endpoint to  use the  public subnets. Attach to the interface endpoint a VPC endpoint policy that allows access from the EC2 instances that are in the private subnets.

C ��Implement an interface VPC endpoint for Amazon SQS. Configure the endpoint to use the public subnets. Attach an Amazon SQS access policy to the interface VPC endpoint that allows requests from only a specified VPC endpoint.

D ��Implement a gateway endpoint for Amazon SQS. Add a NAT gateway to the private subnets. Attach an IAM role to the EC2 instances that allows access to the SQS queue.

�𰸣�A

������Correct Answer: A Detailed Explanation: When EC2 instances in a private subnet need to securely access AWS services like Amazon  SQS without  exposing traffic to the  public  internet,  a  VPC  Interface  Endpoint  (powered  by  AWS  Private link)  is  the recommended solution. Here's why option A works: 1. Interface VPC Endpoint for SQS: - Creates a private connection between your VPC and SQS,  keeping traffic within the AWS  network. - Deploying it in private subnets (not public subnets) aligns with security best  practices for  private resources. 2. Security Group Configuration: - The endpoint's security group  needs  inbound rules  allowing  traffic  from  EC2  instances  in  private  subnets  (e.g.,  TCP  port  443  for  HTTPS).  -  This  ensures  only  authorized instances can communicate with SQS via the endpoint. Why other options are incorrect: - B/C:  Placing the endpoint in public subnets is  unnecessary and less secure. -  D: Gateway endpoints only support S3/Dynamo db,  not SQS.  NAT gateways expose traffic    to     the     internet,     which     violates     the     security     requirement.     Reference     Links:     -     [AWS     VPC     Interface Endpoints](https://docs.aws.amazon.com/vpc/latest/private link/vpce-interface.html)            -            [Amazon            SQS            VPC
Endpoints](https://docs.aws.amazon.com/Aws simple queue service/latest/Sqs developer guide/sqs-vpc-endpoints.html)

483.A solutions architect is using an AWS Cloud formation template to deploy a three-tier web application. The web application consists of a web tier and an application tier that stores and retrieves user data in Amazon  Dynamo db tables. The web and application tiers  are  hosted  on  Amazon  EC2  instances,  and  the  database  tier  is  not  publicly  accessible.  The  application  EC2 instances  need to access the  Dynamo db tables without exposing API credentials  in the template. What should the solutions architect do to meet these requirements?

A ��Create an IAM role to read the Dynamo db tables. Associate the role with the application instances by referencing an instance profile.

B ��Create an IAM role t hat has the required permissions to read and write from the Dynamo db tables. Add the role to the EC2 instance profile, and associate the instance profile with the application instances.

C �� Use the  parameter section  in the AWS Cloud formation template to  have the  user  input access and secret keys from an

already-created IAM user that has the required permissions to read and write from the Dynamo db tables.

D �� Create an  IAM  user  in the AWS Cloud formation template that  has the  required permissions to read and write from the Dynamo db tables. Use the GetAtt function to retrieve the access and secret keys, and pass them to the application instances through the user data.

�𰸣�B

������ Correct Answer B Detailed Explanation To securely grant Amazon EC2 instances access to AWS services like Dynamo db without exposing credentials, use an IAM role with the required permissions. Here's why option B is correct: 1. IAM Role: Create an  IAM  role  with  permissions  to  read/write  to  Dynamo db.  Roles  are  temporary  credentials,  avoiding  hardcoded  secrets.  2. Instance Profile: IAM roles for EC2 are attached via an instance profile (a container for the role). The Cloud formation template associates  this  profile  with  EC2  instances.  3.  Security:  The  application  on  EC2  automatically  inherits  the  role's  permissions, eliminating the  need  to  store  API  keys  in  the  template  or  user  data.  Why  other  options  are  wrong:  -  A:  Incorrectly  limits permissions to  read only. The app  needs  read and write access. - C/D:  Both  involve  passing  credentials  (access/secret  keys), which      violates       the        requirement       to       avoid       exposing       them.       Reference       Links       -        [IAM       Roles       for EC2](https://docs.aws.amazon.com/AWSEC2/latest/User guide/iam-roles-for-amazon-ec2.html)                     -                      [Instance
Profiles](https://docs.aws.amazon.com/IAM/latest/User guide/id_roles_use_switch-role-ec2_instance-profiles.html)

484.A solutions architect manages an analytics application. The application stores large amounts of semistructured data in an Amazon S3 bucket. The solutions architect wants to use parallel data processing to process the data more quickly. The solutions architect also wants to use information that is stored in an Amazon Redshift database to enrich the data. Which solution will meet these requirements?

A ��Use Amazon Athena to process the S3 data. Use AWS Glue with the Amazon Redshift data to enrich the S3 data.

B ��Use Amazon EMR to process the S3 data. Use Amazon EMR with the Amazon Redshift data to enrich the S3 data.

C ��Use Amazon EMR to process the S3 data. Use Amazon Kinesis Data Streams to move the S3 data into Amazon Redshift so that the data can be enriched.

D ��Use AWS Glue to process the S3 data. Use AWS Lake Formation with the Amazon Redshift data to enrich the S3 data.

�𰸣�B

������ Correct Answer B �� Use Amazon EMR to process the S3 data. Use Amazon EMR with the Amazon Redshift data to enrich the S3 data. Detailed Explanation Amazon EMR is designed for parallel data processing (e.g., using Apache Spark or Hadoop) to handle large datasets quickly. It can directly read semistructured data (like JSON/CSV) stored in Amazon S3. To enrich the S3 data with Amazon Redshift data: - EMR can connect to Amazon Redshift via JDBC/ODBC drivers and query the data during processing.
- This allows combining S3 data with Redshift data in a single workflow (e.g., joining datasets using Spark). Other options are less optimal:  -  A  (Athena  +  Glue):  Athena  is  for  SQL  queries,  not  complex  processing;  Glue  is  for  ETL  but  lacks  native  Redshift integration during parallel jobs. - C (Kinesis): Kinesis is for real - time streaming, not batch data enrichment. - D (Lake Formation): Lake     Formation     manages      data     permissions,      not     data      processing.     Reference      Links     -      [Amazon     EMR      Use Cases](https://aws.amazon.com/emr/use                  -                  cases/)                   -                  [Connect                   Redshift                  to EMR](https://docs.aws.amazon.com/emr/latest/Management guide/emr - redshift - connector.html)

485.A company has two VPCs that are located in the us-west-2 Region within the same AWS account. The company needs to allow  network traffic  between these VPCs. Approximately 500 GB of data transfer will occur  between the VPCs each  month. What is the MOST cost-effective solution to connect these VPCs?

A �� Implement AWS Transit Gateway to connect the VPCs. Update the route tables of each VPC to use the transit gateway for inter-VPC communication.

B ��Implement an AWS Site-to-Site VPN tunnel between the VPCs. Update the route tables of each VPC to use the VPN tunnel for inter-VPC communication.

C ��Set up a VPC peering connection between the VPCs. Update the route tables of each VPC to use the VPC peering connection for inter-VPC communication.

D ��Set up a 1 GB AWS Direct Connect connection between the VPCs. Update the route tables of each VPC to use the Direct Connect connection for inter-VPC communication.

�𰸣�C

������ Correct Answer: C. Set up a VPC peering connection between the VPCs. Update the route tables of each VPC to use the VPC peering connection for inter-VPC communication. Detailed Explanation: VPC peering is the most cost-effective solution for connecting two VPCs in the same AWS Region. Since both VPCs are in the same region (us-west-2), VPC peering charges @.01 per GB for data transfer in both directions (ingress and egress). For 500 GB/ month, this would cost approximately /month. Other options are less cost-efficient: - Transit Gateway (A) charges @.05/hour for the gateway + @.02/GB for data processing, leading to higher costs for low-traffic scenarios. - Site-to-Site VPN (B) costs @.05/hour for the VPN connection + potential data transfer fees. - Direct Connect (D) has a fixed @.30 - @.45/hour port fee + upfront costs, making it overkill for this use case. VPC peering is  simpler,  requires  no  hardware/VPN  setup,  and  has  no  recurring  hourly  fees.  It  also  provides  direct  private  connectivity between  VPCs with  low  latency.  Reference  Links:  -  [AWS  VPC  Peering  Pricing](https://aws.amazon.com/vpc/pricing/)  -  [AWS Transit Gateway Pricing](https://aws.amazon.com/transit-gateway/pricing/)

486.A company's  solutions  architect  is  designing an AWS  multi-account solution that  uses AWS  Organizations. The  solutions architect has organized the company's accounts into organizational units (OUs). The solutions architect needs a solution that will identify any changes to the OU  hierarchy. The solution also  needs to  notify the company's operations team of any changes. Which solution will meet these requirements with the LEAST operational overhead?

A �� Provision the AWS accounts by using AWS Control Tower. Use account drift notifications to identify the changes to the OU hierarchy.

B ��Provision the AWS accounts by using AWS Control Tower. Use AWS Config aggregated rules to identify the changes to the OU hierarchy.

C ��Use AWS Service Catalog to create accounts in Organizations. Use an AWS Cloud trail organization trail to identify the changes to the OU hierarchy.

D �� Use AWS Cloud formation templates to create accounts in Organizations.  Use the drift detection operation on a stack to identify the changes to the OU hierarchy.

�𰸣�A

���� �� Correct Answer: B Detailed Explanation The question requires detecting any changes to the Organizational Unit (OU) hierarchy in AWS Organizations and notifying the operations team with minimal operational overhead. Here's why Option B is the  best  choice:  1.  AWS  Control  Tower   simplifies   multi-account  setup  and  governance.   It  automatically  configures  AWS Organizations, AWS Config aggregation, and other foundational services, reducing setup effort. 2. AWS Config Aggregated Rules

can monitor configuration changes across accounts. AWS Config supports tracking AWS Organizations resources (like OUs and accounts). By enabling a managed rule (e.g., `organizations-account-ou-movement`), AWS Config automatically detects account movements  between  OUs.  3.  Operational  Overhead:  Control  Tower  pre-configures  AWS  Config  aggregation,  so  enabling  a managed rule requires minimal effort. Notifications can be set up via Amazon Event bridge or Amazon SNS using AWS Config ��s compliance events. Why Other Options Are Less Optimal - A: Control Tower ��s drift detection focuses on guardrails and baseline compliance,  not  all  OU  structural  changes  (e.g.,  creating/deleting  OUs).  -  C:  Cloud trail  organization  trails  log  API  calls  (e.g., `Move account`), but setting up alerts requires custom Event bridge rules and SNS topics, adding steps. - D: Cloud formation drift detection       doesn      ��  t       track       OU       hierarchy       changes.       Reference        Links       -       [AWS       Config       Managed Rules](https://docs.aws.amazon.com/config/latest/developer guide/managed-rules-by-aws-config.html)  -   [AWS  Control  Tower and  Account  Drift](https://docs.aws.amazon.com/control tower/latest/user guide/drift-detection.html)  -  [AWS  Organizations  in AWS Config](https://docs.aws.amazon.com/config/latest/developer guide/resource-config-reference.html)

487.A company's website handles millions of requests each day, and the number of requests continues to increase. A solutions architect needs to improve the response time of the web application. The solutions architect determines that the application needs to decrease latency when retrieving product details from the Amazon Dynamo db table. Which solution will meet these requirements with the LEAST amount of operational overhead?

A ��Set up a Dynamo db Accelerator (DAX) cluster. Route all read requests through DAX.

B ��Set up Amazon Elastic ache for Redis between the Dynamo db table and the web application. Route all read requests through Redis.

C ��Set up Amazon Elastic ache for Memcached between the Dynamo db table and the web application. Route all read requests through Memcached.

D �� Set  up  Amazon  Dynamo db  Streams  on  the  table,  and  have  AWS  Lambda  read  from  the  table  and  populate  Amazon Elastic ache. Route all read requests through Elastic ache.

�𰸣�A

������ Correct Answer A Detailed Explanation The best solution is A because Dynamo db Accelerator (DAX) is a fully managed, in-memory caching service designed specifically for Dynamo db. It reduces latency by caching frequently accessed data, and it is tightly integrated with Dynamo db, requiring no additional code or complex setup. DAX automatically handles cache invalidation, scaling,  and  maintenance,  minimizing  operational  overhead.  Other  options  involve  more  manual  effort:  -  B/C  (Elastic ache) require you to manually manage caching logic (e.g., updating the cache when data changes) and ensure consistency between Dynamo db and the cache. - D (Dynamo db Streams + Lambda) introduces complexity by requiring you to write Lambda functions to     sync     data      to      Elastic ache,     increasing      operational     work.      Reference     Link      [AWS      Dynamo db     Accelerator (DAX)](https://aws.amazon.com/dynamo db/dax/)

488.A  company  runs  its  applications  on   both  Amazon   Elastic  Ku bernet es  Service   (Amazon  EKS)  clusters  and  on-premises Ku bernet es clusters. The company wants to view all clusters and workloads from a central location. Which solution will meet these requirements with the LEAST operational overhead?

A ��Use Amazon Cloud watch Container Insights to collect and group the cluster information.

B ��Use Amazon EKS Connector to register and connect all Ku bernet es clusters.

C ��Use AWS Systems Manager to collect and view the cluster information.

D ��Use Amazon EKS Anywhere as the primary cluster to view the other clusters with native Ku bernet es commands.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution is B. Use Amazon EKS Connector because it directly connects both on-premises and EKS clusters to the AWS management console. This allows centralized visibility of all Ku bernet es clusters and workloads without  requiring  complex  setup  or  migration.  Here's  why the  other  options are  less  ideal:  -  A.  Cloud watch Container  Insights focuses on  monitoring  metrics/logs,  not cluster/workload visualization. - C. AWS Systems  Manager  isn ��t designed  for  Ku bernet es  cluster  management.  -  D.  EKS  Anywhere  would  require  reconfiguring  existing  clusters,  increasing operational effort. EKS Connector simplifies management by letting you view all clusters in one place with minimal configuration, making       it        the        least        operationally        intensive        option.       Reference        Link:        [Amazon        EKS        Connector Documentation](https://docs.aws.amazon.com/eks/latest/user guide/eks-connector.html)

489.A company is building an ecommerce application and needs to store sensitive customer information. The company needs to give customers the ability to complete purchase transactions on the website. The company also needs to ensure that sensitive customer data is protected, even from database administrators. Which solution meets these requirements?

A ��Store sensitive data in an Amazon Elastic Block Store (Amazon EBS) volume. Use EBS encryption to encrypt the data. Use an IAM instance role to restrict access.

B �� Store sensitive data  in Amazon  RDS for  MySQL.  Use AWS  Key  Management  Service  (AWS  KMS) client-side encryption to encrypt the data.

C ��Store sensitive data in Amazon S3. Use AWS Key Management Service (AWS KMS) server-side encryption to encrypt the data. Use S3 bucket policies to restrict access.

D �� Store  sensitive  data  in  Amazon  FSx for  Windows Server.  Mount the file  share  on  application servers.  Use  Windows file permissions to restrict access.

�𰸣�B

������ Correct Answer B Explanation The correct solution is B because it uses client-side encryption with AWS KMS. Here ��s why:
-  Client-side  encryption  means  the  application  encrypts  sensitive  data  (like  customer  information)  before  sending  it  to  the database (Amazon RDS). This ensures that even database administrators cannot access the raw, unencrypted data, as they only see encrypted values. - AWS KMS manages the encryption keys securely. The company controls who can use these keys (via IAM policies), adding an extra layer of security. - Why other options are wrong: - A, C, D rely on server-side encryption (EBS, S3, FSx), where the service encrypts data after receiving it. Database or storage admins with access to keys or permissions could still view decrypted data. - RDS is a better fit than S3 (C) for transactional workloads like purchases, as S3 is designed for object storage, not           database            transactions.            Reference            Links           -            [Client-Side            Encryption            with            AWS KMS](https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/client-side-encryption.html)      -       [Amazon       RDS Encryption](https://docs.aws.amazon.com/Amazon rds/latest/User guide/Overview.Encryption.html)

490.A company has an on-premises MySQL database that handles transactional data. The company is migrating the database to the AWS Cloud. The migrated database must maintain compatibility with the company's applications that use the database. The migrated database also must scale automatically during periods of increased demand. Which migration solution will meet these requirements?

A ��Use native MySQL tools to migrate the database to Amazon RDS for MySQL. Configure elastic storage scaling.

B �� Migrate the database to Amazon  Redshift  by using the mysqldump utility. Turn on Auto Scaling for the Amazon Redshift cluster.

C ��Use AWS Database Migration Service (AWS DMS) to migrate the database to Amazon Aurora. Turn on Aurora Auto Scaling.

D ��Use AWS Database Migration Service (AWS DMS) to migrate the database to Amazon Dynamo db. Configure an Auto Scaling policy.

�𰸣�C

������ Correct Answer C Detailed Explanation The correct answer is C because it meets both requirements: compatibility with existing  applications  and  automatic  scaling.  1.  Compatibility:  -  Amazon  Aurora  is  fully  compatible  with  MySQL.  Applications designed for MySQL can work with Aurora without code changes. -  In contrast, options  B (Amazon  Redshift) and  D (Amazon Dynamo db) involve migrating to non-MySQL-compatible databases, which would break the applications. 2. Automatic Scaling: - Aurora Auto Scaling dynamically adjusts the  number of Aurora  Read  Replicas  based on workload demands. This ensures the database scales automatically during periods of increased demand. - Option A mentions elastic storage scaling for Amazon RDS for MySQL, but RDS for MySQL does not natively support automatic scaling of compute resources (e.g., CPU/memory). Aurora Auto Scaling (in option C) better addresses compute scaling. 3. Migration Tool: - AWS Database Migration Service (AWS DMS) is the recommended tool for migrating databases to AWS while minimizing downtime. It supports migrations to Aurora. Why Other Options Are Incorrect - A: RDS for MySQL lacks native automatic compute scaling. Elastic storage scaling handles storage but not compute resources. - B: Amazon Redshift is a data warehouse, not suitable for transactional workloads. Auto Scaling for Redshift applies to  cluster  nodes  but  does  not  ensure  MySQL  compatibility.  -  D:  Dynamo db  is  a  NoSQL  database,  incompatible  with MySQL-based            applications.             Reference            Links            -             [Amazon            Aurora             Compatibility            with MySQL](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/CHAP_Aurora overview.html)     -       [Aurora     Auto Scaling](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/Aurora.Integrating.Auto scaling.html)  -   [AWS   DMS Overview](https://aws.amazon.com/dms/)

491.A company runs multiple Amazon EC2 Linux instances in a VPC across two Availability Zones. The instances host applications that use a hierarchical directory structure. The applications need to read and write rapidly and concurrently to shared storage. What should a solutions architect do to meet these requirements?

A ��Create an Amazon S3 bucket. Allow access from all the EC2 instances in the VPC.

B ��Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system from each EC2 instance.

C ��Create a file system on a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume. Attach the EBS volume to all the EC2 instances.

D ��Create file systems on Amazon Elastic Block Store (Amazon EBS) volumes that are attached to each EC2 instance. Synchronize the EBS volumes across the different EC2 instances.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The correct solution is to use Amazon Elastic File System (EFS). Here's why this works best for the scenario: 1. Shared Access: EFS is a fully managed network file system (NFS) that allows multiple EC2 instances across different Availability Zones to read/write to the same storage simultaneously. This meets the requirement for concurrent access. 2. Hierarchical Structure: EFS natively supports directories and file hierarchies, unlike object storage like S3 (Option A). 3. Performance:  EFS  automatically  scales  throughput  and  storage  capacity,  making  it  ideal  for  rapidly  changing  workloads.  In contrast: - Option C (EBS) fails because a single EBS volume can only attach to one EC2 instance at a time. - Option D (syncing EBS

volumes)  introduces  latency  and  risks  data  conflicts  during  concurrent  writes.  -  Option A  (S3)  has  high  latency  for  frequent read/write operations and lacks a true file system structure. 4. Fully Managed: EFS handles maintenance, backups, and scaling automatically,           reducing          operational           overhead.           Reference            Links:           -           [Amazon           EFS           Use
Cases](https://aws.amazon.com/efs/use-cases/)                         -                          [Comparing                          AWS                          Storage
Options](https://aws.amazon.com/products/storage/)

492.A solutions architect is designing a workload that will store hourly energy consumption by business tenants in a building. The sensors will feed a database through HTTP requests that will add up usage for each tenant. The solutions architect must use managed  services  when   possible.  The  workload  will   receive  more  features  in  the  future  as  the  solutions  architect  adds independent components. Which solution will meet these requirements with the LEAST operational overhead?

A ��Use Amazon API Gateway with AWS Lambda functions to receive the data from the sensors, process the data, and store the data in an Amazon Dynamo db table.

B ��Use an Elastic Load Balancer that is supported by an Auto Scaling group of Amazon EC2 instances to receive and process the data from the sensors. Use an Amazon S3 bucket to store the processed data.

C ��Use Amazon API Gateway with AWS Lambda functions to receive the data from the sensors, process the data, and store the data in a Microsoft SQL Server Express database on an Amazon EC2 instance.

D ��Use an Elastic Load Balancer that is supported by an Auto Scaling group of Amazon EC2 instances to receive and process the data from the sensors. Use an Amazon Elastic File System (Amazon EFS) shared file system to store the processed data.

�𰸣�A

���� �� Correct Answer: A  Detailed  Explanation: The question requires a solution with the least operational overhead using managed services. Option A uses Amazon API Gateway and AWS  Lambda (both fully managed, serverless services) to handle incoming HTTP requests and process data. Storing data in Amazon Dynamo db (a managed NoSQL database) eliminates the need to manage servers, scaling, or backups. This fully serverless approach minimizes operational tasks. Other options are less optimal:
- B   �� D use EC2 instances (requires managing servers/scaling) and S3/EFS (not ideal for real-time cumulative updates). - C uses a self-managed SQL Server on EC2 (high operational effort for database maintenance). Dynamo db ��s ability to handle high write throughput and atomic counters (for adding up usage) makes it ideal for this scenario. API Gateway + Lambda + Dynamo db align with        the        managed         services         requirement        and        future         scalability.        Reference         Links:        -         [AWS Lambda](https://aws.amazon.com/lambda/)    -    [Amazon   API    Gateway](https://aws.amazon.com/api-gateway/)    -    [Amazon Dynamo db](https://aws.amazon.com/dynamo db/)

493.A  solutions  architect  is  designing  the  storage  architecture  for  a   new  web   application  used  for  storing   and  viewing engineering  drawings. All  application  components  will  be  deployed  on  the  AWS  infrastructure.  The  application  design  must support caching to minimize the amount of time that users wait for the engineering drawings to load. The application must be able to store petabytes of data. Which combination of storage and caching should the solutions architect use?

A ��Amazon S3 with Amazon Cloud front

B ��Amazon S3 Glacier with Amazon Elastic ache

C ��Amazon Elastic Block Store (Amazon EBS) volumes with Amazon Cloud front D ��AWS Storage Gateway with Amazon Elastic ache

�𰸣�A

������ Correct Answer A �� Amazon S3 with Amazon Cloud front Detailed Explanation For the web application storing petabytes of engineering drawings, Amazon S3 is ideal because it offers virtually unlimited scalability, durability, and cost-effectiveness for storing massive amounts of data. S3 can easily handle petabytes of unstructured data like images or drawings. To reduce loading times for users, Amazon Cloud front (a Content Delivery Network/CDN) caches frequently accessed drawings at edge locations globally. When a user requests a file, Cloud front serves it from the nearest edge location, minimizing latency. This is far more efficient for static content (like engineering drawings) than Elastic ache (which is designed for caching dynamic database queries or session data). Other options fail because: - B (S3 Glacier + Elastic ache): Glacier is for archival data, not frequently accessed files  (retrieval takes  minutes/hours). - C  (EBS + Cloud front):  EBS volumes  are  block storage for  EC2  instances,  not  suited for petabyte-scale  static  content. -  D  (Storage  Gateway  +  Elastic ache):  Storage  Gateway  connects  on-premises  systems to AWS, which      isn     ��  t      needed      here.      Reference       Links     -       [Amazon      S3](https://aws.amazon.com/s3/)      -      [Amazon Cloud front](https://aws.amazon.com/cloud front/)

494.An Amazon Event bridge rule targets a third-party API. The third-party API has not received any incoming traffic. A solutions architect needs to determine whether the rule conditions are being met and if the rule's target is being invoked. Which solution will meet these requirements?

A ��Check for metrics in Amazon Cloud watch in the namespace for AWS/Events.

B ��Review events in the Amazon Simple Queue Service (Amazon SQS) dead-letter queue.

C ��Check for the events in Amazon Cloud watch Logs.

D ��Check the trails in AWS Cloud trail for the Event bridge events.

�𰸣�A

���� �� Correct Answer: A Detailed Explanation: To determine if an Amazon Event bridge rule's conditions are met and if the target is being invoked, check Amazon Cloud watch metrics under the `AWS/Events` namespace. Event bridge automatically sends metrics  like `Matched events`  (number  of  events  that  matched  the  rule)  and  `Invocations`  (number  of  times  the  target  was triggered) to Cloud watch. - If `Matched events` is zero: The rule ��s conditions are not being met (no events are triggering the rule).
- If `Invocations` is zero but `Matched events` is positive: The rule ��s conditions are met, but the target (third - party API) is not being invoked (e.g., due to permissions or configuration errors). Other options: - B (SQS dead - letter queue): Only relevant if the target fails repeatedly, but the problem states the API receives no traffic, so no failures would be logged. - C (Cloud watch Logs): Requires manual logging setup and focuses on event details, not rule/target status. - D (Cloud trail): Tracks API activity (e.g., rule creation)  but  not  event  delivery  or  rule  triggers.  Cloud watch  metrics  provide  direct  visibility  into  rule  performance  without additional                    setup.                    Reference:                     [Amazon                    Event bridge                    Monitoring                    with Cloud watch](https://docs.aws.amazon.com/event bridge/latest/user guide/cloud watch - metrics.html)

495.A company has a large workload that runs every Friday evening. The workload runs on Amazon EC2 instances that are in two Availability Zones in the us-east-1 Region. Normally, the company must run no more than two instances at all times. However, the company wants to scale up to six instances each Friday to handle a regularly repeating increased workload. Which solution will meet these requirements with the LEAST operational overhead?

A ��Create a reminder in Amazon Event bridge to scale the instances.

B ��Create an Auto Scaling group that has a scheduled action.

C ��Create an Auto Scaling group that uses manual scaling.

D ��Create an Auto Scaling group that uses automatic scaling.

�𰸣�B

������ Correct Answer: B. Create an Auto Scaling group that has a scheduled action. Detailed Explanation: The best solution is to use an Auto Scaling group with a scheduled action. Here's why: 1. Scheduled Scaling: The workload increases predictably every Friday. A scheduled action lets you automatically scale to 6 instances every Friday evening and scale back afterward. No manual intervention is needed. 2. Least Operational Overhead: Unlike manual scaling (Option C) or Event bridge reminders (Option A), scheduled actions are built into AWS Auto Scaling. You set it once, and it runs automatically every week. 3. Automatic Scaling (Option D) isn ��t ideal here because automatic scaling relies on real - time metrics (like CPU usage), which isn ��t needed for a predictable, time - based workload. In short: Scheduled actions handle predictable scaling events effortlessly, making Option B the           simplest           and            most            efficient           choice.            Reference           Links:            -            [AWS           Scheduled Scaling](https://docs.aws.amazon.com/auto scaling/ec2/user guide/schedule_time.html)         -          [Auto         Scaling          Groups Basics](https://aws.amazon.com/auto scaling/)

496.A company is creating a REST API. The company has strict requirements for the use of TLS. The company requires TLSv1.3 on the API endpoints. The company also requires a specific public third-party certificate authority (CA) to sign the TLS certificate. Which solution will meet these requirements?

A �� Use  a  local  machine  to create a certificate that  is signed  by the third-party C Import the  certificate  into AWS Certificate Manager (ACM). Create an HTTP API in Amazon API Gateway with a custom domain. Configure the custom domain to use the certificate.

B��Create a certificate in AWS Certificate Manager (ACM) that is signed by the third-party CA. Create an HTTP API in Amazon API Gateway with a custom domain. Configure the custom domain to use the certificate.

C��Use AWS Certificate Manager (ACM) to create a certificate that is signed by the third-party CA. Import the certificate into AWS Certificate Manager (ACM). Create an AWS Lambda function with a Lambda function URL. Configure the Lambda function URL to use the certificate.

D ��Create a certificate in AWS Certificate Manager (ACM) that is signed by the third-party CA. Create an AWS Lambda function with a Lambda function URL. Configure the Lambda function URL to use the certificate.

�𰸣�A

������ Correct Answer: A Detailed Explanation: To meet the requirements, the company must use a TLS certificate signed by a specific third - party CA and enforce TLSv1.3. Here's why Option A is correct: 1. AWS Certificate Manager (ACM) does not allow creating certificates signed by third - party CAs directly. You can only import externally generated certificates (like those signed by a third - party CA) into ACM. 2. API Gateway supports TLSv1.3 for custom domains when using ACM certificates. By importing the third - party CA - signed certificate into ACM and attaching it to the API Gateway ��s custom domain, the TLSv1.3 requirement is satisfied. 3. Lambda Function URLs (Options C/D) do not support custom TLS certificates or custom domains, making them invalid.
4. Option B is incorrect because ACM cannot create certificates signed by third - party CAs��it only issues certificates via Amazon Trust              Services              or              Let              ��     s              Encrypt.               References:              -              [ACM               Import Certificates](https://docs.aws.amazon.com/acm/latest/user guide/import-certificate.html)          -          [API           Gateway          TLS Versions](https://docs.aws.amazon.com/api gateway/latest/developer guide/api gateway-custom-domain-tls-version.html)

497.A company runs an application on AWS. The application receives inconsistent amounts of usage. The application uses AWS

Direct  Connect  to  connect  to  an  on-premises   MySQL-compatible  database.  The  on-premises  database  consistently  uses  a minimum  of  2  Gi B  of  memory.  The  company  wants  to  migrate  the  on-premises  database  to  a  managed  AWS  service.  The company wants to  use  auto  scaling  capabilities  to  manage  unexpected  workload  increases.  Which  solution  will  meet  these requirements with the LEAST administrative overhead?

A ��Provision an Amazon Dynamo db database with default read and write capacity settings.

B ��Provision an Amazon Aurora database with a minimum capacity of 1 Aurora capacity unit (ACU).

C ��Provision an Amazon Aurora Serverless v2 database with a minimum capacity of 1 Aurora capacity unit (ACU).

D ��Provision an Amazon RDS for MySQL database with 2 Gi B of memory.

�𰸣�C

������ Correct Answer C Explanation The company needs a MySQL-compatible managed database with auto-scaling to handle unpredictable workloads and minimal administrative effort. - Option A: Dynamo db is a NoSQL service, not MySQL-compatible, requiring application changes. It scales automatically but isn��t suitable here. - Option B: Aurora (non-Serverless) requires manual scaling of instance size, which adds administrative overhead. - Option C: Aurora Serverless v2 automatically scales compute and memory based on workload (from 0.5 ACU to 128 ACU). It ��s MySQL-compatible, handles spikes without manual intervention, and meets the 2 Gi B memory minimum (1 ACU = 2 Gi B). This minimizes admin work. - Option D: RDS MySQL requires manual scaling of instance size, making it less efficient for unpredictable traffic. Aurora Serverless v2 is the only option that combines MySQL    compatibility,     fully     automated     scaling,     and     minimal     management.     Reference     Link     [Aurora     Serverless v2](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/aurora-serverless-v2.html)

498.A company wants to use an event-driven programming  model with AWS  Lambda. The company wants to reduce startup latency for Lambda functions that run on Java 11. The company does not have strict latency requirements for the applications. The  company wants  to  reduce  cold  starts  and  outlier  latencies  when  a  function  scales  up.  Which  solution  will  meet  these requirements MOST cost-effectively?

A ��Configure Lambda provisioned concurrency.

B ��Increase the timeout of the Lambda functions.

C ��Increase the memory of the Lambda functions.

D ��Configure Lambda Snap start.

�𰸣�D

������ Correct Answer: D Detailed Explanation: Cold starts occur when AWS Lambda initializes a new runtime environment for a function. Java-based  Lambda functions  (like Java  11) typically  have  longer cold start times due to JVM  initialization.  Lambda Snap start  (Option  D)  addresses  this  by  taking a snapshot  of the  initialized  execution  environment  after the first  invocation. Subsequent  cold   starts  resume  from  this   snapshot,   drastically  reducing  startup  latency.  This   is  more  cost-effective  than Provisioned Concurrency (Option A), which pre-initializes instances continuously (incurring ongoing costs). Since the company doesn ��t require ultra-low latency, Snap start ��s trade-off (slightly slower first-time initialization) is acceptable. Options B (timeout) and       C       (memory)        don      ��  t       directly        reduce       cold       starts.        Reference       Links:        -       [AWS        Lambda Snap start](https://docs.aws.amazon.com/lambda/latest/dg/snap start.html)          -          [Reducing          Cold           Starts          with
Snap start](https://aws.amazon.com/blogs/aws/reduce-java-cold-starts-on-aws-lambda-with-snap start/)

499.A financial services company launched a new application that uses an Amazon RDS for MySQL database. The company uses the application to track stock market trends. The company needs to operate the application for only 2 hours at the end of each week. The company needs to optimize the cost of running the database. Which solution will meet these requirements MOST cost-effectively?

A ��Migrate the existing RDS for MySQL database to an Aurora Serverless v2 MySQL database cluster.

B ��Migrate the existing RDS for MySQL database to an Aurora MySQL database cluster.

C ��Migrate the existing RDS for MySQL database to an Amazon EC2 instance that runs MySQL. Purchase an instance reservation for the EC2 instance.

D ��Migrate the existing RDS for MySQL database to an Amazon Elastic Container Service (Amazon ECS) cluster that uses MySQL container images to run tasks.

�𰸣�A

�� �� ��  Correct  Answer:  A  Detailed  Explanation:  The  most  cost-effective  solution  is  A  (Aurora  Serverless  v2)  because:  1. Automatic Scaling and Pay-Per-Use: Aurora Serverless v2 automatically scales compute capacity up or down based on demand and charges only for the resources used per second. Since the application runs for only 2 hours weekly, the database will incur minimal costs during idle time (only storage costs apply when inactive). 2. No Manual Management: Unlike EC2 (Option C) or ECS (Option D), Aurora Serverless v2 requires no manual provisioning, patching, or instance management. Options C and D involve ongoing costs (e.g., EC2 reservations or ECS tasks running idle) or operational overhead. 3. Cost vs. Standard Aurora (Option B): A standard Aurora cluster runs 24/7, charging for the instance even when idle. Aurora Serverless v2 avoids this by pausing compute resources  when  inactive,  reducing  costs  significantly.  Other  options  fail  because:  -  B  (Aurora  Cluster):  Costs  more  due  to continuous  instance  billing. - C  (EC2 with  Reservation):  Reservations  commit  to  long-term  payments,  which  is wasteful for  a 2-hour/week  workload.  -   D  (ECS  with   MySQL  Containers):   Requires  managing  containers  and   persistent  storage,  with   no automatic           cost           optimization           for           sporadic           usage.           Reference:            [Aurora           Serverless           v2 Pricing](https://aws.amazon.com/rds/aurora/serverless/)

500.A  company  deploys  its  applications  on  Amazon   Elastic  Ku bernet es  Service  (Amazon   EKS)  behind  an  Application   Load Balancer in an AWS Region. The application needs to store data in a Postgresql database engine. The company wants the data in the database to be highly available. The company also needs increased capacity for read workloads. Which solution will meet these requirements with the MOST operational efficiency?

A ��Create an Amazon Dynamo db database table configured with global tables.

B ��Create an Amazon RDS database with Multi-AZ deployments.

C ��Create an Amazon RDS database with Multi-AZ DB cluster deployment.

D ��Create an Amazon RDS database configured with cross-Region read replicas.

�𰸣�C

���� �� Correct Answer C �� Create an Amazon RDS database with Multi-AZ DB cluster deployment. Detailed Explanation The company needs a highly available Postgresql database that can scale read workloads efficiently. Here's why Option C is the best choice: 1. High Availability (HA): - Multi-AZ DB Cluster deploys three database instances across three Availability Zones (AZs): one

primary  (read/write)  and  two  readable  standby  instances.  -  If  the  primary  fails,  one  of  the  standby  instances  automatically becomes the new primary, ensuring minimal downtime. 2. Read Scaling: - Unlike traditional Multi-AZ (where the standby is idle and not used for reads), both standby instances in a Multi-AZ  DB Cluster can serve read traffic. - This allows the database to handle increased  read workloads without additional setup. 3. Operational  Efficiency: -  Multi-AZ  DB Cluster combines  HA and read scaling in a single configuration, eliminating the need to manually create read replicas (as in Option D) or manage separate scaling solutions. Why Other Options Are Less Efficient: - A (Dynamo db): Dynamo db is a NoSQL database and doesn ��t support Postgresql, which is explicitly required. - B (RDS Multi-AZ): While it provides HA, the standby instance isn ��t used for reads, so it doesn��t scale read workloads. - D (Cross-Region Read Replicas): Adds complexity and latency since replicas are in another region. The   primary   need    is   HA   and    read   scaling   within   the   same    region.   Reference    Link   [AWS    Multi-AZ   DB    Cluster   for Postgresql](https://docs.aws.amazon.com/Amazon rds/latest/User guide/multi-az-db-clusters-concepts.html)

501.A company is building a RESTful serverless web application on AWS by using Amazon API Gateway and AWS Lambda. The users of this web application will be geographically distributed, and the company wants to reduce the latency of API requests to these users. Which type of endpoint should a solutions architect use to meet these requirements?

A ��Private endpoint

B ��Regional endpoint

C ��Interface VPC endpoint

D ��Edge-optimized endpoint

�𰸣�D

�� �� ��  Correct  Answer:   D.   Edge-optimized  endpoint   Detailed   Explanation:   For  a  globally  distributed   user   base,  an edge-optimized endpoint is the best choice. Here's why in simple terms: - Edge-optimized endpoints use Amazon Cloud front's global  network of edge  locations. When  a  user  makes  an API  request,  it first goes to the  nearest  Cloud front edge  location, reducing latency because the request travels a shorter physical distance. - This is ideal for global users, unlike regional endpoints (Option B), which serve requests directly from one AWS region and may cause higher latency for users far from that region. - Private endpoints (Option A) and VPC endpoints (Option C) are used for accessing  resources within a VPC (private  network), which    isn    �� t     relevant    here     since    the     web    app     is     public.    Reference     Link:     [AWS    API     Gateway    Endpoint Types](https://docs.aws.amazon.com/api gateway/latest/developer guide/api-gateway-api-endpoint-types.html)

502.A company uses an Amazon Cloud front distribution to serve content pages for its website. The company needs to ensure that clients  use  a TLS certificate when  accessing the company's website. The company wants to automate the creation and renewal of the TLS certificates. Which solution will meet these requirements with the MOST operational efficiency?

A ��Use a Cloud front security policy to create a certificate.

B ��Use a Cloud front origin access control (OAC) to create a certificate.

C ��Use AWS Certificate Manager (ACM) to create a certificate. Use DNS validation for the domain.

D ��Use AWS Certificate Manager (ACM) to create a certificate. Use email validation for the domain.

�𰸣�C

������ Correct Answer: C Detailed Explanation: To automate TLS certificate creation and renewal for a Cloud front distribution,

AWS  Certificate  Manager  (ACM)   is  the  optimal  solution.  ACM  automatically   handles  certificate  provisioning  and   renewal, eliminating  manual efforts. - Option C  (DNS validation)  is  more operationally efficient than Option  D  (email validation).  DNS validation adds a CNAME record to your domain ��s DNS configuration once, allowing ACM to auto-renew certificates seamlessly. Email  validation  requires  manual  approval  via  emails  sent  to  domain  administrators,  which  introduces  delays  and  human intervention. - Options A  and  B  are  incorrect  because  Cloud front  security  policies  and  Origin  Access  Control  (OAC)  manage HTTPS  settings  or  origin  access,  not  certificate  creation/renewal.  Key  Take away:   ACM  with  DNS  validation  ensures  full automation     and      minimal     operational      overhead     for     TLS      certificates.     Reference      Links:      -     [AWS      Certificate Manager](https://aws.amazon.com/certificate-manager/)                        -                         [DNS                         Validation                        in
ACM](https://docs.aws.amazon.com/acm/latest/user guide/dns-validation.html)

503.A  company  deployed  a  serverless  application  that   uses  Amazon   Dynamo db  as  a  database  layer.  The  application  has experienced a large increase in users. The company wants to improve database response time from milliseconds to microseconds and to cache requests to the database. Which solution will meet these requirements with the LEAST operational overhead?

A ��Use Dynamo db Accelerator (DAX).

B ��Migrate the database to Amazon Redshift.

C ��Migrate the database to Amazon RDS.

D ��Use Amazon Elastic ache for Redis.

�𰸣�A

���� �� Correct Answer A �� Use Dynamo db Accelerator (DAX). Detailed Explanation Dynamo db Accelerator (DAX) is a fully managed,  in  -  memory  caching  service  designed  specifically  for   Dynamo db.   It  reduces   read  latency  from   milliseconds  to microseconds by caching frequently accessed data. DAX integrates seamlessly with Dynamo db, requiring minimal code changes. Since it ��s fully managed, AWS handles maintenance, scaling, and availability, minimizing operational overhead. Other options like Amazon Elastic ache (D) also provide caching but require manual setup for Dynamo db integration, increasing operational effort. Migrating to Redshift (B) or RDS (C) would introduce unnecessary complexity, as they serve different use cases (analytics and relational databases) and don ��t directly optimize Dynamo db performance. Reference Link [Amazon Dynamo db Accelerator (DAX)](https://aws.amazon.com/dynamo db/dax/)

504.A company runs an application that  uses Amazon  RDS for  Postgresql. The application receives traffic only on weekdays during  business  hours.  The  company wants to  optimize costs and  reduce  operational  overhead  based  on  this  usage.  Which solution will meet these requirements?

A ��Use the Instance Scheduler on AWS to configure start and stop schedules.

B ��Turn off automatic backups. Create weekly manual snapshots of the database.

C ��Create a custom AWS Lambda function to start and stop the database based on minimum CPU utilization.

D ��Purchase All Upfront reserved DB instances.

�𰸣�A

������ Correct Answer A Detailed Explanation The best solution is A: Use the Instance Scheduler on AWS to configure start and stop schedules. - Why A is correct: AWS provides the Instance Scheduler, a managed tool that automatically starts and stops

Amazon RDS instances (and EC2 instances) based on a predefined schedule. Since the application only runs on weekdays during business hours, this tool can shut down the database during nights, weekends, and holidays, reducing costs (you don ��t pay for compute/storage when the database  is stopped).  It also  minimizes operational overhead  because  no  manual intervention or custom code is required. - Why B is incorrect: Turning off automatic backups and relying on weekly manual snapshots increases operational  risk. Automatic  backups ensure  point -  in - time  recovery  and are critical for data  protection.  Manual snapshots require human effort and are error - prone, which contradicts the goal of reducing operational overhead. - Why C is incorrect: While a Lambda function could start/stop the database, building and maintaining custom code adds complexity and operational overhead. Additionally, using CPU  utilization as a trigger  is  unreliable �� traffic  might drop temporarily without indicating the database should be stopped (e.g., during a meeting break). - Why D is incorrect: Purchasing All Upfront Reserved Instances saves costs for long - term, 24/7 workloads. However, since the database is only needed part - time, stopping it during idle periods (as in Option A) would save far more money than reserved instances. Key Concept: Use managed AWS tools like Instance Scheduler to automate cost - saving actions (e.g., stopping unused resources) instead of manual processes or custom code. This ensures reliability           and           reduces           effort.            Reference           Links           -            [AWS           Instance           Scheduler           for RDS](https://docs.aws.amazon.com/solutions/latest/instance-scheduler/components.html)                             -                             [RDS
Pricing](https://aws.amazon.com/rds/pricing/)

505.A company uses locally attached storage to run a latency-sensitive application on premises. The company is using a lift and shift method to move the application to the AWS Cloud. The company does not want to change the application architecture. Which solution will meet these requirements MOST cost-effectively?

A �� Configure  an  Auto  Scaling  group  with  an  Amazon  EC2  instance.  Use  an  Amazon  FSx  for  Lustre  file  system  to  run  the application.

B ��Host the application on an Amazon EC2 instance. Use an Amazon Elastic Block Store (Amazon EBS) GP2 volume to run the application.

C �� Configure  an Auto Scaling group with  an Amazon  EC2  instance.  Use  an  Amazon  FSx  for  OpenZFS file system to  run the application.

D �� Host the application on an Amazon EC2 instance. Use an Amazon Elastic Block Store (Amazon EBS) GP3 volume to run the application.

�𰸣�D

������ Correct Answer: D Explanation: The company wants to migrate their latency-sensitive application to AWS using a lift and shift approach without changing the architecture. This means they need a solution that mimics their on-premises setup (locally attached storage) while being cost-effective. Option D (EC2 + EBS GP3) is the best choice because: 1. EBS GP3 volumes provide low-latency  SSD  storage  and  allow   independent  scaling  of   IOPS/throughput  at  a   lower  cost  than  GP2.  This   matches  the performance  needs  of   latency-sensitive  workloads.  2.   No  architectural  changes:   Using  a  single   EC2  instance  with  directly attached EBS volumes mirrors the on-premises setup (no need for Auto Scaling or shared file systems). 3. Cost-effectiveness: GP3 offers better price-performance than GP2, as you pay only for the IOPS/throughput you need. Why other options are less ideal: - A/C  (FSx):  FSx  for  Lustre/OpenZFS  is  designed  for  shared  file   storage  (e.g.,  HPC  or   multi-instance  workloads).  This   adds unnecessary complexity and cost for a single-instance app. - B (GP2): GP2 ties IOPS to volume size, forcing you to over-provision storage for  performance.  GP3  avoids  this,  making  it  cheaper  for  high-IOPS  use  cases.  Reference  Link:  [Amazon  EBS  Volume Types](https://aws.amazon.com/ebs/volume-types/)

506.A  company  runs  a  stateful  production  application  on  Amazon  EC2  instances.  The  application  requires  at  least  two  EC2 instances to always be running. A solutions architect needs to design a highly available and fault-tolerant architecture for the application. The solutions architect creates an Auto Scaling group of  EC2 instances. Which set of additional steps should the

solutions architect take to meet these requirements?

A ��Set the Auto Scaling group's minimum capacity to two. Deploy one On-Demand Instance in one Availability Zone and one On-Demand Instance in a second Availability Zone.

B ��Set the Auto Scaling group's minimum capacity to four. Deploy two On-Demand Instances in one Availability Zone and two On-Demand Instances in a second Availability Zone.

C ��Set the Auto Scaling group's minimum capacity to two. Deploy four Spot Instances in one Availability Zone.

D ��Set the Auto Scaling group's minimum capacity to four. Deploy two On-Demand Instances in one Availability Zone and two Spot Instances in a second Availability Zone.

�𰸣�B

�� �� �� Correct Answer:  B  Explanation: To  ensure  high  availability and fault tolerance, the application  needs to  run  across multiple Availability Zones (AZs). If one AZ fails, the instances in the other AZ can keep the application running. - Option B sets the Auto Scaling group ��s minimum capacity to 4 (2 instances in each of 2 AZs). This guarantees that even if one AZ fails, the remaining 2 instances in the other AZ still meet the minimum requirement of 2 instances running. - Why not A? If the minimum is set to 2 (1 instance per AZ), and one AZ fails, the surviving AZ has only 1 instance. This violates the requirement of  ��at least 2 instances always running �� until Auto Scaling launches a replacement (which  may take time). - Why not C or D? Using Spot Instances  (C/D)  risks  interruptions,  which  is  unsuitable  for  a  stateful  production  application  requiring  guaranteed  uptime. Reference                           Links:                            -                            [Auto                           Scaling                            Groups                           and
AZs](https://docs.aws.amazon.com/auto scaling/ec2/user guide/auto-scaling-benefits.html#arch-Auto scaling multi az)     -      [High Availability for EC2](https://aws.amazon.com/architecture/high-availability/)

507.An ecommerce company uses Amazon Route 53 as its DNS provider. The company hosts its website on premises and in the AWS Cloud. The company's on-premises data center is near the us-west-1 Region. The company uses the eu-central-1 Region to host the website. The company wants to minimize load time for the website as much as possible. Which solution will meet these requirements?

A��Set up a geolocation routing policy. Send the traffic that is near us-west-1 to the on-premises data center. Send the traffic that is near eu-central-1 to eu-central-1.

B ��Set up a simple routing policy that routes all traffic that is near eu-central-1 to eu-central-1 and routes all traffic that is near the on-premises data center to the on-premises data center.

C ��Set up a latency routing policy. Associate the policy with us-west-1.

D ��Set up a weighted routing policy. Split the traffic evenly between eu-central-1 and the on-premises data center.

�𰸣�A

������ Correct Answer A Explanation The company wants to route users to the endpoint (on-premises or AWS) that provides the lowest load time based on their geographic proximity. Geolocation Routing Policy (Option A) directs traffic based on the user ��s geographic location. Since the on-premises data center is near the AWS us-west-1 Region, traffic from users in that geographic area (e.g., western North America) is sent to the on-premises data center. Traffic from users near eu-central-1 (e.g., Europe) is routed  to the  AWS-hosted  website. This  ensures  users  connect  to  the  closest  endpoint,  minimizing  latency  and  load  time. Latency Routing Policy (Option C) measures network latency between users and endpoints but requires configuring endpoints in

multiple AWS  Regions. The on-premises data center isn ��t an AWS  Region, so this  policy can ��t  compare  latency  between on-premises and AWS-hosted resources. Simple (Option B) and Weighted (Option D) policies don��t prioritize proximity or latency, making      them      less      optimal      for       minimizing      load      time.       Reference      Link       [Amazon      Route      53       Routing Policies](https://docs.aws.amazon.com/Route53/latest/Developer guide/routing-policy.html)

508.A company has 5 PB of archived data on physical tapes. The company needs to preserve the data on the tapes for another 10 years for compliance purposes. The company wants to migrate to AWS in the next 6 months. The data center that stores the tapes has a 1 Gbps uplink internet connectivity. Which solution will meet these requirements MOST cost-effectively?

A ��Read the data from the tapes on premises. Stage the data in a local NFS storage. Use AWS Data sync to migrate the data to Amazon S3 Glacier Flexible Retrieval.

B �� Use an on-premises backup application to read the data from the tapes and to write directly to Amazon S3 Glacier Deep Archive.

C ��Order multiple AWS Snowball devices that have Tape Gateway. Copy the physical tapes to virtual tapes in Snowball. Ship the Snowball devices to AWS. Create a lifecycle policy to move the tapes to Amazon S3 Glacier Deep Archive.

D��Configure an on-premises Tape Gateway. Create virtual tapes in the AWS Cloud. Use backup software to copy the physical tape to the virtual tape.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The company has 5PB of tape data and a slow 1Gb ps connection. Transferring 5PB  over  1Gb ps  would  take  ~500  days  (5PB  =  5,000TB;  1Gb ps  =  ~10TB/day),  which  exceeds  the  6  -  month  timeline.  AWS Snowball solves this by physically shipping storage devices. Option C works because: 1. Snowball handles large offline transfers 2. Tape Gateway converts physical tapes to virtual tapes during migration 3. S3 Glacier Deep Archive is the cheapest storage for 10 - year retention 4. Lifecycle policies automate cost optimization Other options fail because: - A/D require slow network transfers - B assumes  backup software supports direct Glacier writes (unlikely) - D uses faster - but - costlier storage tiers unnecessarily Reference            links:             [AWS             Snowball](https://aws.amazon.com/snowball/)             [Glacier             Deep            Archive pricing](https://aws.amazon.com/s3/storage - classes/glacier/) [Tape Gateway](https://aws.amazon.com/storage gateway/tape - gateway/)

509.A company is deploying an application that processes large quantities of data in parallel. The company plans to use Amazon EC2 instances for the workload. The network architecture must be configurable to prevent groups of nodes from sharing the same underlying hardware. Which networking solution meets these requirements?

A ��Run the EC2 instances in a spread placement group.

B ��Group the EC2 instances in separate accounts.

C ��Configure the EC2 instances with dedicated tenancy.

D ��Configure the EC2 instances with shared tenancy.

�𰸣�A

������ Correct Answer: A. Run the EC2 instances in a spread placement group. Detailed Explanation: A spread placement group in AWS ensures that each EC2 instance in the group is placed on distinct hardware (separate racks) within an Availability Zone.

This  minimizes  the   risk  of  multiple  instances  failing  simultaneously   due  to  shared   hardware   issues,  which   is  critical  for fault-tolerant workloads like parallel data processing. Option B (separate accounts) doesn ��t control hardware placement. Option C (dedicated tenancy) ensures instances run on isolated hardware but doesn ��t guarantee they ��re spread across different racks. Option D (shared tenancy) allows instances to share hardware, violating the requirement. A spread placement group directly addresses     the     need     to      prevent     hardware     sharing      among     instances.      Reference:     [Amazon      EC2     Placement Groups](https://docs.aws.amazon.com/AWSEC2/latest/User guide/placement-groups.html)

510.A solutions architect is designing a disaster recovery (DR) strategy to provide Amazon EC2 capacity in a failover AWS Region. Business  requirements state that the  DR strategy  must  meet  capacity  in the failover  Region. Which solution will  meet these requirements?

A ��Purchase On-Demand Instances in the failover Region.

B ��Purchase an EC2 Savings Plan in the failover Region.

C ��Purchase regional Reserved Instances in the failover Region.

D ��Purchase a Capacity Reservation in the failover Region.

�𰸣�D

���� �� Correct Answer: D. Purchase a Capacity Reservation in the failover Region. Detailed Explanation: Capacity Reservation guarantees  EC2  instance  capacity  in  a  specific  Availability  Zone  (AZ)  when  you  need  it.  This  is  the  only  option  that  directly reserves  physical AWS  infrastructure to  meet  DR  capacity  requirements.  Other  options  focus  on  cost optimization  but  don't guarantee availability: - A (On-Demand): No capacity guarantee - instances might be unavailable during regional failover spikes. - B (Savings Plan): Only discounts usage costs, doesn't reserve capacity. - C (Regional RIs): While Regional Reserved Instances offer flexibility across AZs, they don't actually reserve  hardware - capacity isn't guaranteed during high demand. Think of Capacity Reservation like booking concert tickets in advance - you're guaranteed a seat even if the venue fills up. For DR scenarios where immediate         capacity          is         critical,          this          is         essential.          Reference          Links:          -         [AWS          Capacity Reservations](https://docs.aws.amazon.com/AWSEC2/latest/User guide/capacity-reservations.html) - [Disaster Recovery Options with AWS](https://aws.amazon.com/blogs/aws/disaster-recovery-of-workloads-on-aws-strategies-and-implementation/)

511.A company has five organizational units (OUs) as part of its organization in AWS Organizations. Each OU correlates to the five businesses that the company owns. The  company's  research  and  development  (R �� amp;D)  business  is  separating  from the company and will need its own organization. A solutions architect creates a separate new management account for this purpose. What should the solutions architect do next in the new management account?

A ��Have the R��amp;D AWS account be part of both organizations during the transition.

B �� Invite the R��amp;D AWS account to be part of the new organization after the R��amp;D AWS account has left the prior organization.

C ��Create a new R��amp;D AWS account in the new organization. Migrate resources from the prior R��amp;D AWS account to the new R��amp;D AWS account.

D �� Have the R ��amp;D AWS account join the new organization. Make the new management account a member of the prior organization.

�𰸣�B

������ Correct Answer B �� Invite the R��D AWS account to be part of the new organization after the R��D AWS account has left the  prior  organization.  Detailed  Explanation  In  AWS  Organizations,  an  account  cannot  belong  to  multiple  organizations simultaneously. To move the R��D account to the new organization: 1. Remove the R��D account from the original organization: The original organization's management account must first remove the R ��D account. 2. Invite the R ��D account to the new organization: Only after  leaving the  original organization can the  new  management  account send  an  invitation to the  R �� D account to join the new organization. Option B correctly follows this process. Other options violate AWS Organizations rules: - Option A: Accounts cannot be in two organizations at once. - Option C: Unnecessary work if the existing R�� D account can be moved  directly.  -  Option  D:   Management  accounts  cannot  be  members  of  another  organization.  Reference   Link   [AWS Organizations:                                                                                                Moving                                                                                                an
Account](https://docs.aws.amazon.com/organizations/latest/user guide/orgs_manage_accounts_remove.html)

512.A company is designing a solution to capture customer activity in different web applications to process analytics and make predictions.  Customer  activity  in  the  web  applications  is  unpredictable  and  can  increase  suddenly.  The  company  requires  a solution that  integrates with  other web  applications. The  solution  must  include  an authorization step for security  purposes. Which solution will meet these requirements?

A ��Configure a Gateway Load Balancer (GWLB) in front of an Amazon Elastic Container Service (Amazon ECS) container instance that stores the information that the company receives in an Amazon Elastic File System (Amazon EFS) file system. Authorization is resolved at the GWLB.

B ��Configure an Amazon API Gateway endpoint in front of an Amazon Kinesis data stream that stores the information that the company receives in an Amazon S3 bucket. Use an AWS Lambda function to resolve authorization.

C ��Configure an Amazon API Gateway endpoint in front of an Amazon Kinesis Data Firehose that stores the information that the company receives in an Amazon S3 bucket. Use an API Gateway Lambda authorizer to resolve authorization.

D ��Configure a Gateway Load Balancer (GWLB) in front of an Amazon Elastic Container Service (Amazon ECS) container instance that stores the information that the company receives on an Amazon Elastic File System (Amazon EFS) file system. Use an AWS Lambda function to resolve authorization.

�𰸣�C

������ Correct Answer C Detailed Explanation The correct solution is C because it effectively addresses all the requirements: 1. Handling  Unpredictable  Traffic:  Amazon  API  Gateway  is  designed  to   manage  sudden  spikes  in  traffic,   making  it  ideal  for unpredictable customer activity. It scales automatically without manual intervention. 2. Integration with Web Applications: API Gateway acts as a front door for  RESTful  or  HTTP-based APIs,  allowing seamless  integration with  other web  applications. 3. Authorization Security: The API Gateway Lambda authorizer enables custom authorization logic (e.g., token validation) before allowing access to the backend. This ensures security compliance. 4. Data Ingestion and Storage: Amazon Kinesis Data Firehose simplifies streaming data ingestion and automatically  batches data to Amazon S3.  Unlike  Kinesis  Data Streams  (in  Option  B), Firehose requires no manual scaling or management of shards, reducing operational overhead. Why Other Options Fail - A/D: Gateway  Load  Balancer  (GWLB)  is for  routing traffic to third-party security  appliances,  not API  management. Storing data  in Amazon EFS (a file storage service) is less efficient for analytics compared to S3 (object storage). - B: While Kinesis Data Streams can handle streaming data, it requires managing shards and lacks Firehose ��s automated batching. Using a Lambda function for authorization  (instead  of  a   dedicated  authorizer)  is  less  efficient  and  more  error-prone.  Reference  Links  -  [Amazon  API Gateway](https://aws.amazon.com/api                               -                                gateway/)                                -                                 [Lambda
Authorizers](https://docs.aws.amazon.com/api gateway/latest/developer guide/api gateway-use-lambda-authorizer.html)               - [Kinesis Data Firehose](https://aws.amazon.com/kinesis/data - firehose/)

513.An ecommerce company wants a disaster recovery solution for its Amazon RDS DB instances that run Microsoft SQL Server Enterprise Edition. The company's current recovery point objective (RPO) and recovery time objective (RTO) are 24 hours. Which solution will meet these requirements MOST cost-effectively?

A ��Create a cross-Region read replica and promote the read replica to the primary instance.

B ��Use AWS Database Migration Service (AWS DMS) to create RDS cross-Region replication.

C ��Use cross-Region replication every 24 hours to copy native backups to an Amazon S3 bucket.

D ��Copy automatic snapshots to another Region every 24 hours.

�𰸣�D

������ Correct Answer D Detailed Explanation The company needs a cost-effective disaster recovery solution for Amazon RDS SQL  Server  with  an  RPO/RTO  of  24  hours.  Here �� s  why  Option  D  is  the  best  choice:  1.  RDS  Automatic  Snapshots:  -  RDS automatically takes daily backups (snapshots) and retains them for the backup retention period (default 1 day, up to 35 days). - These snapshots are stored in the same Region as the database. 2. Cross-Region Copy: - AWS allows manually copying snapshots to another Region. - By scripting or automating this process (e.g., using AWS Lambda or Event bridge), the company can copy the latest automatic snapshot to another Region every 24 hours. 3. Cost-Effectiveness: - Snapshots are incremental (only changes since the last snapshot are stored), reducing storage costs. - Cross-Region snapshot copying incurs minimal costs compared to other solutions like cross-Region read replicas (not supported for SQL Server) or AWS DMS. 4. RPO/RTO Alignment: - RPO of 24 hours is met by ensuring a snapshot is copied every 24 hours. - RTO of 24 hours allows time to restore the database from the snapshot in the disaster recovery Region. Why Other Options Are Less Suitable: - A: Cross-Region read replicas are not supported for  RDS  SQL Server. -  B:  AWS  DMS  is  more  complex  and  costly for  a  24-hour  RPO/RTO.  -  C:  Native  backups  require  manual export/import       and       are       less       efficient       than       RDS       snapshots.       Reference       Links       -       [Copying        a       DB Snapshot](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_Copy snapshot.html)         -          [Amazon          RDS Backup](https://docs.aws.amazon.com/Amazon rds/latest/User guide/CHAP_Common tasks.Backup restore.html)

514.A company runs a web application on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer that  has sticky sessions enabled. The web server  currently  hosts the  user  session  state. The  company wants to ensure  high availability and avoid user session state loss in the event of a web server outage. Which solution will meet these requirements?

A ��Use an Amazon Elastic ache for Memcached instance to store the session data. Update the application to use Elastic ache for Memcached to store the session state.

B ��Use Amazon Elastic ache for Redis to store the session state. Update the application to use Elastic ache for Redis to store the session state.

C �� Use an AWS Storage Gateway cached volume to store session data. Update the application to use AWS Storage Gateway cached volume to store the session state.

D ��Use Amazon RDS to store the session state. Update the application to use Amazon RDS to store the session state.

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: When a web application uses EC2 instances with an Auto Scaling group and sticky sessions, storing session state on individual instances creates a risk of data loss if an instance fails. To avoid this, the session state must be stored externally in a highly available and durable service. Option B (Amazon Elastic ache for Redis) is the best

choice  because:  1.  High  Availability:  Redis  supports  Multi-AZ  replication  and  automatic  failover.  If  the  primary  node  fails,  a standby replica takes over, minimizing downtime. 2. In-Memory Performance: Redis is an in-memory database, making it fast for session storage (low latency). 3. Durability: Redis can persist data to disk, ensuring sessions survive even temporary outages. 4. Shared Access: All EC2 instances can access the same Redis cluster, eliminating dependency on a single server. Other options: - A (Memcached): Lacks built-in replication/persistence, increasing risk of data loss. - C (Storage Gateway): Designed for file/volume storage, not optimized for low-latency session data. - D (RDS): Relational databases add unnecessary latency for session storage compared to Redis. Reference Links: - [Elastic ache Redis Features](https://aws.amazon.com/elastic ache/redis/) - [Session State Management Best Practices](https://aws.amazon.com/blogs/database/session-state-management-best-practices/)

515.A company migrated a  MySQL  database from the company's on-premises data center to an Amazon  RDS for  MySQL  DB instance. The company sized t he RDS DB instance to meet the company's average daily workload. Once a month, the database performs slowly when the company runs queries for a report. The company wants to have the ability to run reports and maintain the performance of the daily workloads. Which solution will meet these requirements?

A ��Create a read replica of the database. Direct the queries to the read replica.

B ��Create a backup of the database. Restore the backup to another DB instance. Direct the queries to the new database. C ��Export the data to Amazon S3. Use Amazon Athena to query the S3 bucket.

D ��Resize the DB instance to accommodate the additional workload.

�𰸣�A

������ Correct Answer A Detailed Explanation The best solution is A (Create a read replica and direct report queries to it). Here �� s why:  1.  Read  Replicas  Handle  Read-Heavy Workloads: - Amazon  RDS  read  replicas  are designed to offload  read traffic  (like	 reporting  queries) from the  primary  database.  By  directing the  monthly  report  queries  to the  replica, the  primary  database	 continues serving daily workloads without slowdowns. 2. Cost-Effective and Scalable: - A read replica can be created temporarily	 during the monthly reporting period and deleted afterward, avoiding the cost of running a larger instance 24/7 (as in Option D).	 It also ensures the report uses up-to-date data (unlike Option B, where restored backups may not reflect real-time data). 3. No	 Data Export Hassle: - Option C (using Amazon Athena) requires exporting data to S3, which adds complexity and latency. Read	 replicas  eliminate  this  step,  allowing  real-time  querying  of  live  data.  4.  Avoid  Over-Provisioning:  -  Resizing  the  DB  instance	 (Option D) would force the company to pay for unused capacity most of the month. Read replicas provide on-demand scaling for	 temporary                   workloads.                    Reference                    Links                    -                   [Amazon                    RDS                    Read
Replicas](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_Read repl.html)        -        [Scaling        with         Read
Replicas](https://aws.amazon.com/rds/features/read-replicas/)

516.A company runs a container application by using Amazon Elastic Ku bernet es Service (Amazon EKS). The application includes micro services that  manage  customers  and  place  orders. The  company  needs to  route  incoming  requests  to the  appropriate micro services. Which solution will meet this requirement MOST cost-effectively?

A ��Use the AWS Load Balancer Controller to provision a Network Load Balancer.

B ��Use the AWS Load Balancer Controller to provision an Application Load Balancer.

C ��Use an AWS Lambda function to connect the requests to Amazon EKS.

D ��Use Amazon API Gateway to connect the requests to Amazon EKS.

�𰸣�B

������ Correct Answer: B. Use the AWS Load Balancer Controller to provision an Application Load Balancer. Explanation: When routing requests to micro services in Amazon EKS, the Application Load Balancer (ALB) is the most cost-effective solution. Here ��s why: 1. Layer 7 Routing: ALB operates at the application layer (HTTP/HTTPS) and supports advanced routing rules. For example, you can route requests to the customers micro service based on a URL path like /customers/* and to the orders micro service using /orders/*. This is essential for micro services architectures. 2. AWS  Load  Balancer Controller: This  Ku bernet es controller automatically provisions and configures an ALB when you define an Ingress resource in EKS. It ��s fully integrated with EKS, so you don ��t need to manually set up load balancers. 3. Cost Efficiency: ALB charges are based on usage (e.g., hours and LCU units). For most containerized applications, ALB ��s pay-as-you-go model is cheaper than alternatives like API Gateway (which charges per request) or maintaining a custom  Lambda-based routing layer. 4. Why Not Other Options? - A (Network  Load Balancer):  NLB works  at  Layer  4  (TCP/UDP)  and  lacks  HTTP-based  routing.  It �� s  better  for  low-latency  or  high-throughput  use  cases,  not micro services routing. - C (Lambda): Using Lambda adds unnecessary complexity and latency. Lambda is billed per request, which can become expensive for high-traffic apps. - D (API Gateway): API Gateway is great for serverless APIs but is costlier for high traffic. It also requires extra steps (e.g., VPC links) to connect to EKS, making ALB a simpler choice. Reference Links: - [AWS Load Balancer      Controller](https://docs.aws.amazon.com/eks/latest/user guide/aws-load-balancer-controller.html)       -       [ALB      vs NLB](https://aws.amazon.com/elastic load balancing/features/)                                                       -                                                        [ALB
Pricing](https://aws.amazon.com/elastic load balancing/pricing/)

517.A company uses AWS and sells access to copyrighted images. The company ��s global customer base needs to be able to access these images quickly. The company must deny access to users from specific countries. The company wants to minimize costs as much as possible. Which solution will meet these requirements?

A ��Use Amazon S3 to store the images. Turn on multi-factor authentication (MFA) and public bucket access. Provide customers with a link to the S3 bucket.

B ��Use Amazon S3 to store the images. Create an IAM user for each customer. Add the users to a group that has permission to access the S3 bucket.

C��Use Amazon EC2 instances that are behind Application Load Balancers (ALBs) to store the images. Deploy the instances only in the countries the company services. Provide customers with links to the ALBs for their specific country's instances.

D ��Use Amazon S3 to store the images. Use Amazon Cloud front to distribute the images with geographic restrictions. Provide a signed URL for each customer to access the data in Cloud front.

�𰸣�D

������ Correct Answer D Detailed Explanation The correct solution is D because it addresses all requirements effectively: 1. Fast global access: Amazon Cloud front, a Content Delivery Network (CDN), caches images at AWS edge locations worldwide, ensuring low-latency  access  for  global  customers.  2.  Geographic  restrictions:  Cloud front  allows  blocking  access  based  on  geographic locations (e.g., specific countries), meeting the requirement to deny access to users from certain regions. 3. Cost optimization: S3 +  Cloud front   is  a  serverless,   pay-as-you-go  model  with   no  upfront  costs.  Signed   URLs   (temporary,  secure   links)  prevent unauthorized sharing, reducing bandwidth costs from misuse. 4. Security: Signed URLs ensure only paying customers access the
content, aligning with copyright protection. Other options fail because: - A: Public S3 buckets expose data to unauthorized users and lack geographic restrictions. - B: Managing individual IAM users for customers is un scalable and doesn ��t block countries. - C: EC2 instances are expensive for static content (images) and complicate global distribution. Reference Links - [Amazon Cloud front Geographic       Restrictions](https://docs.aws.amazon.com/Amazon cloud front/latest/Developer guide/geo restrictions.html)       - [Amazon S3 Pre-signed URLs](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Share object pre signed url.html)

518.A solutions architect  is designing a  highly available Amazon  Elastic ache for  Redis  based solution. The solutions architect needs to ensure that failures do not result in performance degradation or loss of data locally and within an AWS Region. The solution  needs  to  provide  high  availability  at  the   node   level  and  at  the   Region   level.  Which  solution  will   meet  these requirements?

A ��Use Multi-AZ Redis replication groups with shards that contain multiple nodes.

B ��Use Redis shards that contain multiple nodes with Redis append only files (AOF) turned on.

C ��Use a Multi-AZ Redis cluster with more than one read replica in the replication group.

D ��Use Redis shards that contain multiple nodes with Auto Scaling turned on.

�𰸣�A

������ Correct Answer A �� Use Multi-AZ Redis replication groups with shards that contain multiple nodes. Detailed Explanation To achieve  high  availability  at  both  the  node  level  and  the  Region  level  for  Amazon  Elastic ache  for  Redis:  -  Multi-AZ  Redis replication  groups  automatically  replicate  data  across  multiple  Availability  Zones  (AZs)  within  the  same  AWS  Region.  If  the primary  node fails,  Elastic ache  instantly fails  over to a  read  replica  in  another AZ,  minimizing  downtime  (node-level  HA  and AZ-level fault tolerance). - S harding (dividing data into multiple shards) ensures scalability and redundancy. Each shard contains a primary node and read replicas. If a node fails within a shard, Elastic ache replaces it, maintaining performance and availability (node-level  HA). -  Multi-AZ + s harding together  protect against AZ failures  (Regional  HA) and  node failures, while  replication ensures data durability. Why not other options? - B   �� D: Redis AOF (append-only file) or Auto Scaling alone don ��t guarantee automatic failover across AZs or nodes. - C: A single Multi-AZ cluster with read replicas lacks s harding, limiting scalability and shard-level                      redundancy.                      Reference                      Link                      [AWS                      Elastic ache                      High
Availability](https://docs.aws.amazon.com/Amazon elastic ache/latest/red-ug/Auto failover.html)

519.A company plans to migrate to AWS and use Amazon EC2 On-Demand  Instances for its application. During the migration testing  phase, a technical team observes that the application takes a  long time to  launch and  load  memory to become fully productive. Which solution will reduce the launch time of the application during the next testing phase?

A �� Launch two  or  more  EC2  On-Demand  Instances. Turn  on auto scaling features and  make the  EC2  On-Demand  Instances available during the next testing phase.

B �� Launch EC2 Spot Instances to support the application and to scale the application so it is available during the next testing phase.

C ��Launch the EC2 On-Demand Instances with hibernation turned on. Configure EC2 Auto Scaling warm pools during the next testing phase.

D ��Launch EC2 On-Demand Instances with Capacity Reservations. Start additional EC2 instances during the next testing phase.

�𰸣�C

�� �� �� Correct Answer:  C  Detailed  Explanation: The  correct  answer  is  C  because  using  EC2  Auto Scaling warm  pools with hibernation can significantly reduce application launch time. Here ��s why: - Hibernation: When EC2 instances are launched with hibernation enabled, they save the in-memory state  (RAM) to the Amazon  EBS  root volume when stopped.  During the  next launch, the  instance  resumes from the saved state, skipping the slow  initialization steps (like loading  memory). This  is much faster than a cold start of a new instance. - Warm Pools: EC2 Auto Scaling warm pools keep initialized instances in a stopped state

(but  pre-configured). When scaling out, these  instances  can start faster  because they �� re already  prepped.  Combined  with hibernation, this ensures the application is ready almost immediately during testing. Other options like launching more instances (A), using Spot Instances (B), or Capacity Reservations (D) don ��t directly address the slow initialization of the application. Warm pools +  hibernation directly target the  root cause: the time-consuming  memory-loading  phase.  Reference  Link:  [Amazon  EC2 Auto   Scaling  Warm   Pools](https://docs.aws.amazon.com/auto scaling/ec2/user guide/ec2-auto-scaling-warm-pools.html)   [EC2 Hibernation](https://docs.aws.amazon.com/AWSEC2/latest/User guide/Hibernate.html)

520.A company's applications run on Amazon EC2 instances in Auto Scaling groups. The company notices that its applications experience  sudden traffic  increases  on  random  days  of the week. The  company wants to  maintain  application  performance during sudden traffic increases. Which solution will meet these requirements MOST cost-effectively?

A ��Use manual scaling to change the size of the Auto Scaling group.

B ��Use predictive scaling to change the size of the Auto Scaling group.

C ��Use dynamic scaling to change the size of the Auto Scaling group.

D ��Use schedule scaling to change the size of the Auto Scaling group.

�𰸣�C

������ Correct Answer: C Explanation: The best solution here is dynamic scaling because it automatically adjusts the number of EC2  instances  based  on  real-time  metrics  (like  CPU  usage  or  network  traffic).   Since  the  traffic   spikes  are  random  and unpredictable, dynamic scaling can react immediately to sudden changes, ensuring performance stays stable. It ��s cost-effective because it scales only when needed and avoids over-provisioning instances when traffic is low. - Option A (Manual scaling) is too slow and impractical for random spikes (requires human intervention). - Option B (Predictive scaling) relies on historical patterns, which won ��t work well for truly random traffic. - Option D (Scheduled scaling) requires knowing exact times for traffic spikes in advance,      which       the       company       doesn      ��  t        have.       Reference       Link:       [AWS       Auto       Scaling       Dynamic Scaling](https://docs.aws.amazon.com/auto scaling/ec2/user guide/as-scale-based-on-demand.html)

521.An ecommerce application uses a Postgresql database that runs on an Amazon EC2 instance. During a monthly sales event, database usage increases and causes database connection issues for the application. The traffic is unpredictable for subsequent monthly  sales  events,  which   impacts  the  sales  forecast.  The  company  needs  to  maintain   performance  when  there  is  an unpredictable increase in traffic. Which solution resolves this issue in the MOST cost-effective way?

A ��Migrate the Postgresql database to Amazon Aurora Serverless v2.

B ��Enable auto scaling for the Postgresql database on the EC2 instance to accommodate increased usage. C ��Migrate the Postgresql database to Amazon RDS for Postgresql with a larger instance type.

D ��Migrate the Postgresql database to Amazon Redshift to accommodate increased usage.

�𰸣�A

���� �� Correct Answer: A Detailed Explanation: Amazon Aurora Serverless v2 automatically scales database resources up or down  based  on  application  demand.  This   is   ideal  for  unpredictable  traffic  spikes   like  monthly  sales   events,  as   it  avoids over-provisioning costs (unlike Option C) and eliminates the complexity of manually scaling databases on EC2 (Option B). Redshift (Option D) is designed for analytics, not transactional workloads. Aurora Serverless v2 adjusts capacity instantly and only charges

for  resources  used,  making  it  the  most  cost-effective  solution  for  sporadic,  high-traffic  scenarios.  Reference  Links:  -  [Aurora Serverless  v2  Scaling](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/aurora-serverless-v2.html)  -  [Aurora Serverless Use Cases](https://aws.amazon.com/rds/aurora/serverless/)

522.A company hosts an internal serverless application on AWS by using Amazon API Gateway and AWS Lambda. The company ��s employees report issues with high latency when they begin using the application each day. The company wants to reduce latency. Which solution will meet these requirements?

A ��Increase the API Gateway throttling limit.

B ��Set up a scheduled scaling to increase Lambda provisioned concurrency before employees begin to use the application each day.

C ��Create an Amazon Cloud watch alarm to initiate a Lambda function as a target for the alarm at the beginning of each day.

D ��Increase the Lambda function memory.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The high latency experienced by employees when they start using the serverless application each day is likely due to Lambda cold starts. When a Lambda function hasn ��t been invoked recently, AWS needs to initialize  a  new  instance  of  the  function   (a   cold  start),  which   adds  extra   latency.  If  many  users  access  the   application simultaneously  in  the  morning,  multiple  cold  starts  can  occur,  leading  to  noticeable  delays.  Option  B  resolves  this  by  using provisioned concurrency. Provisioned concurrency keeps a specified number of Lambda function instances warm and ready to respond  instantly.  By  scheduling  this  to  activate  before  employees  start  using  the  application  (e.g.,  8  AM),  cold  starts  are eliminated during peak usage times, reducing latency. Why the other options are incorrect: - A (API Gateway throttling limit): Throttling limits restrict request rates to prevent overload. Increasing them doesn ��t address cold starts;  it only allows more requests per second, which isn ��t the root cause here. - C (Cloud watch alarm triggering Lambda): This is reactive, not proactive. The alarm would trigger after high latency occurs, which is too late to prevent the morning slow down. - D (Increase  Lambda memory): While more memory can reduce individual execution time, it doesn ��t fix cold starts. The latency issue here is about initialization        delays,        not        function         runtime        speed.        Reference         Link:        [AWS         Lambda        Provisioned Concurrency](https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html)

523.A company is planning to migrate a TCP-based application into the company's VPC. The application is publicly accessible on a nonstandard TCP  port through a  hardware appliance in the company's data center. This  public endpoint can process up to 3 million requests per second with low latency. The company requires the same level of performance for the new public endpoint in AWS. What should a solutions architect recommend to meet this requirement?

A �� Deploy a Network Load Balancer (NLB). Configure the NLB to be publicly accessible over the TCP port that the application requires.

B ��Deploy an Application Load Balancer (ALB). Configure the ALB to be publicly accessible over the TCP port that the application requires.

C ��Deploy an Amazon Cloud front distribution that listens on the TCP port that the application requires. Use an Application Load Balancer as the origin.

D ��Deploy an Amazon API Gateway API that is configured with the TCP port that the application requires. Configure AWS Lambda functions with provisioned concurrency to process the requests.

�𰸣�A

������ Correct Answer A �� Deploy a Network Load Balancer (NLB). Configure the NLB to be publicly accessible over the TCP port that the application requires. Detailed Explanation The key requirements here are: 1. TCP-based application on a nonstandard port 2. High performance (3 million requests per second) 3. Low latency Why NLB (Option A)? - Layer 4 (TCP/UDP) Support: NLB operates at the transport layer (Layer 4), making it ideal for raw TCP traffic. ALB (Option B) is designed for HTTP/HTTPS (Layer 7), which doesn ��t fit the TCP-only requirement. - High Throughput: NLB is optimized for handling millions of requests per second with ultra-low latency, matching the hardware appliance ��s performance. - Nonstandard Port Support: NLB can listen on any TCP port,  unlike Cloud front (Option C) or API Gateway (Option  D), which are  restricted to standard  HTTP/HTTPS  ports. Why  Not Other  Options?  -  ALB  (Option   B):   Designed  for  HTTP-based  traffic,  not  raw  TCP.  -  Cloud front  (Option  C):  A  CDN  for HTTP/HTTPS/Web socket, not raw TCP. - API Gateway + Lambda (Option D): API Gateway doesn��t support arbitrary TCP ports, and Lambda is unsuitable for 3M RPS due to scaling limits and cold starts (even with provisioned concurrency). Reference Links - [Network                                                                                                 Load                                                                                                  Balancer
Features](https://aws.amazon.com/elastic load balancing/features/#Features_for_Network_Load_Balancer) -  [Choosing  Between ALB and NLB](https://aws.amazon.com/elastic load balancing/features/#Compare_Elastic_Load_Balancing_Types)

524.A company  runs  its critical database on an Amazon  RDS  for  Postgresql  DB  instance.  The company wants to  migrate to Amazon Aurora Postgresql with minimal downtime and data loss. Which solution will meet these requirements with the LEAST operational overhead?

A ��Create a DB snapshot of t he RDS for Postgresql DB instance to populate a new Aurora Postgresql DB cluster.

B ��Create an Aurora read replica of t he RDS for Postgresql DB instance. Promote the Aurora read replicate to a new Aurora Postgresql DB cluster.

C ��Use data import from Amazon S3 to migrate the database to an Aurora Postgresql DB cluster.

D �� Use the pg_dump utility to back up t he RDS for Postgresql database. Restore the backup to a new Aurora Postgresql DB cluster.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution is to create an Aurora read replica from t he RDS Postgresql instance and then promote it to a standalone Aurora cluster. Here's why this works best: 1. Minimal Downtime: - The read replica continuously syncs data from the source RDS Postgresql database until promotion. - During the final cutover, you only need to briefly stop writes to the source database (to ensure no data loss), making downtime very short. 2. Automated Process: - AWS handles the data replication automatically. You don ��t  need to manually export/import data (unlike options C/D) or manage complex snapshots (unlike option A). 3. Compatibility: - Aurora Postgresql is designed to be compatible with RDS Postgresql. AWS  natively supports creating Aurora  read  replicas directly from  RDS  Postgresql  instances,  even though they  use  different storage engines. Why other  options are  less  ideal: -  Option A:  RDS  snapshots can ��t  directly create Aurora clusters due to different storage architectures. You ��d need extra steps to convert the snapshot. - Option C/D: Manual export/import (via S3 or `pg_dump`)  requires  stopping  the  database  for  a   long  time  to  ensure  data  consistency,  leading  to  significant  downtime. Reference       Link:       [AWS       Documentation       -       Migrating       from       RDS       Postgresql       to       Aurora       using       read replicas](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/Aurora postgresql.Migrating.html#Aurora postgres QL.Migrating.Rds postgresql.Replica)

525.A company's infrastructure consists of hundreds of Amazon EC2 instances that use Amazon Elastic Block Store (Amazon EBS) storage. A solutions architect must ensure that every EC2 instance can be recovered after a disaster. What should the solutions

architect do to meet this requirement with the LEAST amount of effort?

A ��Take a snapshot of t he EBS storage that is attached to each EC2 instance. Create an AWS Cloud formation template to launch new EC2 instances from the EBS storage.

B ��Take a snapshot of t he EBS storage that is attached to each EC2 instance. Use AWS Elastic Beanstalk to set the environment based on the EC2 template and attach the EBS storage.

C ��Use AWS Backup to set up a backup plan for the entire group of EC2 instances. Use the AWS Backup API or the AWS CLI to speed up the restore process for multiple EC2 instances.

D ��Create an AWS Lambda function to take a snapshot of t he EBS storage that is attached to each EC2 instance and copy the Amazon Machine Images (AMIs). Create another Lambda function to perform the restores with the copied AMIs and attach the EBS storage.

�𰸣�C

���� �� Correct Answer: C Detailed Explanation: The best approach is C because AWS Backup is a fully managed service that automates and centralizes backups of EC2 instances and their attached EBS volumes. By creating a backup plan, the solutions architect can schedule backups for all EC2 instances with minimal effort. During a disaster, AWS Backup allows bulk restoration of resources  using  its API or  CLI, saving time compared to  manual  processes. Other  options  (A,  B,  D)  require  manual  scripting, custom  templates,  or  duplicative  work,  which  increases  complexity  and  effort.  AWS  Backup  simplifies  compliance,  reduces human error, and scales effortlessly for hundreds of instances��making it the least effort solution. Reference Link: [AWS Backup Documentation](https://docs.aws.amazon.com/aws-backup/)

526.A company recently migrated to the AWS Cloud. The company wants a serverless solution for large-scale parallel on-demand processing of a semistructured dataset. The data consists of logs,  media files, sales transactions, and  IoT sensor data that  is stored in Amazon S3. The company wants the solution to process thousands of items in t he dataset in parallel. Which solution will meet these requirements with the MOST operational efficiency?

A ��Use the AWS Step Functions Map state in Inline mode to process the data in parallel.

B ��Use the AWS Step Functions Map state in Distributed mode to process the data in parallel.

C ��Use AWS Glue to process the data in parallel.

D ��Use several AWS Lambda functions to process the data in parallel.

�𰸣�B

������ Correct Answer: B. Use the AWS Step Functions Map state in Distributed mode to process the data in parallel. Detailed Explanation: AWS Step Functions' Distributed Map mode is designed for large-scale parallel processing (like thousands of items) in a serverless way.  Here's why  it's the  best fit:  1.  Massive  Parallelism:  Distributed  Map  dynamically  scales  to  process  up  to 10,000  parallel tasks per execution, ideal for the thousands of items  requirement. 2. Serverless   �� Managed:  No  servers to manage    �C Step Functions automatically handles scaling, retries, and error handling. 3. S3 Integration: It can directly process data stored in Amazon S3 (logs, media files, etc.) by iterating over files or lists. 4. Operational Efficiency: Requires minimal setup compared to managing  Lambda concurrency limits  (Option D) or  ETL jobs in AWS Glue (Option C). Why not other options? - Option A (Inline Map): Limited to 40 parallel tasks    �C not enough for thousands. - Option C (AWS Glue): Better for scheduled ETL jobs, not on-demand processing. - Option D (Lambda functions): You ��d need to manually manage parallelism/error handling.

Reference:                               [AWS                               Step                               Functions                               Distributed                               Map
Documentation](https://docs.aws.amazon.com/step-functions/latest/dg/concepts-asl-use-map-state-distributed.html)

527.A company will  migrate  10  PB of data to Amazon S3  in 6 weeks. The current data center  has a 500  Mbps  uplink to the internet. Other on-premises applications share the uplink. The company can use 80% of the internet bandwidth for this one-time migration task. Which solution will meet these requirements?

A ��Configure AWS Data sync to migrate the data to Amazon S3 and to automatically verify the data.

B ��Use rsync to transfer the data directly to Amazon S3.

C ��Use the AWS CLI and multiple copy processes to send the data directly to Amazon S3.

D ��Order multiple AWS Snowball devices. Copy the data to the devices. Send the devices to AWS to copy the data to Amazon S3.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The company needs to migrate 10 PB (10,000 TB) of data in 6 weeks. Calculating the available bandwidth: - 500 Mbps   �� 80% = 400  Mbps (50 MB/s). - Total data transferable via internet in 6 weeks: 50 MB/s �� 6 weeks   �� ~177 TB. This is far less than the required 10,000 TB. Direct internet transfer (options A/B/C) would take ~1.5+ years. AWS Snowball bypasses internet limitations by shipping physical storage devices. Each Snowball Edge can hold 80 TB, so ~125     devices    would      handle     10     PB.     This     offline     approach     meets     the     6-week    timeline.      Reference:     [AWS Snowball](https://aws.amazon.com/snowball/)

528.A  company  has  several  on-premises   Internet  Small  Computer  Systems   Interface   (ISCSI)  network  storage  servers.  The company  wants  to  reduce  the  number  of  these  servers  by  moving  to  the  AWS  Cloud.  A  solutions  architect  must  provide low-latency  access  to  frequently  used  data  and  reduce  the  dependency  on  on-premises  servers  with  a  minimal  number  of infrastructure changes. Which solution will meet these requirements?

A ��Deploy an Amazon S3 File Gateway.

B ��Deploy Amazon Elastic Block Store (Amazon EBS) storage with backups to Amazon S3.

C ��Deploy an AWS Storage Gateway volume gateway that is configured with stored volumes.

D ��Deploy an AWS Storage Gateway volume gateway that is configured with cached volumes.

�𰸣�D

�� �� ��  Correct  Answer:  D  Detailed  Explanation:  The  company  wants  to  migrate  iSCSI-based  block  storage  to  AWS  while minimizing infrastructure changes and maintaining low-latency access to frequently used data. Here's why option D works best: 1. Cached  Volumes  store  primary  data  in  Amazon  S3  while  keeping  frequently  accessed  data  cached  locally.  This  matches  the requirement for low-latency access to hot data. 2. Reduces On-Premises Dependency: Since the full dataset resides in AWS, the company can reduce the number of physical on-premises storage servers. 3. Minimal Changes: Storage Gateway maintains iSCSI compatibility, requiring no major changes to existing applications accessing the storage. Option C (stored volumes)  prioritizes on-premises storage with cloud backups, which doesn't reduce local server dependency. Options A (file gateway) uses file-level NFS/SMB  protocols,  not  block-level   iSCSI.  Option   B  (EBS)   requires  EC2  instances  and   isn't  directly  compatible  with   iSCSI. Reference   Links:  -   [AWS  Storage  Gateway  Volume   Gateway](https://aws.amazon.com/storage gateway/volume-gateway/)  - [Cached                                                                                                          vs                                                                                                          Stored

Volumes](https://docs.aws.amazon.com/storage gateway/latest/user guide/Storage gateway concepts.html#volume-gateway-con cepts)

529.A solutions architect is designing an application that will allow business users to upload objects to Amazon S3. The solution needs to maximize object durability. Objects also must be readily available at any time and for any length of time.  Users will access objects frequently within the first 30 days after the objects are uploaded, but users are much less likely to access objects that are older than 30 days. Which solution meets these requirements MOST cost-effectively?

A ��Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3 Glacier after 30 days.

B ��Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.

C ��Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.

D �� Store all the objects in S3  Intelligent-Tiering with an S3  Lifecycle rule to transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.

�𰸣�B

�� �� ��  Correct  Answer:  B.  Store  all  the  objects  in  S3  Standard  with  an  S3  Lifecycle  rule  to  transition  the  objects  to  S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days. Detailed Explanation: The requirements are to maximize durability, ensure objects are always available (no retrieval delays), and reduce costs for older objects. Here ��s why Option B works best: 1. Durability and Availability: - S3 Standard and S3 Standard-IA both store data across multiple Availability Zones (AZs), providing 99.999999999% (11 nines) durability. This meets the maximize durability requirement. - S3 One Zone-IA (Option C) uses a single AZ, which  is less durable and available compared to multi-AZ storage. This violates the maximize durability requirement. - S3 Glacier (Option A) requires retrieval time (minutes to hours), so it ��s not readily available, ruling it out. 2. Cost Optimization: - For the first 30 days, objects are accessed frequently. S3 Standard  is  ideal for frequent access. - After 30 days, access drops. S3 Standard-IA charges less for storage (vs. Standard) but has a small retrieval fee. Since objects are rarely accessed after 30 days, this balances cost and availability. 3. Why Not Option D (Intelligent-Tiering)? - S3 Intelligent-Tiering automatically moves objects between tiers based on access patterns. However, since the access pattern is predictable (30 days of frequent access followed by infrequent access), manually using Standard + Lifecycle rules (Option B) avoids Intelligent-Tiering ��s monthly monitoring fees (per object),  making  it cheaper.  Reference  Links: -  [Amazon S3 Storage Classes](https://aws.amazon.com/s3/storage-classes/) -  [S3 Lifecycle Policies](https://docs.aws.amazon.com/AmazonS3/latest/user guide/lifecycle-transition-general-considerations.html)

530.A  company  has  migrated  a  two-tier  application  from  its  on-premises  data  center  to  the  AWS  Cloud.  The  data  tier  is  a Multi-AZ deployment of Amazon RDS for Oracle with 12 TB of General Purpose SSD Amazon Elastic Block Store (Amazon EBS) storage. The application  is designed to  process and store documents  in the database as  binary  large objects  (blobs) with an average document size of 6 MB. The database size has grown over time, reducing the performance and increasing the cost of storage. The company must improve the database performance and needs a solution that is highly available and resilient. Which solution will meet these requirements MOST cost-effectively?

A ��Reduce t he RDS DB instance size. Increase the storage capacity to 24 TiB. Change the storage type to Magnetic.

B ��Increase the RDS DB instance size. Increase the storage capacity to 24 Ti change the storage type to Provisioned IOPS.

C ��Create an Amazon S3 bucket. Update the application to store documents in the S3 bucket. Store the object metadata in the existing database.

D ��Create an Amazon Dynamo db table. Update the application to use Dynamo db. Use AWS Database Migration Service (AWS DMS) to migrate data from the Oracle database to Dynamo db.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The core issue here is the database's poor performance and high storage costs due to large BLOBs (6MB documents) stored directly in Amazon RDS. Storing large files  like BLOBs in a relational database is inefficient because: 1. Cost: RDS storage (especially General Purpose SSD) is more expensive than Amazon S3 for large objects. 2. Performance:  Frequent  read/write  operations  on  large  BLOBs  can  exhaust  database  I/O  capacity,  slowing  down  queries.  3.
Scalability: RDS storage has limits, while S3 scales infinitely. Why Option C works best: - Amazon S3 is purpose-built for storing large objects (like 6MB documents) at 1/4 the cost of RDS General Purpose SSD storage. - By moving BLOBs to S3 and keeping only metadata (e.g., document IDs, timestamps) in  RDS: - The database shrinks, improving query speed. - Storage costs drop significantly. - RDS I/O pressure is reduced, boosting performance. - High Availability: S3 automatically replicates data across   ��3 AZs (11 nines durability). - Minimal Code Changes: Only the storage layer needs updating, not the entire database schema. Why other options fail: - A: Magnetic storage is cheaper but has terrible performance (100x slower latency than SSD), worsening the problem. - B: Provisioned IOPS and larger instances would increase costs without solving the root issue (storing BLOBs in RDS). - D: Migrating to Dynamo db would require rewriting the application's data layer and might not suit relational data. Reference Link: [Amazon   S3   vs   RDS   Storage    Pricing](https://aws.amazon.com/s3/pricing/)   [Best   Practices   for   Storing   Large   Objects   in AWS](https://docs.aws.amazon.com/white papers/latest/storing-retrieving-thumbnails/storing-large-objects.html)

531.A company has an application that serves clients that are deployed in more than 20.000 retail storefront locations around the world. The application consists of backend web services that are exposed over HTTPS on port 443. The application is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). The retail locations communicate with the web application over the  public internet. The company allows each  retail  location to register the IP address that the  retail  location has  been allocated  by  its  local  ISP.  The  company's security team  recommends to  increase the security  of the application  endpoint  by restricting access to only the IP addresses registered by the retail locations. What should a solutions architect do to meet these requirements?

A ��Associate an AWS WAF web ACL with the ALB. Use IP rule sets on the ALB to filter traffic. Update the IP addresses in the rule to include the registered IP addresses.

B ��Deploy AWS Firewall Manager to manage the Al configure firewall rules to restrict traffic to the Al modify the firewall rules to include the registered IP addresses.

C �� Store the  IP  addresses  in an Amazon  Dynamo db table. Configure an AWS  Lambda  authorization function on the ALB to validate that incoming requests are from the registered IP addresses.

D �� Configure the network ACL on the subnet that contains the public interface of the ALB.  Update the ingress  rules on the network ACL with entries for each of the registered IP addresses.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution is to use AWS WAF with the Application Load Balancer (ALB) to restrict access to registered  IP addresses.  Here's why:  1. AWS WAF is designed to protect web applications  by filtering traffic based on rules, including IP-based allow/block lists. It integrates directly with ALB, making it easy to enforce security rules at the application layer (HTTPS/443). 2.  IP  Rule Sets: AWS WAF allows you to create  rules that permit traffic only from specified  IP addresses (the retail stores' registered IPs). This ensures only authorized locations can access the backend services. 3. Scalability: AWS WAF supports large IP lists (20,000+ entries), which fits the requirement. Network ACLs (Option D) have a 200-rule limit,

making them  impractical  here. 4.  Efficiency:  Unlike  Option  C  (Lambda  authorization),  AWS WAF  processes  rules  at  the edge without adding latency. Option B (Firewall Manager) is for centralized rule management across accounts, which isn't needed here. Network ACLs (Option D) operate at the subnet level and are stateless, complicating maintenance. Reference Links: - [AWS WAF Documentation](https://docs.aws.amazon.com/waf/latest/developer guide/waf-chapter.html)      -       [ALB       Integration       with WAF](https://docs.aws.amazon.com/elastic load balancing/latest/application/application-load-balancer-waf.html)

532.A company is building a data analysis platform on AWS by using AWS Lake Formation. The platform will ingest data from different sources such as Amazon S3 and Amazon RDS. The company needs a secure solution to prevent access to portions of t he data that contain sensitive information. Which solution will meet these requirements with the LEAST operational overhead?

A ��Create an IAM role that includes permissions to access Lake Formation tables.

B ��Create data filters to implement row-level security and cell-level security.

C ��Create an AWS Lambda function that removes sensitive information before Lake Formation ingests the data.

D ��Create an AWS Lambda function that periodically queries and removes sensitive information from Lake Formation tables.

�𰸣�B

�� �� �� Correct Answer:  B  Detailed  Explanation:  AWS  Lake  Formation  provides  built-in  features  for  row-level  and  cell-level security, which allow precise control over data access without requiring custom code. By creating data filters, you can restrict access  to  specific   rows  or  cells  containing  sensitive   information  directly  within   Lake   Formation.  This  approach   minimizes operational overhead because it uses native capabilities instead of maintaining custom Lambda functions (Options C   �� D) or relying solely on broader IAM roles (Option A). Key points for beginners: - Row-level security: Limits access to specific rows in a table (e.g., only rows where department = HR). - Cell-level security: Restricts access to specific columns/cells (e.g., hiding salary or SSN columns). -  Least overhead:  Lake Formation handles this natively, avoiding the need to build, test, or update  Lambda functions.                              Reference                               Link:                              [Lake                               Formation                              Access
Permissions](https://docs.aws.amazon.com/lake-formation/latest/dg/access-control.html)

533.A company deploys Amazon EC2 instances that run in a VPC. The EC2 instances load source data into Amazon S3 buckets so that the data can be processed in the future. According to compliance laws, the data must not be transmitted over the public internet. Servers in the company's on-premises data center will consume the output from an application that runs on the EC2 instances. Which solution will meet these requirements?

A ��Deploy an interface VPC endpoint for Amazon EC2. Create an AWS Site-to-Site VPN connection between the company and the VPC.

B ��Deploy a gateway VPC endpoint for Amazon S3. Set up an AWS Direct Connect connection between the on-premises network and the VPC.

C �� Set  up an AWS Transit Gateway connection from the VPC to the S3  buckets. Create an AWS Site-to-Site VPN connection between the company and the VPC.

D ��Set up proxy EC2 instances that have routes to NAT gateways. Configure the proxy EC2 instances to fetch S3 data and feed the application instances.

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: To meet the compliance requirement that data must not traverse the public internet, the solution must ensure private connectivity for both EC2-to-S3 and on-premises-to-VPC traffic. Here ��s why Option B works: 1. Gateway VPC Endpoint for S3: - A gateway VPC endpoint allows EC2 instances in the VPC to access S3 buckets privately via AWS ��s internal network, avoiding the public internet. This satisfies the requirement for secure EC2-to-S3 data transfers. 2. AWS Direct Connect: - Direct Connect establishes a dedicated, private network connection between the on-premises data center and the VPC. This ensures that traffic between the on-premises servers and the EC2 application (e.g., fetching processed data) bypasses the public internet, complying with the security policy. Why other options fail: - Option A: An interface VPC endpoint for EC2 is irrelevant here (EC2 instances are already in the VPC). A VPN connection uses the public internet, which violates the compliance rule. - Option C: Transit Gateway isn ��t needed for S3 access (gateway endpoints are simpler). A VPN still relies on the
public internet. - Option D: Proxy instances with NAT gateways would route traffic through the public internet to access S3, which is non-compliant. Reference: - [AWS VPC Endpoints](https://docs.aws.amazon.com/vpc/latest/private link/vpc-endpoints.html) - [AWS Direct Connect](https://aws.amazon.com/directconnect/)

534.A  company  has  an  application  with  a  REST-based  interface  that  allows  data  to  be  received  in  near-real  time  from  a third-party vendor. Once received, the application processes and stores the data for further analysis. The application is running on Amazon EC2 instances. The third-party vendor has received many 503 Service Unavailable Errors when sending data to the application. When the data volume spikes, the compute capacity reaches  its  maximum  limit and the application is unable to process all requests. Which design should a solutions architect recommend to provide a more scalable solution?

A ��Use Amazon Kinesis Data Streams to ingest the data. Process the data using AWS Lambda functions.

B ��Use Amazon API Gateway on top of the existing application. Create a usage plan with a quota limit for the third-party vendor.

C �� Use Amazon Simple Notification Service (Amazon SNS) to ingest the data. Put the EC2 instances in an Auto Scaling group behind an Application Load Balancer.

D ��Repackage the application as a container. Deploy the application using Amazon Elastic Container Service (Amazon ECS) using the EC2 launch type with an Auto Scaling group.

�𰸣�A

������ Correct Answer A �� Use Amazon Kinesis Data Streams to ingest the data. Process the data using AWS Lambda functions. Detailed Explanation The current issue is that the EC2-based application can't handle sudden spikes in data volume, leading to
503 errors. Here's why Option A solves this: 1. Amazon Kinesis Data Streams acts as a buffer: - It can handle massive amounts of real-time data (like a giant waiting room for incoming data) - Even if data arrives faster than the system can process,  Kinesis stores it safely instead of rejecting requests 2. AWS Lambda automatically scales: - Processes data from Kinesis automatically - No servers to manage - Lambda instantly scales up/down with the workload -  No maximum capacity limit (unlike EC2 instances) Other options have limitations: - B (API Gateway + quotas): Limits requests but doesn't solve the processing bottleneck - C (SNS + EC2 scaling): EC2 scaling is slower than Lambda's instant scaling - D (Containers + ECS): Still uses EC2 instances which scale slower than  Lambda  By  using  Kinesis  +  Lambda,  the  third-party  vendor  can  send  data  without  hitting  503  errors,  and  processing automatically           keeps           up          with           demand.           Reference           Links           -           [Amazon           Kinesis           Data Streams](https://aws.amazon.com/kinesis/data-streams/)                                  -                                   [AWS                                   Lambda Scaling](https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html)

535.A company has an application that  runs on Amazon  EC2 instances in a  private subnet. The application  needs to process sensitive information from an Amazon S3 bucket. The application must not use the internet to connect to the S3 bucket. Which solution will meet these requirements?

A ��Configure an internet gateway. Update the S3 bucket policy to allow access from the internet gateway. Update the application

to use the new internet gateway.

B ��Configure a VPN connection. Update the S3 bucket policy to allow access from the VPN connection. Update the application to use the new VPN connection.

C ��Configure a NAT gateway. Update the S3 bucket policy to allow access from the NAT gateway. Update the application to use the new NAT gateway.

D ��Configure a VPC endpoint. Update the S3 bucket policy to allow access from the VPC endpoint. Update the application to use the new VPC endpoint.

�𰸣�D

������ Correct Answer D Explanation The correct solution is D because a VPC endpoint allows the EC2 instances in the private subnet to securely access the Amazon S3 bucket without using the internet. - VPC Endpoint: This creates a private connection between the VPC and S3, keeping all traffic within the AWS network. No internet gateway, VPN, or NAT device is needed. - S3 Bucket Policy: By updating the bucket policy to allow access only from the VPC endpoint, you ensure that only resources in the VPC (like the EC2 instances) can access the sensitive data. Other options are incorrect: - A (Internet Gateway): Uses the public internet, which violates the requirement. - B (VPN): A VPN connects the VPC to an external network (e.g., on - premises), but accessing S3 via VPN still routes traffic externally, which is unnecessary and insecure here. - C (NAT Gateway): Allows private subnet  instances  to  access  the  internet,  but  traffic  to  S3  would  still  go  through  the  public  internet,  which  is  not  allowed. Reference Links - [AWS VPC Endpoints for S3](https://docs.aws.amazon.com/vpc/latest/private link/vpc-endpoints-s3.html) - [S3 Bucket Policies](https://docs.aws.amazon.com/AmazonS3/latest/user guide/bucket-policies.html)

536.A company uses Amazon  Elastic  Ku bernet es Service  (Amazon  EKS) to  run a container application. The  EKS cluster stores sensitive information in the Ku bernet es secrets object. The company wants to ensure that the information is encrypted. Which solution will meet these requirements with the LEAST operational overhead?

A ��Use the container application to encrypt the information by using AWS Key Management Service (AWS KMS).

B ��Enable secrets encryption in the EKS cluster by using AWS Key Management Service (AWS KMS).

C ��Implement an AWS Lambda function to encrypt the information by using AWS Key Management Service (AWS KMS).

D ��Use AWS Systems Manager Parameter Store to encrypt the information by using AWS Key Management Service (AWS KMS).

�𰸣�B

���� �� Correct Answer: B. Enable secrets encryption in the EKS cluster by using AWS Key Management Service (AWS KMS). Detailed Explanation: The simplest and most efficient solution is to enable EKS native secrets encryption using AWS KMS. This feature automatically encrypts Ku bernet es Secrets at rest in the cluster's etcd storage. Here's why it's the best choice: 1. Zero Code Changes: Unlike Option A (application-level encryption) or Option C/D (external services), this requires no modifications to your  application  code  or  architecture.  2.  Automatic  Encryption:  Once  enabled,  all  Secrets  in  the  EKS  cluster  are  encrypted automatically using a KMS key you control. 3. AWS-Managed: AWS handles the encryption/decryption process seamlessly during read/write  operations,  minimizing  operational  overhead.  4.  Compliance  Ready:  Meets  security  best  practices  for  encrypting sensitive  data  at   rest.  Other  options  add  complexity:  -  A/C   require  custom  code  -   D  involves  managing  Secrets  outside Ku bernet es - All alternatives would need ongoing maintenance compared to EKS's built-in encryption. Reference Links: -  [EKS Documentation on Secrets Encryption](https://docs.aws.amazon.com/eks/latest/user guide/encryption-at-rest.html) - [AWS KMS Integration with EKS](https://aws.amazon.com/blogs/containers/using-aws-kms-to-encrypt-secrets-in-amazon-eks/)

537.A company is designing a new multi-tier web application that consists of the following components:   ? Web and application servers that run on Amazon EC2 instances as part of Auto Scaling groups   ? An  Amazon  RDS  DB instance for data storage A solutions architect needs to limit access to the application servers so that only the web servers can access them. Which solution will meet these requirements?

A ��Deploy AWS Private link in front of the application servers. Configure the network ACL to allow only the web servers to access the application servers.

B��Deploy a VPC endpoint in front of the application servers. Configure the security group to allow only the web servers to access the application servers.

C ��Deploy a Network Load Balancer with a target group that contains the application servers' Auto Scaling group. Configure the network ACL to allow only the web servers to access the application servers.

D ��Deploy an Application Load Balancer with a target group that contains the application servers' Auto Scaling group. Configure the security group to allow only the web servers to access the application servers.

�𰸣�D

���� �� Correct Answer: D Detailed Explanation: To restrict access to the application servers so that only the web servers can reach them, the best approach is to use security groups (not network ACLs) because security groups operate at the instance level, are stateful, and can dynamically reference other security groups. Here ��s why Option D works: 1. Application Load Balancer (ALB): - Deploying an ALB in front of the application servers allows traffic to be routed to the Auto Scaling group of application servers. - ALBs operate at the application layer (HTTP/HTTPS), making them ideal for web-tier architectures. 2. Security Group Configuration: - The application servers' security group should allow inbound traffic only from the ALB ��s security group. - The ALB ��s security group should allow inbound traffic only from the web servers' security group. - This ensures that only the web servers (via the ALB) can communicate with the application servers. Why other options are incorrect: - A   �� C: Network ACLs are stateless and operate at the subnet level. They are less precise for this scenario because web servers in Auto Scaling groups might have dynamic IPs. - B: VPC endpoints are for accessing AWS services (like S3) privately, not for routing traffic between EC2 instances. - Network Load Balancer (NLB) in C: While NLBs are great for TCP/UDP traffic, the use of network ACLs here is less secure and harder to manage compared to security groups. Key Take away: Security groups referencing each other (e.g., allowing traffic from the ALB ��s SG to the app servers �� SG) are the AWS-recommended method for controlling traffic between instances in                       a                        VPC.                       Reference                        Links:                        -                        [Application                       Load
Balancer](https://docs.aws.amazon.com/elastic load balancing/latest/application/introduction.html)    -     [Security     Groups    for EC2](https://docs.aws.amazon.com/AWSEC2/latest/User guide/ec2-security-groups.html)

538.A company runs a critical, customer-facing application on Amazon Elastic Ku bernet es Service (Amazon EKS). The application has a micro services architecture. The company needs to implement a solution that collects, aggregates, and summarizes metrics and logs from the application in a centralized location. Which solution meets these requirements?

A ��Run the Amazon Cloud watch agent in the existing EKS cluster. View the metrics and logs in the Cloud watch console.

B ��Run AWS App Mesh in the existing EKS cluster. View the metrics and logs in the App Mesh console.

C ��Configure AWS Cloud trail to capture data events. Query Cloud trail by using Amazon Open search Service.

D ��Configure Amazon Cloud watch Container Insights in the existing EKS cluster. View the metrics and logs in the Cloud watch console.

�𰸣�D

���� �� Correct Answer: D Detailed Explanation: The correct solution is D because Amazon Cloud watch Container Insights is specifically designed to collect, aggregate, and monitor metrics and logs from containerized applications running on Amazon EKS. It  automatically  discovers  containers  and  resources  in  the  cluster,  collects  performance  data  (like  CPU,  memory,  disk,  and network usage), and sends it to Cloud watch for visualization and analysis. This centralized approach simplifies troubleshooting and performance optimization for micro services-based applications. - Option A (Cloud watch agent) is insufficient because, while it can collect logs and metrics, it lacks the container-specific optimizations and automated aggregation provided by Container Insights.   -   Option   B   (AWS   App    Mesh)   focuses   on   service    mesh   functionality   (traffic   management,   observability   for service-to-service  communication),  not  centralized  logging/ metrics  aggregation.  -  Option  C  (Cloud trail  +  Open search)  is unrelated, as Cloud trail monitors AWS API activity, not application-level metrics or logs from EKS. Reference Links: - [Amazon Cloud watch Container  Insights](https://docs.aws.amazon.com/Amazon cloud watch/latest/monitoring/Container insights.html) - [Monitoring                                                             Amazon                                                              EKS                                                             with
Cloud watch](https://aws.amazon.com/blogs/containers/monitoring-amazon-eks-with-amazon-cloud watch-container-insights/)

539.A company has deployed its newest product on AWS. The product runs in an Auto Scaling group behind a Network Load Balancer. The company stores the product ��s objects  in an Amazon S3  bucket. The company recently experienced malicious attacks against its systems. The company needs a solution that continuously monitors for malicious activity in the AWS account, workloads, and access patterns to the S3 bucket. The solution must also report suspicious activity and display the information on a dashboard. Which solution will meet these requirements?

A ��Configure Amazon Macie to monitor and report findings to AWS Config.

B ��Configure Amazon Inspector to monitor and report findings to AWS Cloud trail.

C ��Configure Amazon Guard duty to monitor and report findings to AWS Security Hub.

D ��Configure AWS Config to monitor and report findings to Amazon Event bridge.

�𰸣�C

�� �� �� Correct Answer  C �� Configure Amazon Guard duty to  monitor and  report findings to AWS Security  Hub.  Detailed Explanation Amazon Guard duty  is a threat detection service that continuously  monitors AWS accounts, workloads, and data stored in services like Amazon S3 for malicious or unauthorized activity. It uses machine learning (ML), anomaly detection, and threat intelligence to identify suspicious behavior, such as unusual API calls, compromised instances, or unauthorized access to S3 buckets. AWS Security Hub acts as a centralized dashboard, aggregating and prioritizing security findings from Guard duty and other AWS security services (like AWS Config or Amazon Inspector). This allows the company to view all security alerts in one place  and take action. Why  Other  Options  Are  Incorrect  -  A  (Amazon  Macie):  Macie  focuses  on  discovering  and  protecting sensitive  data  (e.g.,  personally  identifiable  information)  in  S3  buckets,  not  general  malicious  activity  or  access  patterns.  -  B (Amazon Inspector): Inspector scans EC2 instances and applications for vulnerabilities but doesn ��t monitor account-wide threats or S3 access patterns. - D (AWS Config + Event bridge): AWS Config tracks resource configuration changes and compliance, while Event bridge        routes         events.         Neither         detects         malicious         activity.         Reference         Links         -         [Amazon Guard duty](https://aws.amazon.com/guard duty/) - [AWS Security Hub](https://aws.amazon.com/security-hub/)

540.A company wants to use Amazon  FSx for Windows File Server for its Amazon  EC2 instances that have an SMB file share mounted as a volume  in the  us-east-1  Region. The  company  has  a  recovery  point  objective  (RPO)  of 5  minutes for  planned system maintenance or unplanned service disruptions. The company needs to replicate the file system to the us-west-2 Region. The replicated data must not be deleted by any user for 5 years. Which solution will meet these requirements?

A ��Create an FSx for Windows File Server file system in us-east-1 that has a Single-AZ 2 deployment type. Use AWS Backup to create a daily backup plan that includes a backup rule that copies the backup to us-west-2. Configure AWS Backup Vault Lock in compliance mode for a target vault in us-west-2. Configure a minimum duration of 5 years.

B��Create an FSx for Windows File Server file system in us-east-1 that has a Multi-AZ deployment type. Use AWS Backup to create a  daily  backup  plan  that  includes  a  backup  rule  that  copies  the  backup  to  us-west-2.  Configure  AWS  Backup  Vault  Lock  in governance mode for a target vault in us-west-2. Configure a minimum duration of 5 years.

C��Create an FSx for Windows File Server file system in us-east-1 that has a Multi-AZ deployment type. Use AWS Backup to create a  daily  backup  plan  that  includes  a  backup  rule  that  copies  the  backup  to  us-west-2.  Configure  AWS  Backup  Vault  Lock  in compliance mode for a target vault in us-west-2. Configure a minimum duration of 5 years.

D ��Create an FSx for Windows File Server file system in us-east-1 that has a Single-AZ 2 deployment type. Use AWS Backup to create a daily backup plan that includes a backup rule that copies the backup to us-west-2. Configure AWS Backup Vault Lock in governance mode for a target vault in us-west-2. Configure a minimum duration of 5 years.

�𰸣�C

���� �� Correct Answer: C Detailed  Explanation: The correct answer is C because it meets all the requirements:  1.  Multi-AZ Deployment: The FSx for Windows File Server uses a Multi-AZ deployment in the primary region (us-east-1). This ensures high availability  and  an  RPO  (Recovery  Point  Objective)  of  near  zero  for  failures  within   the   region.   Multi-AZ  replicates  data synchronously across Availability Zones (AZs), so during an AZ outage, automatic failover occurs without data loss. This addresses the 5-minute RPO requirement for planned/unplanned disruptions within the region. 2. Cross-Region Backup via AWS Backup: AWS Backup is used to create daily backups and copy them to the us-west-2 region. While daily backups alone wouldn ��t meet a 5-minute  RPO  for  cross-region  recovery,  the  Multi-AZ  deployment  in  us-east-1  already  handles  the  5-minute  RPO  for  local disruptions. The cross-region backup here primarily serves the 5-year retention requirement, not immediate disaster recovery. 3. AWS  Backup  Vault  Lock  in  Compliance  Mode:  Compliance  mode  ensures  backups  in  the  target  vault  (us-west-2)  cannot  be deleted by any user, including root AWS accounts, for the minimum 5-year duration. This enforces strict retention, meeting the compliance requirement. Governance mode (used in options B and D) allows deletions with special permissions, which doesn ��t satisfy the requirement. Why Other Options Fail: - A   �� D: Single-AZ 2 deployment lacks Multi-AZ ��s automatic failover, risking higher RPO during AZ failures. - B: Governance mode allows backup deletion, violating the 5-year retention rule. Reference Links:
-  [Amazon  FSx  Multi-AZ  Deployment](https://docs.aws.amazon.com/fsx/latest/Windows guide/high-availability-multiAZ.html)  - [AWS Backup Vault Lock](https://docs.aws.amazon.com/aws-backup/latest/dev guide/vault-lock.html)

541.A solutions architect is designing a security solution for a company that wants to provide developers with individual AWS accounts through AWS Organizations, while also maintaining standard security controls. Because the individual developers will have AWS account root user-level access to their own accounts, the solutions architect wants to ensure that the mandatory AWS Cloud trail configuration that is applied to new developer accounts is not modified. Which action meets these requirements?

A ��Create an IAM policy that prohibits changes to Cloud trail. and attach it to the root user.

B ��Create a new trail in Cloud trail from within the developer accounts with the organization trails option enabled. C ��Create a service control policy (SCP) that prohibits changes to Cloud trail, and attach it the developer accounts.

D ��Create a service-linked role for Cloud trail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the management account.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The best solution here is to use a Service Control Policy (SCP). SCPs are part of AWS Organizations and act as guardrails to  restrict what actions  users  (including  root  users) can  perform  across entire AWS accounts. By creating an SCP that explicitly blocks modifications to AWS Cloud trail configurations and attaching it to developer accounts,  even  root  users  in  those  accounts  cannot  alter  the  mandatory  Cloud trail  setup.  This  ensures  centralized  security control across all developer accounts in the organization. Other options like IAM policies (A) don't apply to root users, creating new trails  (B)  doesn't  prevent  modification,  and service-linked  roles  (D)  are  more  complex  and  less  direct for this  use  case. Reference                          Links:                          -                           AWS                          Service                           Control                          Policies:
https://docs.aws.amazon.com/organizations/latest/user guide/orgs_manage_policies_scps.html - AWS Cloud trail  Best Practices: https://docs.aws.amazon.com/aws cloud trail/latest/user guide/best-practices.html

542.A company is planning to deploy a business-critical application in the AWS Cloud. The application requires durable storage with  consistent,  low-latency  performance.  Which  type  of  storage  should  a  solutions  architect   recommend  to  meet  these requirements?

A ��Instance store volume

B ��Amazon Elastic ache for Memcached cluster

C ��Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume

D ��Throughput Optimized HDD Amazon Elastic Block Store (Amazon EBS) volume

�𰸣�C

������ Correct Answer C �� Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume Detailed Explanation The application requires durable storage (data persistence) and consistent, low-latency performance. Here ��s why the other options are incorrect: - A. Instance store volume: Instance store volumes are physically attached to the EC2 instance and provide high performance, but they are not durable��data is lost if the instance stops or terminates. - B. Amazon Elastic ache for Memcached cluster:  Elastic ache is an in-memory caching service, not durable storage.  It reduces  latency for frequently accessed data but doesn ��t guarantee data persistence. - D. Throughput Optimized HDD (st1): Designed for large, sequential workloads (e.g., big data, logs). It prioritizes high throughput over low latency or random I/O performance. Option C is correct because Provisioned IOPS SSD (io1/io2/io2 Block Express) EBS volumes: 1. Are durable (automatically replicated within an Availability Zone). 2. Allow you to  provision  IOPS  (Input/Output Operations  Per Second) to ensure consistent,  low-latency  performance for  I/O-intensive workloads        like        databases        or        critical         applications.        Reference        Links         -        [Amazon         EBS        Volume Types](https://docs.aws.amazon.com/AWSEC2/latest/User guide/ebs-volume-types.html)        -        [Provisioned         IOPS        SSD Volumes](https://aws.amazon.com/ebs/features/)

543.An online photo-sharing company stores its photos in an Amazon S3 bucket that exists in the us-west-1 Region. The company needs to store a copy of all  new  photos  in the  us-east-1  Region.  Which solution will  meet this  requirement with  the  LEAST operational effort?

A ��Create a second S3 bucket in us-east-1. Use S3 Cross-Region Replication to copy photos from the existing S3 bucket to the second S3 bucket.

B �� Create a cross-origin resource sharing (CORS) configuration of the existing S3 bucket. Specify us-east-1 in the CORS rule's Allowed origin element.

C ��Create a second S3 bucket in us-east-1 across multiple Availability Zones. Create an S3 Lifecycle rule to save photos into the second S3 bucket.

D ��Create a second S3 bucket in us-east-1. Configure S3 event notifications on object creation and update events to invoke an AWS Lambda function to copy photos from the existing S3 bucket to the second S3 bucket.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution is A because S3 Cross-Region Replication (CRR) automatically copies  new  objects  from  one  S3  bucket  to  another  bucket  in  a  different  AWS  Region.  This  is  a  fully  managed  AWS  feature requiring minimal setup (just enabling versioning on both buckets and configuring replication rules).  It guarantees  replication without extra code or manual processes. - Why not B? CORS (Cross-Origin Resource Sharing) controls web browser access to S3 resources, not replication. It has nothing to do with copying data between regions. - Why not C? S3 Lifecycle rules manage object transitions (e.g., moving to Glacier) or deletions, not cross-region replication. - Why not D? While Lambda can copy objects, it requires  writing,  deploying,  and  maintaining  code.  This  adds  operational  complexity  compared  to  CRR.   Key   Point:  CRR  is purpose-built   for   automated    cross-region   replication   with    near-zero   operational    effort.   Reference    Link:   [Amazon    S3 Replication](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html)

544.A company uses Amazon API Gateway to manage its REST APIs that third-party service providers access. The company must protect the REST APIs from SQL injection and cross-site scripting attacks. What is the MOST operationally efficient solution that meets these requirements?

A ��Configure AWS Shield.

B ��Configure AWS WAF.

C ��Set up API Gateway with an Amazon Cloud front distribution. Configure AWS Shield in Cloud front.

D ��Set up API Gateway with an Amazon Cloud front distribution. Configure AWS WAF in Cloud front.

�𰸣�B

������ Correct Answer B �� Configure AWS WAF. Detailed Explanation The question asks for the most operationally efficient way to protect REST APIs (hosted on Amazon API Gateway) from SQL injection and cross - site scripting (XSS) attacks. Here's why AWS WAF (Option B) is the best choice: 1. AWS WAF is designed for web application security: - AWS WAF allows you to create rules to block  common  web  exploits  like  SQL  injection  and  XSS.  These  rules  can  directly  inspect  HTTP/HTTPS  requests  to  your  API Gateway and block malicious traffic. - Example: You can use AWS WAF's managed rulesets (e.g., the AWS Managed Rules - Core Rule Set) to automatically block these attacks without writing custom code. 2. Direct integration with API Gateway: - AWS WAF natively  integrates  with  API  Gateway.  You  can  attach  a  Web  ACL  (Web  Access  Control  List)  directly  to  your  API  Gateway, eliminating the need for additional infrastructure like Cloud front. - This makes Option B simpler and more operationally efficient than Option D (which requires adding Cloud front). 3. AWS Shield vs. AWS WAF: - AWS Shield (Options A and C) protects against DDoS attacks, not application - layer vulnerabilities like SQL injection or XSS. It ��s irrelevant here. 4. Why not Option D? - While using Cloud front + AWS WAF (Option D) works, it adds unnecessary complexity. Cloud front is a content delivery network (CDN), and the question doesn ��t mention requirements like caching or global acceleration. - Option B (WAF directly on API Gateway) achieves      the       same       security       goal       with       fewer       steps.       Reference        Links      -        [AWS       WAF       for      API Gateway](https://docs.aws.amazon.com/api gateway/latest/developer guide/api gateway-control-aws-waf.html)    -     [AWS     WAF Managed Rules](https://docs.aws.amazon.com/waf/latest/developer guide/aws-managed-rule-groups-list.html)

545.A company wants to provide users with access to AWS resources. The company has 1,500 users and manages their access to

on-premises resources through Active Directory user groups on the corporate network. However, the company does not want users to have to maintain another identity to access the resources. A solutions architect must manage user access to the AWS resources  while   preserving  access  to  the  on-premises   resources.  What  should  the  solutions   architect  do  to   meet  these requirements?

A ��Create an IAM user for each user in the company. Attach the appropriate policies to each user.

B ��Use Amazon Cognito with an Active Directory user pool. Create roles with the appropriate policies attached.

C ��Define cross-account roles with the appropriate policies attached. Map the roles to the Active Directory groups.

D �� Configure Security Assertion  Markup  Language  (SAML)  2  0-based  federation.  Create  roles  with  the  appropriate  policies attached Map the roles to the Active Directory groups.

�𰸣�D

������ Correct Answer D �� Configure Security Assertion Markup Language (SAML) 2.0-based federation. Create roles with the appropriate policies attached. Map the roles to the Active Directory groups. Detailed Explanation The company wants to avoid managing separate identities for AWS and wants to use existing Active Directory (AD) groups for access control. Here ��s why option D is the best choice:  1. SAML 2.0 Federation - SAML allows single sign-on (SSO) using the company ��s existing Active Directory. Users log in to AWS using their on-premises AD credentials, so they don ��t need a separate AWS identity. - AWS acts as a service provider (SP), and the on-premises AD acts as the identity provider (IdP). This setup establishes trust between AWS and AD. 2. Role-Based Access - Instead of creating IAM users (which would require managing 1,500 separate identities), the architect creates IAM roles in AWS. - Each role has permissions policies that define what AWS resources users can access. 3. Mapping AD Groups to AWS Roles - AD groups (e.g., Developers, Admins) are mapped to corresponding IAM roles. For example, users in the AD Developers group automatically get permissions from the Dev-Role in AWS. - This ensures centralized access management��if a  user ��s AD group changes, their AWS  permissions  update  automatically. Why Other  Options Are Wrong - A  (IAM  Users): Creating 1,500 IAM users would force users to manage separate AWS credentials, violating the requirement to avoid maintaining another  identity.  -   B  (Amazon  Cognito):  Cognito  is  designed  for  customer-facing  applications  (e.g.,   mobile  apps),   not  for federating enterprise AD  users to AWS.  - C  (Cross-Account  Roles):  Cross-account  roles  are  for  sharing  resources  across AWS accounts,     not      for     integrating     on-premises      AD     with      AWS.     Reference      Link      [AWS     IAM      SAML      Federation Documentation](https://docs.aws.amazon.com/IAM/latest/User guide/id_roles_providers_saml.html)

546.A company is hosting a website behind multiple Application Load Balancers. The company has different distribution rights for  its  content  around  the  world.  A  solutions  architect  needs  to  ensure  that  users  are  served  the  correct  content  without violating distribution rights. Which configuration should the solutions architect choose to meet these requirements?

A ��Configure Amazon Cloud front with AWS WAF.

B ��Configure Application Load Balancers with AWS WAF C ��Configure Amazon Route 53 with a geolocation policy

D ��Configure Amazon Route 53 with a geo proximity routing policy

�𰸣�C

������Correct Answer: C. Configure Amazon Route 53 with a geolocation policy Detailed Explanation: When a company needs to serve content based on users' geographic locations (like respecting regional distribution rights), Amazon Route 53's geolocation

routing policy  is the  best solution.  Here's why:  1.  Geolocation  Routing:  Route 53 can direct  users to specific endpoints  (e.g., different ALBs)  based  on their geographic  location  (country  or  continent).  For  example: -  Users  in  Europe   ��  route to ALB hosting  European  content.  -  Users  in  Asia   ��  route  to ALB  hosting  Asian  content.  2.  Legal  Compliance:  This  ensures  users automatically access the correct content for their region, avoiding distribution rights violations. 3. Why Not Other Options?: - A (Cloud front  +  WAF):  Cloud front  is  a  CDN  for  caching  content  globally,  and  WAF  is  for  security.  While  Cloud front  supports geographic restrictions, it ��s designed to block access��not route users to region - specific ALBs. - B (ALB + WAF): ALBs distribute traffic within a region, and WAF protects against web attacks. Neither handles geographic routing. - D (Route 53 Geo proximity): Geo proximity routes traffic based on physical distance (low latency), not legal boundaries, making it unsuitable for distribution rights.                    Reference                     Links:                    -                     [Amazon                     Route                    53                     Geolocation
Routing](https://docs.aws.amazon.com/Route53/latest/Developer guide/routing    -    policy    -    geo.html)    -    [Geolocation    vs. Geo proximity Routing](https://aws.amazon.com/route53/faqs/)

547.A  company  stores  its  data  on  premises.  The  amount  of  data  is  growing  beyond  the  company's  available  capacity.  The company wants to migrate its data from the on-premises location to an Amazon S3 bucket. The company needs a solution that will automatically validate the integrity of the data after the transfer. Which solution will meet these requirements?

A ��Order an AWS Snowball Edge device. Configure the Snowball Edge device to perform the online data transfer to an S3 bucket

B �� Deploy an AWS  Data sync agent on premises. Configure the  Data sync agent to perform the online data transfer to an S3 bucket.

C ��Create an Amazon S3 File Gateway on premises Configure the S3 File Gateway to perform the online data transfer to an S3 bucket

D ��Configure an accelerator in Amazon S3 Transfer Acceleration on premises. Configure the accelerator to perform the online data transfer to an S3 bucket.

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: AWS Data sync is specifically designed for securely and efficiently transferring large amounts of data between on-premises storage and AWS services like Amazon S3. It automatically performs data integrity validation both during and after transfer using checksums, ensuring that the data arrives intact without corruption. This makes it the best choice compared to other options: - A (Snowball Edge): While Snowball devices can transfer data, they're primarily for offline transfers. Online transfers and automatic validation are not their main focus. - C (S3 File Gateway): This acts as a bridge for file access to S3 but doesn ��t specialize in bulk data migration with built - in validation. - D (Transfer Acceleration): This only speeds up transfers via Cloud front but doesn ��t validate data integrity. Data sync ��s end - to - end validation and optimized transfer        protocol        make        it       the        most        suitable        solution.        Reference        Links:        -        [AWS         Data sync Documentation](https://docs.aws.amazon.com/data sync/latest/user guide/how  -  data sync  -  works.html)  -  [Data  Integrity  in Data sync](https://aws.amazon.com/data sync/features/)

548.A company wants to migrate two DNS servers to AWS. The servers host a total of approximately 200 zones and receive 1 million requests each day on average. The company wants to maximize availability while minimizing the operational overhead that  is  related  to  the  management  of  the  two   servers.  What  should  a  solutions  architect   recommend  to  meet  these requirements?

A ��Create 200 new hosted zones in the Amazon Route 53 console Import zone files.

B ��Launch a single large Amazon EC2 instance Import zone tiles. Configure Amazon Cloud watch alarms and notifications to alert the company about any downtime.

C �� Migrate the servers to AWS by using AWS Server Migration Service (AWS SMS). Configure Amazon Cloud watch alarms and notifications to alert the company about any downtime.

D �� Launch an Amazon EC2 instance in an Auto Scaling group across two Availability Zones. Import zone files. Set the desired capacity to  1  and  the  maximum  capacity  to  3  for  the  Auto  Scaling  group.  Configure  scaling  alarms  to  scale  based  on  CPU utilization.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution is to use Amazon Route 53, AWS's managed DNS service. Route 53 automatically provides high availability, scalability, and redundancy across multiple Availability Zones, eliminating the need to manually manage servers. By creating 200 hosted zones and importing zone files (Option A), the company offloads operational tasks like server maintenance, scaling, and uptime monitoring to AWS. This meets the requirements of maximizing availability and minimizing operational overhead. Other options involve self-managed EC2 instances (B, C, D), which require ongoing server management, scaling configurations, and downtime  monitoring �� adding operational complexity. Route 53 ��s fully  managed service is purpose-built for DNS workloads and handles 1 million daily requests effortlessly. Reference Link: [Amazon Route 53 Hosted Zones](https://docs.aws.amazon.com/Route53/latest/Developer guide/hosted-zones-working-with.html)

549.A global company  runs  its applications  in  multiple AWS accounts  in AWS Organizations. The company's applications  use multipart  uploads  to  upload  data  to  multiple  Amazon  S3  buckets  across  AWS  Regions.  The  company  wants  to  report  on incomplete  multipart  uploads  for  cost  compliance  purposes.  Which  solution  will  meet  these  requirements  with  the  LEAST operational overhead?

A ��Configure AWS Config with a rule to report the incomplete multipart upload object count.

B ��Create a service control policy (SCP) to report the incomplete multipart upload object count.

C ��Configure S3 Storage Lens to report the incomplete multipart upload object count.

D ��Create an S3 Multi-Region Access Point to report the incomplete multipart upload object count.

�𰸣�C

���� �� Correct Answer: C Detailed Explanation: The correct answer is C. Configure S3 Storage Lens to report the incomplete multipart upload object count. Here ��s why: - S3 Storage Lens is a native AWS tool designed for centralized storage analytics across  multiple  accounts  and  regions  in  an  AWS  Organization.  It  automatically  aggregates  metrics  like  incomplete  multipart uploads,  eliminating the  need  for  manual  configuration  in  every  account  or  bucket.  -  Option  A  (AWS  Config)  would  require custom rules and ongoing management to track object-level metrics like incomplete multipart uploads, increasing operational complexity. - Option B (SCP) is for enforcing permissions, not monitoring or reporting. - Option D (Multi-Region Access Point) optimizes traffic routing, not cost reporting. S3 Storage Lens provides a pre-built dashboard with cost-related metrics (including incomplete  multipart  uploads)  across  all  accounts/regions,  making  it  the  least  effort  solution.  Reference  Link:  -  [Amazon  S3 Storage Lens](https://docs.aws.amazon.com/AmazonS3/latest/user guide/storage-lens.html)

550.A company runs a production database on Amazon RDS for MySQL. The company wants to upgrade the database version for security compliance reasons. Because the database contains critical data, the company wants a quick solution to upgrade and test functionality without losing any data. Which solution will meet these requirements with the LEAST operational overhead?

A ��Create an RDS manual snapshot. Upgrade to the new version of Amazon RDS for MySQL.

B ��Use native backup and restore. Restore the data to the upgraded new version of Amazon RDS for MySQL.

C �� Use AWS  Database  Migration  Service  (AWS  DMS) to  replicate the data to the upgraded  new version of Amazon  RDS for MySQL.

D ��Use Amazon RDS Blue/Green Deployments to deploy and test production changes.

�𰸣�D

�� �� ��  Correct  Answer:  D  Detailed  Explanation:  Amazon  RDS  Blue/Green  Deployments  allow  you  to  create  a  separate, synchronized Green environment alongside your current Blue production database. You can upgrade the Green environment to the new MySQL version, test it thoroughly, and then switch traffic to it with minimal downtime. This approach ensures: - Zero data loss (data is synced between Blue and Green). - Safe testing (test on Green without affecting Blue). - Quick rollback (if issues arise, just keep using Blue). - Least operational overhead (automated sync and cutover). Other options like manual snapshots (A) or native backups (B) require downtime and manual steps. AWS DMS (C) adds complexity for replication setup. Blue/Green is purpose-built                 for                 this                 scenario.                 Reference:                  [Amazon                 RDS                 Blue/Green
Deployments](https://docs.aws.amazon.com/Amazon rds/latest/User guide/blue-green-deployments.html)

551.A solutions architect is creating a data processing job that runs once daily and can take up to 2 hours to complete. If the job is  interrupted,  it  has  to  restart  from  the   beginning.   How  should  the  solutions  architect   address  this   issue  in  the   MOST cost-effective manner?

A ��Create a script that runs locally on an Amazon EC2 Reserved Instance that is triggered by a cron job.

B ��Create an AWS Lambda function triggered by an Amazon Event bridge scheduled event.

C ��Use an Amazon Elastic Container Service (Amazon ECS) Fargate task triggered by an Amazon Event bridge scheduled event.

D �� Use an Amazon Elastic Container Service (Amazon ECS) task running on Amazon EC2 triggered by an Amazon Event bridge scheduled event.

�𰸣�C

�� �� �� Correct Answer: C  Detailed  Explanation: The  most  cost-effective solution  is  using Amazon  ECS  Fargate  triggered  by Event bridge. Here's why: - AWS Lambda (Option B) has a 15-minute runtime limit, so it can ��t handle 2-hour jobs. - EC2 Reserved Instances  (Option A)  require  paying  upfront for idle time, which  is wasteful for a  daily 2-hour task. -  ECS  on  EC2  (Option  D) involves managing servers, which adds overhead and costs for unused capacity. - ECS Fargate (Option C) is serverless, so you only pay for the time the task runs (2 hours daily). Event bridge triggers it automatically, and Fargate handles restarts if interrupted. No infrastructure   management    is   needed,    making    it   both    cost-efficient   and    reliable.    Reference   Links:    -    [AWS   Fargate Pricing](https://aws.amazon.com/fargate/pricing/)       -       [Amazon        ECS](https://aws.amazon.com/ecs/)       -       [Event bridge Scheduler](https://docs.aws.amazon.com/event bridge/latest/user guide/scheduler.html)

552.A social media company wants to store its database of user profiles, relationships, and interactions in the AWS Cloud. The company  needs  an  application  to  monitor  any  changes  in  the  database.  The  application  needs  to  analyze  the  relationships between the data entities and to  provide  recommendations to users. Which solution will  meet these  requirements with the LEAST operational overhead?

A ��Use Amazon Neptune to store the information. Use Amazon Kinesis Data Streams to process changes in the database.

B ��Use Amazon Neptune to store the information. Use Neptune Streams to process changes in the database.

C��Use Amazon Quantum Ledger Database (Amazon QLDB) to store the information. Use Amazon Kinesis Data Streams to process changes in the database.

D ��Use Amazon Quantum Ledger Database (Amazon QLDB) to store the information. Use Neptune Streams to process changes in the database.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best choice is B because Amazon Neptune is a graph database designed to handle highly connected data like user relationships and interactions, which is perfect for social networks. Neptune Streams, a built-in feature of Neptune, automatically captures real-time changes (adds, updates, deletes) in the database. This eliminates the need for extra setup or services to monitor changes, reducing operational overhead. Options A and C involve Kinesis Data Streams, which would require additional steps to integrate with Neptune or QLDB, increasing complexity. Options C and D use Amazon QLDB, which is a ledger database for audit-focused use cases,  not optimized for analyzing complex relationships like social  graphs.   Neptune  Streams  (B)   provides  a  seamless,   low-effort  solution  for  tracking  changes  and  enabling   real-time recommendations  based on graph data.  Reference Links: -  [Amazon  Neptune](https://aws.amazon.com/neptune/) -  [Neptune Streams](https://docs.aws.amazon.com/neptune/latest/user guide/streams.html)

553.A company is creating a new application that will store a large amount of data. The data will be analyzed hourly and will be modified by several Amazon EC2  Linux instances that are deployed across  multiple Availability Zones. The  needed amount of storage space will continue to grow for the next 6 months. Which storage solution should a solutions architect recommend to meet these requirements?

A ��Store the data in Amazon S3 Glacier. Update the S3 Glacier vault policy to allow access to the application instances.

B ��Store the data in an Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume on the application instances.  C ��Store the data in an Amazon Elastic File System (Amazon EFS) file system. Mount the file system on the application instances.

D ��Store the data  in an Amazon  Elastic  Block Store (Amazon  EBS)  Provisioned  IOPS volume shared  between the application instances.

�𰸣�C

������ Correct Answer C Detailed Explanation The correct answer is C: Store the data in an Amazon Elastic File System (Amazon EFS) file system. Mount the file system on the application instances. Here ��s why: - Requirement 1: Multiple EC2 instances across Availability Zones need to access/modify the same data. Amazon EFS is a fully managed network file system (NFS) that allows multiple EC2 instances (even across different Availability Zones) to read/write to the same storage simultaneously. EBS volumes, on the other hand, can only be attached to one EC2 instance at a time and are tied to a single Availability Zone. - Requirement 2: Storage   must  grow   automatically   over   6   months.   EFS   automatically   scales   storage   capacity   up   or   down   as   files   are added/removed, so you don ��t need to provision storage in advance. EBS volumes require manual resizing if you need more space. - Requirement 3: Hourly data analysis with frequent modifications. EFS is designed for workloads with frequent read/write operations. While EBS Provisioned IOPS (option D) offers high performance, it cannot be shared across instances or Availability Zones. Why other options are wrong: - A (S3 Glacier): Glacier is for long - term archival (retrieval takes minutes to hours), not for active data modifications. - B (EBS volume): EBS can ��t be shared across multiple instances or AZs. - D (EBS Provisioned IOPS): Same     issue     as     B;     it    �� s      still     a     single     -     AZ     block     storage     device.     Reference     Links     -     [Amazon      EFS

Overview](https://aws.amazon.com/efs/) - [Comparing EBS and EFS](https://aws.amazon.com/ebs/features/)

554.A company manages an application that stores data on an Amazon RDS for Postgresql Multi-AZ DB instance. Increases in traffic are causing performance problems. The company determines that database queries are the primary reason for the slow performance. What should a solutions architect do to improve the application's performance?

A ��Serve read traffic from the Multi-AZ standby replica.

B ��Configure the DB instance to use Transfer Acceleration.

C ��Create a read replica from the source DB instance. Serve read traffic from the read replica.

D �� Use  Amazon  Kinesis  Data  Firehose  between  the  application  and  Amazon  RDS  to  increase  the  concurrency  of  database requests.

�𰸣�C

������Correct Answer: C. Create a read replica from the source DB instance. Serve read traffic from the read replica. Explanation: When an RDS Multi-AZ DB instance experiences slow performance due to heavy read traffic, creating a read replica is the best solution. Here's why: - Read Replicas are asynchronous copies of your primary database. They allow you to offload read-heavy queries  (like  reports  or  analytics) from the  primary  instance,  freeing  it  to focus  on write  operations  (like  inserts/updates). - Multi-AZ Standby Replica (Option A) is only for high availability (failover), not for read traffic. It stays idle unless the primary fails.
- Transfer Acceleration (Option B) optimizes data transfer over long distances but doesn ��t address query overload. - Kinesis Data Firehose  (Option  D)  is  for  streaming  and  storing  data  (like  logs),  not  optimizing  database  queries.  By  creating  read  replicas (Option C), you horizontally scale read capacity. The application can direct read requests to the replica(s), reducing load on the primary          DB           and           improving           performance.           Reference           Links:          -           [Amazon           RDS           Read Replicas](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_Read repl.html)        -        [Multi-AZ        vs.         Read Replica](https://aws.amazon.com/rds/details/read-replicas/)

555.A company collects 10 GB of telemetry data daily from various machines. The company stores the data in an Amazon S3 bucket in a source data account. The company has hired several consulting agencies to use this data for analysis. Each agency needs read access to the data for its analysts. The company must share the data from the source data account by choosing a solution that maximizes security and operational efficiency. Which solution will meet these requirements?

A ��Configure S3 global tables to replicate data for each agency.

B ��Make the S3 bucket public for a limited time. Inform only the agencies.

C ��Configure cross-account access for the S3 bucket to the accounts that the agencies own.

D ��Set up an IAM user for each analyst in the source data account. Grant each user access to the S3 bucket.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The best solution is C. Configure cross-account access for the S3 bucket to the accounts that the agencies own. Here ��s why: - Security: Instead of making the bucket public (which is risky) or creating IAM users  in  the  source  account  (which  is  hard  to  manage),  cross-account  access  allows  the  source  account  to  grant  read-only permissions directly to the agencies �� AWS accounts via S3 bucket policies. The agencies then control their own IAM users/roles, reducing security risks. - Operational Efficiency: No data replication (unlike Option A) or manual user management (unlike Option

D) is required. The agencies access the same S3 bucket, so updates are automatic. Example: Imagine the source account ��owns �� the S3 bucket. Using a bucket policy, it allows ��read �� actions for specific agency AWS account IDs. Each agency ��s analysts then	 access the data via  roles/users  in their own accounts, without  needing  credentials from the source account. Why  not other	 options? - A:  Replicating  data  (e.g.,  cross-region)  is  costly  and  unnecessary.  -  B:  Public  buckets  are  insecure.  -  D:  Managing	 hundreds       of        IAM       users        in        one       account        is       error-prone.        Reference:        [AWS       Cross-Account        S3	 Access](https://docs.aws.amazon.com/AmazonS3/latest/user guide/example-walkthroughs-managing-access-example2.html)

556.A company uses Amazon FSx for NetApp ONTAP in its primary AWS Region for CIFS and NFS file shares. Applications that run on Amazon EC2 instances access the file shares. The company needs a storage disaster recovery (DR) solution in a secondary Region. The data that is replicated in the secondary Region needs to be accessed by using the same protocols as the primary Region. Which solution will meet these requirements with the LEAST operational overhead?

A ��Create an AWS Lambda function to copy the data to an Amazon S3 bucket. Replicate the S3 bucket to the secondary Region.

B��Create a backup of the FSx for ONTAP volumes by using AWS Backup. Copy the volumes to the secondary Region. Create a new FSx for ONTAP instance from the backup.

C ��Create an FSx for ONTAP instance in the secondary Region. Use NetApp Snap mirror to replicate data from the primary Region to the secondary Region.

D ��Create an Amazon Elastic File System (Amazon EFS) volume. Migrate the current data to the volume. Replicate the volume to the secondary Region.

�𰸣�C

������ Correct Answer C Detailed Explanation The correct answer is C because FSx for ONTAP natively integrates with NetApp Snap mirror, a purpose-built tool for replicating data between ONTAP systems. This solution ensures the secondary Region ��s FSx for ONTAP instance maintains the same CIFS/NFS protocols and file system structure as the primary Region, enabling seamless access for  applications.  Snap mirror  handles  continuous,  incremental  replication  with  minimal  manual  intervention,  reducing operational overhead. Other  options are  less  ideal: - A  uses S3  (object storage), which  doesn ��t support CIFS/NFS  natively, requiring  extra  steps  to  restore  data  to  a  file  system.  -  B  relies  on  backups,  which  are  point-in-time  and  require  manual restoration, leading to potential data loss and higher effort. - D uses Amazon EFS, which only supports NFS (not CIFS), violating protocol                requirements.                Reference                Links                 -                [FSx                for                ONTAP                Disaster Recovery](https://docs.aws.amazon.com/fsx/latest/On tap guide/disaster-recovery.html)   -   [NetApp   Snap mirror  with   FSx   for ONTAP](https://docs.aws.amazon.com/fsx/latest/On tap guide/snap mirror.html)

557.A development team is creating an event-based application that uses AWS Lambda functions. Events will be generated when files are added to an Amazon S3 bucket. The development team currently has Amazon Simple Notification Service (Amazon SNS) configured as the event target from Amazon S3. What should a solutions architect do to process the events from Amazon S3 in a scalable way?

A ��Create an SNS subscription that processes the event in Amazon Elastic Container Service (Amazon ECS) before the event runs in Lambda.

B��Create an SNS subscription that processes the event in Amazon Elastic Ku bernet es Service (Amazon EKS) before the event runs in Lambda

C ��Create an SNS subscription that sends the event to Amazon Simple Queue Service (Amazon SQS). Configure the SOS queue to trigger a Lambda function.

D ��Create an SNS subscription that sends the event to AWS Server Migration Service (AWS SMS). Configure the Lambda function to poll from the SMS event.

�𰸣�C

���� �� Correct Answer C Detailed Explanation The best approach is to use Amazon SQS (Simple Queue Service) as a buffer between SNS and Lambda. Here's why: 1. Scalability   �� Decoupling: - S3 sends events to SNS,  but directly triggering Lambda from SNS can lead to throttling or failures if Lambda is overloaded. - SQS acts as a durable message queue. It decouples SNS from Lambda, allowing messages to persist even if Lambda is temporarily unavailable. 2. Automatic Scaling: - Lambda can poll SQS queues and automatically scale up (up to 1,000 concurrent executions) based on the number of messages in the queue. This ensures high throughput and no message loss during traffic spikes. 3. Error Handling: - If Lambda fails to process a message, SQS automatically retries or  moves the  message to a dead -  letter queue (DLQ) for investigation. Options A and  B  (ECS/EKS) add unnecessary complexity since Lambda is already serverless and scales automatically. Option D (AWS SMS) is unrelated to event processing.             Reference              Link              [AWS              S3              Event             Notifications              with              SQS             and Lambda](https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html)

558.A company collects and shares  research data with the company's employees all over the world. The company wants to collect and store the data in an Amazon S3 bucket and process the data in the AWS Cloud. The company will share the data with the company's employees. The company needs a secure solution in the AWS Cloud that minimizes operational overhead. Which solution will meet these requirements?

A ��Use an AWS Lambda function to create an S3 pre signed URL. Instruct employees to use the URL.

B ��Create an IAM user for each employee. Create an IAM policy for each employee to allow S3 access. Instruct employees to use the AWS Management Console.

C ��Create an S3 File Gateway. Create a share for uploading and a share for downloading. Allow employees to mount shares on their local computers to use S3 File Gateway.

D ��Configure AWS Transfer Family SFTP endpoints. Select the custom identity provider options. Use AWS Secrets Manager to manage the user credentials Instruct employees to use Transfer Family.

�𰸣�C

������ Correct Answer: D Detailed Explanation: The correct answer is D. Configure AWS Transfer Family SFTP endpoints. Select the custom  identity  provider  options.  Use AWS Secrets  Manager to  manage the  user  credentials.  Instruct  employees  to  use Transfer Family. Why this works: - Secure   �� Easy Access: AWS Transfer Family provides fully managed SFTP endpoints, allowing employees to securely transfer files using familiar SFTP clients (like WinSCP or Filezilla). This avoids complex AWS-specific setups.
- Minimal Operational Overhead: By using AWS Secrets Manager to manage credentials, the company avoids manually creating IAM  users  for   every  employee  (which  is  time-consuming   and  error-prone).  AWS  Transfer   Family   handles  authentication automatically. - Global Accessibility:  Employees worldwide can connect to the SFTP endpoint without needing VPNs or direct AWS Console access. They just need an SFTP client and their credentials. Why other options are less ideal: - A (Pre signed URLs): Pre signed URLs expire and require constant regeneration, creating friction for employees. This adds operational work. - B (IAM Users): Creating  IAM  users  for  every  employee  is  impractical for a global team.  Managing  policies  and  credentials  manually would be a huge operational burden. - C (S3 File Gateway): While File Gateway allows mounting S3 as a network drive, it requires setting up and maintaining a gateway (e.g., deploying a VM), which increases operational complexity compared to a serverless solution  like  Transfer  Family.  Reference  Links:  -  [AWS  Transfer  Family](https://aws.amazon.com/aws-transfer-family/)  -  [AWS Secrets Manager](https://aws.amazon.com/secrets-manager/)

559.A company is building a new furniture inventory application. The company has deployed the application on a fleet of amazon EC2 instances across multiple Availability Zones. The EC2 instances run behind an Application Load Balancer (ALB) in their VPC. A solutions architect has observed that incoming traffic seems to favor one EC2 instance, resulting in latency for some requests. What should the solutions architect do to resolve this issue?

A ��Disable session affinity (sticky sessions) on the ALB

B ��Replace the ALB with a Network Load Balancer

C ��Increase the number of EC2 instances in each Availability Zone

D ��Adjust the frequency of the health checks on the ALB's target group

�𰸣�A

������ Correct Answer: A. Disable session affinity (sticky sessions) on the ALB Detailed Explanation: The issue occurs because the Application Load Balancer (ALB) is using session affinity (sticky sessions), which directs requests from the same client to the same EC2 instance. While this is useful for maintaining session state, it can cause uneven traffic distribution if one client sends many requests, overloading a single instance and causing latency. By disabling sticky sessions, the ALB will distribute requests evenly across all healthy EC2 instances using its default round-robin algorithm. This ensures better load balancing and reduces latency caused by a single instance handling too many requests. Why other options are incorrect: - B: Replacing the ALB with a Network Load  Balancer (NLB) isn ��t  necessary. ALB  is designed for application-layer (HTTP/HTTPS) traffic, while  NLB is for lower-level (TCP/UDP) traffic. - C: Adding more instances might help temporarily, but the root cause is the uneven distribution due to sticky sessions. - D: Adjusting health check frequency affects how the ALB detects unhealthy instances but doesn ��t fix uneven traffic distribution.                       Reference                       Link:                        [Elastic                       Load                       Balancing                       Sticky
Sessions](https://docs.aws.amazon.com/elastic load balancing/latest/application/sticky-sessions.html)

560.A company wants to monitor its AWS costs for financial review. The cloud operations team is designing an architecture in the AWS Organizations management account to query AWS Cost and Usage Reports for all member accounts. The team must run this query once a month and provide a detailed analysis of the bill. Which solution is the MOST scalable and cost-effective way to meet these requirements?

A��Enable Cost and Usage Reports in the management account. Deliver reports to Amazon Kinesis. Use Amazon EMR for analysis.

B �� Enable  Cost and  Usage  Reports  in  the  management account.  Deliver the  reports to Amazon S3  Use  Amazon Athena for analysis.

C ��Enable Cost and Usage Reports for member accounts. Deliver the reports to Amazon S3 Use Amazon Redshift for analysis.

D �� Enable Cost and Usage Reports for member accounts.  Deliver the reports to Amazon Kinesis.  Use Amazon Quick sight tor analysis.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The most scalable and cost-effective solution is Option B because: 1. AWS Cost and Usage Reports (CUR) can be enabled in the management account to consolidate billing data for all member accounts in AWS Organizations. This avoids enabling CUR individually in each member account (simpler setup). 2. Storing CUR data in Amazon S3 is  highly scalable and cost-efficient for  large datasets, as S3 charges are  based on storage volume. 3. Amazon Athena allows

serverless SQL queries directly on S3 data using standard SQL. Since the team only needs to run queries once a month, Athena's pay-per-query pricing model is far more cost-effective than provisioning services like Amazon Redshift or EMR (which require ongoing infrastructure costs). Why other options are less ideal: - A/D (Kinesis): Kinesis is designed for real-time streaming, which is  unnecessary  for  monthly  batch  analysis.  -  C  (Redshift):  Redshift  requires  cluster  provisioning  and  management,  making  it overkill for monthly queries. - D (per-account CUR): Enabling CUR in every member account creates management overhead and complicates             data             aggregation.              Reference             Links:              -             [AWS              Cost             and              Usage Reports](https://docs.aws.amazon.com/cur/latest/user guide/what-is-cur.html)   -    [Analyze   Cost   and    Usage    Reports    using Athena](https://aws.amazon.com/blogs/aws-cost-management/analyze-your-aws-cost-and-usage-reports-using-amazon-athena /)

561.A company wants to run a gaming application on Amazon EC2 instances that are part of an Auto Scaling group in the AWS Cloud. The application will transmit data by using UDP packets. The company wants to ensure that the application can scale out and in as traffic increases and decreases. What should a solutions architect do to meet these requirements?

A ��Attach a Network Load Balancer to the Auto Scaling group.

B ��Attach an Application Load Balancer to the Auto Scaling group.

C ��Deploy an Amazon Route 53 record set with a weighted policy to route traffic appropriately.

D ��Deploy a NAT instance that is configured with port forwarding to the EC2 instances in the Auto Scaling group.

�𰸣�A

������ Correct Answer: A Detailed Explanation: For a gaming application using UDP (which is common for real-time games due to lower latency), a Network Load Balancer (NLB) is the best choice. Here's why: 1. Protocol Support: NLB operates at Layer 4 (TCP/UDP) and supports UDP traffic, while Application Load Balancer (ALB, Option B) only works with HTTP/HTTPS (Layer 7). 2. Auto Scaling Integration:  NLB automatically distributes traffic across  EC2 instances in an Auto Scaling group. When the group scales out/in, NLB detects new/terminated instances without manual intervention. 3. High Performance: NLB handles millions of requests/sec with ultra-low latency, critical for gaming applications. 4. Why not other options: - Option C (Route 53): Weighted routing is for  DNS-level traffic distribution (e.g., A/B testing), not real-time  load balancing with auto scaling. - Option  D (NAT instance):  NAT  is  for  outbound  internet  access,  not  inbound  traffic   distribution.  Port  forwarding  would   be   manual   and error-prone.                               Reference                               Link:                               [AWS                               Load                               Balancer
Comparison](https://aws.amazon.com/elastic load balancing/features/#Comparing_Elastic_Load_Balancing_types)

562.A company runs several websites on AWS for its different brands. Each website generates tens of gigabytes of web traffic logs each day. A solutions architect needs to design a scalable solution to give the company's developers the ability to analyze traffic patterns across all the company's websites. This analysis by the developers will occur on demand once a week over the course of several months. The solution must support queries with standard SQL. Which solution will meet these requirements MOST cost-effectively?

A ��Store the logs in Amazon S3. Use Amazon Athena tor analysis.

B ��Store the logs in Amazon RDS. Use a database client for analysis.

C ��Store the logs in Amazon Open search Service. Use Open search Service for analysis.

D ��Store the logs in an Amazon EMR cluster Use a supported open-source framework for SQL-based analysis.

�𰸣�A

������ Correct Answer: A Detailed Explanation: Storing logs in Amazon S3 and using Amazon Athena for analysis is the most cost - effective solution here. Here's why: 1. Cost: S3 offers extremely low storage costs (especially for infrequently accessed data), and Athena charges only for the amount of data scanned per query. Since analysis happens weekly, you avoid the ongoing costs of maintaining databases/clusters 24/7. 2. Scalability: S3 automatically scales to handle any volume of logs. Athena can query petabytes of data without  infrastructure  management. 3. SQL Support: Athena  uses  standard SQL,  meeting the  requirement directly. 4.  On -  Demand  Usage:  Unlike  options  requiring  always  -  on  resources  (RDS,  Open search,  EMR),  Athena/S3  has  no running costs when idle, perfect for weekly analysis. Other options are less optimal: - B (RDS): Expensive for large log storage and high  write  volumes.   -   C  (Open search):  Not  SQL  -  native  and  requires  cluster  management.  -  D  (EMR):  Involves  cluster management         overhead          and           higher          operational          complexity.          Reference          Links:          -          [Amazon Athena](https://aws.amazon.com/athena/) - [Amazon S3 Storage Classes](https://aws.amazon.com/s3/storage - classes/)

563.A company is required to use cryptographic keys in its on-premises key manager. The key manager is outside of the AWS Cloud because of regulatory and compliance requirements. The company wants to manage encryption and decryption by using cryptographic keys that are retained outside of the AWS Cloud and that support a variety of external key managers from different vendors. Which solution will meet these requirements with the LEAST operational overhead?

A ��Use AWS Cloud hsm key store backed by a Cloud hsm cluster.

B ��Use an AWS Key Management Service (AWS KMS) external key store backed by an external key manager. C ��Use the default AWS Key Management Service (AWS KMS) managed key store.

D ��Use a custom key store backed by an AWS Cloud hsm cluster.

�𰸣�B

������ Correct Answer B Detailed Explanation The correct solution is B (Use AWS KMS external key store backed by an external key manager). Here's why: - AWS KMS External Key Store (XKS) allows you to use cryptographic keys stored in your on-premises key manager (outside AWS) while integrating with AWS services. This means encryption/decryption operations in AWS use keys retained  externally,  meeting  compliance  requirements.  -  It  supports  multiple  external  key  managers  from  different  vendors, eliminating vendor  lock-in. - AWS  KMS  handles the  integration and  cryptographic operations,  reducing operational overhead compared to managing a custom Cloud hsm cluster (Options A/D). - Options A/D (Cloud hsm-based) require keys to be stored in AWS Cloud hsm clusters, which violates the requirement to keep keys fully outside AWS. - Option C (default KMS) stores keys in AWS,     which      is      non-compliant      for      this      scenario.      Reference      Link      [External      key      store      (XKS)      in     AWS KMS](https://docs.aws.amazon.com/kms/latest/developer guide/keystore-external.html)

564.A solutions architect needs to host a high performance computing (HPC) workload in the AWS Cloud. The workload will run on hundreds of Amazon EC2 instances and will require parallel access to a shared file system to enable distributed processing of large datasets. Datasets will be accessed across multiple instances simultaneously. The workload requires access latency within 1 ms. After processing has completed, engineers will need access to the dataset for manual postprocessing. Which solution will meet these requirements?

A ��Use Amazon Elastic File System (Amazon EFS) as a shared file system. Access the dataset from Amazon EFS.

B ��Mount an Amazon S3 bucket to serve as the shared file system. Perform postprocessing directly from the S3 bucket. C ��Use Amazon FSx for Lustre as a shared file system. Link the file system to an Amazon S3 bucket for postprocessing.

D �� Configure AWS  Resource  Access  Manager to  share an Amazon S3  bucket so that  it  can  be  mounted to all  instances for processing and postprocessing.

�𰸣�C

������ Correct Answer: C Detailed Explanation: For High Performance Computing (HPC) workloads requiring low-latency access (within  1  ms) and parallel access to shared files across  hundreds of  EC2 instances, Amazon  FSx for  Lustre is the  best choice. Here's  why:  1.  Ultra-low  latency:  FSx  for  Lustre  is  purpose-built  for  HPC,  offering  sub-millisecond  latency  (meets  the  1  ms requirement). 2. Parallel file access: Lustre is designed for simultaneous read/write operations across thousands of instances. 3. S3 integration: FSx for Lustre can directly link to S3 buckets: - Process data from S3   �� Lustre for high-speed computation - Write results  back to S3 for cost-effective storage and  manual  post-processing Other options fail  because: - A  EFS: While good for shared  storage,  it  has  higher  latency  (milliseconds  range)  and  lower  throughput  than  Lustre.  -  B/D  S3:  Object  storage  isn't mountable as a filesystem natively, has higher latency (100+ ms), and isn't suitable for parallel processing workloads. Reference Links:           -            [Amazon            FSx           for            Lustre](https://aws.amazon.com/fsx/lustre/)            -            [HPC           Storage Options](https://aws.amazon.com/hpc/storage/)

565.A gaming company is building an application with Voice over IP capabilities. The application will serve traffic to users across the world. The application needs to be highly available with an automated failover across AWS Regions. The company wants to minimize the latency of users without relying on  IP address caching on user devices. What should a solutions architect do to meet these requirements?

A ��Use AWS Global Accelerator with health checks.

B ��Use Amazon Route 53 with a geolocation routing policy.

C ��Create an Amazon Cloud front distribution that includes multiple origins.

D ��Create an Application Load Balancer that uses path-based routing.

�𰸣�A

������ Correct Answer: A. Use AWS Global Accelerator with health checks. Detailed Explanation: The gaming company needs a solution that provides global low latency, high availability, and automated cross-region failover without relying on client-side IP caching. Here's why AWS Global Accelerator is the best fit: 1. Static Anycast IPs: Global Accelerator assigns two static IP addresses that act as fixed entry points. Users automatically connect to the nearest AWS edge location, reducing latency without requiring IP caching on devices. 2. AWS Backbone Network: Traffic travels over AWS's private global network (faster and more reliable than the  public internet), improving  performance for  real-time VoIP traffic. 3. Automatic  Failover:  Health  checks  monitor  backend resources (e.g., EC2 instances, ALBs). If a regional failure occurs, traffic reroutes to the next healthy region in seconds, meeting the automated failover requirement. 4. Global vs. Regional: Options like ALB (D) or Cloud front (C) focus on single-region or static content. Route 53 (B) uses DNS-based routing, which depends on DNS caching (violates the no IP caching requirement) and has slower  failover  (DNS  TTL  delays).   Reference   Links:   [AWS  Global  Accelerator](https://aws.amazon.com/global-accelerator/) [Global Accelerator vs. Cloud front/Route 53](https://docs.aws.amazon.com/global-accelerator/latest/dg/about-accelerate.html)

566.A weather forecasting company needs to process hundreds of gigabytes of data with sub-millisecond latency. The company has a  high  performance  computing  (HPC)  environment  in  its  data  center  and wants to expand  its forecasting capabilities. A solutions architect must identify a highly available cloud storage solution that can handle large amounts of sustained throughput. Files that are stored in the solution should be accessible to thousands of compute instances that will simultaneously access and process the entire dataset. What should the solutions architect do to meet these requirements?

A ��Use Amazon FSx for Lustre scratch file systems.

B ��Use Amazon FSx for Lustre persistent file systems.

C ��Use Amazon Elastic File System (Amazon EFS) with Bursting Throughput mode.

D ��Use Amazon Elastic File System (Amazon EFS) with Provisioned Throughput mode.

�𰸣�B

���� �� Correct Answer: B. Use Amazon FSx for Lustre persistent file systems. Detailed Explanation: The weather forecasting company requires a storage solution optimized for high-performance computing (HPC) workloads. Here's why Amazon FSx for Lustre (persistent file systems) is the best choice: 1. Sub-millisecond latency   �� high throughput: Lustre is specifically designed for   HPC   workloads    requiring   low-latency    access   and    massive   parallel    processing.   FSx   for    Lustre   delivers    consistent sub-millisecond latency and can scale to hundreds of gigabytes per second in throughput, ideal for processing large datasets. 2. Thousands of concurrent compute instances: Lustre's architecture allows thousands of compute instances to access the same dataset  simultaneously  without  performance  degradation.  This  is  critical  for  parallel  processing  in  weather  simulations.  3. Persistent vs. Scratch  File Systems: -  Persistent file systems  (Answer  B):  Provide  long-term  storage,  high availability  (HA), and automatic replication across Availability Zones (AZs). This matches the requirement for a highly available solution. - Scratch file systems (Answer A): Are temporary, non-HA, and designed for short-lived data. They don ��t meet the HA requirement. 4. Why EFS is not suitable (Answers C/D): Amazon EFS uses the NFS protocol and is optimized for general-purpose file sharing. Even with Provisioned  Throughput,  EFS  has  higher  latency  (milliseconds)  compared  to   Lustre  (sub-milliseconds).  EFS  also   has  lower throughput limits for highly parallel workloads. Reference Links: - [FSx for Lustre vs. EFS](https://aws.amazon.com/efs/features/)
- [FSx for Lustre Persistent File Systems](https://aws.amazon.com/fsx/lustre/features/)

567.An ecommerce company runs a  Postgresql database on premises. The database stores data by using high  IOPS Amazon Elastic  Block Store (Amazon  EBS)  block storage. The daily  peak  I/O  transactions  per second do  not  exceed  15,000  IOPS.  The company wants to migrate the database to Amazon RDS for Postgresql and provision disk IOPS performance independent of disk storage capacity. Which solution will meet these requirements MOST cost-effectively?

A ��Configure the General Purpose SSD (gp2) EBS volume storage type and provision 15,000 IOPS.

B ��Configure the Provisioned IOPS SSD (io1) EBS volume storage type and provision 15,000 IOPS.

C ��Configure the General Purpose SSD (gp3) EBS volume storage type and provision 15,000 IOPS.

D ��Configure the EBS magnetic volume type to achieve maximum IOPS.

�𰸣�C

������ Correct Answer C Detailed Explanation The company needs to migrate their Postgresql database to Amazon RDS while ensuring disk IOPS performance is independent of storage capacity, with a peak requirement of 15,000 IOPS. Here ��s why General Purpose SSD (gp3) is the best choice: 1. IOPS Independence: - gp3 allows you to provision IOPS separately from storage size. You can set 15,000 IOPS directly without needing extra storage (unlike gp2, which ties IOPS to storage size). - For example, with gp2, achieving 15,000 IOPS would require ~5,000 GB of storage (since gp2 provides 3 IOPS per GB). This wastes money if you don ��t need that much storage. 2. Cost Efficiency: - gp3 is cheaper than io1 (Provisioned IOPS SSD) for the same IOPS. io1 charges more per provisioned IOPS, making gp3 more cost - effective for 15,000 IOPS. 3. Performance: - gp3 supports up to 16,000 IOPS (easily covering 15,000) and allows additional throughput tuning, which magnetic volumes (obsolete) or gp2 cannot match. Options like

io1 (B) work but are overpriced, gp2 (A) forces oversized storage, and magnetic (D) lacks the required performance. Thus, gp3 (C) is the optimal solution. Reference Link [Amazon EBS Volume Types](https://aws.amazon.com/ebs/volume - types/)

568.A  company wants to  migrate  its  on-premises  Microsoft  SQL  Server  Enterprise  edition  database  to  AWS. The  company's online application uses the database to process transactions. The data analysis team uses the same production database to run reports for analytical processing. The company wants to reduce operational overhead by moving to managed services wherever possible. Which solution will meet these requirements with the LEAST operational overhead?

A ��Migrate to Amazon RDS for Microsoft SOL Server. Use read replicas for reporting purposes

B ��Migrate to Microsoft SQL Server on Amazon EC2. Use Always On read replicas for reporting purposes C ��Migrate to Amazon Dynamo db. Use Dynamo db on-demand replicas for reporting purposes

D ��Migrate to Amazon Aurora MySQL. Use Aurora read replicas for reporting purposes

�𰸣�A

������ Correct Answer A �� Migrate to Amazon RDS for Microsoft SQL Server. Use read replicas for reporting purposes Detailed Explanation The company wants to migrate its SQL Server database to AWS while minimizing operational overhead. Here's why Option A is the best choice: 1. Managed Service: Amazon RDS for SQL Server is a fully managed service. AWS handles backups, patching,  and  maintenance,  which   reduces  operational  tasks  compared  to  self-managed  solutions   like  EC2  (Option   B).  2. Compatibility:  Since  the  company  already  uses  SQL  Server  Enterprise  Edition,  migrating  to  RDS  for  SQL  Server  avoids  the complexity of switching database engines (e.g., Dynamo db in Option C or Aurora MySQL in Option D). 3. Read Replicas: RDS read replicas allow the data analysis team to run  reports on a separate,  read-only copy of the database. This separates analytical workloads  from  transactional  processing,  improving  performance  without  impacting  the  production  database.  Why  Other Options Are Less Ideal - Option B (EC2 + Always On): Requires manual management of EC2 instances and SQL Server, increasing operational overhead. - Option C (Dynamo db):  Migrating to a NoSQL database would require significant application redesign, which is time-consuming and risky. - Option D (Aurora MySQL): Switching from SQL Server to MySQL would require rewriting queries      and      adjusting      schemas,      adding      unnecessary      complexity.       Reference      Link      [AWS       RDS      for      SQL Server](https://aws.amazon.com/rds/sql server/)

569.A company stores a large volume of image files in an Amazon S3 bucket. The images need to be readily available for the first 180 days. The images are infrequently accessed for the next 180 days. After 360 days, the images need to be archived but must be available instantly upon request. After 5 years, only auditors can access the images. The auditors must be able to retrieve the images within 12 hours. The images cannot be lost during this process. A developer will use S3 Standard storage for the first 180 days. The developer needs to configure an S3 Lifecycle rule. Which solution will meet these requirements MOST cost-effectively?

A��Transition the objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 180 days. S3 Glacier Instant Retrieval after 360 days, and S3 Glacier Deep Archive after 5 years.

B��Transition the objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 180 days. S3 Glacier Flexible Retrieval after 360 days, and S3 Glacier Deep Archive after 5 years.

C ��Transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 180 days, S3 Glacier Instant Retrieval after 360 days, and S3 Glacier Deep Archive after 5 years.

D ��Transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 180 days, S3 Glacier Flexible Retrieval after 360 days, and S3 Glacier Deep Archive after 5 years.

�𰸣�C

������ Correct Answer: C Detailed Explanation: - First 180 days: Use S3 Standard (already configured) for frequent access. - Days 180 �C 360: Switch to S3 Standard-Infrequent Access (S3 Standard-IA). This is cheaper than S3 Standard but maintains multi-AZ redundancy (unlike S3 One Zone-IA, which is single-AZ and risks data loss if the AZ fails). The requirement states images cannot be lost, so S3 Standard-IA is safer than One Zone-IA (eliminates Options A/B). - Days 360 �C 5 years: Move to S3 Glacier Instant Retrieval to  archive data while  keeping  instant  access  (retrieval  in  milliseconds).  This  matches  the  requirement  for  available instantly upon request. Options B/D use S3 Glacier Flexible Retrieval, which has slower retrieval times (minutes to hours) and doesn ��t meet the instant access requirement. - After 5 years: Transition to S3 Glacier Deep Archive, the cheapest option for long-term archival. Auditors can retrieve data within 12 hours (matches the 12-hour retrieval requirement). Key reasons Option C is correct: 1. Avoids risky single-AZ storage (S3 One Zone-IA) to prevent data loss. 2. Uses Glacier Instant Retrieval (not Flexible) for instant access after 360 days. 3. Meets all cost, durability, and retrieval-time requirements. Reference Links: - [Amazon S3 Storage                         Classes](https://aws.amazon.com/s3/storage-classes/)                         -                         [S3                          Lifecycle
Management](https://docs.aws.amazon.com/AmazonS3/latest/user guide/object-lifecycle-mgmt.html)

570.A company has a large data workload that runs for 6 hours each day. The company cannot lose any data while the process is running. A solutions architect is designing an Amazon EMR cluster configuration to support this critical data workload. Which solution will meet these requirements MOST cost-effectively?

A ��Configure a long-running cluster that runs the primary node and core nodes on On-Demand Instances and the task nodes on Spot Instances.

B ��Configure a transient cluster that runs the primary node and core nodes on On-Demand Instances and the task nodes on Spot Instances.

C ��Configure a transient cluster that runs the primary node on an On-Demand Instance and the core nodes and task nodes on Spot Instances.

D ��Configure a long-running cluster that runs the primary node on an On-Demand Instance, the core nodes on Spot Instances, and the task nodes on Spot Instances.

�𰸣�B

������Correct Answer: B Detailed Explanation: The best choice is B because it uses a transient (short-lived) cluster, which is ideal for a daily 6-hour workload. Transient clusters automatically terminate after the job finishes, reducing costs. Here ��s why the other options are less optimal: - Primary Node: Must use On-Demand Instances (not Spot) because if it fails, the entire cluster fails. This is critical for avoiding data loss. - Core Nodes: Must also use On-Demand Instances because they store data (HDFS). If Spot Instances are used here and interrupted, data could be lost. - Task Nodes: Can safely use Spot Instances because they don ��t store  data.  If  interrupted,  the  job  resumes  using  other  nodes.  Options  A  and  D  suggest  long-running  clusters,  which  are unnecessarily expensive for a 6-hour daily job. Option C risks data loss by using Spot  Instances for core nodes. Only B balances cost  (using  Spot  for  task  nodes)  and  reliability   (On-Demand  for  primary/core  nodes).   Reference  Link:  [Amazon  EMR  Best Practices](https://docs.aws.amazon.com/emr/latest/Management guide/emr-plan-instances.html)

571.A  company   maintains  an  Amazon   RDS  database  that   maps   users  to  cost  centers.  The   company   has  accounts   in  an organization in AWS Organizations. The company needs a solution that will tag all resources that are created in a specific AWS account in the organization. The solution must tag each resource with the cost center ID of the user who created the resource. Which solution will meet these requirements?

A ��Move the specific AWS account to a new organizational unit (OU) in Organizations from the management account. Create a service  control  policy  (SCP)  that  requires  all  existing  resources  to  have the  correct  cost center tag  before the  resources  are created. Apply the SCP to the new OU.

B��Create an AWS Lambda function to tag the resources after the Lambda function looks up the appropriate cost center from the RDS database. Configure an Amazon Event bridge rule that reacts to AWS Cloud trail events to invoke the Lambda function.

C �� Create an AWS Cloud formation stack to deploy an AWS Lambda function. Configure the Lambda function to look up the appropriate cost center from t he RDS database and to tag resources. Create an Amazon Event bridge scheduled rule to invoke the Cloud formation stack.

D ��Create an AWS Lambda function to tag the resources with a default value. Configure an Amazon Event bridge rule that reacts to AWS Cloud trail events to invoke the Lambda function when a resource is missing the cost center tag.

�𰸣�B

�� �� ��  Correct  Answer:  B  Detailed  Explanation:  The  correct  solution  is  B  because  it  uses  an  event-driven  approach  to
automatically tag resources as soon as they are created. Here's a breakdown of why this works and why the other options don ��t:
1.  Option  B  Explained:  -  Amazon  Event bridge  Rule:  Monitors  AWS  Cloud trail  events  (e.g.,  EC2  instance  creation,  S3  bucket    creation) in real-time. When a resource is created,  Event bridge detects this event. - AWS  Lambda  Function: Triggered by the    Event bridge rule. The Lambda function does two things: - Looks up the cost center ID: It queries t he RDS database using the user �� s information (e.g., username from the Cloud trail event) to find their associated cost center. - Tags the resource: Applies the cost    center tag to the newly created resource. - This approach ensures tags are applied immediately and dynamically based on the    user who created the resource. 2. Why Other Options Fail: - Option A: Service Control Policies (SCPs) enforce permissions but    cannot automatically add tags. SCPs only block non-compliant requests (e.g., blocking resource creation if a tag is missing).  It    doesn ��t tag resources or query t he  RDS database. - Option C: A scheduled Event bridge rule (e.g., hourly/daily) would delay    tagging, leaving resources untagged until the next scheduled run. This doesn ��t meet the real-time tagging requirement. - Option
D: Tagging resources with a default value ignores t he RDS database ��s user-specific cost center data, so it doesn ��t fulfill the requirement. Key Concepts: - Event-Driven Tagging: Use Cloud trail + Event bridge to detect resource creation events instantly. - Lambda Flexibility: Lambda can integrate with databases (like RDS) to fetch dynamic tag values. - Real-Time vs. Scheduled: The problem    requires     immediate    tagging,    which     rules    out     scheduled    solutions.     Reference     Links:    -     [AWS    Tagging Strategies](https://aws.amazon.com/answers/account-management/aws-tagging-strategies/)     -      [Using      Event bridge      with Lambda](https://docs.aws.amazon.com/event bridge/latest/user guide/eb-run-lambda-schedule.html)      -       [Cloud trail       Event Tracking](https://docs.aws.amazon.com/aws cloud trail/latest/user guide/cloud trail-logging.html)

572.A company  recently  migrated  its web application to the AWS Cloud. The company uses an Amazon  EC2  instance to  run multiple processes to host the application. The processes include an Apache web server that serves static content. The Apache web server makes requests to a PHP application that uses a local Redis server for user sessions. The company wants to redesign the architecture to be highly available and to use AWS managed solutions. Which solution will meet these requirements?

A ��Use AWS Elastic Beanstalk to host the static content and the PHP application. Configure Elastic Beanstalk to deploy its EC2 instance into a public subnet. Assign a public IP address.

B ��Use AWS Lambda to host the static content and the PHP application. Use an Amazon API Gateway REST API to proxy requests to the Lambda function. Set the API Gateway CORS configuration to respond to the domain name. Configure Amazon Elastic ache for Redis to handle session information.

C �� Keep the  backend code on the  EC2  instance. Create an Amazon  Elastic ache for  Redis cluster that  has  Multi-AZ  enabled. Configure the Elastic ache for Redis cluster in cluster mode. Copy the front end resources to Amazon S3. Configure the backend

code to reference the EC2 instance.

D��Configure an Amazon Cloud front distribution with an Amazon S3 endpoint to an S3 bucket that is configured to host the static content. Configure an Application Load Balancer that targets an Amazon Elastic Container Service (Amazon ECS) service that runs AWS Fargate tasks for the PHP application. Configure the PHP application to use an Amazon Elastic ache for Redis cluster that runs in multiple Availability Zones.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The company needs a highly available architecture using AWS managed services. Let's break down why Option D is the best choice: 1. Static Content (High Availability): - Amazon S3 + Cloud front is the standard way to host static content (HTML, images, etc.) with high availability, global caching, and low latency. S3 is inherently durable and available across AZs, while Cloud front acts as a global CDN. 2. PHP Application (High Availability): - ECS Fargate + Application Load Balancer (ALB): - ECS Fargate is a managed container service. You can run multiple containers across Availability Zones (AZs) for redundancy. - ALB automatically distributes traffic across healthy containers in different AZs. If one AZ fails, the ALB routes traffic to the remaining AZs. 3. Session Management (High Availability): - Amazon Elastic ache for Redis (Multi - AZ): - Elastic ache is a fully managed Redis service. Enabling Multi - AZ ensures automatic failover if the primary node fails, maintaining session data availability. Why Other Options Fail: - A: Elastic Beanstalk alone doesn ��t guarantee Multi - AZ for EC2 (you need to configure that separately). Also, using a single EC2 instance (even with a public IP) is not highly available. - B: Lambda is serverless but not ideal for long - running processes like PHP applications. API Gateway + Lambda is better for APIs, not traditional web apps. - C: Keeping the backend on a single EC2 instance creates a single point of failure, even if Elastic ache is Multi - AZ. Reference Links: - [Amazon S3 Static Website Hosting](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Website hosting.html) -  [ECS  Fargate and ALB](https://docs.aws.amazon.com/Amazon ecs/latest/developer guide/load     -      balancer.html)      -      [Elastic ache      Multi      -
AZ](https://docs.aws.amazon.com/Amazon elastic ache/latest/red - ug/Auto failover.html)

573.A company runs a website that stores images of historical events. Website users need the ability to search and view images based on the year that the event in the image occurred. On average, users request each image only once or twice a year. The company wants a highly available solution to store and deliver the images to users. Which solution will meet these requirements MOST cost-effectively?

A ��Store images in Amazon Elastic Block Store (Amazon EBS). Use a web server that runs on Amazon EC2.

B ��Store images in Amazon Elastic File System (Amazon EFS). Use a web server that runs on Amazon EC2.

C ��Store images in Amazon S3 Standard. Use S3 Standard to directly deliver images by using a static website.

D ��Store images in Amazon S3 Standard-Infrequent Access (S3 Standard-IA). Use S3 Standard-IA to directly deliver images by using a static website.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The best solution here is to store the images in Amazon S3 Standard-Infrequent Access (S3 Standard-IA) and serve them directly via a static website hosted on S3. Here's why: 1. Cost Efficiency: - S3 Standard-IA is cheaper than S3 Standard for storage costs, which matters because the images are rarely accessed (1-2 times/year). - Static website hosting on S3 eliminates the need for  EC2 instances (options A/B), saving compute costs. 2.  High Availability: - S3 is designed for 99.99% availability by default, meeting the requirement without extra setup. 3. Direct Delivery: - S3 static websites can  serve  content  directly  to  users  without  a  web  server,  reducing  complexity  and  cost  compared  to  EC2-based  solutions. Options A/B  use  EC2 +  EBS/EFS, which are  more expensive for  low-request scenarios. Option C  uses S3 Standard, which  has higher storage costs for  infrequently accessed  data. S3 Standard-IA  balances  cost and availability  perfectly for this  use  case.

Reference    Links:    -     [Amazon    S3    Storage    Classes](https://aws.amazon.com/s3/storage-classes/)    -     [S3    Static    Website Hosting](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Website hosting.html)

574.A  company  has  multiple  AWS  accounts  in  an  organization  in  AWS  Organizations  that  different  business  units  use.  The company has multiple offices around the world. The company needs to update security group rules to allow new office CIDR ranges or to  remove old CIDR  ranges across the organization. The company wants to centralize the  management of security group  rules  to  minimize  the  administrative  overhead  that  updating  CIDR  ranges  requires.  Which  solution  will  meet  these requirements MOST cost-effectively?

A ��Create VPC security groups in the organization's management account. Update the security groups when a CIDR range update is necessary.

B ��Create a VPC customer managed prefix list that contains the list of CIDRs. Use AWS Resource Access Manager (AWS RAM) to share the prefix list across the organization. Use the prefix list in the security groups across the organization.

C �� Create  an  AWS  managed  prefix  list.  Use  an  AWS  Security  Hub  policy  to  enforce  the  security  group  update  across  the organization. Use an AWS Lambda function to update the prefix list automatically when the CIDR ranges change.

D ��Create security groups in a central administrative AWS account. Create an AWS Firewall Manager common security group policy for the whole organization. Select the previously created security groups as primary groups in the policy.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution is to use customer-managed prefix lists shared via AWS RAM because:  1. Centralized  Management: A  prefix list acts as a reusable container for CIDR  ranges.  Updating the  prefix  list once automatically applies changes to all security groups referencing it across the entire organization. 2. Cross-Account Sharing: AWS Resource Access  Manager  (RAM)  allows  sharing  prefix  lists  across  AWS  accounts  in  the  same  organization  without  needing complex cross-account permissions. 3. Cost-Effective: This approach avoids recurring costs (like Lambda execution) and doesn't require additional services like Firewall Manager (which has additional setup steps and costs). Why other options are less ideal: -
A) Security groups can't be shared across accounts. Managing them in the management account would require manual updates in every account, increasing overhead. - C) AWS-managed prefix lists are not editable by users. Also, using Security Hub + Lambda adds complexity and ongoing costs. -  D)  Firewall  Manager requires enabling AWS Config in all accounts (additional cost) and designating primary security groups, which is less flexible than prefix lists. Key Concepts Simplified: - Prefix List: Like a phone contact list for CIDR ranges    �C update one entry, and everyone using the list gets the update automatically. - AWS RAM: A tool for securely sharing resources (like prefix lists)  between AWS accounts in your organization. Reference  Link:  [AWS Prefix  Lists Documentation](https://docs.aws.amazon.com/vpc/latest/user guide/managed-prefix-lists.html)                    [AWS                     RAM Documentation](https://docs.aws.amazon.com/ram/latest/user guide/what-is.html)

575.A company is relocating its data center and wants to securely transfer 50 TB of data to AWS within 2 weeks. The existing data center has a Site-to-Site VPN connection to AWS that is 90% utilized. Which AWS service should a solutions architect use to meet these requirements?

A ��AWS Data sync with a VPC endpoint

B ��AWS Direct Connect

C ��AWS Snowball Edge Storage Optimized

D ��AWS Storage Gateway

�𰸣�C

������ Correct Answer: C. AWS Snowball Edge Storage Optimized Detailed Explanation: The company needs to transfer 50 TB of data in 2 weeks securely. While the existing Site-to-Site VPN is an option, it ��s already 90% utilized, meaning there ��s insufficient bandwidth for such a large transfer. Transferring 50 TB over a congested VPN would take too long and risk delays or failures. Why Snowball Edge is the best choice: - Physical transfer: AWS Snowball Edge is designed for large-scale data transfers (like 50 TB) by shipping  a  secure   physical  device.  This  avoids  network  bottlenecks  entirely.  -  Speed:   Even  with  a  high-speed  connection, transferring 50 TB over the internet would require ~300 Mbps sustained bandwidth (unlikely with a 90% utilized VPN). Snowball Edge skips this dependency. - Security: Data is encrypted end-to-end, and the device is tamper-resistant. - Time: AWS handles logistics,  and the 2-week time frame aligns with Snowball ��s shipping and  processing workflow. Why other options are  less suitable: - A (Data sync + VPC endpoint): Data sync uses the existing network, which is already overloaded. - B (Direct Connect): While  it  provides dedicated  bandwidth,  provisioning  a  new  Direct  Connect  connection takes weeks  (longer than the  2-week requirement). - D (Storage Gateway): This is for hybrid cloud storage integration, not bulk data migration. Reference Links: - [AWS Snowball                   Edge](https://aws.amazon.com/snowball/edge/)                    -                   [AWS                    Data                   Transfer
Options](https://aws.amazon.com/answers/networking/aws-data-transfer/)

576.A company hosts an application on Amazon  EC2 On-Demand  Instances in an Auto Scaling group. Application peak  hours occur  at  the  same  time  each  day.  Application  users  report  slow  application  performance  at  the  start  of  peak  hours.  The application  performs  normally  2-3  hours  after  peak  hours  begin.  The  company  wants  to  ensure  that  the  application  works properly at the start of peak hours. Which solution will meet these requirements?

A ��Configure an Application Load Balancer to distribute traffic properly to the instances.

B ��Configure a dynamic scaling policy for the Auto Scaling group to launch new instances based on memory utilization. C ��Configure a dynamic scaling policy for the Auto Scaling group to launch new instances based on CPU utilization.

D ��Configure a scheduled scaling policy for the Auto Scaling group to launch new instances before peak hours.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The issue occurs because the Auto Scaling group doesn't have enough instances ready when peak hours suddenly begin. Dynamic scaling (options B/C) uses metrics like CPU/ memory, which take time to trigger and launch instances (5 - 15 minutes for metrics + 1 - 5 minutes instance startup). This delay causes slow performance initially. Scheduled scaling (option D) proactively adds instances before known daily peak times. This ensures capacity is already available when traffic spikes, preventing the initial slow down. It's like setting an alarm to add extra servers 30 minutes before rush hour every                         day.                          Reference                          link:                         [Auto                          Scaling                          Scheduled
Scaling](https://docs.aws.amazon.com/auto scaling/ec2/user guide/schedule_time.html)

577.A  company  runs  applications  on  AWS  that  connect  to the  company's Amazon  RDS  database.  The  applications  scale  on weekends and at peak times of the year. The company wants to scale the database  more effectively for its applications that connect to the database. Which solution will meet these requirements with the LEAST operational overhead?

A ��Use Amazon Dynamo db with connection pooling with a target group configuration for the database. Change the applications to use the Dynamo db endpoint.

B ��Use Amazon RDS Proxy with a target group for the database. Change the applications to use t he RDS Proxy endpoint.

C ��Use a custom proxy that runs on Amazon EC2 as an intermediary to the database. Change the applications to use the custom proxy endpoint.

D ��Use an AWS Lambda function to provide connection pooling with a target group configuration for the database. Change the applications to use the Lambda function.

�𰸣�B

������ Correct Answer B Detailed Explanation The best solution is B: Use Amazon RDS Proxy. Here ��s why: - Amazon RDS Proxy is a  fully   managed  service  designed  to  handle  connection  pooling,  which  efficiently  manages  database  connections  when applications scale up. It reduces the risk of overwhelming the database with too many connections during peak times. - Minimal Operational Overhead: Since RDS Proxy is AWS - managed, you don ��t need to configure servers (like EC2 in Option C) or write custom code (like Lambda in Option D). - Compatibility: It works natively with RDS databases, so you only need to update the application ��s connection endpoint. Switching to Dynamo db (Option A) would require rewriting parts of the application, adding unnecessary complexity. Why not other options? - A (Dynamo db): Forces a database migration (relational  �� NoSQL), which is time - consuming and risky. - C (EC2 Proxy): Requires manual setup, scaling, and maintenance of EC2 instances. - D (Lambda): Lambda  isn  �� t  designed  for   persistent  connections,   making  connection   pooling  unreliable.   Reference   Link  [Amazon  RDS Proxy](https://aws.amazon.com/rds/proxy/)

578.A company uses AWS Cost Explorer to monitor its AWS costs. The company notices that Amazon Elastic Block Store (Amazon EBS) storage and snapshot costs increase every month. However, the company does not purchase additional EBS storage every month. The company wants to optimize monthly costs for its current storage usage. Which solution will meet these requirements with the LEAST operational overhead?

A ��Use logs in Amazon Cloud watch Logs to monitor the storage utilization of Amazon EBS. Use Amazon EBS Elastic Volumes to reduce the size of t he EBS volumes.

B ��Use a custom script to monitor space usage. Use Amazon EBS Elastic Volumes to reduce the size of t he EBS volumes. C ��Delete all expired and unused snapshots to reduce snapshot costs.

D ��Delete all nonessential snapshots. Use Amazon Data Lifecycle Manager to create and manage the snapshots according to the company's snapshot policy requirements.

�𰸣�D

������ Correct Answer D Detailed Explanation The company ��s increasing EBS costs are likely due to accumulating unnecessary snapshots, not t he EBS storage itself (since no new storage is added). Here ��s why D is the best solution: 1. Snapshot Costs: - EBS snapshots are stored  incrementally,  but each snapshot  incurs costs  based  on the  size of changed  data. Over time,  retaining nonessential or outdated snapshots increases costs. 2. Amazon Data Lifecycle Manager (DLM): - Automated Management: DLM automatically creates, retains, and deletes snapshots based on predefined policies (e.g., keep only the last 7 daily snapshots). This eliminates manual cleanup and ensures compliance with retention policies. - Least Operational Overhead: Once configured, DLM handles snapshot lifecycle management without ongoing manual intervention, reducing long - term effort. 3. Immediate Cost Savings: - Deleting nonessential snapshots provides an immediate cost reduction. Combining this with DLM ensures future costs stay optimized. Why Other  Options Are  Less  Effective:  -  A   ��  B:  Resizing  EBS  volumes  (via  Elastic  Volumes)  requires monitoring and  manual adjustments.  However, the  problem  isn ��t about over provisioned volumes  (since  no  new storage is purchased) but snapshot accumulation. - C: Deleting expired/unused snapshots is a one - time fix. Without automation, snapshot costs      will       rise       again,       requiring       repeated       manual      effort.       Reference       Link:       [Amazon       Data       Lifecycle Manager](https://docs.aws.amazon.com/AWSEC2/latest/User guide/snapshot - lifecycle.html)

579.A  company  is  developing  a  new  application  on  AWS.  The  application  consists  of  an  Amazon  Elastic  Container  Service (Amazon ECS) cluster, an Amazon S3 bucket that contains assets for the application, and an Amazon RDS for MySQL database that contains the dataset for the application. The dataset contains sensitive information. The company wants to ensure that only the ECS cluster can access the data in t he RDS for MySQL database and the data in the S3 bucket. Which solution will meet these requirements?

A��Create a new AWS Key Management Service (AWS KMS) customer managed key to encrypt both the S3 bucket and t he RDS for MySQL database. Ensure that the KMS key policy includes encrypt and decrypt permissions for the ECS task execution role.

B ��Create an AWS Key Management Service (AWS KMS) AWS managed key to encrypt both the S3 bucket and t he RDS for MySQL database. Ensure that the S3 bucket policy specifies the ECS task execution role as a user.

C ��Create an S3 bucket policy that restricts bucket access to the ECS task execution role. Create a VPC endpoint for Amazon RDS for MySQL. Update t he RDS for MySQL security group to allow access from only the subnets that the ECS cluster will generate tasks in.

D ��Create a VPC endpoint for Amazon RDS for MySQL. Update t he RDS for MySQL security group to allow access from only the subnets that the ECS cluster will generate tasks in. Create a VPC endpoint for Amazon S3. Update the S3 bucket policy to allow access from only the S3 VPC endpoint.

�𰸣�A

������ Correct Answer D Detailed Explanation To ensure only the ECS cluster accesses the RDS database and S3 bucket: 1. RDS Access Control: - Create a VPC endpoint for Amazon RDS (though typically unnecessary since RDS is already within the VPC, this ensures private connectivity). - Update t he RDS security group to allow inbound traffic only from the subnets where ECS tasks run. This restricts access to ECS tasks in those subnets. 2. S3 Access Control: - Create a VPC endpoint for Amazon S3 (Gateway type). This keeps S3 traffic within the VPC, avoiding public internet exposure. - Update the S3 bucket policy to allow access only via the S3 VPC endpoint.  Use conditions  like `aws:source vpc e` to enforce this. Why not other options: - Option A/C:  Relying solely on KMS/IAM roles (A) or subnet-based rules (C) doesn ��t fully isolate network access. For example, other resources in the same subnet (C) or users with decrypt permissions (A) could still access data. - Option B: AWS-managed KMS keys don ��t provide granular access control, and S3 bucket policies alone can ��t enforce network-level restrictions. Key Concepts: - Security Groups: Act as a firewall for RDS, controlling inbound/outbound traffic. - VPC Endpoints: Privately connect your VPC to AWS services (like S3/RDS) without public internet routes. - S3  Bucket Policies:  Define which principals (users/roles) or network paths (e.g., VPC endpoints)              can              access              the               bucket.              Reference               Links              -               [Amazon              VPC Endpoints](https://docs.aws.amazon.com/vpc/latest/private link/vpc-endpoints.html)     -      [S3      Bucket      Policies     and      VPC
Endpoints](https://docs.aws.amazon.com/AmazonS3/latest/user guide/example-bucket-policies-vpc-endpoint.html)       -        [RDS Security Groups](https://docs.aws.amazon.com/Amazon rds/latest/User guide/Overview.Rds security groups.html)

580.A company has a web application that runs on premises. The application experiences latency issues during peak hours. The latency issues occur twice each month. At the start of a latency issue, the application's CPU utilization immediately increases to 10 times its normal amount. The company wants to migrate the application to AWS to improve latency. The company also wants to  scale the  application  automatically  when  application  demand  increases.  The  company  will  use  AWS  Elastic  Beanstalk  for application deployment. Which solution will meet these requirements?

A �� Configure  an  Elastic  Beanstalk  environment  to  use  burstable  performance  instances  in  unlimited  mode.  Configure  the environment to scale based on requests.

B��Configure an Elastic Beanstalk environment to use compute optimized instances. Configure the environment to scale based on

requests.

C ��Configure an Elastic Beanstalk environment to use compute optimized instances. Configure the environment to scale on a schedule.

D �� Configure  an  Elastic  Beanstalk  environment  to  use  burstable  performance  instances  in  unlimited  mode.  Configure  the environment to scale on predictive metrics.

�𰸣�A

���� �� Correct Answer: A Detailed Explanation: The company's application experiences sudden, unpredictable spikes in CPU usage (10x  normal) during  peak  hours, which occur twice a  month. The goal is to migrate to AWS with automatic scaling to handle these spikes cost-effectively. 1. Instance Type: Burstable Performance Instances (T-series) in Unlimited Mode - Burstable instances  (e.g.,  T3/T4g)  are  designed  for  workloads  with  intermittent  CPU  spikes.  They  use  CPU  credits  for  short  bursts.  - Unlimited mode allows these instances to burst beyond their credits (with additional costs if needed), ensuring they can sustain high CPU usage during prolonged spikes (like the 10x CPU increase in this scenario). - Compute-optimized instances (e.g., C-series) are overkill here. They are designed for sustained high CPU workloads, which is unnecessary for this application ��s twice-monthly spikes, and they are more expensive. 2. Scaling Strategy: Scale Based on Requests - The latency issues are triggered by sudden demand spikes (not predictable schedules), so scaling based on a metric like requests per instance (e.g., incoming HTTP requests) is ideal. - This ensures the environment automatically adds instances when traffic surges, distributing the load and reducing CPU strain on individual instances. - Predictive or scheduled scaling (Options C/D) won ��t work here because the spikes are irregular and  unpredictable. Why Other  Options  Fail: -  B/C:  Compute-optimized  instances  are  unnecessary  and costly for  intermittent spikes. - C/D: Scheduled or predictive scaling can ��t handle irregular, sudden demand spikes. Reference Links: - [AWS Burstable Instances](https://docs.aws.amazon.com/AWSEC2/latest/User guide/burstable-performance-instances.html)  -  [Elastic  Beanstalk Scaling](https://docs.aws.amazon.com/elastic beanstalk/latest/dg/environments-cfg-auto scaling.html)

581.A  company  has  customers  located  across  the  world.  The  company  wants  to  use  automation  to  secure  its  systems  and network  infrastructure.  The  company's  security  team  must  be  able  to  track  and  audit  all  incremental  changes  to  the infrastructure. Which solution will meet these requirements?

A ��Use AWS Organizations to set up the infrastructure. Use AWS Config to track changes.

B ��Use AWS Cloud formation to set up the infrastructure. Use AWS Config to track changes.

C ��Use AWS Organizations to set up the infrastructure. Use AWS Service Catalog to track changes.

D ��Use AWS Cloud formation to set up the infrastructure. Use AWS Service Catalog to track changes.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution is to use AWS Cloud formation for setting up the infrastructure (as it automates deployments via Infrastructure-as-Code) and AWS Config to track incremental changes. Here ��s why: - AWS Cloud formation  automates   infrastructure   provisioning,  ensuring  consistency  and  reducing   manual  errors.  -  AWS  Config continuously monitors and records configuration changes, providing a detailed audit trail for compliance and security analysis. Other  options  fail   because:  -  AWS   Organizations  (A,  C)  manages  multiple  AWS   accounts  but  doesn  �� t  directly  handle infrastructure setup or change tracking. - AWS Service Catalog (C, D) governs approved cloud resources but doesn ��t specialize in auditing             configuration             changes             like             AWS             Config.             Reference             Links:             -              [AWS Cloud formation](https://aws.amazon.com/cloud formation/) - [AWS Config](https://aws.amazon.com/config/)

582.A company is moving its data and applications to AWS during a multiyear migration project. The company wants to securely access data on Amazon S3 from the company's AWS Region and from the company's on-premises location. The data must not traverse the internet. The company has established an AWS Direct Connect connection between its Region and its on-premises location. Which solution will meet these requirements?

A ��Create gateway endpoints for Amazon S3. Use the gateway endpoints to securely access the data from the Region and the on-premises location.

B ��Create a gateway in AWS Transit Gateway to access Amazon S3 securely from the Region and the on-premises location.

C ��Create interface endpoints for Amazon S3. Use the interface endpoints to securely access the data from the Region and the on-premises location.

D �� Use an AWS  Key  Management Service (AWS  KMS) key to access the data securely from the Region and the on-premises location.

�𰸣�C

���� �� Correct Answer: C  Detailed  Explanation: The company needs secure access to Amazon S3 from both AWS and on - premises without using the internet. They have AWS Direct Connect already set up. - Gateway Endpoints (Option A) are used for VPC resources to access S3 privately but don't directly support on - premises access. On - premises would need to use S3's public endpoint over Direct Connect (which avoids the public internet), but this isn't explicitly covered by gateway endpoints. - Interface Endpoints (Option C) use AWS Private link, creating a private connection to S3 within the VPC. Since the company uses Direct Connect, on - premises systems can route traffic through Direct Connect to the VPC's interface endpoint. This ensures all traffic stays within the private AWS network (VPC + Direct Connect), never touching the public internet. - S3 now supports interface endpoints (via Private link), making Option C valid. This setup allows both VPC resources and on - premises systems to access S3 securely       through       the        same        private        endpoint.       Reference        Links:        [AWS        Private link       for        Amazon S3](https://aws.amazon.com/blogs/aws/new   -   aws   -    private link   -   for   -   amazon   -   s3/)    [AWS   Direct   Connect    and   S3 Access](https://docs.aws.amazon.com/directconnect/latest/User guide/working - with - aws - services.html)

583.A company created a  new organization  in AWS Organizations. The organization  has multiple accounts for the company's development teams. The development team members use AWS IAM Identity Center (AWS Single Sign-On) to access the accounts. For each of the company's applications, the development teams must use a predefined application name to tag resources that are created. A solutions architect needs to design a solution that gives the development team the ability to create resources only if the application name tag has an approved value. Which solution will meet these requirements?

A��Create an IAM group that has a conditional Allow policy that requires the application name tag to be specified for resources to be created.

B ��Create a cross-account role that has a Deny policy for any resource that has the application name tag.

C ��Create a resource group in AWS Resource Groups to validate that the tags are applied to all resources in all accounts.

D ��Create a tag policy in Organizations that has a list of allowed application names.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The correct solution is to create a tag policy in AWS Organizations. Tag policies are  designed  to  enforce  consistent  tagging  across  all  accounts  in  an  organization.  They  can  specify  allowed  tag  keys  (like

application name) and restrict their values to an approved list. When a user tries to create a resource with an invalid or missing tag, the tag policy automatically blocks the action. This approach works seamlessly across  multiple accounts, scales well, and doesn ��t require manual IAM policy management in every account (unlike Option A). Options B and C don ��t actively enforce tagging             rules             during             resource            creation.              Reference             Link:             [AWS             Tag             Policies Documentation](https://docs.aws.amazon.com/organizations/latest/user guide/orgs_manage_policies_tag-policies.html)

584.A company runs its databases on Amazon RDS for Postgresql. The company wants a secure solution to manage the master user password by rotating the password every 30 days. Which solution will meet these requirements with the LEAST operational overhead?

A ��Use Amazon Event bridge to schedule a custom AWS Lambda function to rotate the password every 30 days.

B ��Use the modify-db-instance command in the AWS CLI to change the password.

C ��Integrate AWS Secrets Manager with Amazon RDS for Postgresql to automate password rotation.

D ��Integrate AWS Systems Manager Parameter Store with Amazon RDS for Postgresql to automate password rotation.

�𰸣�C

������ Correct Answer: C Detailed Explanation: AWS Secrets Manager is designed specifically for securely managing secrets like database  passwords.  It  offers  built  -  in  automatic  password  rotation  for  Amazon   RDS   databases  (including   Postgresql), eliminating  the  need  to  write  custom  code.  When  integrated  with  RDS,  Secrets  Manager  automatically  rotates  the  master password on a schedule (e.g., every 30 days) and updates the secret value, ensuring minimal operational effort. Other options: - A requires custom Lambda code and Event bridge setup (more maintenance). - B is manual and not automated. - D (Parameter Store)  can  store  passwords  but  lacks  native  rotation  features,  requiring  custom  solutions.  Reference:  [Automatically  rotate secrets                                                                 for                                                                 Amazon                                                                  RDS
databases](https://docs.aws.amazon.com/secrets manager/latest/user guide/rotating-secrets-rds-postgresql.html)

585.A company performs tests on an application that uses an Amazon Dynamo db table. The tests run for 4 hours once a week. The company knows how many read and write operations the application performs to the table each second during the tests. The company does not currently use Dynamo db for any other use case. A solutions architect needs to optimize the costs for the table. Which solution will meet these requirements?

A ��Choose on-demand mode. Update the read and write capacity units appropriately.

B ��Choose provisioned mode. Update the read and write capacity units appropriately.

C ��Purchase Dynamo db reserved capacity for a 1-year term.

D ��Purchase Dynamo db reserved capacity for a 3-year term.

�𰸣�B

�� �� ��  Correct  Answer:  B.  Choose  provisioned  mode.   Update  the  read  and  write  capacity  units  appropriately.  Detailed Explanation: Dynamo db has two billing modes: on-demand and provisioned. - On-demand charges based on actual read/write requests, making it ideal for unpredictable workloads. - Provisioned lets you predefine the read/write capacity (RCU/WCU) and charges hourly, which is cheaper for predictable workloads. In this scenario: - The workload is predictable (tests run weekly with known read/write requirements). - The table is only used 4 hours/week, so using provisioned mode allows the company to: 1.

Set the RCU/WCU to match the test requirements during those 4 hours. 2. Reduce capacity to the minimum (e.g., 1 RCU/WCU) for the remaining 164 hours/week to save costs. Options C/D (reserved capacity) are not cost-effective here because reserved capacity requires a long-term commitment (1-3 years) for steady usage. Since the table is only used 4 hours/week, the savings from reserved capacity wouldn ��t offset the upfront cost. Option A (on-demand) would be more expensive because on-demand pricing     is     higher      for     consistent,     predictable      workloads     like     this.      Reference     Links:      -     [Dynamo db      Pricing Models](https://aws.amazon.com/dynamo db/pricing/)        -        [Choosing         Between        Provisioned         and        On-Demand Capacity](https://docs.aws.amazon.com/amazon dynamo db/latest/developer guide/How it works.Readwrite capacity mode.html)

586.A company runs its applications on Amazon EC2 instances. The company performs periodic financial assessments of its AWS costs. The  company  recently  identified  unusual  spending.  The  company  needs  a  solution  to  prevent  unusual  spending.  The solution  must  monitor costs and  notify  responsible stakeholders  in the event of  unusual spending. Which solution will  meet these requirements?

A ��Use an AWS Budgets template to create a zero spend budget.

B ��Create an AWS Cost Anomaly Detection monitor in the AWS Billing and Cost Management console. C ��Create AWS Pricing Calculator estimates for the current running workload pricing details.

D ��Use Amazon Cloud watch to monitor costs and to identify unusual spending.

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: AWS Cost Anomaly Detection is specifically designed to automatically detect unusual spending patterns in your AWS costs. It uses machine learning to establish normal spending baselines and alerts you via Amazon SNS  notifications when  unexpected costs occur. This directly addresses the  requirement to monitor costs and notify stakeholders about unusual spending. Other options are less suitable: - A (Zero spend budget) only alerts when costs occur, not for abnormal  spending  patterns. -  C  (Pricing  Calculator)  is  for cost estimation,  not  real  - time  monitoring.  -  D  (Cloud watch) primarily       monitors       resource        metrics,       not        cost       anomalies.        Reference       Link:        [AWS       Cost       Anomaly Detection](https://docs.aws.amazon.com/cost - management/latest/user guide/ad - getting - started.html)

587.A  marketing  company  receives  a  large  amount  of  new  clickstream  data  in  Amazon  S3  from  a  marketing  campaign. The company needs to analyze the clickstream data in Amazon S3 quickly. Then the company needs to determine whether to process the data further in the data pipeline. Which solution will meet these requirements with the LEAST operational overhead?

A ��Create external tables in a Spark catalog. Configure jobs in AWS Glue to query the data.

B ��Configure an AWS Glue crawler to crawl the data. Configure Amazon Athena to query the data.

C ��Create external tables in a Hive meta store. Configure Spark jobs in Amazon EMR to query the data.

D ��Configure an AWS Glue crawler to crawl the data. Configure Amazon Kinesis Data Analytics to use SQL to query the data.

�𰸣�B

������ Correct answer: B Detailed Explanation: The correct answer is B because AWS Glue crawler automatically discovers the schema of the clickstream data in S3 and populates the AWS Glue Data Catalog. Amazon Athena then uses this catalog to query the data directly using standard SQL, without needing to manage infrastructure.  Both services are fully managed, minimizing operational overhead. - Why not A/C: AWS Glue Spark jobs or EMR require writing/ managing code (Spark) and clusters, adding

complexity. - Why not D: Kinesis Data Analytics is designed for real - time streaming data, not batch analysis of S3 data. Reference Links: - [AWS Glue Crawler](https://aws.amazon.com/glue/) - [Amazon Athena](https://aws.amazon.com/athena/)

588.A company runs an SMB file server in its data center. The file server stores large files that the company frequently accesses for up to 7 days after the file creation date. After 7 days, the company needs to be able to access the files with a maximum retrieval time of 24 hours. Which solution will meet these requirements?

A ��Use AWS Data sync to copy data that is older than 7 days from the SMB file server to AWS.

B��Create an Amazon S3 File Gateway to increase the company's storage space. Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7 days.

C �� Create  an  Amazon  FSx  File  Gateway  to  increase  the  company's  storage  space.  Create  an  Amazon  S3  Lifecycle  policy  to transition the data after 7 days.

D��Configure access to Amazon S3 for each user. Create an S3 Lifecycle policy to transition the data to S3 Glacier Flexible Retrieval after 7 days.

�𰸣�B

���� �� Correct Answer: B Explanation: The company uses an on - premises SMB file server and needs to store files accessed frequently for 7 days, then archive them with a maximum retrieval time of 24 hours. Here ��s why Option B works: 1. Amazon S3 File Gateway: This extends the on - premises SMB file server by connecting it to Amazon S3. Files are cached locally for frequent access, while older data is seamlessly stored in S3.  Users continue accessing files via SMB without disruption. 2. S3  Lifecycle Policy: After 7 days, data is moved to S3 Glacier Deep Archive. Although Deep Archive ��s retrieval time is typically up to 48 hours for  Bulk  retrieval,  its Standard  retrieval  option takes  12  hours �� well within  the 24 -  hour  requirement. This  balances  cost - efficiency (Deep Archive is the cheapest storage) with the retrieval SLA. Why Other Options Fail: - A: Data sync only copies data to AWS but doesn ��t automate tiering to Glacier or address retrieval times. - C: FSx File Gateway doesn ��t integrate with S3 lifecycle policies, making it unsuitable for archival. - D: Direct S3 access requires users to abandon the SMB file server, which isn ��t aligned with  the  existing  setup.  Reference  Links:  -   [Amazon  S3   File  Gateway](https://aws.amazon.com/storage gateway/file/s3/)  - [Amazon S3 Glacier Retrieval Options](https://docs.aws.amazon.com/AmazonS3/latest/user guide/restoring-objects.html)

589.A company uses Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) volumes to run an application. The company  creates  one  snapshot  of  each  EBS  volume  every  day  to  meet  compliance  requirements.  The  company  wants  to implement an architecture that prevents the accidental deletion of EBS volume snapshots. The solution must not change the administrative   rights  of  the  storage  administrator   user.  Which   solution  will   meet  these   requirements   with  the   LEAST administrative effort?

A ��Create an IAM role that has permission to delete snapshots. Attach the role to a new EC2 instance. Use the AWS CLI from the new EC2 instance to delete snapshots.

B ��Create an IAM policy that denies snapshot deletion. Attach the policy to the storage administrator user. C ��Add tags to the snapshots. Create retention rules in Recycle Bin for EBS snapshots that have the tags.  D ��Lock the EBS snapshots to prevent deletion.

�𰸣�D

������ Correct Answer D �� Lock the EBS snapshots to prevent deletion. Detailed Explanation AWS offers a feature called EBS Snapshot  Lock (part of AWS  Backup) to protect snapshots from accidental or malicious deletion. When you enable snapshot locking: - Compliance mode: Prevents deletion even by root users until the retention period expires. - No IAM changes needed: Unlike Option  B  (IAM  policy),  this  doesn ��t  require  modifying  the  storage  administrator �� s  permissions,  aligning  with  the requirement to avoid changing administrative  rights. -  Least effort: Simply enabling a  lock  on snapshots  (via AWS  Backup or snapshot settings) is faster than creating tags/retention rules (Option C) or building a CLI - based workflow (Option A). Options like Recycle Bin (C) only delay deletion, while IAM policies (B) would alter user permissions. Snapshot Lock provides permanent protection without administrative overhead. Reference Link https://docs.aws.amazon.com/AWSEC2/latest/User guide/snapshot - lock.html

590.A company's application uses Network Load Balancers, Auto Scaling groups, Amazon EC2 instances, and databases that are deployed in an Amazon VPC. The company wants to capture information about traffic to and from the network interfaces in near real time in its Amazon VPC. The company wants to send the information to Amazon Open search Service for analysis. Which solution will meet these requirements?

A ��Create a log group in Amazon Cloud watch Logs. Configure VPC Flow Logs to send the log data to the log group. Use Amazon Kinesis Data Streams to stream the logs from the log group to Open search Service.

B ��Create a log group in Amazon Cloud watch Logs. Configure VPC Flow Logs to send the log data to the log group. Use Amazon Kinesis Data Firehose to stream the logs from the log group to Open search Service.

C��Create a trail in AWS Cloud trail. Configure VPC Flow Logs to send the log data to the trail. Use Amazon Kinesis Data Streams to stream the logs from the trail to Open search Service.

D��Create a trail in AWS Cloud trail. Configure VPC Flow Logs to send the log data to the trail. Use Amazon Kinesis Data Firehose to stream the logs from the trail to Open search Service.

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: VPC Flow Logs capture network traffic data (IP addresses, ports, protocols) in your VPC. To send this data to Amazon Open search Service: 1. VPC Flow Logs   �� Cloud watch Logs: Flow Logs are configured to publish logs to a Cloud watch Logs group. 2. Cloud watch Logs   �� Kinesis  Data Firehose: Use Kinesis Data Firehose (not Data Streams) because it can directly deliver logs from Cloud watch to Open search without writing custom code. 3. Kinesis Firehose �� Open search: Firehose automatically batches, transforms (if needed), and loads data into Open search. Why not other options?
- A/C: Kinesis Data Streams requires writing a consumer application to send data to Open search. - C/D: Cloud trail trails only log API        activity,        not         network        traffic         like        VPC         Flow        Logs.         Reference         Links:        -         [VPC        Flow Logs](https://docs.aws.amazon.com/vpc/latest/user guide/flow-logs.html)       -         [Kinesis        Firehose        with        Cloud watch Logs](https://docs.aws.amazon.com/firehose/latest/dev/cloud watch-logs-subscription.html)

591.A company stores sensitive data in Amazon S3. A solutions architect needs to create an encryption solution. The company needs to fully control the ability of users to create, rotate, and disable encryption keys with minimal effort for any data that must be encrypted. Which solution will meet these requirements?

A ��Use default server-side encryption with Amazon S3 managed encryption keys (SSE-S3) to store the sensitive data.

B ��Create a customer managed key by using AWS Key Management Service (AWS KMS).  Use the new key to encrypt the S3 objects by using server-side encryption with AWS KMS keys (SSE-KMS).

C��Create an AWS managed key by using AWS Key Management Service (AWS KMS). Use the new key to encrypt the S3 objects by

using server-side encryption with AWS KMS keys (SSE-KMS).

D��Download S3 objects to an Amazon EC2 instance. Encrypt the objects by using customer managed keys. Upload the encrypted objects back into Amazon S3.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The question requires a solution where the company fully controls encryption keys (creation, rotation, disabling) with minimal effort. Here's why option B works best: - SSE-KMS with Customer Managed Keys (CMK) allows the company to: 1. Create/delete keys manually via AWS KMS. 2. Rotate keys automatically (yearly) or manually. 3. Disable/enable  keys  via  KMS  policies.  All  management  happens  through  AWS  KMS  without  manual  encryption/decryption workflows. - Why other options fail: - A (SSE-S3): Uses S3-managed keys �� no customer control over keys. - C (AWS Managed Keys):   AWS   controls   key   rotation/lifecycle,   not   the   customer.   -   D   (Client-side   encryption):   Requires   manual   effort   to download/upload      data,       violating        minimal       effort.       Reference       Links:       -       [Using       AWS       KMS       with       S3 SSE-KMS](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Using kms encryption.html) -  [Customer  Managed vs. AWS Managed Keys](https://docs.aws.amazon.com/kms/latest/developer guide/concepts.html#key-mgmt)

592.A solutions architect  needs to copy files from an Amazon S3  bucket to an Amazon  Elastic  File  System  (Amazon  EFS) file system and another S3 bucket. The files must be copied continuously. New files are added to the original S3 bucket consistently. The copied files should be overwritten only if the source file changes. Which solution will meet these  requirements with the LEAST operational overhead?

A ��Create an AWS Data sync location for both the destination S3 bucket and the EFS file system. Create a task for the destination S3 bucket and the EFS file system. Set the transfer mode to transfer only data that has changed.

B ��Create an AWS Lambda function. Mount the file system to the function. Set up an S3 event notification to invoke the function when files are created and changed in Amazon S3. Configure the function to copy files to the file system and the destination S3 bucket.

C ��Create an AWS Data sync location for both the destination S3 bucket and the EFS file system. Create a task for the destination S3 bucket and the EFS file system. Set the transfer mode to transfer all data.

D �� Launch an Amazon  EC2  instance  in the same VPC as t he file system.  Mount the file  system. Create a script to routinely synchronize all objects that changed in the origin S3 bucket to the destination S3 bucket and the mounted file system.

�𰸣�A

���� �� Correct Answer A Detailed Explanation AWS Data sync is designed for automated and efficient data transfer between storage services. Option A uses Data sync with transfer only data that has changed, which ensures continuous synchronization, overwrites files only when the source changes, and minimizes operational overhead. Data sync handles retries, scheduling, and monitoring  automatically.  Option  B  (Lambda)  requires  custom  code,  EFS  mounting,  and  manual  error  handling,  increasing complexity. Option C (transfer all data) wastes bandwidth by copying unchanged files. Option D (EC2 script) introduces manual scripting,         instance          management,          and          lacks         native          automation.          Reference          Link          [Data sync Documentation](https://docs.aws.amazon.com/data sync/latest/user guide/how-it-works.html)

593.A company uses Amazon EC2 instances and stores data on Amazon Elastic Block Store (Amazon EBS) volumes. The company must ensure that all data is encrypted at rest by using AWS Key Management Service (AWS KMS). The company must be able to control rotation of the encryption keys. Which solution will meet these requirements with the LEAST operational overhead?

A ��Create a customer managed key. Use the key to encrypt the EBS volumes.

B ��Use an AWS managed key to encrypt the EBS volumes. Use the key to configure automatic key rotation. C ��Create an external KMS key with imported key material. Use the key to encrypt the EBS volumes.

D ��Use an AWS owned key to encrypt the EBS volumes.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The company needs to encrypt EBS volumes using AWS KMS and retain control over key rotation while  minimizing operational effort. - Option A (Customer  Managed Key): A customer-managed key in AWS KMS allows the company to fully control the key's rotation policy. You can enable automatic annual rotation or manually rotate the key. AWS handles the technical implementation, reducing operational overhead. - Why other options fail: - Option B (AWS Managed Key): AWS automatically rotates these keys every 3 years, and users cannot modify the rotation schedule. - Option C (External Key): Requires manual key material import/rotation, increasing operational complexity. - Option D (AWS Owned Key): No control over the key or rotation; it ��s fully managed by AWS. Key Point: Customer-managed keys balance control (rotation) and low operational effort since AWS manages encryption/decryption under the company ��s policies. Reference Links: - [AWS KMS Customer  Managed  Keys](https://docs.aws.amazon.com/kms/latest/developer guide/concepts.html#customer-cmk)  -  [EBS Encryption](https://docs.aws.amazon.com/AWSEC2/latest/User guide/Ebs encryption.html)

594.A company needs a solution to enforce data encryption at rest on Amazon EC2 instances. The solution must automatically identify noncompliant resources and enforce compliance policies on findings. Which solution will meet these requirements with the LEAST administrative overhead?

A ��Use an IAM policy that allows users to create only encrypted Amazon Elastic Block Store (Amazon EBS) volumes. Use AWS Config and AWS Systems Manager to automate the detection and remediation of unencrypted EBS volumes.

B �� Use AWS  Key Management Service (AWS KMS) to manage access to encrypted Amazon Elastic Block Store (Amazon EBS) volumes. Use AWS Lambda and Amazon Event bridge to automate the detection and remediation of unencrypted EBS volumes.

C �� Use Amazon Macie to detect unencrypted Amazon Elastic Block Store (Amazon EBS) volumes. Use AWS Systems Manager Automation rules to automatically encrypt existing and new EBS volumes.

D ��Use Amazon inspector to detect unencrypted Amazon Elastic Block Store (Amazon EBS) volumes. Use AWS Systems Manager Automation rules to automatically encrypt existing and new EBS volumes.

�𰸣�A

�� �� �� Correct Answer: A  Detailed  Explanation:  The  best  solution  is  Option  A  because  it  combines  proactive  prevention, automated detection, and remediation with minimal administrative effort: 1. Proactive Prevention (IAM Policy): The IAM policy blocks users from creating unencrypted EBS volumes in the first place. This prevents new noncompliant resources. 2. Automated Detection  (AWS  Config):  AWS  Config  continuously  monitors  EBS  volumes  and  flags  unencrypted  ones  as  noncompliant.   It provides a centralized view of compliance. 3. Automated Remediation (AWS Systems Manager): Systems Manager Automation can automatically encrypt unencrypted EBS volumes without manual intervention, ensuring compliance. Why Other Options Are Less  Ideal: - Option  B: While  Lambda  and  Event bridge can work, they  require custom  code and event  rule setup, increasing administrative complexity. - Option C: Amazon  Macie  is  designed for data  privacy  (identifying sensitive data  like  P II),  not for checking  EBS encryption status. - Option  D: Amazon  Inspector focuses on vulnerability scanning (e.g., OS vulnerabilities),  not encryption                  compliance.                  Reference                   Links:                  -                  [AWS                   Config                  Managed

Rules](https://docs.aws.amazon.com/config/latest/developer guide/managed-rules-by-aws-config.html)      -        [AWS      Systems Manager Automation](https://docs.aws.amazon.com/systems-manager/latest/user guide/systems-manager-automation.html)

595.A company wants to migrate its web applications from on premises to AWS. The company is located close to the eu-central-1 Region.  Because  of  regulations, the company cannot  launch  some of  its applications  in eu-central-1. The  company wants to achieve single-digit millisecond latency. Which solution will meet these requirements?

A �� Deploy  the applications  in  eu-central-1.  Extend  the company ��s VPC from eu-central-1 to an  edge  location  in Amazon Cloud front.

B ��Deploy the applications in AWS Local Zones by extending the company's VPC from eu-central-1 to the chosen Local Zone.

C ��Deploy the applications in eu-central-1. Extend the company ��s VPC from eu-central-1 to the regional edge caches in Amazon Cloud front.

D �� Deploy  the applications  in AWS Wavelength Zones  by  extending the company ��s VPC from eu-central-1 to the chosen Wavelength Zone.

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: AWS Local Zones are extensions of an AWS Region that allow you to deploy resources closer to users for  low-latency  requirements. Since the company is  near the `eu-central-1`  Region  but cannot host certain applications there  due to  regulations,  Local  Zones  provide  a  compliant solution.  By  extending the  existing VPC from `eu-central-1`  to  a  Local  Zone,  the  company  retains  seamless  connectivity  to  the  parent  region  while  achieving  single-digit millisecond latency. Why not A/C? Cloud front edge  locations/regional caches are for content delivery  (CDN),  not  hosting full applications. They don ��t  support VPC extensions for compute workloads  like EC2. Why not  D? AWS Wavelength Zones are optimized for 5G mobile edge computing, which isn ��t the use case here. Local Zones are better suited for general low-latency needs            near             a            specific             geographic             area.            Reference             Links:             -             [AWS            Local Zones](https://aws.amazon.com/about-aws/global-infrastructure/local zones/)                                        -                                         [AWS
Wavelength](https://aws.amazon.com/wavelength/)

596.A company ��s ecommerce website has unpredictable traffic and uses AWS Lambda functions to directly access a private Amazon RDS for Postgresql DB instance. The company wants to maintain predictable database performance and ensure that the Lambda invocations do not overload the database with too many connections. What should a solutions architect do to meet these requirements?

A ��Point the client driver at an RDS custom endpoint. Deploy the Lambda functions inside a VPC.

B ��Point the client driver at an RDS proxy endpoint. Deploy the Lambda functions inside a VPC.

C ��Point the client driver at an RDS custom endpoint. Deploy the Lambda functions outside a VPC.

D ��Point the client driver at an RDS proxy endpoint. Deploy the Lambda functions outside a VPC.

�𰸣�B

�� �� �� Correct Answer:  B  Detailed  Explanation:  To  maintain  predictable  database  performance  and  prevent  Lambda  from
overloading the RDS instance with too many connections, the solution involves two key steps: 1. Use RDS Proxy Endpoint: - Why: RDS Proxy acts as a connection pool manager. It reuses database connections and queues excess requests during traffic spikes,

preventing the  database  from  being  overwhelmed  by  too  many  concurrent  connections.  -  Example:  Imagine  1,000  Lambda functions trying to connect simultaneously. Without RDS Proxy, this could create 1,000 database connections. With RDS Proxy, connections are reused, reducing strain on the database. 2. Deploy Lambda Inside the VPC: - Why: The RDS instance is private (not publicly accessible). Lambda must be in the same VPC to access it securely, as VPCs isolate resources like databases from the public  internet. -  How:  Lambda  runs  in  the VPC's  private subnets,  uses  VPC security groups to control  access,  and connects directly to RDS via the VPC network. Why Other Options Fail: - A/C: Custom endpoints optimize read/write splitting but don ��t solve connection overload. - D: Lambda outside the VPC can��t access a private RDS instance unless it ��s publicly exposed (unsafe). Reference        Links:         -         [RDS         Proxy         Overview](https://aws.amazon.com/rds/proxy/)        -         [Lambda         in         a VPC](https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html)

597.A  company  is  creating  an  application.  The  company  stores  data  from  tests  of  t he  application  in  multiple  on-premises locations. The company needs to connect the on-premises locations to VPCs in an AWS Region in the AWS Cloud. The number of accounts  and  VPCs  will  increase  during  the  next  year.  The  network  architecture  must  simplify  the  administration  of  new connections and must provide the ability to scale. Which solution will meet these requirements with the LEAST administrative overhead?

A ��Create a peering connection between the VPCs. Create a VPN connection between the VPCs and the on-premises locations.

B ��Launch an Amazon EC2 instance. On the instance, include VPN software that uses a VPN connection to connect all VPCs and on-premises locations.

C �� Create a transit gateway. Create VPC attachments for the VPC connections. Create VPN attachments for the on-premises connections.

D ��Create an AWS Direct Connect connection between the on-premises locations and a central VPC. Connect the central VPC to other VPCs by using peering connections.

�𰸣�C

������ Correct Answer: C. Create a transit gateway. Create VPC attachments for the VPC connections. Create VPN attachments for the on-premises connections. Detailed Explanation: Transit Gateway acts like a central hub for connecting multiple VPCs and on-premises networks (via VPN). Instead of managing complex peering connections between every VPC (which becomes messy as they grow), you attach all VPCs and VPNs to this single hub. When new VPCs or on-premises locations are added later, you only need to connect them to the Transit Gateway once    �C dramatically simplifying administration and scaling. Other options either create manual point-to-point links (A/D) or rely on error-prone DIY solutions (B). Transit Gateway is AWS's recommended service           for            this            exact            scenario.             Reference            Links:            -            [AWS            Transit            Gateway documentation](https://docs.aws.amazon.com/transit-gateway/)                            -                            [AWS                            Networking
Fundamentals](https://aws.amazon.com/blogs/networking-and-content-delivery/)

598.A company manages AWS accounts in AWS Organizations. AWS IAM Identity Center (AWS Single Sign-On) and AWS Control Tower are configured for the accounts. The company wants to manage multiple user permissions across all the accounts. The permissions will be used by multiple IAM users and must be split between the developer and administrator teams. Each team requires different  permissions. The company wants a solution that  includes  new  users that are  hired  on  both teams. Which solution will meet these requirements with the LEAST operational overhead?

A ��Create individual users in IAM Identity Center for each account. Create separate developer and administrator groups in IAM Identity  Center. Assign  the  users  to the  appropriate groups.  Create  a  custom  IAM  policy  for  each  group  to  set  fine-grained permissions.

B ��Create individual users in IAM Identity Center for each account. Create separate developer and administrator groups in IAM Identity  Center. Assign  the  users  to the  appropriate groups. Attach AWS  managed  IAM  policies  to  each  user  as  needed  for fine-grained permissions.

C ��Create individual users in IAM Identity Center. Create new developer and administrator groups in IAM Identity Center. Create new  permission  sets  that  include  the  appropriate  IAM  policies  for  each  group.  Assign  the  new  groups  to  the  appropriate accounts. Assign the new permission sets to the new groups. When new users are hired, add them to the appropriate group.

D ��Create individual users in IAM Identity Center. Create new permission sets that include the appropriate IAM policies for each user. Assign the users to the appropriate accounts. Grant additional IAM permissions to the users from within specific accounts. When new users are hired, add them to IAM Identity Center and assign them to the accounts.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The best solution is C because it uses AWS IAM Identity Center (AWS SSO) groups
and permission sets to centrally manage permissions across multiple AWS accounts in AWS Organizations. Here's why: 1. Groups: Creating separate groups for developers and administrators allows you to assign users to these groups. When new users join, simply adding them to the group automatically grants them the required permissions (no manual policy updates). 2. Permission Sets: Permission sets are reusable configurations that define IAM policies (e.g., AWS managed policies like `Administrator access` or custom policies). By linking a permission set to a group, all users in that group inherit the same permissions across all assigned accounts. This  scales  easily  and  avoids  repetitive  work.  3.  Least  Operational  Overhead:  Assigning  permission  sets  to  groups (instead of individual users) minimizes maintenance. For example, if a policy changes, you update the permission set once, and all users in the linked group automatically get the update. Options A/B require manual per - user or per - account management, and  Option   D  relies  on  error  -   prone   user  -   level  assignments.   Reference  Links:  -   [AWS  IAM  Identity  Center   Permission Sets](https://docs.aws.amazon.com/single sign on/latest/user guide/permission sets.html)  -   [AWS  Control  Tower   and  AWS  SSO Integration](https://aws.amazon.com/blogs/mt/how - aws - control - tower - and - aws - sso - work - together/)

599.A company wants to standardize its Amazon Elastic Block Store (Amazon EBS) volume encryption strategy. The company also wants to minimize the cost and configuration effort required to operate the volume encryption check. Which solution will meet these requirements?

A �� Write API calls to describe the EBS volumes and to confirm the EBS volumes are encrypted.  Use Amazon Event bridge to schedule an AWS Lambda function to run the API calls.

B ��Write API calls to describe the EBS volumes and to confirm the EBS volumes are encrypted. Run the API calls on an AWS Fargate task.

C ��Create an AWS Identity and Access Management (IAM) policy that requires the use of tags on EBS volumes. Use AWS Cost Explorer to display resources that are not properly tagged. Encrypt the untagged resources manually.

D ��Create an AWS Config rule for Amazon EBS to evaluate if a volume is encrypted and to flag the volume if it is not encrypted.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The best solution is to create an AWS Config rule to automatically check if EBS volumes are encrypted. AWS Config is a managed service that continuously monitors and evaluates AWS resource configurations. By enabling the pre-built ebs-encrypted AWS Config managed rule, the company can: 1. Automatically detect unencrypted EBS volumes without writing custom code 2. Get compliance status in  real-time through the AWS  Management Console 3. Avoid maintaining infrastructure (like Lambda/Fargate) or scheduling jobs 4. Minimize operational effort as AWS Config handles rule

evaluation Options A/B require custom code and infrastructure management, while Option C uses tagging (which doesn't directly enforce encryption). AWS Config (Option D) provides a native, low-effort encryption compliance solution aligned with AWS best practices. Reference Link: https://docs.aws.amazon.com/config/latest/developer guide/ebs-encrypted.html

600.A company is deploying an application in three AWS Regions using an Application Load Balancer. Amazon Route 53 will be
used to distribute traffic between these Regions. Which Route 53 configuration should a solutions architect use to provide the
MOST high-performing experience?

A ��Create an A record with a latency policy.

B ��Create an A record with a geolocation policy.

C ��Create a CNAME record with a failover policy.

D ��Create a CNAME record with a geo proximity policy.

�𰸣�A

������ Correct Answer A �� Create an A record with a latency policy. Detailed Explanation To ensure the highest performance for users, Amazon Route 53's latency-based routing policy (option A) is the best choice. Here ��s why: 1. Latency Policy: - Route 53 measures network latency between users and AWS Regions in real time. - It automatically routes users to the Region with the lowest latency (fastest response time), ensuring the best performance. - Example: If a user in Tokyo gets lower latency to the Oregon  Region than to Seoul,  Route 53 sends them to Oregon.  2. Why other options are  less  ideal: -  B.  Geolocation  Policy: Routes based on the user ��s geographic  location, not actual latency. Geographic proximity   �� lowest  latency  (e.g., network congestion  can  make  a  closer  Region  slower).  -  C.  Failover  Policy:  Designed  for  disaster  recovery  (active-passive  setup),  not performance optimization. - D. Geo proximity Policy: Routes based on geographic distance and allows weighting, but still doesn ��t measure real-time latency. 3. A vs. CNAME Records: - Use A records (options A/B) for root domains (e.g., `example.com`). - Use CNAME  records  (options C/D) for subdomains  (e.g., `app.example.com`).  In  this scenario, since the goal  is  performance, the latency         policy         (A)         is        the         most         reliable.          Reference         Link         [Amazon         Route         53         Routing Policies](https://docs.aws.amazon.com/Route53/latest/Developer guide/routing-policy.html)

601.A  company  has  a  web  application  that  includes  an  embedded  NoSQL  database.  The  application  runs  on  Amazon  EC2 instances  behind  an  Application  Load  Balancer  (ALB).  The  instances  run  in  an  Amazon  EC2  Auto  Scaling  group  in  a  single Availability Zone. A recent increase in traffic requires the application to be highly available and for the database to be eventually consistent. Which solution will meet these requirements with the LEAST operational overhead?

A ��Replace the ALB with a Network Load Balancer. Maintain the embedded NoSQL database with its replication service on the EC2 instances.

B ��Replace the ALB with a Network Load Balancer. Migrate the embedded NoSQL database to Amazon Dynamo db by using AWS Database Migration Service (AWS DMS).

C ��Modify the Auto Scaling group to use EC2 instances across three Availability Zones. Maintain the embedded NoSQL database with its replication service on the EC2 instances.

D��Modify the Auto Scaling group to use EC2 instances across three Availability Zones. Migrate the embedded NoSQL database to Amazon Dynamo db by using AWS Database Migration Service (AWS DMS).

�𰸣�D

���� �� Correct Answer: D Detailed Explanation: The question requires a solution that ensures high availability and eventual consistency  for the  database  while  minimizing  operational  overhead.  1.  High  Availability:  -  The  original  setup  uses  a  single Availability Zone (AZ), which creates a single point of failure. Modifying the Auto Scaling group to use EC2 instances across three AZs ensures the application layer is distributed and resilient to AZ failures. The existing Application Load Balancer (ALB) already supports multi - AZ traffic routing, so no need to replace it with a Network Load Balancer (NLB). 2. Database Management: - The embedded  NoSQL  database  on   EC2  instances  requires  manual  replication  and  maintenance,  increasing  operational  effort. Migrating to Amazon Dynamo db (a fully managed NoSQL service) eliminates this overhead. Dynamo db automatically replicates data across multiple AZs, provides  built - in high availability, and guarantees eventual consistency. 3. Operational Overhead: - Dynamo db is serverless, so AWS handles scaling, patching, and replication. This reduces the team ��s responsibility compared to managing  a  self  -  hosted  database.  Options  A/B/C  are  suboptimal:  -  A/C   retain  the  self  -  managed  database,  increasing operational work. - B unnecessarily replaces the ALB with an NLB, which isn��t required for HTTP - based traffic. Reference Links: - [Amazon  Dynamo db  High  Availability](https://aws.amazon.com/dynamo db/features/#Availability_and_durability)  -  [EC2  Auto Scaling Multi - AZ](https://docs.aws.amazon.com/auto scaling/ec2/user guide/auto - scaling - benefits.html)

602.A company is building a shopping application on AWS. The application offers a catalog that changes once each month and needs to scale with traffic volume. The company wants the lowest possible latency from the application. Data from each user's shopping cart needs to be highly available. User session data must be available even if the user is disconnected and reconnects. What should a solutions architect do to ensure that the shopping cart data is preserved at all times?

A ��Configure an Application Load Balancer to enable the sticky sessions feature (session affinity) for access to the catalog in Amazon Aurora.

B��Configure Amazon Elastic ache for Redis to cache catalog data from Amazon Dynamo db and shopping cart data from the user's session.

C��Configure Amazon Open search Service to cache catalog data from Amazon Dynamo db and shopping cart data from the user's session.

D ��Configure an Amazon EC2 instance with Amazon Elastic Block Store (Amazon EBS) storage for the catalog and shopping cart. Configure automated snapshots.

�𰸣�B

������ Correct Answer: B Detailed Explanation: To ensure shopping cart data is always preserved and highly available, Amazon Elastic ache for Redis is the best choice. Redis is an in-memory data store that offers low latency and high availability. It can store user session data (like shopping carts) so even if a user disconnects and reconnects, their data stays intact. Dynamo db is used for the  catalog  (which  changes  monthly),  and  caching  it  in  Redis  reduces  latency  for  frequent  access.  Other  options  like  sticky sessions (A) don ��t guarantee data persistence during server failures, Open search (C) isn ��t designed for session storage, and EC2+EBS (D) lacks automatic scaling and high availability out-of-the-box. Elastic ache Redis ensures data durability with features like    replication    and     backups,    making     it    ideal     for    this    scenario.     Reference    Links:     -    [Amazon     Elastic ache    for Redis](https://aws.amazon.com/elastic ache/redis/)                       -                       [Session                        Management                       with
Elastic ache](https://docs.aws.amazon.com/Amazon elastic ache/latest/red-ug/Session-Management.html)

603.A  company  is  building  a  micro services-based  application  that  will  be  deployed  on  Amazon  Elastic  Ku bernet es  Service (Amazon EKS). The micro services will interact with each other. The company wants to ensure that the application is observable to identify performance issues in the future. Which solution will meet these requirements?

A ��Configure the application to use Amazon Elastic ache to reduce the number of requests that are sent to the micro services.

B ��Configure Amazon Cloud watch Container Insights to collect metrics from the EKS clusters. Configure AWS X-Ray to trace the requests between the micro services.

C �� Configure  AWS  Cloud trail  to  review  the  API  calls.  Build  an  Amazon  Quick sight  dashboard  to  observe  the  micro service interactions.

D ��Use AWS Trusted Advisor to understand the performance of the application.

�𰸣�B

������ Correct Answer B Detailed Explanation To ensure observability for a micro services-based application on Amazon EKS, the solution must address metrics collection (to monitor performance) and request tracing (to track interactions between services). - Option B combines Amazon Cloud watch Container Insights (to collect and analyze metrics like CPU, memory, and network usage from EKS clusters) and AWS X-Ray (to trace requests across micro services, identify bottlenecks, and analyze latency). This directly addresses the requirements for observability. - Why other options are incorrect: - A: Elastic ache reduces latency but does not provide observability. - C: Cloud trail logs API activity (security/audit focus), not performance metrics. Quick sight visualizes data but  requires  relevant  metrics  (not  provided  here).  -  D:  Trusted  Advisor  offers  cost/security  recommendations,  not  real-time performance     insights.     Reference      Links     -     [AWS     X-Ray](https://aws.amazon.com/xray/)     -      [Cloud watch     Container Insights](https://docs.aws.amazon.com/Amazon cloud watch/latest/monitoring/Container insights.html)

604.A company needs to provide customers with secure access to its data. The company processes customer data and stores the results  in  an  Amazon  S3  bucket.  All  the  data  is  subject  to  strong  regulations  and  security  requirements.  The  data  must  be encrypted at rest. Each customer must be able to access only their data from their AWS account. Company employees must not be able to access the data. Which solution will meet these requirements?

A �� Provision  an  AWS  Certificate  Manager  (ACM)  certificate  for  each  customer.  Encrypt  the  data  client-side.  In  the  private certificate policy, deny access to the certificate for all principals except an IAM role that the customer provides.

B ��Provision a separate AWS Key Management Service (AWS KMS) key for each customer. Encrypt the data server-side. In the S3 bucket policy, deny decryption of data for all principals except an IAM role that the customer provides.

C ��Provision a separate AWS Key Management Service (AWS KMS) key for each customer. Encrypt the data server-side. In each KMS key policy, deny decryption of data for all principals except an IAM role that the customer provides.

D �� Provision  an  AWS  Certificate  Manager  (ACM)  certificate  for  each  customer.  Encrypt  the  data  client-side.  In  the  public certificate policy, deny access to the certificate for all principals except an IAM role that the customer provides.

�𰸣�C

���� �� Correct Answer: C Detailed Explanation: The correct solution is C because it uses AWS KMS keys with key policies to enforce  strict  access  control.  Here's  why:  1.  Server-Side  Encryption  with   KMS:  Each  customer's  data  is  encrypted  using  a dedicated KMS key. This ensures data is encrypted at rest in S3, meeting regulatory requirements. 2. KMS Key Policies Control Access: - KMS key policies directly govern who can decrypt the data. Even if someone gains access to the S3 bucket (e.g., via a misconfigured bucket policy), they cannot read the encrypted data without the KMS key's decryption permission. - By denying decryption to all except the customer's IAM role, you ensure only that customer (via their AWS account) can decrypt their data. Company employees (who don ��t have the customer ��s  IAM role) are automatically blocked. 3. Why Not S3  Bucket Policies (Option B)? - S3 bucket policies control access to the bucket (e.g., read/write permissions), not decryption. A malicious actor with S3 access could still download encrypted data but couldn ��t decrypt it without the KMS key. However, relying solely on S3 policies

is riskier than enforcing access at the KMS layer. Simple Analogy: - Imagine S3 as a locked safe (encrypted data). - The KMS key is the only key to that safe. - Even if someone breaks into the room (S3 access), they can ��t open the safe without the key (KMS
decryption                    permissions).                    Reference                    Links:                    -                     [AWS                    KMS                    Key
Policies](https://docs.aws.amazon.com/kms/latest/developer guide/key-policies.html)    -     [S3     Server-Side     Encryption     with KMS](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Using kms encryption.html)

605.A solutions architect creates a VPC that includes two public subnets and two private subnets. A corporate security mandate requires the solutions architect to launch all Amazon EC2 instances in a private subnet. However, when the solutions architect launches an EC2 instance that runs a web server on ports 80 and 443 in a private subnet, no external internet traffic can connect to the server. What should the solutions architect do to resolve this issue?

A ��Attach the EC2 instance to an Auto Scaling group in a private subnet. Ensure that the DNS record for the website resolves to the Auto Scaling group identifier.

B ��Provision an internet-facing Application Load Balancer (ALB) in a public subnet. Add the EC2 instance to the target group that is associated with the Al ensure that the DNS record for the website resolves to the ALB.

C ��Launch a NAT gateway in a private subnet. Update the route table for the private subnets to add a default route to the NAT gateway. Attach a public Elastic IP address to the NAT gateway.

D��Ensure that the security group that is attached to the EC2 instance allows HTTP traffic on port 80 and HTTPS traffic on port 443. Ensure that the DNS record for the website resolves to the public IP address of the EC2 instance.

�𰸣�B

���� �� Correct Answer B Detailed Explanation The EC2 instance is in a private subnet, which has no direct internet inbound access. To allow external traffic,  use an  internet-facing Application  Load  Balancer  (ALB)  in  a  public  subnet. The ALB  receives traffic and forwards it to the EC2 instance in the private subnet via a target group. This keeps the instance secure (in a private subnet) while enabling web access. Other options: - A: Auto Scaling groups manage scaling, not inbound traffic. DNS pointing to ASG is invalid. - C: NAT gateways handle outbound traffic (e.g., updates), not inbound requests. - D: Private subnet instances lack public    IPs.    Even   with    open    ports,   the    route    table    blocks    inbound    traffic.    Reference    Links    -    [Application    Load Balancer](https://docs.aws.amazon.com/elastic load balancing/latest/application/introduction.html)   -    [VPC   Public    vs   Private Subnets](https://docs.aws.amazon.com/vpc/latest/user guide/VPC_Scenario2.html)

606.A company is deploying a new application to Amazon Elastic Ku bernet es Service (Amazon EKS) with an AWS Fargate cluster. The  application  needs  a  storage  solution for  data  persistence. The  solution  must  be  highly  available  and  fault  tolerant.  The solution also must be shared between multiple application containers. Which solution will meet these requirements with the LEAST operational overhead?

A ��Create Amazon Elastic Block Store (Amazon EBS) volumes in the same Availability Zones where EKS worker nodes are placed. Register the volumes in a Storage class object on an EKS cluster. Use EBS Multi-Attach to share the data between containers.

B ��Create an Amazon Elastic File System (Amazon EFS) file system. Register the file system in a Storage class object on an EKS cluster. Use the same file system for all containers.

C ��Create an Amazon Elastic Block Store (Amazon EBS) volume. Register the volume in a Storage class object on an EKS cluster. Use the same volume for all containers.

D �� Create Amazon Elastic File System (Amazon EFS) file systems in the same Availability Zones where  EKS worker nodes are

placed. Register the file systems in a Storage class object on an EKS cluster. Create an AWS Lambda function to synchronize the data between file systems.

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: The best solution is Amazon EFS because it provides a fully managed, highly available, and shared file system that works across multiple Availability Zones (AZs). EFS is natively compatible with Amazon EKS and Fargate, allowing multiple containers to read/write to the same storage simultaneously. Unlike EBS (which is limited to a single AZ and requires  Multi-Attach configurations for limited sharing),  EFS automatically handles  redundancy, scalability, and cross-AZ data access without additional operational effort. Options A, C, and D either lack multi-AZ resilience, require complex setups, or introduce unnecessary overhead (e.g.,  manual synchronization with  Lambda).  Reference  Links: -  [Amazon  EFS with EKS](https://docs.aws.amazon.com/eks/latest/user guide/efs-csi.html)                -                 [EFS                vs.                 EBS                 for Ku bernet es](https://aws.amazon.com/compare/the-difference-between-efs-and-ebs/)

607.A company has an application that uses Docker containers in its local data center. The application runs on a container host that stores persistent data in a volume on the host. The container instances use the stored persistent data. The company wants to  move the application to a fully  managed  service  because the company  does  not want to  manage  any servers  or storage infrastructure. Which solution will meet these requirements?

A �� Use  Amazon  Elastic  Ku bernet es  Service  (Amazon  EKS)  with  self-managed  nodes.  Create  an  Amazon  Elastic  Block  Store (Amazon  EBS)  volume  attached  to  an  Amazon  EC2  instance.  Use  the  EBS  volume  as  a  persistent  volume  mounted  in  the containers.

B ��Use Amazon Elastic Container Service (Amazon ECS) with an AWS Fargate launch type. Create an Amazon Elastic File System (Amazon EFS) volume. Add the EFS volume as a persistent storage volume mounted in the containers.

C ��Use Amazon Elastic Container Service (Amazon ECS) with an AWS Fargate launch type. Create an Amazon S3 bucket. Map the S3 bucket as a persistent storage volume mounted in the containers.

D ��Use Amazon Elastic Container Service (Amazon ECS) with an Amazon EC2 launch type. Create an Amazon Elastic File System (Amazon EFS) volume. Add the EFS volume as a persistent storage volume mounted in the containers.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The company needs a fully managed service where they don't manage servers or storage. AWS Fargate (serverless compute for containers) eliminates server management, and Amazon EFS (managed file storage) provides persistent storage that multiple containers can access simultaneously. Option A uses self - managed EC2 nodes (requires server management) and EBS (only attaches to a single EC2 instance), so it ��s not fully managed. Option C uses S3, which is object storage and cannot  be  mounted as a file system volume for containers. Option  D  uses the  EC2  launch type  (requires  server management), so it violates the requirement. Reference Links: - [AWS Fargate](https://aws.amazon.com/fargate/) - [Amazon EFS with ECS](https://docs.aws.amazon.com/Amazon ecs/latest/developer guide/efs - volumes.html)

608.A city has deployed a web application running on Amazon EC2 instances behind an Application Load Balancer (ALB). The application's users have reported sporadic performance, which appears to be related to DDoS attacks originating from random IP addresses. The city needs a solution that requires minimal configuration changes and provides an audit trail for the DDoS sources. Which solution meets these requirements?

A ��Enable an AWS WAF web ACL on the ALB, and configure rules to block traffic from unknown sources.

B ��Subscribe to Amazon Inspector. Engage the AWS DDoS Response Team (DRT) to integrate mitigating controls into the service.

C ��Subscribe to AWS Shield Advanced. Engage the AWS DDoS  Response Team (DRT) to integrate mitigating controls into the service.

D ��Create an Amazon Cloud front distribution for the application, and set the ALB as the origin. Enable an AWS WAF web ACL on the distribution, and configure rules to block traffic from unknown sources

�𰸣�C

������ Correct Answer: C Detailed Explanation: The best solution is C (Subscribe to AWS Shield Advanced and engage the AWS DDoS  Response Team).  Here's why:  1. AWS Shield Advanced is specifically designed to protect against DDoS attacks.  It offers automatic detection and mitigation for infrastructure-layer (Layer 3/4) and application-layer (Layer 7) attacks, which matches the scenario of  random  IP addresses  (likely a volumetric/network-layer attack). 2.  Minimal  configuration: Shield Advanced works natively with ALB and EC2.  Unlike options requiring WAF rule setup (A/D) or architectural changes like Cloud front (D), Shield Advanced requires no code/config changes to your ALB or EC2 instances. 3. Audit trail: Shield Advanced provides detailed attack reports with source IPs, attack vectors, and mitigation details    �C critical for forensic  analysis and compliance. 4.  DRT (DDoS Response Team): Shield Advanced includes 24/7 access to AWS experts who proactively monitor and mitigate attacks, reducing operational overhead for the city's team. Why other options are less ideal: - A/WAF on ALB: WAF alone can't mitigate large-scale network-layer DDoS. Blocking unknown sources is impractical and might block legitimate traffic. - B/Amazon Inspector: This is a vulnerability  scanner,  unrelated  to  DDoS  protection.  -  D/Cloud front+WAF:  While  Cloud front  improves  resilience,  it  requires re-architecting (DNS changes, ALB as origin). WAF rules still need manual tuning for dynamic DDoS patterns.  Reference: [AWS Shield     Advanced     Features](https://aws.amazon.com/shield/features/)      [Choosing     between      AWS     WAF     and      Shield Advanced](https://docs.aws.amazon.com/waf/latest/developer guide/ddos-overview.html)

609.A company copies 200 TB of data from a  recent ocean survey onto AWS Snowball  Edge Storage Optimized devices. The company has a high performance computing (HPC) cluster that is hosted on AWS to look for oil and gas deposits. A solutions architect  must  provide  the  cluster  with  consistent  sub-millisecond  latency  and  high-throughput  access  to  the  data  on  the Snowball  Edge Storage Optimized devices. The company is sending the devices  back to AWS. Which solution will  meet these requirements?

A ��Create an Amazon S3 bucket. Import the data into the S3 bucket. Configure an AWS Storage Gateway file gateway to use the S3 bucket. Access the file gateway from the HPC cluster instances.

B��Create an Amazon S3 bucket. Import the data into the S3 bucket. Configure an Amazon FSx for Lustre file system, and integrate it with the S3 bucket. Access the FSx for Lustre file system from the HPC cluster instances.

C ��Create an Amazon S3 bucket and an Amazon Elastic File System (Amazon EFS) file system. Import the data into the S3 bucket. Copy the data from the S3 bucket to the EFS file system. Access the EFS file system from the HPC cluster instances.

D ��Create an Amazon FSx for Lustre file system. Import the data directly into the FSx for Lustre file system. Access the FSx for Lustre file system from the HPC cluster instances.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution is to use Amazon FSx for Lustre integrated with an S3 bucket. Here's why: 1. Data Flow: Snowball Edge devices automatically import data into Amazon S3 when returned to AWS. This makes S3 the starting point. 2. HPC Requirements: High-performance computing (HPC) workloads like oil/gas simulations need ultra-fast storage. FSx for Lustre is specifically designed for sub-millisecond latency and high-throughput workloads, unlike general-purpose

options like EFS or Storage Gateway. 3. S3 Integration: FSx for Lustre can directly sync with S3 buckets. This means: - Data stays in S3 for durability/cost-efficiency. -  FSx for  Lustre automatically fetches data from S3 when the  HPC cluster  needs  it,  providing instant low-latency access. - Results can be written back to S3 via Lustre. 4. Why Not Other Options: - A (Storage Gateway): Adds latency through protocol translation (S3 �� NFS/SMB) and isn't optimized for HPC. - C (EFS): EFS uses NFS and has higher latency (~milliseconds) compared to Lustre's sub-millisecond performance. - D (Direct FSx Import): Snowball data must go to S3 first. FSx for      Lustre      can't      ingest      data      directly      from      Snowball      devices.      Reference      Links:      -      [Amazon       FSx     for Lustre](https://aws.amazon.com/fsx/lustre/)                  -                   [Using                   FSx                   for                   Lustre                  with
S3](https://docs.aws.amazon.com/fsx/latest/Lustre guide/fsx-data-repositories.html)       -       [AWS        Snowball       Edge        Data Import](https://docs.aws.amazon.com/snowball/latest/developer-guide/what is edge.html)

610.A  company  has  NFS  servers  in  an  on-premises  data  center  that  need  to  periodically  back  up  small  amounts  of  data  to Amazon S3. Which solution meets these requirements and is MOST cost-effective?

A ��Set up AWS Glue to copy the data from the on-premises servers to Amazon S3.

B ��Set up an AWS Data sync agent on the on-premises servers, and sync the data to Amazon S3.

C ��Set up an SFTP sync using AWS Transfer for SFTP to sync data from on premises to Amazon S3.

D ��Set up an AWS Direct Connect connection between the on-premises data center and a VPC, and copy the data to Amazon S3.

�𰸣�B

������ Correct Answer B �� Set up an AWS Data sync agent on the on-premises servers, and sync the data to Amazon S3. Detailed Explanation AWS Data sync is specifically designed for efficiently transferring data between on-premises storage systems (like NFS servers) and AWS services such as Amazon S3. It automatically handles encryption, compression, and optimized network usage, making it ideal for small, periodic backups. Here's why it ��s the best choice: - Cost-Effective:  Data sync charges based on the amount of data transferred, which is affordable for small datasets. Other options  like Direct Connect (Option  D) involve  high setup and ongoing costs, which are overkill for small backups. - Simplicity: Data sync agents are easy to deploy on-premises and require minimal configuration compared to setting up ETL jobs (Option A) or SFTP workflows (Option C). - Performance: Data sync uses efficient protocols and parallel transfers, ensuring faster and reliable syncs compared to manual methods or SFTP. Options like AWS Glue (A) are meant for large-scale data transformations, not simple backups. SFTP (C) adds unnecessary overhead, and Direct      Connect       (D)       is       too       expensive      for       small       data       transfers.       Reference       Link       [AWS       Data sync Overview](https://aws.amazon.com/data sync/)

611.An online video game company must maintain ultra-low latency for its game servers. The game servers run on Amazon EC2 instances. The company needs a solution that can handle millions of UDP internet traffic requests each second. Which solution will meet these requirements MOST cost-effectively?

A ��Configure an Application Load Balancer with the required protocol and ports for the internet traffic. Specify the EC2 instances as the targets.

B ��Configure a Gateway Load Balancer for the internet traffic. Specify the EC2 instances as the targets.

C ��Configure a Network Load Balancer with the required protocol and ports for the internet traffic. Specify the EC2 instances as the targets.

D ��Launch an identical set of game servers on EC2 instances in separate AWS Regions. Route internet traffic to both sets of EC2

�𰸣�C

������ Correct Answer: C Detailed Explanation: For ultra-low latency and handling millions of UDP requests per second, Network Load Balancer (NLB) is the best choice. Here ��s why: 1. Protocol Support: NLB natively supports UDP (unlike Application Load Balancer, which only handles HTTP/HTTPS). 2. Performance: NLB is designed for extreme performance��it can handle millions of requests per second with minimal latency. 3. Cost-Effectiveness: NLB uses a  ��pay-as-you-go �� model and scales automatically, avoiding over provisioning costs. 4. Preserves Source IP: NLB forwards traffic without altering source IP addresses, which is critical for  game  servers  using  UDP  (e.g.,  for  real-time  player  interactions).  Other  options  fail  because:  -  A  (ALB):  Only  works  for HTTP/HTTPS, not UDP. - B (GWLB): Built for deploying third-party security appliances (e.g., firewalls), not high-volume UDP traffic.
- D (Multi-Region): Unnecessary complexity and cost if low latency is achievable within a single region using NLB. Reference Links: [Network  Load  Balancer  Documentation](https://aws.amazon.com/elastic load balancing/network-load-balancer/)  [Choosing  an AWS Load Balancer](https://aws.amazon.com/elastic load balancing/features/#compare)

612.A company  hosts a  database that  runs  on  an Amazon  RDS  instance that  is  deployed  to  multiple Availability Zones. The company periodically runs a script against the database to report new entries that are added to the database. The script that runs against the database negatively affects the performance of a critical application. The company needs to improve application performance with minimal costs. Which solution will meet these requirements with the LEAST operational overhead?

A ��Add functionality to the script to identify the instance that has the fewest active connections. Configure the script to read from that instance to report the total new entries.

B ��Create a read replica of the database. Configure the script to query only the read replica to report the total new entries. C ��Instruct the development team to manually export the new entries for the day in the database at the end of each day. D ��Use Amazon Elastic ache to cache the common queries that the script runs against the database.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution is to create a read replica. Here's why: 1. Problem: The script causes performance issues because it competes for resources with the critical application on the primary RDS instance. 2. Read Replica:  A  read  replica  is  a  copy  of  the  primary  database  that  handles  read-only  queries.   By  offloading  the  script's  read operations to the replica, the primary instance can focus on the critical application's write/read traffic, improving performance. 3. Minimal Cost/Effort: RDS automates replication, so there ��s no manual maintenance. You only pay for the replica instance, which is cheaper than re architecting the system. 4. Why Other Options Fail: - A: Multi-AZ RDS has a standby replica for failover only (not readable), so this doesn ��t work. - C: Manual exports add operational overhead and delay reporting. - D: Caching (Elastic ache) helps for repeated queries, but scripts pulling new entries likely need fresh data, making caching ineffective. Reference Links: - [Amazon RDS Read Replicas](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_Read repl.html) - [Multi-AZ vs. Read Replicas](https://aws.amazon.com/rds/details/read-replicas/)

613.A  company  is  using  an  Application  Load  Balancer  (ALB)  to  present  its  application  to  the  internet.  The  company  finds abnormal traffic access patterns across the application. A solutions architect needs to improve visibility into the infrastructure to help the company understand these abnormalities better. What is the MOST operationally efficient solution that meets these requirements?

A ��Create a table in Amazon Athena for AWS Cloud trail logs. Create a query for the relevant information.

B ��Enable ALB access logging to Amazon S3. Create a table in Amazon Athena, and query the logs.

C ��Enable ALB access logging to Amazon S3. Open each file in a text editor, and search each line for the relevant information.

D ��Use Amazon EMR on a dedicated Amazon EC2 instance to directly query the ALB to acquire traffic access log information.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The most efficient solution is to enable ALB access logging to Amazon S3 and use Amazon Athena to query the logs.  Here ��s why: - ALB Access  Logs  capture detailed  request-level data (e.g., client  IP, paths, response codes), which is critical for analyzing traffic patterns. - Amazon S3 stores these logs cost-effectively. - Amazon Athena allows SQL-based querying of  logs directly  in S3 without  manual  processing,  saving time  compared to options  like  manually searching  files  (C)  or  complex  setups  like  EMR/EC2  (D).  Option  A  uses  Cloud trail  logs,  which  track  AWS  API  activity,  not application traffic. Option C  is  manual  and  inefficient.  Option  D  over complicates  log  analysis.  Reference  Links:  -  [ALB  Access Logs](https://docs.aws.amazon.com/elastic load balancing/latest/application/load-balancer-access-logs.html)  -  [Query  ALB   Logs with Athena](https://docs.aws.amazon.com/athena/latest/ug/application-load-balancer-logs.html)

614.A company wants to use NAT gateways in its AWS environment. The company's Amazon EC2 instances in private subnets must be able to connect to the public internet through the NAT gateways. Which solution will meet these requirements?

A ��Create public NAT gateways in the same private subnets as the EC2 instances.

B ��Create private NAT gateways in the same private subnets as the EC2 instances.

C ��Create public NAT gateways in public subnets in the same VPCs as the EC2 instances.

D ��Create private NAT gateways in public subnets in the same VPCs as the EC2 instances.

�𰸣�C

�� �� �� Correct Answer:  C. Create  public  NAT gateways  in  public  subnets  in  the same VPCs  as the  EC2  instances.  Detailed Explanation: To allow  EC2  instances  in  private  subnets  to  access the  public  internet  securely, you  must:  1.  Place  public  NAT gateways (not private ones) in public subnets (not private subnets). 2. Public subnets have a route to an internet gateway, which is  required for internet access. 3.  Private subnets  lack internet gateways, so  NAT gateways can ��t work there. 4.  Public  NAT gateways handle outbound internet traffic for private instances, while blocking inbound traffic for security. Private NAT gateways (options B/D) are used for complex private network routing, not public internet access. Options A/B fail because NAT gateways must                  be                  in                  public                  subnets.                  Reference:                  [AWS                  NAT                  Gateway
Basics](https://docs.aws.amazon.com/vpc/latest/user guide/vpc-nat-gateway.html)

615.A company ��s website  hosted on Amazon  EC2 instances  processes classified data stored in Amazon S3.  Due to security concerns, the company requires a private and secure connection between  its  EC2  resources and Amazon S3. Which solution meets these requirements?

A ��Set up S3 bucket policies to allow access from a VPC endpoint.

B ��Set up an IAM policy to grant read-write access to the S3 bucket.

C ��Set up a NAT gateway to access resources outside the private subnet.

D ��Set up an access key I D and a secret access key to access the S3 bucket.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution is to use a VPC endpoint for S3. Here's why: 1. VPC Endpoint creates a private connection between your VPC (where EC2 instances are) and S3. Traffic stays within the AWS network, avoiding the public internet, which  is crucial for security. 2. S3  Bucket Policies can then restrict access to only the VPC endpoint. This ensures only resources in the approved VPC can access the classified data in S3. Why other options are wrong: - B (IAM Policy): IAM controls who (users/roles) can access S3, but doesn't secure the network path. Data could still travel over the public internet.
- C (NAT Gateway): NAT allows private subnets to access the internet, but S3 is an AWS service. Using NAT would force traffic through the public internet, which is less secure. - D (Access Keys): Access keys are for authentication, not network security. They don   �� t    prevent    data    from     being    exposed    during    transmission.    Reference    Links:    -     [AWS    VPC    Endpoints    for S3](https://docs.aws.amazon.com/AmazonS3/latest/user guide/private link-interface-endpoints.html)   -   [Securing   S3   with  VPC Endpoints](https://aws.amazon.com/blogs/security/how-to-restrict-amazon-s3-bucket-access-to-a-specific-vpc/)

616.An ecommerce company runs its application on AWS. The application uses an Amazon Aurora Postgresql cluster in Multi-AZ mode for the underlying database.  During a  recent  promotional campaign, the application experienced  heavy  read  load  and write  load.  Users experienced timeout issues when they attempted to access the application. A solutions architect  needs to make the application architecture  more scalable and highly available. Which solution will  meet these  requirements with the LEAST downtime?

A ��Create an Amazon Event bridge rule that has the Aurora cluster as a source. Create an AWS Lambda function to log the state change events of the Aurora cluster. Add the Lambda function as a target for the Event bridge rule. Add additional reader nodes to fail over to.

B ��Modify the Aurora cluster and activate the zero-downtime restart (ZDR) feature. Use Database Activity Streams on the cluster to track the cluster status.

C ��Add additional reader instances to the Aurora cluster. Create an Amazon RDS Proxy target group for the Aurora cluster.

D �� Create  an Amazon  Elastic ache  for  Redis  cache.  Replicate  data from the  Aurora cluster to  Redis  by  using  AWS  Database Migration Service (AWS DMS) with a write-around approach.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The correct answer is C because it directly addresses both scalability and high availability with minimal downtime. Here's why: 1. Add Reader Instances: Aurora allows you to easily add read replicas (reader nodes)  to  offload   read  traffic  from  the   primary   (writer)  instance.  This   reduces  the   load  on  the  writer   node,   improving performance during heavy read operations. Adding read replicas is a quick, online operation with no downtime. 2. RDS Proxy: Amazon  RDS  Proxy  acts   as  a  connection  pooler,  managing  database  connections   efficiently.  During  spikes   in  traffic   (like promotional campaigns),  RDS  Proxy  prevents  connection overload on the Aurora cluster  by  reusing  existing connections and distributing traffic. This  reduces  timeouts  and  improves  application  responsiveness.  RDS  Proxy  also  routes  traffic  to  healthy instances  automatically,  enhancing  availability.  3.  Why  Other  Options  Are  Less  Ideal:  -  Option  A:  Event bridge  and  Lambda automate adding nodes, but failover to reader nodes isn ��t a standard Aurora feature for write-heavy loads. This adds complexity without directly solving the connection/timeout issue. - Option B: Zero-Downtime Restart (ZDR) applies to maintenance tasks (e.g., parameter updates), not scalability. Database Activity Streams monitor activity but don ��t improve performance. - Option D: Elastic ache  and  DMS  introduce  caching   but  require  significant  application  changes  and  data  synchronization  efforts.  This increases     complexity      and      potential      downtime     during      setup.      Reference      Links:      -      [Amazon     Aurora      Read Replicas](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/Aurora.Replication.html)       -        [Amazon        RDS Proxy](https://aws.amazon.com/rds/proxy/)

617.A company is designing a web application on AWS. The application will use a VPN connection between the company ��s existing data centers and the company's VPCs. The company uses Amazon Route 53 as its DNS service. The application must use private DNS records to communicate with the on-premises services from a VPC. Which solution will meet these requirements in the MOST secure manner?

A ��Create a Route 53 Resolver outbound endpoint. Create a resolver rule. Associate the resolver rule with the VPC.

B ��Create a Route 53 Resolver inbound endpoint. Create a resolver rule. Associate the resolver rule with the VPC. C ��Create a Route 53 private hosted zone. Associate the private hosted zone with the VPC.

D ��Create a Route 53 public hosted zone. Create a record for each service to allow service communication

�𰸣�A

������ Correct Answer A �� Create a Route 53 Resolver outbound endpoint. Create a resolver rule. Associate the resolver rule with the VPC.  Detailed  Explanation The question requires a secure way for a VPC to communicate with on-premises services using private DNS records via a VPN. Here's why Option A is correct: 1. Route 53 Resolver Outbound Endpoint: - An outbound endpoint allows DNS queries from the VPC to be forwarded to on-premises DNS servers over the encrypted VPN connection. This ensures that private DNS records for on-premises services (e.g., `service.internal.company.com`) are resolved securely without exposing   DNS   traffic   to  the   public   internet.   2.   Resolver   Rule:   -   A   resolver   rule   specifies   which   DNS   domains   (e.g., `internal.company.com`) should  be forwarded to the on-premises  DNS servers via the outbound  endpoint. This ensures only relevant queries  are sent over the VPN,  maintaining security and efficiency. 3. Why Other Options  Fail: - Option  B  (Inbound Endpoint): Inbound endpoints allow on-premises systems to resolve DNS records in the VPC (e.g., private hosted zones), not the other  way  around.  -  Option  C  (Private  Hosted  Zone):  Private  hosted  zones  are  for  resolving  DNS  records  within  AWS  (e.g., VPC-specific domains). They can ��t directly resolve on-premises DNS records without additional forwarding. - Option D (Public Hosted  Zone):  Public  hosted  zones  expose   DNS  records  to  the  internet,  which  violates  the   requirement  for   private  DNS communication.                  Reference                   Links                  -                   [Route                  53                   Resolver                  Outbound
Endpoints](https://docs.aws.amazon.com/Route53/latest/Developer guide/resolver-outbound-endpoints.html)    -     [Forwarding DNS                                                              Queries                                                               to                                                              On-Premises
Networks](https://aws.amazon.com/blogs/networking-and-content-delivery/how-to-set-up-dns-resolution-between-on-premise s-networks-and-aws-using-amazon-route-53-resolver/)

618.A company is running a photo hosting service in the us-east-1 Region. The service enables users across multiple countries to upload  and  view  photos.  Some   photos  are  heavily  viewed  for  months,  and  others  are  viewed  for  less  than  a  week.  The application allows uploads of up to 20 MB for each photo. The service uses the photo metadata to determine which photos to display to each user. Which solution provides the appropriate user access MOST cost-effectively?

A ��Store the photos in Amazon Dynamo db. Turn on Dynamo db Accelerator (DAX) to cache frequently viewed items.

B �� Store  the  photos  in  the  Amazon  S3  Intelligent-Tiering  storage  class.  Store  the  photo  metadata  and  its  S3  location  in Dynamo db.

C ��Store the photos in the Amazon S3 Standard storage class. Set up an S3 Lifecycle policy to move photos older than 30 days to the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Use the object tags to keep track of metadata.

D ��Store the photos in the Amazon S3 Glacier storage class. Set up an S3 Lifecycle policy to move photos older than 30 days to the S3 Glacier Deep Archive storage class. Store the photo metadata and its S3 location in Amazon Open search Service.

�𰸣�B

�� �� ��  Correct  Answer  B  Detailed  Explanation  The  best  solution  is  B  because  it  uses  Amazon  S3  Intelligent-Tiering  and Dynamo db effectively for the photo hosting service. Here's why: 1. Amazon S3 Intelligent-Tiering: - Automatically moves photos between storage tiers (e.g., Frequent Access, Infrequent Access) based on access patterns. This is ideal because some photos are heavily viewed for months (frequent access), while others are viewed for less than a week (infrequent access). - Avoids manual lifecycle policies and minimizes costs by optimizing storage tiers without retrieval fees or delays. 2. Dynamo db for Metadata: - Storing photo metadata (e.g., tags, user preferences) in Dynamo db allows fast querying, which is critical for determining which photos to display to users. - Dynamo db scales seamlessly and integrates well with applications needing low-latency access to structured data. 3. Why Other Options Are Less Suitable: - A: Storing photos in Dynamo db is expensive and inefficient for large files (up to 20 MB). DAX only caches metadata, not the photos themselves. - C: S3 Standard + lifecycle policies requires manual tuning (e.g., 30-day threshold) and risks moving frequently accessed photos to Standard-IA, which has retrieval fees. Tag-based metadata management is less efficient than Dynamo db. - D: S3 Glacier is for archival data (retrieval takes hours/days), which is unsuitable     for       photos     that       might      still      be      viewed      within      months.      Reference      Links      -       [Amazon     S3 Intelligent-Tiering](https://aws.amazon.com/s3/storage-classes/intelligent-tiering/)         -          [Amazon          Dynamo db          Use Cases](https://aws.amazon.com/dynamo db/use-cases/)

619.A company  runs  a  highly  available  web  application  on Amazon  EC2  instances  behind  an  Application  Load  Balancer.  The company  uses  Amazon  Cloud watch  metrics.  As  the  traffic  to  the  web  application  increases,  some   EC2  instances  become overloaded with many outstanding requests. The Cloud watch metrics show that the number of requests processed and the time to receive the responses from some EC2 instances are  both higher compared to other  EC2 instances. The company does  not want  new  requests  to  be  forwarded  to  the   EC2   instances  that  are  already  overloaded.  Which  solution  will   meet  these requirements?

A��Use the round robin routing algorithm based on the Request count per target and Active connection count Cloud watch metrics.

B ��Use the least outstanding requests algorithm based on the Request count per target and Active connection count Cloud watch metrics.

C ��Use the round robin routing algorithm based on the Request count and Target response time Cloud watch metrics.

D ��Use the least outstanding requests algorithm based on the Request count and Target response time Cloud watch metrics.

�𰸣�B

�� �� ��  Correct  Answer   B  ��  Use  the   least  outstanding   requests  algorithm   based  on  the   Request count per target  and Active connection count Cloud watch metrics. Detailed Explanation The Application Load Balancer (ALB) has a routing algorithm called least outstanding requests that automatically sends new requests to the EC2 instance with the fewest active/in-progress requests.   This   is   perfect   for   avoiding   overloaded   instances   because   it   prioritizes   instances   with   lighter   workloads.   - Request count per target and Active connection count are Cloud watch metrics that directly track how many requests/connections each EC2 instance is handling right now. These metrics help the ALB make real-time decisions to avoid overloaded instances. - The  least  outstanding  requests  algorithm  uses  these  metrics  to  dynamically  balance  traffic,  unlike  the  round  robin  method (which  blindly rotates  requests without checking instance  load). Options C and D  mention Request count (total requests over time) and Target response time (how slow/fast instances respond). While useful for monitoring, they don ��t directly show current overloaded    states,    making    them    less    effective    for    real-time    routing    decisions.     Reference    Links    -    [ALB    Routing Algorithms](https://docs.aws.amazon.com/elastic load balancing/latest/application/load-balancer-target-groups.html#routing-alg orithms)                                            -                                             [Cloud watch                                             Metrics                                            for
ALB](https://docs.aws.amazon.com/elastic load balancing/latest/application/load-balancer-cloud watch-metrics.html)

620.A company uses Amazon EC2, AWS Fargate, and AWS Lambda to run multiple workloads in the company's AWS account. The company wants to fully make use of its Compute Savings Plans. The company wants to receive notification when coverage of the Compute Savings Plans drops. Which solution will meet these requirements with the MOST operational efficiency?

A ��Create a daily budget for the Savings Plans by using AWS Budgets. Configure the budget with a coverage threshold to send notifications to the appropriate email message recipients.

B ��Create a Lambda function that runs a coverage report against the Savings Plans. Use Amazon Simple Email Service (Amazon SES) to email the report to the appropriate email message recipients.

C ��Create an AWS Budgets report for the Savings Plans budget. Set the frequency to daily.

D ��Create a Savings Plans alert subscription. Enable all notification options. Enter an email address to receive notifications.

�𰸣�A

������ Correct Answer A Explanation The correct solution is to use AWS Budgets to set up a daily budget for Compute Savings Plans with a coverage threshold. AWS Budgets natively supports monitoring Savings Plans coverage and can automatically send email  notifications  when  coverage  drops  below  the  specified  threshold.  This  requires  no  custom  code,  reduces  operational overhead,  and  aligns with  AWS  best  practices.  Options  B  and  D  are  incorrect  because  they  involve  unnecessary  complexity (writing a Lambda function) or refer to non-existent features (Savings Plans alert subscriptions). Option C only generates reports but doesn ��t trigger alerts, failing the requirement for proactive notifications. Reference Link [AWS Budgets for Savings Plans Monitoring](https://docs.aws.amazon.com/cost-management/latest/user guide/budgets-savings-plans.html)

621.A company runs a real-time data ingestion solution on AWS. The solution consists of the most recent version of Amazon Managed  Streaming  for  Apache   Kafka  (Amazon   MSK).  The  solution  is  deployed  in  a  VPC  in  private  subnets  across  three Availability Zones. A solutions architect needs to redesign the data ingestion solution to be publicly available over the internet. The data in transit must also be encrypted. Which solution will meet these requirements with the MOST operational efficiency?

A ��Configure public subnets in the existing VPC. Deploy an MSK cluster in the public subnets. Update the MSK cluster security settings to enable mutual TLS authentication.

B ��Create a new VPC that has public subnets. Deploy an MSK cluster in the public subnets. Update the MSK cluster security settings to enable mutual TLS authentication.

C ��Deploy an Application Load Balancer (ALB) that uses private subnets. Configure an ALB security group inbound rule to allow inbound traffic from the VPC CIDR block for HTTPS protocol.

D��Deploy a Network Load Balancer (NLB) that uses private subnets. Configure an NLB listener for HTTPS communication over the internet.

�𰸣�A

������ Correct Answer: A. Configure public subnets in the existing VPC. Deploy an MSK cluster in the public subnets. Update the MSK cluster security settings to enable mutual TLS authentication. Detailed Explanation: The question requires making an MSK cluster publicly accessible over the internet while encrypting data in transit. Here ��s why Option A is correct: 1. Public Subnets in Existing VPC: MSK clusters can be deployed in public subnets with public IPs, allowing direct internet access. Reusing the existing VPC avoids the operational overhead  of creating a  new VPC  (as  in  Option  B).  2.  Mutual  TLS  (mTLS):  Enabling  mTLS  ensures

encryption for data in transit and adds client authentication, which meets the security requirement. 3. Operational Efficiency: Modifying  the  existing  VPC  (adding  public  subnets)  is  simpler  than  setting  up  load  balancers  (Options  C/D)  or  rebuilding infrastructure in a new VPC (Option B). Why Not Other Options? - Option B: Creating a new VPC adds unnecessary complexity. - Options C/D:  Load  balancers  (ALB/ NLB)  would  require additional configuration  (e.g., TLS termination,  routing) and  introduce latency. NLB in private subnets (Option D) cannot receive public traffic unless placed in public subnets, which the option doesn ��t mention. Reference: [Amazon MSK Public Access](https://docs.aws.amazon.com/msk/latest/developer guide/public-access.html) [MSK Encryption with TLS](https://docs.aws.amazon.com/msk/latest/developer guide/msk-encryption.html)

622.A company wants to migrate an on-premises legacy application to AWS. The application ingests customer order files from an on-premises enterprise resource planning (ERP) system. The application then uploads the files to an SFTP server. The application uses a scheduled job that checks for order files every hour. The company already has an AWS account that has connectivity to the  on-premises  network.  The  new  application  on  AWS  must  support  integration  with  the  existing  ERP  system.  The  new application must be secure and resilient and must use the SFTP protocol to process orders from the ERP system immediately. Which solution will meet these requirements?

A ��Create an AWS Transfer Family SFTP internet-facing server in two Availability Zones. Use Amazon S3 storage. Create an AWS Lambda function to process order files. Use S3 Event Notifications to send s3:Object created:* events to the Lambda function.

B ��Create an AWS Transfer Family SFTP internet-facing server in one Availability Zone. Use Amazon Elastic File System (Amazon EFS) storage. Create an AWS  Lambda function to  process order files.  Use a Transfer  Family  managed workflow to  invoke the Lambda function.

C ��Create an AWS Transfer Family SFTP internal server in two Availability Zones. Use Amazon Elastic File System (Amazon EFS) storage. Create an AWS Step Functions state machine to process order files. Use Amazon Event bridge Scheduler to invoke the state machine to periodically check Amazon EFS for order files.

D ��Create an AWS Transfer Family SFTP internal server in two Availability Zones. Use Amazon S3 storage. Create an AWS Lambda function to process order files. Use a Transfer Family managed workflow to invoke the Lambda function.

�𰸣�D

������ Correct Answer D Detailed Explanation The correct answer is D because it meets all the requirements: 1. SFTP Protocol & Secure Connectivity: The AWS Transfer Family internal SFTP server is deployed within the VPC, ensuring secure connectivity to the on-premises network (via AWS Direct Connect/VPN). An internet-facing server (as in A/B) is unnecessary and less secure. 2. Resilience: The  server  is  deployed  in two Availability Zones  (AZs),  providing  high  availability.  Options  B/C  use  a  single  AZ  or non-resilient storage (EFS in C). 3.  Immediate  Processing: A Transfer  Family  managed workflow triggers the  Lambda function immediately when files are uploaded (event-driven). Options A/C  rely on S3 events (delayed) or hourly polling (Event bridge), which don ��t guarantee immediate processing. 4. Storage: Amazon S3 (D) is more scalable and cost-effective for object storage compared to  EFS  (B/C), which  is designed for shared file systems.  For a  beginner: Think of this as a secure, always-available dropbox (SFTP server) that instantly processes files using serverless functions (Lambda) without manual checks. Reference Links - [AWS                Transfer                 Family](https://aws.amazon.com/aws-transfer-family/)                -                 [Transfer                 Family Workflows](https://docs.aws.amazon.com/transfer/latest/user guide/workflows.html)                              -                               [Lambda
Integration](https://docs.aws.amazon.com/lambda/latest/dg/services-transfer.html)

623.A company ��s applications use Apache Hadoop and Apache Spark to process data on premises. The existing infrastructure is not scalable and is complex to manage. A solutions architect must design a scalable solution that reduces operational complexity. The solution must keep the data processing on premises. Which solution will meet these requirements?

A �� Use AWS Site-to-Site VPN to access the on-premises Hadoop Distributed File System (HDFS) data and application. Use an

Amazon EMR cluster to process the data.

B �� Use AWS Data sync to connect to the on-premises Hadoop Distributed File System (HDFS) cluster. Create an Amazon EMR cluster to process the data.

C ��Migrate the Apache Hadoop application and the Apache Spark application to Amazon EMR clusters on AWS Outposts. Use the EMR clusters to process the data.

D ��Use an AWS Snowball device to migrate the data to an Amazon S3 bucket. Create an Amazon EMR cluster to process the data.

�𰸣�C

�� �� �� Correct Answer: C  Detailed  Explanation: The  company needs to  keep data  processing on-premises while  improving scalability  and  reducing  management  complexity.  AWS   Outposts  allows  running  AWS   services  like  Amazon   EMR  (Elastic Map reduce) directly in the on-premises environment. By migrating Hadoop/Spark workloads to EMR clusters on Outposts, the solution maintains on-premises data processing while benefiting from AWS-managed infrastructure (automatic scaling, simplified cluster management). Other options involve moving data or processing to the cloud (A/B/ D), which violates the keep processing on-premises  requirement.  Outposts  brings  AWS  infrastructure  to  the  customer's  data  center,  fulfilling   all   requirements. Reference Links: - AWS Outposts: https://aws.amazon.com/outposts/ - Amazon EMR: https://aws.amazon.com/emr/

624.A company is migrating a large amount of data from on-premises storage to AWS. Windows, Mac, and Linux based Amazon EC2 instances in the same AWS Region will access the data by using SMB and NFS storage protocols. The company will access a portion of the data routinely. The company will access the remaining data infrequently. The company needs to design a solution to host the data. Which solution will meet these requirements with the LEAST operational overhead?

A��Create an Amazon Elastic File System (Amazon EFS) volume that uses EFS Intelligent-Tiering. Use AWS Data sync to migrate the data to the EFS volume.

B ��Create an Amazon FSx for ONTAP instance. Create an FSx for ONTAP file system with a root volume that uses the auto tiering policy. Migrate the data to the FSx for ONTAP volume.

C �� Create an Amazon S3 bucket that uses S3  Intelligent-Tiering.  Migrate the data to the S3 bucket by using an AWS Storage Gateway Amazon S3 File Gateway.

D ��Create an Amazon FSx for OpenZFS file system. Migrate the data to the new volume.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution here is B (Amazon FSx for ONTAP) because: 1. Multi - protocol support: FSx for ONTAP natively supports both SMB (used by Windows) and NFS (used by Linux/Mac) simultaneously, meeting the requirement for cross - OS access. 2. Automatic tiering: The auto tiering policy automatically moves infrequently accessed data to lower - cost storage tiers while keeping frequently accessed data in performance tiers, optimizing costs without manual intervention. 3. Single storage system: Unlike options requiring multiple services (like C with Storage Gateway), FSx for ONTAP handles everything in one managed service, reducing operational complexity. Why not others: - A (EFS): Only supports NFS, not SMB. - C (S3 + Gateway): Adds complexity with Storage Gateway and isn't ideal for routine file access patterns. - D (OpenZFS): Only supports NFS, not SMB. Reference link: https://aws.amazon.com/fsx/ontap/features/

625.A manufacturing company runs its report generation application on AWS. The application generates each report in about 20 minutes. The application is built as a  monolith that runs on a single Amazon  EC2 instance. The application requires frequent

updates to its tightly coupled modules. The application becomes complex to maintain as the company adds new features. Each time the company patches a software module, the application experiences downtime. Report generation must restart from the beginning after any interruptions. The company wants to redesign the application so that the application can be flexible, scalable, and gradually improved. The company wants to minimize application downtime. Which solution will meet these requirements?

A ��Run the application on AWS Lambda as a single function with maximum provisioned concurrency.

B ��Run the application on Amazon EC2 Spot Instances as micro services with a Spot Fleet default allocation strategy. C ��Run the application on Amazon Elastic Container Service (Amazon ECS) as micro services with service auto scaling.

D ��Run the application on AWS Elastic Beanstalk as a single application environment with an all-at-once deployment strategy.

�𰸣�C

������ Correct Answer C Detailed Explanation The best solution is to use Amazon ECS with micro services and auto scaling. Here �� s why: 1. Micro services Architecture: - The current monolithic application is tightly coupled, making updates and maintenance	 difficult. By breaking it into micro services (small, independent services), each module can be updated, scaled, and maintained	 separately. This eliminates downtime caused by patching the entire monolith. 2. Amazon ECS (Elastic Container Service): - ECS	 allows you to run applications in containers (e.g., Docker). Containers package code and dependencies, making it easier to deploy	 updates  to  specific   modules  without   restarting  the  entire  application.  -   ECS  supports   rolling  deployments,  which   update	 containers incrementally (e.g., one at a time) to minimize downtime. If a deployment fails, ECS can automatically roll back to the	 previous version. 3. Auto Scaling: - Service Auto Scaling adjusts the number of containers  based on demand.  For example, if	 report generation requires more resources, ECS can automatically spin up additional containers to handle the load. This ensures	 scalability and cost efficiency. 4. Avoiding  Restarts: - With  micro services, only the  updated  module  needs to  restart,  not the	 entire application. This prevents report generation from restarting from scratch after a patch. Why the Other Options Fail: - A	 (Lambda with provisioned concurrency): Lambda functions have a 15 - minute timeout, but report generation takes 20 minutes.	 Lambda is unsuitable here. - B (EC2 Spot Instances): Spot Instances can be interrupted, which would force report generation to	 restart. This violates the requirement to avoid restarts. -  D (Elastic  Beanstalk with all - at - once deployment): All - at - once	 deployments  cause  downtime  because  all  instances  are  updated  simultaneously.  Elastic  Beanstalk  also  doesn ��t  solve  the	 monolithic   architecture    problem.    Reference    Links   -    [Amazon    ECS](https://aws.amazon.com/ecs/)   -    [Micro services    on	 AWS](https://aws.amazon.com/micro services/)                                          -                                           [ECS                                           Rolling
Deployments](https://docs.aws.amazon.com/Amazon ecs/latest/developer guide/deployment - type - ecs.html)

626.A company wants to re architect a large-scale web application to a serverless micro services architecture. The application uses Amazon  EC2  instances  and  is  written  in  Python.  The  company  selected  one  component  of the web  application to test  as  a micro service.  The   component  supports  hundreds  of  requests  each  second.  The   company  wants  to   create  and  test  the micro service  on  an  AWS  solution  that  supports   Python.  The  solution   must  also  scale  automatically  and   require  minimal infrastructure and minimal operational support. Which solution will meet these requirements?

A ��Use a Spot Fleet with auto scaling of EC2 instances that run the most recent Amazon Linux operating system.

B ��Use an AWS Elastic Beanstalk web server environment that has high availability configured.

C ��Use Amazon Elastic Ku bernet es Service (Amazon EKS). Launch Auto Scaling groups of self-managed EC2 instances.

D ��Use an AWS Lambda function that runs custom developed code.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The question asks for a serverless solution that automatically scales, supports Python,  and  requires  minimal  infrastructure/operations.  Here's  why  each  option  fits  or  doesn't:  -  A  (Spot  Fleet  +  EC2  Auto Scaling):  While  EC2  Auto  Scaling  reduces  manual  intervention,  Spot  Fleets  still  require  managing  servers,  OS  updates,  and capacity  planning.  Not  fully  serverless.  -  B  (Elastic  Beanstalk):  Elastic  Beanstalk  simplifies  deployment  but  still  relies  on  EC2 instances (or other compute resources) under the hood. You must manage the runtime environment. Not fully serverless. - C (EKS + Self-Managed EC2): Ku bernet es (EKS) adds complexity with cluster management, pods, and node groups. Self-managed EC2 instances require patching, scaling, and maintenance. High operational overhead. - D (Lambda): AWS Lambda is serverless, automatically   scales   to   thousands   of   concurrent   requests,   supports   Python   natively,   and   requires   zero   infrastructure management.  You  only  deploy  code,  and  AWS  handles  scaling,  security  patches,  and  resource  allocation.  This  matches  all requirements:      minimal      infrastructure,      auto-scaling,      and       Python      support.      Reference      Link:       [AWS      Lambda Features](https://aws.amazon.com/lambda/)

627.A company has an AWS Direct Connect connection from its on-premises location to an AWS account. The AWS account has 30 different VPCs in the same AWS Region. The VPCs use private virtual interfaces (VI Fs). Each VPC has a CIDR block that does not overlap with other networks under the company's control. The company wants to centrally manage the networking architecture while still allowing each VPC to communicate with all other VPCs and on-premises  networks. Which solution will  meet these requirements with the LEAST amount of operational overhead?

A ��Create a transit gateway, and associate the Direct Connect connection with a new transit V IF. Turn on the transit gateway's route propagation feature.

B��Create a Direct Connect gateway. Recreate the private VI Fs to use the new gateway. Associate each VPC by creating new virtual private gateways.

C ��Create a transit Vp connect the Direct Connect connection to the transit Vp create a peering connection between all other VPCs in the Region. Update the route tables.

D �� Create AWS Site-to-Site VPN connections from on  premises to each VPC.  Ensure that  both VPN tunnels are  UP for each connection. Turn on the route propagation feature.

�𰸣�A

�� �� �� Correct Answer: A  Detailed  Explanation: The  best  solution  is  A  because  using  a Transit  Gateway allows centralized management of network traffic between  multiple VPCs and on-premises  networks via AWS  Direct Connect.  Here ��s why:  1. Transit Gateway Simplifies Architecture: - A Transit Gateway acts as a central hub, connecting all 30 VPCs and the on-premises network.  Instead of  manually configuring  peering  connections  between all VPCs  (which would  be complex for  30 VPCs), the Transit  Gateway automatically  routes traffic  between  them.  2.  Direct  Connect  Integration  with  a  Transit  V IF:  -  By  creating  a Transit  Virtual  Interface  (V IF)  for  the  Direct  Connect  connection,  the  on-premises  network  can  communicate  with  all  VPCs attached to the Transit Gateway. This avoids recreating private VI Fs for each VPC (as in Option B) or setting up VPNs for every VPC (as  in  Option   D).  3.   Route  Propagation  Reduces  Overhead:  -  Enabling  the  Transit  Gateway  �� s  route  propagation  feature automatically shares routes between VPCs and the on-premises network. This eliminates manual route table updates (required in  Options  C  and  D)  and  ensures  seamless  communication.  Why  Other  Options  Fail:  -  Option  B  (Direct  Connect  Gateway): Requires recreating private V IFs and manually associating each VPC with a virtual private gateway, which is time-consuming for 30 VPCs. - Option C (Transit VPC): Requires complex peering connections between all 30 VPCs, leading to a mesh architecture that is hard to manage. - Option D (Site-to-Site VPN): Setting up VPNs for 30 VPCs creates operational complexity and costs, as each VPN connection requires monitoring. Reference Links: - [AWS Transit Gateway](https://aws.amazon.com/transit-gateway/) - [Direct Connect and Transit Gateway Integration](https://docs.aws.amazon.com/directconnect/latest/User guide/transit-vif.html)

628.A company has applications that run on Amazon EC2 instances. The  EC2 instances connect to Amazon RDS databases  by using an IAM role that has associated policies. The company wants to use AWS Systems  Manager to patch the EC2 instances without disrupting the running applications. Which solution will meet these requirements?

A ��Create a new IAM role. Attach the Amazon ssm managed instance core policy to the new IAM role. Attach the new IAM role to the EC2 instances and the existing IAM role.

B ��Create an IAM user. Attach the Amazon ssm managed instance core policy to the IAM user. Configure Systems Manager to use the IAM user to manage the EC2 instances.

C ��Enable Default Host Configuration Management in Systems Manager to manage the EC2 instances.

D ��Remove the existing policies from the existing IAM role. Add the Amazon ssm managed instance core policy to the existing IAM role.

�𰸣�C

������ Correct Answer: D. Remove the existing policies from the existing IAM role. Add the Amazon ssm managed instance core policy  to the  existing  IAM  role.  Detailed  Explanation:  To  use  AWS  Systems  Manager  (SSM)  for  patching  EC2  instances,  the instances need the Amazon ssm managed instance core policy attached to their IAM role. This policy grants SSM the permissions to manage the instances. - Why not A? EC2 instances can only have one IAM role at a time. You can ��t attach both a new role and the existing  role. - Why  not  B? SSM  requires an  IAM  role  (not  a user)  because  roles are assigned to AWS  resources  like EC2 instances. - Why not C? Default Host Configuration Management is not a valid SSM feature. This is a distractor. - Why D? The existing  IAM  role  must  include  the  Amazon ssm managed instance core  policy.  Instead  of  removing  all  existing  policies  (which might  break  t he  RDS  connection),  the  correct  approach  is to  add t he  SSM  policy  to the  existing  role.  However,  the  answer assumes the existing policies are incompatible and need replacement (this is a nuance in the question's phrasing). Reference: [AWS                                                                                                  Systems                                                                                                  Manager
prerequisites](https://docs.aws.amazon.com/systems-manager/latest/user guide/systems-manager-setting-up.html)

629.A  company  runs  container  applications  by  using  Amazon  Elastic  Ku bernet es  Service  (Amazon  EKS)  and  the  Ku bernet es Horizontal Pod Auto scaler. The workload is not consistent throughout the day. A solutions architect notices that the number of nodes does not automatically scale out when the existing nodes have reached maximum capacity in the cluster, which causes performance issues. Which solution will resolve this issue with the LEAST administrative overhead?

A ��Scale out the nodes by tracking the memory usage.

B ��Use the Ku bernet es Cluster Auto scaler to manage the number of nodes in the cluster.

C ��Use an AWS Lambda function to resize the EKS cluster automatically.

D ��Use an Amazon EC2 Auto Scaling group to distribute the workload.

�𰸣�B

���� �� Correct Answer B �� Use the Ku bernet es Cluster Auto scaler to manage the number of nodes in the cluster. Detailed Explanation The Ku bernet es Cluster Auto scaler (CA) is designed to automatically adjust the number of nodes in an EKS cluster based on pod resource demands. Here's why it ��s the best choice: 1. Works with HPA: The Horizontal Pod Auto scaler (HPA) scales pods  horizontally,  but  when  pods  can ��t  be  scheduled  due  to  insufficient  node  resources  (e.g.,  CPU/memory),  the  Cluster Auto scaler detects this and adds nodes to the cluster. This ensures the workload scales seamlessly without manual intervention.

2. Minimal Overhead: The CA integrates natively with EKS and requires minimal setup. You configure it once, and it automatically manages node scaling based on pod scheduling needs. 3. Other Options: - A (track memory usage) doesn ��t address node scaling ��it ��s redundant if HPA is already handling pod scaling. - C (Lambda function) adds complexity (writing code, setting triggers, permissions). - D (EC2 Auto Scaling) alone can ��t dynamically respond to pod scheduling needs��it scales nodes based on metrics like CPU, not Ku bernet es - specific resource requests. The Cluster Auto scaler is purpose - built for this scenario, ensuring nodes scale  automatically  when   pods  are   pending  due  to   resource  constraints.   Reference  Link  [Amazon  EKS  Cluster  Auto scaler Documentation](https://docs.aws.amazon.com/eks/latest/user guide/cluster - auto scaler.html)

630.A company maintains about 300 TB in Amazon S3 Standard storage month after month. The S3 objects are each typically around 50 GB in size and are frequently replaced with multipart uploads by their global application. The number and size of S3 objects remain constant, but the company's S3 storage costs are increasing each month. How should a solutions architect reduce costs in this situation?

A ��Switch from multipart uploads to Amazon S3 Transfer Acceleration.

B ��Enable an S3 Lifecycle policy that deletes incomplete multipart uploads.

C ��Configure S3 inventory to prevent objects from being archived too quickly.

D ��Configure Amazon Cloud front to reduce the number of objects stored in Amazon S3.

�𰸣�B

���� �� Correct Answer:  B  Detailed  Explanation: The company's increasing S3 storage costs are  likely caused  by incomplete multipart uploads. When a multipart upload is started but not completed, the uploaded parts (fragments of the object) remain in S3 and incur storage costs. These incomplete uploads are not visible in the normal S3 object list but still contribute to storage usage. Option B suggests enabling an S3 Lifecycle policy to delete incomplete multipart uploads. This automatically cleans up orphaned parts after a specified period (e.g., 7 days), eliminating unnecessary storage costs. - Why not other options: - A: S3 Transfer Acceleration optimizes upload speed but doesn ��t reduce storage costs. - C: S3 Inventory tracks objects but doesn ��t directly affect costs. Archiving too quickly is irrelevant here, as the company uses Standard storage (no archiving mentioned). - D: Cloud front caches frequently accessed content but doesn ��t  reduce storage usage in S3.  Reference Links: -  [Automating the cleanup                                                            of                                                            incomplete                                                            multipart
uploads](https://docs.aws.amazon.com/AmazonS3/latest/user guide/mpu-abort-incomplete.html)      -      [Managing       multipart
uploads](https://docs.aws.amazon.com/AmazonS3/latest/user guide/mpu overview.html)

631.A company has deployed a multiplayer game for mobile devices. The game requires live location tracking of players based on latitude and longitude. The data store for the game must support rapid updates and retrieval of locations. The game uses an Amazon RDS for Postgresql DB instance with read replicas to store the location data. During peak usage periods, the database is unable to maintain the performance that is needed for reading and writing updates. The game's user base is increasing rapidly. What should a solutions architect do to improve the performance of the data tier?

A ��Take a snapshot of the existing DB instance. Restore the snapshot with Multi-AZ enabled.

B ��Migrate from Amazon RDS to Amazon Open search Service with Open search Dashboards.

C ��Deploy Amazon Dynamo db Accelerator (DAX) in front of the existing DB instance. Modify the game to use DAX.

D ��Deploy an Amazon Elastic ache for Redis cluster in front of the existing DB instance. Modify the game to use Redis.

�𰸣�D

�� �� �� Correct Answer  D  Detailed  Explanation  The  best  solution  is  to  use  Amazon  Elastic ache  for  Redis,  a  fully  managed in-memory  data  store that  provides  sub-millisecond  latency  for  high-performance  use  cases  like  real-time  location  tracking. Here's  why:  1.  Problem  with  RDS  Postgresql:  Relational  databases  (like  Postgresql)  are  not  optimized  for  rapid,  frequent updates/reads of simple key-value data (e.g., latitude/longitude). During peak usage, this leads to performance bottlenecks. 2. Why Redis? - Redis is an in-memory database, making it 100x faster than disk-based databases for read/write operations. - It excels at handling real-time data (e.g., player locations) with low latency. - Redis supports data structures like geospatial indexes (e.g., `GEOADD`, `Geo radius`), which are perfect for location-based queries. 3. How It Works - Deploy Elastic ache for Redis as a caching layer in front of RDS. - Game servers read/write location data directly to Redis (fast access). - Redis can asynchronously persist  data  to  RDS  Postgresql  for  long-term  storage  (if  needed).  4.  Scalability:  Elastic ache  automatically  scales  to  handle increasing users, avoiding the limitations of RDS read replicas. Why other options are wrong - (A) Multi-AZ: Improves availability (not  performance)  by  creating  a  standby  replica.  Doesn't  solve  read/write  bottlenecks.  -  (B)  Open search:  Designed  for search/analytics, not  high-frequency  updates.  Unnecessary overhead for simple  location tracking. - (C)  DAX: Only works with Dynamo db  (not   RDS).  The  question  doesn't   mention  migrating  to  Dynamo db.   Reference   Link   [Amazon  Elastic ache  for Redis](https://aws.amazon.com/elastic ache/redis/)

632.A company stores critical data in Amazon Dynamo db tables in the company's AWS account. An IT administrator accidentally deleted a Dynamo db table. The deletion caused a significant loss of data and disrupted the company's operations. The company wants to prevent this type of disruption in the future. Which solution will meet this requirement with the LEAST operational overhead?

A ��Configure a trail in AWS Cloud trail. Create an Amazon Event bridge rule for delete actions. Create an AWS Lambda function to automatically restore deleted Dynamo db tables.

B ��Create a backup and restore plan for the Dynamo db tables. Recover the Dynamo db tables manually. C ��Configure deletion protection on the Dynamo db tables.

D ��Enable point-in-time recovery on the Dynamo db tables.

�𰸣�C

������ Correct Answer C �� Configure deletion protection on the Dynamo db tables. Detailed Explanation Deletion protection for Dynamo db tables is a straightforward and effective solution to prevent accidental table deletions. When enabled, this feature blocks any Delete table API call unless deletion protection is explicitly disabled first. This means even if someone tries to delete the table (intentionally or accidentally), the operation will fail, preventing data loss. For a beginner: - Why C is best: It directly blocks accidental deletions with minimal setup. You just enable the setting once, and no further operational effort is needed. - Other options: - A: Requires configuring Cloud trail, Event bridge, and Lambda, which is complex and error-prone. - B:  Manual recovery is slow, risky, and adds operational overhead. -  D:  Point-in-time  recovery (PITR) allows restoring data, but it doesn't prevent    the    deletion     itself.     Recovery    still     takes    time     and    effort.     Reference    Link     [AWS     Dynamo db     Deletion Protection](https://docs.aws.amazon.com/amazon dynamo db/latest/developer guide/Point in time recovery.html#Point in time rec overy.Deletion protection)

633.A company has an on-premises data center that is running out of storage capacity. The company wants to migrate its storage infrastructure to AWS while minimizing bandwidth costs. The solution must allow for immediate retrieval of data at no additional cost. How can these requirements be met?

A ��Deploy Amazon S3 Glacier Vault and enable expedited retrieval. Enable provisioned retrieval capacity for the workload.

B ��Deploy AWS Storage Gateway using cached volumes. Use Storage Gateway to store data in Amazon S3 while retaining copies of frequently accessed data subsets locally.

C �� Deploy AWS Storage Gateway using stored volumes to store data locally. Use Storage Gateway to asynchronously back up point-in-time snapshots of the data to Amazon S3.

D��Deploy AWS Direct Connect to connect with the on-premises data center. Configure AWS Storage Gateway to store data locally. Use Storage Gateway to asynchronously back up point-in-time snapshots of the data to Amazon S3.

�𰸣�B

������ Correct Answer B Detailed Explanation The company needs to migrate storage to AWS while minimizing bandwidth costs and ensuring immediate data retrieval at no additional cost. Here ��s why Option B is the best choice: 1. AWS Storage Gateway with Cached Volumes: - Cached volumes store only frequently accessed data subsets locally on-premises, reducing the need for on-premises storage capacity. The  majority of the data resides in Amazon S3. - This  minimizes  bandwidth costs because only frequently used data is cached locally, and changes are synced incrementally to S3. 2. Immediate Retrieval at No Extra Cost: - Frequently accessed data is available locally for immediate retrieval. - For less frequently accessed data stored in S3, retrieval is still fast  (S3  Standard  offers  millisecond  access)  and  incurs  no  additional  fees  beyond  standard  storage  costs.  3.  Why  Other Options Fail: - A: Amazon S3 Glacier is for archival data, not immediate retrieval. Expedited retrievals incur extra costs. - C/D: Stored volumes (Options C and D) keep all data on-premises and back up snapshots to AWS. This does not reduce on-premises storage       needs,       violating       the        core       requirement.       Reference        Link        [AWS       Storage       Gateway       Cached Volumes](https://aws.amazon.com/storage gateway/cached-volumes/)

634.A company runs a three-tier web application in a VPC across multiple Availability Zones. Amazon EC2 instances run in an Auto  Scaling  group  for the  application  tier. The  company  needs  to  make  an  automated  scaling  plan  that  will  analyze  each resource's daily and weekly historical workload trends. The configuration must scale resources appropriately according to both the  forecast  and  live  changes  in  utilization.  Which  scaling  strategy  should  a  solutions  architect  recommend  to  meet  these requirements?

A ��Implement dynamic scaling with step scaling based on average CPU utilization from the EC2 instances.

B ��Enable predictive scaling to forecast and scale. Configure dynamic scaling with target tracking

C ��Create an automated scheduled scaling action based on the traffic patterns of the web application.

D ��Set up a simple scaling policy. Increase the cool down period based on the EC2 instance startup time.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best approach here is to use predictive scaling along with dynamic scaling (target tracking). Here's why: - Predictive Scaling uses machine learning to analyze your EC2 instance's daily and weekly historical usage  patterns.  It  automatically  forecasts  future traffic  and scales your  resources  in  advance  to  maintain  performance.  This handles the analyze historical trends requirement. - Target Tracking Scaling complements this by handling real-time changes in utilization. It automatically adjusts the number of EC2 instances to keep a specific metric (like CPU usage) close to a target value. This  handles the  live  changes  in  utilization  requirement. Other  options fall short: - A  (Step Scaling): Only  reacts to  real-time metrics, no forecasting. - C (Scheduled Scaling): Requires manual analysis of patterns to create schedules. - D (Simple Scaling): Basic  reactive  scaling  with  delays  (cool down  periods),  no  forecasting.  Predictive  +  Target  Tracking  (B)  gives  both  automated forecasting                            and                             real-time                             adjustments.                            Reference                             Link:

https://docs.aws.amazon.com/auto scaling/ec2/user guide/ec2-auto-scaling-predictive-scaling.html

635.A package delivery company has an application that uses Amazon EC2 instances and an Amazon Aurora MySQL DB cluster. As the application becomes more popular, EC2 instance usage increases only slightly. DB cluster usage increases at a much faster rate. The  company  adds  a  read  replica,  which  reduces  the  DB  cluster  usage  for  a  short  period  of  time.  However,  the  load continues to  increase. The operations that cause the  increase  in  DB  cluster  usage are all  repeated  read  statements that are related to delivery details. The company needs to alleviate the effect of repeated reads on the DB cluster. Which solution will meet these requirements MOST cost-effectively?

A ��Implement an Amazon Elastic ache for Redis cluster between the application and the DB cluster.

B ��Add an additional read replica to the DB cluster.

C ��Configure Aurora Auto Scaling for the Aurora read replicas.

D ��Modify the DB cluster to have multiple writer instances.

�𰸣�A

���� �� Correct Answer: A.  Implement an Amazon Elastic ache for Redis cluster between the application and the DB cluster. Detailed Explanation: The problem involves repetitive read operations overwhelming the Aurora DB cluster. While adding read replicas (Option B/C) temporarily reduces load, it becomes costly as usage grows. Elastic ache for Redis (Option A) is a cache layer that stores frequently accessed data (like delivery details) in memory. This allows repeated read requests to be served directly from the cache instead of querying the database, significantly reducing DB load at a lower cost than continuously scaling replicas. Options B/C are less cost - effective for repetitive reads, and Option D (multi - writer) addresses write scalability, which isn ��t the issue here. Reference Links: - [Amazon Elastic ache Use Cases](https://aws.amazon.com/elastic ache/use - cases/) - [Aurora Read Replicas vs Caching](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/Aurora.Overview.html)

636.A company has an application that uses an Amazon Dynamo db table for storage. A solutions architect discovers that many requests to the table are not returning the latest data. The company's users have not reported any other issues with database performance. Latency is in an acceptable range. Which design change should the solutions architect recommend?

A ��Add read replicas to the table.

B ��Use a global secondary index (GSI).

C ��Request strongly consistent reads for the table.

D ��Request eventually consistent reads for the table.

�𰸣�C

���� �� Correct Answer: C. Request strongly consistent reads for the table. Explanation: Dynamo db offers two types of read consistency: eventually consistent (default) and strongly consistent. - Eventually consistent reads might return slightly outdated data because Dynamo db replicates data across multiple servers for availability. This means your read request might hit a replica that hasn ��t received the latest update yet. This is why users aren ��t seeing the latest data in the problem scenario. - Strongly consistent reads ensure you always get the most up-to-date data by querying the primary storage node. However, this might slightly increase latency or reduce availability in rare cases (like network issues). Since the problem states latency is acceptable, switching to strongly consistent reads is the simplest fix. Why the other options are wrong: - A. Add read replicas: Read replicas

in Dynamo db (Global Tables) replicate data across regions for disaster recovery, but they still use eventual consistency. This won �� t solve the stale data issue. - B. Use a GSI: A Global Secondary Index (GSI) helps query data using alternate keys, but it also uses	 eventual consistency. It doesn ��t address the need for fresh data. - D. Eventually consistent reads: This is already the default   behavior     causing     the      problem.      Switching     to     this      makes     no      sense.      Reference:     [AWS      Dynamo db      Read  Consistency](https://docs.aws.amazon.com/amazon dynamo db/latest/developer guide/How it works.Read consistency.html)

637.A company has deployed its application on Amazon EC2 instances with an Amazon RDS database. The company used the principle  of  least  privilege  to  configure  the  database  access  credentials. The  company's  security  team  wants to  protect  the application and the database from SQL injection and other web-based attacks. Which solution will meet these requirements with the LEAST operational overhead?

A ��Use security groups and network ACLs to secure the database and application servers.

B ��Use AWS WAF to protect the application. Use RDS parameter groups to configure the security settings. C ��Use AWS Network Firewall to protect the application and the database.

D �� Use different database accounts in the application code for different functions. Avoid granting excessive privileges to the database users.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The question asks for a solution to protect against SQL injection and web-based attacks with minimal operational overhead. Here's why option B is the best choice:  1. AWS WAF (Web Application Firewall) is specifically designed to block common web exploits like SQL injection by inspecting HTTP/HTTPS traffic. It uses managed rulesets (e.g., OWASP Top 10) to filter malicious requests automatically, reducing manual configuration. 2. RDS Parameter Groups allow you to enforce security settings (e.g., forcing SSL connections, enabling audit logging) at the database level. While they don ��t directly block SQL injection, they complement AWS WAF by hardening the database ��s security posture. Other options fall short: - A  (Security  Groups/NACLs)  only  control  network traffic,  not  application-layer  attacks.  -  C  (Network  Firewall)  operates  at  the network layer, not the application layer. - D (Database accounts) limits damage after a breach but doesn ��t prevent attacks. AWS WAF requires minimal setup (attach to an Application Load Balancer or Cloud front), making it the most efficient choice.  RDS parameter    groups    are    also    pre-configured    templates,    adding    no    significant    overhead.     Reference    Links:    -    [AWS WAF](https://aws.amazon.com/waf/)                                              -                                               [RDS                                              Parameter
Groups](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_Working with param groups.html)

638.An  ecommerce  company  runs applications  in AWS  accounts that  are  part  of  an organization  in AWS  Organizations. The applications  run  on Amazon Aurora  Postgresql  databases  across  all the accounts. The  company  needs to  prevent  malicious activity  and  must  identify  abnormal  failed  and  incomplete  login  attempts  to the  databases. Which solution will  meet  these requirements in the MOST operationally efficient way?

A ��Attach service control policies (SCPs) to the root of the organization to identity the failed login attempts.

B ��Enable the Amazon RDS Protection feature in Amazon Guard duty for the member accounts of the organization.

C��Publish the Aurora general logs to a log group in Amazon Cloud watch Logs. Export the log data to a central Amazon S3 bucket.

D ��Publish all the Aurora Postgresql database events in AWS Cloud trail to a central Amazon S3 bucket.

�𰸣�B

������ Correct Answer: B Detailed Explanation To detect abnormal failed/incomplete login attempts to Aurora databases in an AWS Organizations setup, enabling Amazon Guard duty's RDS Protection is t he most efficient solution. Here's why: - Guard duty RDS Protection automatically analyzes database activity (like login attempts) using machine learning to flag suspicious behavior (e.g., brute-force attacks, unusual login patterns). It works across all accounts in an organization without manual log analysis. - Why  not  other  options:  -  A:  SCPs  enforce  permission  guardrails  but  cannot  detect  or  log  activity.  -  C/D:  Exporting  logs  to Cloud watch/S3  (via  Cloud trail)  requires  manual  setup  for   parsing,  monitoring,  and  alerting,  which  is  less  efficient  than Guard duty's          automated          threat          detection.           Reference           Links          -           [Amazon          Guard duty          RDS Protection](https://docs.aws.amazon.com/guard duty/latest/ug/rds-protection.html)                 -                   [Guard duty                 vs.
Cloud trail/Cloud watch](https://aws.amazon.com/guard duty/faqs/)

639.A company has an AWS Direct Connect connection from its corporate data center to its VPC in the us-east-1 Region. The company recently acquired a corporation that has several VPCs and a Direct Connect connection between its on-premises data center  and  the  eu-west-2  Region.  The  CIDR  blocks  for  the  VPCs  of  t he  company  and  the  corporation  do  not  overlap.  The company requires connectivity between two Regions and the data centers. The company needs a solution that is scalable while reducing operational overhead. What should a solutions architect do to meet these requirements?

A ��Set up inter-Region VPC peering between the VPC in us-east-1 and the VPCs in eu-west-2.

B ��Create private virtual interfaces from the Direct Connect connection in us-east-1 to the VPCs in eu-west-2 .

C ��Establish VPN appliances in a fully meshed VPN network hosted by Amazon EC2. Use AWS VPN Cloud hub to send and receive data between the data centers and each VPC.

D��Connect the existing Direct Connect connection to a Direct Connect gateway. Route traffic from the virtual private gateways of the VPCs in each Region to the Direct Connect gateway.

�𰸣�D

������ Correct Answer D Detailed Explanation The best solution is to use a Direct Connect Gateway. Here's why: - Direct Connect Gateway allows you to connect multiple AWS Regions and VPCs through a single Direct Connect connection. Both existing Direct Connect connections (from the two companies) can be attached to this gateway. - How It Works: 1. The existing Direct Connect connections from both data centers are linked to the Direct Connect Gateway. 2. Virtual Private Gateways (VGWs) from both Regions (us - east - 1 and eu - west - 2) are associated with the Direct Connect Gateway. 3. Traffic between the data centers and VPCs in both Regions automatically routes through the Direct Connect Gateway, enabling seamless cross - Region communication.
- Scalability: Adding new VPCs or Regions only requires associating their VGWs with the existing Direct Connect Gateway (no new physical connections or complex configurations). - Reduced Overhead: No need to manage VPNs (Option C) or set up multiple peering connections (Option A). Why Other Options Fail: - A: Inter - Region VPC peering only connects two VPCs at a time and doesn ��t connect on - premises data centers. - B: Private virtual interfaces are Region - specific and can ��t directly connect to another Region ��s VPCs. - C: A fully meshed VPN network with EC2 instances adds management complexity and isn ��t as reliable or          scalable          as          a          Direct          Connect          Gateway.          Reference           Link          [AWS          Direct          Connect Gateway](https://docs.aws.amazon.com/directconnect/latest/User guide/direct - connect - gateways.html)

640.A company is developing a mobile game that streams score updates to a backend processor and then posts results on a leaderboard. A  solutions  architect  needs  to  design  a  solution  that  can  handle  large  traffic  spikes,  process  the  mobile  game updates in order of receipt, and store the processed updates in a highly available database. The company also wants to minimize the  management  overhead   required  to  maintain  the  solution.  What  should  the  solutions  architect  do  to  meet  these requirements?

A��Push score updates to Amazon Kinesis Data Streams. Process the updates in Kinesis Data Streams with AWS Lambda. Store the processed updates in Amazon Dynamo db.

B��Push score updates to Amazon Kinesis Data Streams. Process the updates with a fleet of Amazon EC2 instances set up for Auto Scaling. Store the processed updates in Amazon Redshift.

C ��Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe an AWS Lambda function to the SNS topic to process the updates. Store the processed updates in a SQL database running on Amazon EC2.

D ��Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue. Use a fleet of Amazon EC2 instances with Auto Scaling to process the updates in the SQS queue. Store the processed updates in an Amazon RDS Multi-AZ DB instance.

�𰸣�A

���� �� Correct Answer: A Detailed Explanation: The best solution is A because it uses AWS managed services that minimize management overhead while meeting all requirements. Here's why: 1. Kinesis Data Streams: Handles high - volume, real - time data streams while maintaining order of records within each shard. This ensures mobile game updates are processed in the order they're received. 2. AWS  Lambda:  Processes updates server lessly, automatically scaling during traffic spikes without requiring infrastructure  management  (unlike  EC2   in  options   B/D).  3.  Amazon   Dynamo db:  A  fully   managed,  highly  available   NoSQL database  that  scales  automatically,  ideal  for  leaderboards  requiring  fast  reads/writes.  Options  using  EC2  -  based  databases (B/C/D) add management complexity. Other options fail because: - B uses EC2 (management overhead) and Redshift (analytical database,  not  optimal  for  leaderboards).  -  C  uses  SNS  (pub/sub  service,  no  ordering  guarantee)  and  EC2  -  hosted  SQL  DB (management overhead). - D uses SQS (standard queues don't guarantee ordering; FI FO queues have throughput limits) and EC2 (management overhead). Reference Links: - [Amazon Kinesis Data Streams](https://aws.amazon.com/kinesis/data - streams/) - [AWS Lambda](https://aws.amazon.com/lambda/) - [Amazon Dynamo db](https://aws.amazon.com/dynamo db/)

641.A  company  has  multiple  AWS  accounts  with  applications  deployed  in  the  us-west-2  Region.  Application  logs  are  stored within Amazon S3 buckets in each account. The company wants to build a centralized log analysis solution that uses a single S3 bucket. Logs must not leave us-west-2, and the company wants to incur minimal operational overhead. Which solution meets these requirements and is MOST cost-effective?

A ��Create an S3 Lifecycle policy that copies the objects from one of the application S3 buckets to the centralized S3 bucket.

B��Use S3 Same-Region Replication to replicate logs from the S3 buckets to another S3 bucket in us-west-2. Use this S3 bucket for log analysis.

C��Write a script that uses the Put object API operation every day to copy the entire contents of t he buckets to another S3 bucket in us-west-2 . Use this S3 bucket for log analysis.

D �� Write  AWS  Lambda  functions   in  these  accounts  that  are  triggered  every  time   logs  are  delivered  to  the  S3   buckets (s3:Object created:* event). Copy the logs to another S3 bucket in us-west-2. Use this S3 bucket for log analysis.

�𰸣�B

���� �� Correct Answer:  B  Detailed  Explanation: The  best solution  is  B (Use S3 Same-Region  Replication).  Here's why:  1. S3 Same-Region Replication (SRR) automatically copies objects from source S3 buckets (in the same AWS account or across accounts) to  a  destination  bucket  in  the  same  region  (us-west-2).  This  ensures  logs  do  not  leave  the  region.  2.  Minimal  Operational Overhead: SRR is managed by AWS. Once configured, it works without manual intervention or custom scripts. Options C and D require writing and maintaining code (scripts/Lambda), which adds complexity and costs. 3. Cost-Effectiveness: SRR avoids costs

from  unnecessary API  calls  (like  in  C/D). You only  pay for  storage  and  replication  requests,  which  are  minimal  compared  to running Lambda functions or scripts. 4. Cross-Account Support: SRR can replicate logs from S3 buckets in multiple accounts to a centralized bucket, meeting the multi-account requirement. Other options are less ideal: - A: S3 Lifecycle policies cannot copy objects  to  another  bucket;  they  only  manage  storage  classes  or  deletions.  -  C/D:  Custom  scripts  or  Lambda  functions  are error-prone,        incur        higher        costs,        and         require        ongoing        maintenance.        Reference         Links:        -        [S3 Replication](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html)                            -                             [Same-Region
Replication](https://aws.amazon.com/blogs/aws/new-amazon-s3-same-region-replication/)

642.A company has a new mobile app. Anywhere in the world, users can see local news on topics they choose. Users also can post photos and videos from inside the app. Users access content often in the first minutes after the content is posted. New content quickly replaces older content, and then the older content disappears. The local nature of the news means that users consume 90% of the content within the AWS Region where it is uploaded. Which solution will optimize the user experience by providing the LOWEST latency for content uploads?

A ��Upload and store content in Amazon S3. Use Amazon Cloud front for the uploads.

B ��Upload and store content in Amazon S3. Use S3 Transfer Acceleration for the uploads.

C ��Upload content to Amazon EC2 instances in the Region that is closest to the user. Copy the data to Amazon S3.

D �� Upload  and  store content  in Amazon  S3  in the  Region  that  is closest to the  user.  Use  multiple  distributions  of Amazon Cloud front.

�𰸣�B

���� �� Correct Answer:  B.  Upload and store content in Amazon S3.  Use S3 Transfer Acceleration for the  uploads.  Detailed Explanation: The question asks for the lowest-latency solution for global users uploading content, while 90% of the content is consumed within the same AWS Region where it is uploaded. Here ��s why S3 Transfer Acceleration (Option B) is the best choice:
1. Problem with Latency for Global Uploads: Users are spread worldwide, but content must quickly reach the target AWS Region (e.g., if a user in Europe uploads content to an S3 bucket in the US-East Region, the upload could be slow due to long-distance network hops). 2.  How S3 Transfer Acceleration Works: - Transfer Acceleration uses  Cloud front ��s  globally distributed edge locations as entry points for uploads. - When a user uploads a file, it first goes to the nearest edge location (e.g., London for a user in Europe). - From there, AWS ��s optimized backbone network transfers the file to the target S3 bucket (e.g., US-East) much faster than a direct upload over the public internet. - This reduces latency for users far from the target Region. 3. Why Other Options Are Worse: - Option A (Cloud front for uploads): Cloud front is designed for downloading cached content, not optimized for uploads. - Option C (EC2 in closest Region): Uploading to EC2 adds complexity (you must manage servers, storage, and data replication to S3). This is slower and less scalable than using S3 directly. - Option D (Multiple Cloud front distributions): Multiple Cloud front distributions are unnecessary because 90% of content stays in one Region. Cloud front would only help if you needed to distribute content globally, not for uploading. Key Take away: S3 Transfer Acceleration is purpose-built to accelerate uploads to S3 from anywhere in the world, making it the simplest and most effective solution for low-latency uploads. Reference Link: [AWS S3 Transfer Acceleration](https://docs.aws.amazon.com/AmazonS3/latest/user guide/transfer-acceleration.html)

643.A company is building a new application that uses serverless architecture. The architecture will consist of an Amazon API Gateway REST API and AWS Lambda functions to manage incoming requests. The company wants to add a service that can send messages received from the API Gateway REST API to multiple target Lambda functions for processing. The service must offer message filtering that gives the target  Lambda functions the ability to receive only the  messages the functions  need. Which solution will meet these requirements with the LEAST operational overhead?

A ��Send the requests from the API Gateway REST API to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe

Amazon  Simple  Queue  Service  (Amazon  SQS)  queues  to  the  SNS  topic.  Configure  the  target  Lambda  functions  to  poll  the different SQS queues.

B��Send the requests from the API Gateway REST API to Amazon Event bridge. Configure Event bridge to invoke the target Lambda functions.

C ��Send the requests from the API Gateway REST API to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Configure Amazon MSK to publish the messages to the target Lambda functions.

D��Send the requests from the API Gateway REST API to multiple Amazon Simple Queue Service (Amazon SQS) queues. Configure the target Lambda functions to poll the different SQS queues.

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: The best solution is to use Amazon Event bridge because it provides built-in content filtering and routing capabilities. Here's why: 1. Event bridge Rules: You can create rules that filter incoming messages (events) based on their content. For example, a rule could match messages with a specific attribute (e.g., type: order) and route only those messages to a specific Lambda function. This eliminates the need for manual filtering in Lambda or managing multiple queues. 2. Direct Integration: Event bridge can directly invoke Lambda functions without requiring intermediate services like SQS queues. This reduces operational complexity because you don ��t have to set up and manage queues, subscriptions, or polling mechanisms. 3. Scalability   �� Cost: Event bridge automatically scales with your workload and charges based on the number of events processed, making it cost-effective and low-maintenance. Why Other Options Are Less Optimal: - Option A (SNS+SQS): While  SNS  supports  message  filtering,  it  requires  setting  up  SQS  queues,  configuring  subscriptions  with  filter  policies,  and managing Lambda polling. This adds operational overhead. - Option C (MSK): Amazon MSK is overkill for simple message filtering and routing. It ��s designed for complex streaming scenarios, requiring significant setup and management. - Option D (Multiple SQS Queues): API Gateway cannot natively send messages to multiple SQS queues. You ��d need a custom solution (e.g., Lambda to       duplicate       messages),        which       increases        complexity.       Reference        Link:       [Amazon        Event bridge       Event Patterns](https://docs.aws.amazon.com/event bridge/latest/user guide/eb-event-patterns.html)

644.A company migrated millions of archival files to Amazon S3. A solutions architect needs to implement a solution that will encrypt all the archival data  by  using  a customer-provided  key. The  solution  must  encrypt  existing  unencrypted  objects and future objects. Which solution will meet these requirements?

A �� Create a list of unencrypted objects by filtering an Amazon S3 Inventory report. Configure an S3 Batch Operations job to encrypt the objects from the list with a server-side encryption with a customer-provided key (SSE-C). Configure the S3 default encryption feature to use a server-side encryption with a customer-provided key (SSE-C).

B �� Use  S3  Storage  Lens  metrics  to  identify  unencrypted  S3  buckets.  Configure  the  S3  default  encryption  feature  to  use  a server-side encryption with AWS KMS keys (SSE-KMS).

C ��Create a list of unencrypted objects by filtering the AWS usage report for Amazon S3. Configure an AWS Batch job to encrypt the  objects from the  list  with  a  server-side  encryption  with  AWS  KMS  keys  (SSE-KMS).  Configure  the  S3  default  encryption feature to use a server-side encryption with AWS KMS keys (SSE-KMS).

D ��Create a list of unencrypted objects by filtering the AWS usage report for Amazon S3. Configure the S3 default encryption feature to use a server-side encryption with a customer-provided key (SSE-C).

�𰸣�A

������ Correct Answer A Detailed Explanation To meet the requirements of encrypting existing and future objects in Amazon S3 using a customer-provided key (SSE-C), the solution must address two key aspects: 1. Encrypt existing unencrypted objects: Since SSE-C requires the customer to provide the encryption key during upload, existing unencrypted objects must be re-encrypted. This can be done efficiently using S3 Batch Operations, which processes a list of objects (identified via an S3 Inventory report) and applies encryption. 2. Encrypt future objects: Enabling S3 default encryption with SSE-C ensures all newly uploaded objects are automatically encrypted with the customer-provided key. Why Other Options Are Incorrect: - Option B/C: These use SSE-KMS
(AWS-managed keys), which violates the requirement to use a customer-provided key. - Option D: The AWS usage report does not  track  object-level  encryption  status,  and  it  lacks  the  S3   Batch  Operations  step  to  re-encrypt  existing  objects.   Default encryption alone only affects new objects, not existing ones. Key Services: - S3 Inventory: Generates a list of objects and their properties  (e.g.,  encryption  status).  - S3  Batch  Operations:  Re processes  existing  objects  at  scale  (e.g.,  encrypting  them  with SSE-C). - S3  Default  Encryption:  Enforces encryption for all  new objects in the  bucket.  Reference  Links: -  [Amazon S3  Default Encryption](https://docs.aws.amazon.com/AmazonS3/latest/user guide/bucket-encryption.html)      -        [Amazon       S3       Batch Operations](https://docs.aws.amazon.com/AmazonS3/latest/user guide/batch-ops.html)          -            [Using           SSE-C           with S3](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Server side encryption customer keys.html)

645.The DNS provider that hosts a company's domain name records is experiencing outages that cause service disruption for a website running on AWS. The company needs to migrate to a more resilient managed DNS service and wants the service to run on AWS. What should a solutions architect do to rapidly migrate the DNS hosting service?

A ��Create an Amazon Route 53 public hosted zone for the domain name. Import the zone file containing the domain records hosted by the previous provider.

B ��Create an Amazon Route 53 private hosted zone for the domain name. Import the zone file containing the domain records hosted by the previous provider.

C��Create a Simple AD directory in AWS. Enable zone transfer between the DNS provider and AWS Directory Service for Microsoft Active Directory for the domain records.

D �� Create an Amazon Route 53  Resolver inbound endpoint in the VPC. Specify the IP addresses that the provider's DNS will forward  DNS  queries  to.  Configure the  provider's  DNS  to  forward  DNS  queries  for the domain to the  IP  addresses  that  are specified in the inbound endpoint.

�𰸣�A

������ Correct Answer: A. Create an Amazon Route 53 public hosted zone for the domain name. Import the zone file containing the domain  records  hosted  by the  previous  provider.  Detailed  Explanation:  To  migrate  DNS  hosting to AWS  quickly, the  best solution is to use Amazon Route 53, AWS's managed DNS service. Here's why: 1. Public Hosted Zone: A public hosted zone in Route 53 is designed to manage DNS records for domains accessible on the public internet (like a website). A private hosted zone (Option B) is only for internal networks (VPCs), so it ��s irrelevant here. 2. Import Zone File: Route 53 allows you to import an existing DNS zone file (a list of DNS records like A, CNAME, MX, etc.) from the old provider. This avoids manual reconfiguration and ensures rapid migration. 3. Update Name servers: After importing, Route 53 will assign new name servers. The company must update their domain registrar to point to these Route 53 name servers, completing the migration. Other options are incorrect: - Option C (Simple AD) is unrelated to DNS hosting. - Option D (Resolver inbound endpoint) is for hybrid setups (e.g., forwarding queries      from      on       -      premises       to      AWS),       not      full       migration.      Reference       Link:       [Amazon      Route       53 Documentation](https://docs.aws.amazon.com/Route53/latest/Developer guide/Migrating dns.html)

646.A company is building an application on AWS that connects to an Amazon RDS database. The company wants to manage the application configuration and to securely store and retrieve credentials for the database and other services. Which solution will meet these requirements with the LEAST administrative overhead?

A ��Use AWS Appconfig to store and manage the application configuration. Use AWS Secrets Manager to store and retrieve the credentials.

B��Use AWS Lambda to store and manage the application configuration. Use AWS Systems Manager Parameter Store to store and retrieve the credentials.

C ��Use an encrypted application configuration file. Store the file in Amazon S3 for the application configuration. Create another S3 file to store and retrieve the credentials.

D��Use AWS Appconfig to store and manage the application configuration. Use Amazon RDS to store and retrieve the credentials.

�𰸣�A

������ Correct Answer A Detailed Explanation The best solution is A because it uses purpose-built AWS services that minimize administrative  effort:  -  AWS  Appconfig  is  designed  specifically  for  managing  application  configurations.  It  allows  dynamic updates  without  redeploying  code,  supports  environment-specific  settings,  and  integrates  with  other  AWS  services.  -  AWS Secrets Manager securely stores credentials (like RDS passwords), automatically encrypts them, and enables automatic rotation of secrets, reducing manual overhead. Other options are less efficient: - B uses Lambda (not meant for configuration storage) and Parameter Store (lacks built-in rotation). - C relies on manual S3 file management, increasing operational complexity. - D incorrectly     uses      RDS     for      credential     storage,      which      isn't     secure      or     scalable.      Reference      Links     -      [AWS Appconfig](https://aws.amazon.com/systems-manager/features/appconfig/)                       -                        [AWS                        Secrets
Manager](https://aws.amazon.com/secrets-manager/)

647.To meet security requirements, a company needs to encrypt all of its application data in transit while communicating with an Amazon RDS MySQL DB instance. A recent security audit revealed that encryption at rest is enabled using AWS Key Management Service (AWS KMS), but data in transit is not enabled. What should a solutions architect do to satisfy the security requirements?

A ��Enable IAM database authentication on the database.

B ��Provide self-signed certificates. Use the certificates in all connections to t he RDS instance.

C ��Take a snapshot of t he RDS instance. Restore the snapshot to a new instance with encryption enabled.

D ��Download AWS-provided root certificates. Provide the certificates in all connections to t he RDS instance.

�𰸣�D

������ Correct Answer: D Detailed Explanation: To encrypt data in transit between an application and an Amazon RDS MySQL DB instance, SSL/TLS encryption must be enabled. AWS provides root certificates that are managed and regularly updated by AWS Certificate Manager (ACM). These certificates are trusted and eliminate the complexity of managing self-signed certificates. The steps                    involve:                    1.                     Downloading                    AWS-provided                     certificates:                    Available
[here](https://docs.aws.amazon.com/Amazon rds/latest/User guide/Using with rds.SSL.html). 2. Configuring the RDS instance to enforce  SSL:   Modify  the   DB   parameter  group  to   require  SSL   connections.  3.  Updating  application  connections:  Use  the AWS-provided certificates in the application ��s database connection strings to establish encrypted connections. Why not other options? - A: IAM database authentication improves authentication security but doesn ��t encrypt data in transit. - B: Self-signed certificates are error-prone, require manual management, and are less secure than AWS-managed certificates. - C: Snapshots and   restoring   only   affect   encryption   at   rest   (storage),   not   in   transit   (network).   Reference   Link:   [Using   SSL/TLS   with RDS](https://docs.aws.amazon.com/Amazon rds/latest/User guide/Using with rds.SSL.html)

648.A company is designing a new web service that will run on Amazon EC2 instances behind an Elastic Load Balancing (ELB) load balancer. However, many of the web service clients can only reach  IP addresses authorized on their firewalls. What should a solutions architect recommend to meet the clients �� needs?

A ��A Network Load Balancer with an associated Elastic IP address.

B ��An Application Load Balancer with an associated Elastic IP address.

C ��An A record in an Amazon Route 53 hosted zone pointing to an Elastic IP address.

D ��An EC2 instance with a public IP address running as a proxy in front of the load balancer.

�𰸣�A

�� �� �� Correct Answer: A  Detailed  Explanation: The  clients  need  to whitelist specific  IP  addresses  on their firewalls. AWS Application Load Balancers (ALB) and Classic Load Balancers do not support static/Elastic IP addresses��they use dynamic DNS hostnames. A Network Load Balancer (NLB), however, can be assigned Elastic IP addresses (E IPs), providing fixed IPs that clients can whitelist. This makes Option A the best choice. Other options: - B: ALBs cannot use E IPs. - C: Route 53 A records point to DNS names (not fixed IPs), and ALBs/NLBs use dynamic DNS. - D: Adding a proxy instance adds complexity, cost, and a single point of failure. Reference Link: [AWS Load Balancer Features](https://aws.amazon.com/elastic load balancing/features/)

649.A company has established a new AWS account. The account is newly provisioned and no changes have been made to the default settings. The company is concerned about the security of the AWS account root user. What should be done to secure the root user?

A ��Create IAM users for daily administrative tasks. Disable the root user.

B ��Create IAM users for daily administrative tasks. Enable multi-factor authentication on the root user.

C ��Generate an access key for the root user. Use the access key for daily administration tasks instead of the AWS Management Console.

D��Provide the root user credentials to the most senior solutions architect. Have the solutions architect use the root user for daily administration tasks.

�𰸣�B

������ Correct Answer: B. Create IAM users for daily administrative tasks. Enable multi-factor authentication on the root user. Detailed Explanation: The root user has full access to all AWS resources and services in the account, making it the most privileged account. To secure it: 1. Do NOT use the root user for daily tasks (eliminates options C and D). 2. Create IAM users for routine work to follow the principle of least privilege. 3. Enable Multi-Factor Authentication (MFA) on the root user to add an extra layer of security. Why other options are wrong: - A: AWS does not allow disabling the root user. You can only secure it. - C: Root access keys are extremely high-risk and should never be used for daily tasks. - D: Sharing root credentials violates security best practices, regardless                of                seniority.                 Reference:                [AWS                 Root                 User                Security                 Best Practices](https://docs.aws.amazon.com/accounts/latest/reference/root-user-best-practices.html)

650.A financial services company wants to shut down two data centers and migrate more than 100 TB of data to AWS. The data has an  intricate  directory  structure with  millions  of  small  files  stored  in  deep  hierarchies  of  subfolders.  Most  of  the  data  is

unstructured, and the company ��s file storage consists of SMB-based storage types from multiple vendors. The company does not want  to  change  its  applications  to  access  the  data  after   migration.  What  should  a  solutions  architect  do  to  meet  these requirements with the LEAST operational overhead?

A ��Use AWS Direct Connect to migrate the data to Amazon S3.

B ��Use AWS Data sync to migrate the data to Amazon FSx for Lustre.

C ��Use AWS Data sync to migrate the data to Amazon FSx for Windows File Server.

D ��Use AWS Direct Connect to migrate the data on-premises file storage to an AWS Storage Gateway volume gateway.

�𰸣�C

������ Correct Answer: C. Use AWS Data sync to migrate the data to Amazon FSx for Windows File Server. Detailed Explanation: The company needs to migrate SMB-based file storage (common in Windows environments) without changing their applications. Here's why option C is  best:  1. SMB Compatibility: Amazon  FSx for Windows  File Server  natively supports the SMB  protocol, which  matches  the  company's  existing  on-premises  setup. Apps  can  access files the  same  way  after  migration.  2.  Directory Structure   �� Small  Files:  AWS Data sync is optimized for transferring large datasets with millions of files and complex folder hierarchies.  It  handles  metadata  (e.g.,  permissions,  timestamps)  and  small  files  efficiently.  3.  Least  Operational  Overhead: Data sync  automates  data  transfer,   retries  failures,   and  validates   data  integrity.  FSx  for  Windows   requires  minimal  setup compared to Storage Gateway (option D), which adds ongoing management. Why other options are wrong: - A (S3): S3 is object storage, not file storage. Apps would need changes to use S3 APIs. - B (FSx Lustre): Lustre is for high-performance computing (HPC), not SMB. Apps would break. - D (Storage Gateway): This creates a hybrid setup (not full migration) and requires managing a  virtual   appliance,  increasing  operational  work.   Reference   Links:  -  [AWS  Data sync](https://aws.amazon.com/data sync/)  - [Amazon FSx for Windows File Server](https://aws.amazon.com/fsx/windows/)

651.A company uses an organization in AWS Organizations to manage AWS accounts that contain applications. The company sets up a dedicated monitoring member account in the organization. The company wants to query and visualize observability data across the accounts by using Amazon Cloud watch. Which solution will meet these requirements?

A �� Enable  Cloud watch  cross-account  observability  for  the  monitoring  account.  Deploy  an  AWS  Cloud formation  template provided by the monitoring account in each AWS account to share the data with the monitoring account.

B ��Set up service control policies (SCPs) to provide access to Cloud watch in the monitoring account under the Organizations root organizational unit (OU).

C ��Configure a new IAM user in the monitoring account. In each AWS account, configure an IAM policy to have access to query and visualize the Cloud watch data in the account. Attach the new IAM policy to the new IAM user.

D ��Create a new IAM user in the monitoring account. Create cross-account IAM policies in each AWS account. Attach the IAM policies to the new IAM user.

�𰸣�A

�� �� �� Correct Answer: A  Detailed  Explanation: Amazon  Cloud watch Cross-Account Observability  is  designed to centralize monitoring across multiple AWS accounts in an organization. By enabling this feature in the monitoring account and deploying the AWS-provided Cloud formation template in each source account, the monitoring account gains secure access to metrics, logs, and traces from all linked accounts. This approach automates permissions and resource policies, avoiding complex manual IAM

configurations. Other options (B, C, D) rely on error-prone methods like SCPs (which restrict permissions, not grant access) or cross-account  IAM  users  (which  are  less  scalable  and  secure  compared  to  AWS-managed  solutions).  Reference  Links:  -
[Cloud watch                                                                                                                                                                                                Cross-Account
Observability](https://docs.aws.amazon.com/Amazon cloud watch/latest/monitoring/Cross-Account-Observability.html)               -
[Setting                                                                                                Up                                                                                                Cross-Account
Monitoring](https://docs.aws.amazon.com/Amazon cloud watch/latest/monitoring/Cross-Account-Observability-Getting-Set-Up. html)

652.A company ��s website is used to sell products to the public. The site runs on Amazon EC2 instances in an Auto Scaling group behind an Application  Load  Balancer (ALB). There is also an Amazon Cloud front distribution, and AWS WAF is  being used to protect  against SQL  injection  attacks. The ALB  is  the  origin for the  Cloud front  distribution. A  recent  review  of  security  logs revealed an external malicious IP that needs to be blocked from accessing the website. What should a solutions architect do to protect the application?

A ��Modify the network ACL on the Cloud front distribution to add a deny rule for the malicious IP address.

B ��Modify the configuration of AWS WAF to add an IP match condition to block the malicious IP address.

C ��Modify the network ACL for the EC2 instances in the target groups behind the ALB to deny the malicious IP address.

D ��Modify the security groups for the EC2 instances in the target groups behind the ALB to deny the malicious IP address.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution is to block the malicious IP address using AWS WAF. Here's why:
1. AWS WAF Purpose: AWS WAF is designed to protect web applications by inspecting HTTP/HTTPS requests and applying rules (like blocking specific IPs). Since the architecture already uses AWS WAF for SQL injection protection, adding an IP blocking rule here is the most direct and effective method. 2. Traffic Flow: The traffic flows through Cloud front   �� ALB   �� EC2. Network ACLs (A, C) and security groups (D) operate at the network layer (IP/port level). However: - EC2 instances behind the ALB only see the ALB's IP (not the original client IP), so blocking the malicious IP at the EC2 level (C, D) won't work. - Cloud front doesn't use network ACLs (A is invalid because network ACLs are VPC - specific, and Cloud front is a global service). 3. AWS WAF Integration: AWS WAF can be attached to Cloud front or the ALB. Blocking the IP in AWS WAF ensures the malicious request is stopped before reaching the ALB or EC2 instances, regardless of the entry point (Cloud front or ALB). Reference Links: - [AWS WAF I P Matching Conditions](https://docs.aws.amazon.com/waf/latest/developer guide/waf - rule - statement - type - ipmatch.html) - [How AWS WAF Works with Cloud front and ALB](https://aws.amazon.com/waf/faqs/)

653.A company sets up an organization in AWS Organizations that contains 10 AWS accounts. A solutions architect must design a solution to provide access to the accounts for several thousand employees. The company has an existing identity provider (IdP). The company wants to use the existing IdP for authentication to AWS. Which solution will meet these requirements?

A ��Create IAM users for the employees in the required AWS accounts. Connect IAM users to the existing IdP. Configure federated authentication for the IAM users.

B ��Set up AWS account root users with user email addresses and passwords that are synchronized from the existing IdP.

C ��Configure AWS IAM Identity Center (AWS Single Sign-On). Connect IAM Identity Center to the existing IdP. Provision users and groups from the existing IdP.

D ��Use AWS Resource Access Manager (AWS RAM) to share access to the AWS accounts with the users in the existing IdP.

�𰸣�C

������ Correct Answer: C Detailed Explanation: AWS IAM Identity Center (formerly AWS Single Sign-On) is designed to simplify managing access across multiple AWS accounts. It integrates with existing identity providers (IdPs) like Microsoft Active Directory or O kta via SAML.  By connecting  IAM  Identity Center to the company's existing  IdP, employees can log in to AWS using their familiar corporate credentials without creating separate IAM users in each account. This solution scales efficiently for thousands of users and centralizes access control, aligning with AWS best practices. Options A and D are incorrect because they involve manual user management (A) or resource sharing (D), which don ��t address federated authentication. Option  B is  risky and violates AWS security best practices (root users should never be used for daily access). Reference Link: [AWS IAM Identity Center Documentation](https://docs.aws.amazon.com/single sign on/latest/user guide/what-is.html)

654.A solutions architect is designing an AWS Identity and Access Management (IAM) authorization model for a company's AWS account. The company has designated five specific employees to have full access to AWS services and resources  in the AWS account. The solutions architect has created an IAM user for each of the five designated employees and has created an IAM user group. Which solution will meet these requirements?

A��Attach the Administrator access resource-based policy to the IAM user group. Place each of the five designated employee IAM users in the IAM user group.

B ��Attach the System administrator identity-based policy to the IAM user group. Place each of the five designated employee IAM users in the IAM user group.

C ��Attach the Administrator access identity-based policy to the IAM user group. Place each of the five designated employee IAM users in the IAM user group.

D��Attach the System administrator resource-based policy to the IAM user group. Place each of the five designated employee IAM users in the IAM user group.

�𰸣�C

������ Correct Answer: C. Attach the Administrator access identity-based policy to the IAM user group. Place each of the five designated employee  IAM  users  in  the  IAM  user  group.  Detailed  Explanation:  To  grant  full AWS  access, you should  use the Administrator access AWS-managed policy, which is an identity-based policy (attached to IAM users/groups/roles). User groups can't use resource-based policies (those are for AWS resources like S3 buckets). The System administrator policy exists but has narrower  permissions  (e.g.,  EC2/network  management),  not  full  access.  By  attaching  the  identity-based  Administrator access policy    to     the    group,     all     5     users     in    the     group     inherit    full     permissions.     Reference     Link:     [AWS     Managed Policies](https://docs.aws.amazon.com/IAM/latest/User guide/access_policies_managed-vs-inline.html)

655.A company has a nightly batch processing routine that analyzes report files that an on-premises file system receives daily through SFTP. The company wants to move the solution to the AWS Cloud. The solution must be highly available and resilient. The solution also must minimize operational effort. Which solution meets these requirements?

A ��Deploy AWS Transfer for SFTP and an Amazon Elastic File System (Amazon EFS) file system for storage. Use an Amazon EC2 instance in an Auto Scaling group with a scheduled scaling policy to run the batch operation.

B ��Deploy an Amazon EC2 instance that runs Linux and an SFTP service. Use an Amazon Elastic Block Store (Amazon EBS) volume for storage. Use an Auto Scaling group with the minimum number of instances and desired number of instances set to 1.

C �� Deploy an Amazon EC2 instance that runs Linux and an SFTP service. Use an Amazon Elastic File System (Amazon EFS) file system for storage. Use an Auto Scaling group with the minimum number of instances and desired number of instances set to 1.

D �� Deploy AWS Transfer for SFTP and an Amazon S3  bucket for storage.  Modify the application to  pull the  batch files from Amazon S3 to an Amazon EC2 instance for processing. Use an EC2 instance in an Auto Scaling group with a scheduled scaling policy to run the batch operation.

�𰸣�A

������Correct Answer A ��Deploy AWS Transfer for SFTP and an Amazon Elastic File System (Amazon EFS) file system for storage. Use an Amazon  EC2  instance  in  an  Auto Scaling group with  a scheduled  scaling  policy to  run the  batch  operation.  Detailed Explanation The requirements are high availability, resilience, and minimal operational effort. Here ��s why Option A is the best choice:  1. AWS Transfer for SFTP: - This  is a fully  managed SFTP service that automatically  handles  high availability  (HA) and eliminates the need to manually configure or maintain EC2 instances for SFTP. This reduces operational effort. 2. Amazon EFS: - EFS  is  a  shared,  scalable,  and  fully  managed  file  system  that  works  across  multiple  Availability  Zones  (AZs).  It  ensures  high availability and durability for the uploaded files. 3. EC2 with Auto Scaling: - Using an Auto Scaling group with a scheduled scaling policy  ensures  the  EC2  instance  runs  only  during  the  nightly  batch  window,  reducing  costs.  Auto  Scaling  also  guarantees resilience  by  replacing  failed  instances  automatically. Why  Other  Options  Fail:  -  Options  B/C:  Both  use  a  self-managed  SFTP service on  EC2, which  requires  manual setup,  patching,  and  HA  configurations. This  increases operational effort. - Option  D: While S3 is highly durable, modifying the application to pull files from S3 adds development work. EFS (in Option A) allows the existing batch process to work without code changes, as it mimics an on-premises file system. Key Take away: Option A uses fully managed  services  (AWS  Transfer,   EFS,  Auto  Scaling)  to   minimize  operational  overhead  while   ensuring   HA  and   resilience. Reference         Links:         -          [AWS         Transfer         Family](https://aws.amazon.com/aws-transfer-family/)          -         [Amazon EFS](https://aws.amazon.com/efs/) - [EC2 Auto Scaling](https://aws.amazon.com/ec2/auto scaling/)

656.A  company  has  users  all  around  the  world  accessing  its  HTTP-based  application  deployed  on  Amazon  EC2  instances  in multiple AWS Regions. The company wants to improve the availability and performance of the application. The company also wants to protect the application against common web exploits that may affect availability, compromise security, or consume excessive resources. Static IP addresses are required. What should a solutions architect recommend to accomplish this?

A �� Put the  EC2  instances  behind  Network  Load  Balancers  (NLBs)  in  each  Region.  Deploy  AWS  WAF  on  the  NLBs.  Create an accelerator using AWS Global Accelerator and register the NLBs as endpoints.

B �� Put the EC2 instances behind Application Load Balancers (ALBs) in each Region. Deploy AWS WAF on the ALBs. Create an accelerator using AWS Global Accelerator and register the ALBs as endpoints.

C��Put the EC2 instances behind Network Load Balancers (NLBs) in each Region. Deploy AWS WAF on the NLBs. Create an Amazon Cloud front distribution with an origin that uses Amazon Route 53 latency-based routing to route requests to the NLBs.

D �� Put the EC2 instances behind Application Load Balancers (ALBs) in each Region. Create an Amazon Cloud front distribution with  an  origin  that  uses  Amazon  Route  53  latency-based  routing  to  route  requests  to  the  ALBs.  Deploy  AWS  WAF  on  the Cloud front distribution.

�𰸣�B

���� �� Correct Answer:  B Detailed Explanation: To meet the requirements of global availability, performance improvement, protection against web exploits, and static IP addresses, the best solution combines Application Load Balancers (ALBs), AWS WAF, and AWS Global Accelerator. Here��s why: 1. Application Load Balancer (ALB): - ALBs operate at Layer 7 (HTTP/HTTPS), making them ideal for HTTP-based applications. They support advanced routing, SSL termination, and integration with AWS WAF (unlike

Network Load Balancers, which work at Layer 4 and don ��t support WAF directly). - Deploying ALBs in each region ensures traffic is distributed efficiently to EC2 instances. 2. AWS WAF: - AWS WAF protects against common web exploits (e.g., SQL injection, XSS).  It  can  be  directly  deployed  on  ALBs  (or  Cloud front),  blocking  malicious  requests  before  they  reach  the  application.  - Options A and C use NLBs, which cannot host AWS WAF, so they ��re invalid. 3. AWS Global Accelerator: - Global Accelerator provides static  IP addresses and  routes traffic to the  nearest  healthy endpoint  using AWS ��s global  network. This  improves performance  (lower  latency)  and  availability  (failover  between   regions).  -   Registering  ALBs  as  endpoints  ensures  traffic  is optimized globally. - Options C and D use Cloud front + Route 53, which lacks static IPs controlled by the company (Cloud front uses  CDN  edge  IPs,  not  static  IPs  allocated  to  the  user).  4.  Why  Not  Cloud front  (Option  D)?  -  While  Cloud front  improves performance via caching, the requirement for static IP addresses isn ��t fully met here. Global Accelerator explicitly provides static IPs, whereas Cloud front ��s IPs are dynamic (managed by AWS). - AWS WAF on Cloud front (Option D) works but doesn ��t address the         static         IP         need.         Reference         Links:          -         [AWS         Global         Accelerator           -        Static          IP
Addresses](https://docs.aws.amazon.com/global-accelerator/latest/dg/introduction.html)    -     [AWS     WAF     Integration     with ALB](https://docs.aws.amazon.com/waf/latest/developer guide/alb-features.html)           -            [ALB            vs            NLB            Use Cases](https://aws.amazon.com/elastic load balancing/features/)

657.A company ��s data platform uses an Amazon Aurora MySQL database. The database has multiple read replicas and multiple DB instances across different Availability Zones. Users have recently reported errors from the database that indicate that there are too many connections. The company wants to reduce the failover time by 20% when a read replica is promoted to primary writer. Which solution will meet this requirement?

A ��Switch from Aurora to Amazon RDS with Multi-AZ cluster deployment.

B ��Use Amazon RDS Proxy in front of the Aurora database.

C ��Switch to Amazon Dynamo db with Dynamo db Accelerator (DAX) for read connections.

D ��Switch to Amazon Redshift with relocation capability.

�𰸣�B

������ Correct Answer B �� Use Amazon RDS Proxy in front of the Aurora database. Detailed Explanation The problem involves two issues: too many database connections and the need to reduce failover time by 20% when promoting a read replica. Here ��s why Amazon RDS Proxy (Option B) is the best solution: 1. Connection Pooling: RDS Proxy acts as an intermediary layer between the application and the database.  It manages a pool of database connections, reusing them efficiently. This  prevents the too many connections error by reducing the direct load on the database. 2. Failover Acceleration: During a failover (e.g., promoting a read   replica  to  the  primary  writer),  applications  typically   need  to  reconnect  to  the  new  primary  instance.   RDS   Proxy automatically  redirects  connections  to  the  new  primary,  eliminating  the  need  for  applications  to  manually  reconnect.  This reduces downtime and meets the 20% faster failover requirement. 3. Compatibility with Aurora:  RDS  Proxy works seamlessly with Amazon Aurora, requiring no major architectural changes. Other options like switching to Dynamo db (C) or Redshift (D) would force a costly migration and redesign, while switching to RDS Multi-AZ (A) offers slower failover than Aurora. Why Other Options Fail - A (RDS Multi-AZ): Aurora already has faster failover than RDS Multi-AZ. Switching would likely increase failover time.
- C (Dynamo db + DAX):  Migrating to a  NoSQL database is impractical for relational workloads and doesn ��t directly  address connection pooling or failover speed. - D (Redshift): Redshift is a data warehouse, not a transactional database. This would break the existing application. Reference Links - [Amazon RDS Proxy](https://aws.amazon.com/rds/proxy/) - [Aurora Failover with RDS Proxy](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/rds-proxy.html#rds-proxy-failover)

658.A company stores text files in Amazon S3. The text files include customer chat messages, date and time information, and customer personally identifiable information (P II). The company needs a solution to provide samples of t he conversations to an external service provider for quality control. The external service provider needs to randomly pick sample conversations up to

the most recent conversation. The company must not share the customer  P II with the external service provider. The solution must scale when the number of customer conversations increases. Which solution will meet these requirements with the LEAST operational overhead?

A ��Create an Object Lambda Access Point. Create an AWS Lambda function that redacts the P II when the function reads the file. Instruct the external service provider to access the Object Lambda Access Point.

B ��Create a batch process on an Amazon EC2 instance that regularly reads all new files, redacts the P II from the files, and writes the redacted files to a different S3 bucket. Instruct the external service provider to access the bucket that does not contain the P II. B. Create a web application on an Amazon EC2 instance that presents a list of the files, redacts the P II from the files, and allows the external service provider to download new versions of the files that have the P II redacted.

C��Create an Amazon Dynamo db table. Create an AWS Lambda function that reads only the data in the files that does not contain P II. Configure the Lambda function to store the non-PII data in t he Dynamo db table when a new file is written to Amazon S3. Grant the external service provider access to the Dynamo db table.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution is to use an Amazon S3 Object Lambda Access Point with an AWS Lambda function.  Here's why: 1.  Real-Time Redaction: When the external provider accesses files via the Object  Lambda Access Point, the Lambda function automatically removes PII on-the-fly. This ensures they always see the most up-to-date data without delays. 2. Zero Data Duplication: Unlike Option B (which requires copying files to a new bucket), this approach doesn't create redundant data. The original files stay secure in S3. 3. Serverless Scaling: Both Lambda and S3 automatically scale with usage spikes. If conversations increase 10x tomorrow, no infrastructure changes are needed (unlike EC2-based solutions). 4. Least Maintenance: No servers to manage (EC2 in Option B/C requires patching/scaling), no batch scheduling, and no database schema design (Dynamo db in Option C adds complexity for simple file access). Example: Imagine the external provider opens a file. The Lambda acts like a filter - it grabs the original chat log, digitally blacks out sensitive info like credit card numbers, then instantly serves this cleaned version. The  provider  never sees  P II, and your team  never  manages servers.  Reference  Link:  [Amazon  S3 Object Lambda](https://docs.aws.amazon.com/AmazonS3/latest/user guide/transforming-objects.html)

659.A company is running a legacy system on an Amazon EC2 instance. The application code cannot be modified, and the system cannot run on more than one instance. A solutions architect must design a resilient solution that can improve the recovery time for the system. What should the solutions architect recommend to meet these requirements?

A ��Enable termination protection for the EC2 instance.

B ��Configure the EC2 instance for Multi-AZ deployment.

C ��Create an Amazon Cloud watch alarm to recover the EC2 instance in case of failure.

D ��Launch the EC2 instance with two Amazon Elastic Block Store (Amazon EBS) volumes that use RAID configurations for storage redundancy.

�𰸣�C

�� �� �� Correct  Answer:  C.  Create  an  Amazon  Cloud watch  alarm  to  recover  the  EC2  instance  in  case  of  failure.  Detailed Explanation: The application cannot run on multiple instances (eliminating Multi-AZ/load balancing) and cannot be modified. The goal is to reduce recovery time during failures. Here ��s why option C works best: - Option C (Correct): A Cloud watch alarm with EC2   instance   recovery   automatically   detects   hardware/software   failures   and   relaunches   the   instance   on   healthy   AWS

infrastructure. This requires no code changes, uses a single instance, and reduces downtime by automating recovery (typically in minutes). - Option A (Wrong): Termination protection only blocks accidental manual termination, not system failures. - Option B (Wrong): Multi-AZ for EC2 requires multiple instances (against the requirement) and is typically used for databases (e.g., RDS), not standalone EC2. - Option D (Partial): RAID improves storage redundancy but doesn ��t protect against instance-level failures (e.g.,             OS             crash,              network             issues).              Reference              Link:             [Amazon              EC2             Instance Recovery](https://docs.aws.amazon.com/AWSEC2/latest/User guide/ec2-instance-recover.html)

660.A company wants to deploy its containerized application workloads to a VPC across three Availability Zones. The company needs a solution that is highly available across Availability Zones. The solution must require minimal changes to the application. Which solution will meet these requirements with the LEAST operational overhead?

A��Use Amazon Elastic Container Service (Amazon ECS). Configure Amazon ECS Service Auto Scaling to use target tracking scaling. Set the minimum capacity to 3. Set the task placement strategy type to spread with an Availability Zone attribute.

B ��Use Amazon Elastic Ku bernet es Service (Amazon EKS) self-managed nodes. Configure Application Auto Scaling to use target tracking scaling. Set the minimum capacity to 3.

C �� Use Amazon EC2 Reserved Instances. Launch three EC2 instances in a spread placement group. Configure an Auto Scaling group to use target tracking scaling. Set the minimum capacity to 3.

D ��Use an AWS Lambda function. Configure the Lambda function to connect to a VPC. Configure Application Auto Scaling to use Lambda as a scalable target. Set the minimum capacity to 3.

�𰸣�A

�� �� ��  Correct  Answer:  A   Detailed   Explanation:  Amazon   ECS  (Elastic  Container  Service)   is  a  fully   managed   container orchestration service that minimizes operational overhead. By configuring Service Auto Scaling with a minimum capacity of 3 and using the spread placement strategy across Availability Zones, ECS automatically distributes tasks evenly across all three AZs. This ensures  high  availability  with  minimal  manual  intervention.  -  Why  not  B?   EKS  self-managed  nodes  require  managing  the Ku bernet es control plane and worker nodes, increasing operational complexity. - Why not C? EC2 Reserved Instances + spread placement groups involve manual infrastructure management, which is less efficient for containerized workloads. - Why not D? Lambda isn ��t designed for long-running containerized applications and would require significant code changes. Reference Links: - [Amazon  ECS  Task  Placement](https://docs.aws.amazon.com/Amazon ecs/latest/developer guide/task-placement.html)  -   [ECS Service Auto Scaling](https://docs.aws.amazon.com/Amazon ecs/latest/developer guide/service-auto-scaling.html)

661.A media company stores movies in Amazon S3. Each movie is stored in a single video file that ranges from 1 GB to 10 GB in size. The company must be able to provide the streaming content of a movie within 5 minutes of a user purchase. There is higher demand for movies that are less than 20 years old than for movies that are  more than 20 years old. The company wants to minimize hosting service costs based on demand. Which solution will meet these requirements?

A ��Store all media content in Amazon S3. Use S3 Lifecycle policies to move media data into the Infrequent Access tier when the demand for a movie decreases.

B ��Store newer movie video files in S3 Standard. Store older movie video files in S3 Standard-infrequent Access (S3 Standard-IA). When a user orders an older movie, retrieve the video file by using standard retrieval.

C ��Store newer movie video files in S3 Intelligent-Tiering. Store older movie video files in S3 Glacier Flexible Retrieval. When a user orders an older movie, retrieve the video file by using expedited retrieval.

D��Store newer movie video files in S3 Standard. Store older movie video files in S3 Glacier Flexible Retrieval. When a user orders an older movie, retrieve the video file by using bulk retrieval.

�𰸣�B

�� �� �� Correct Answer  B  Detailed  Explanation  The  company  needs  to  stream  movies  within  5  minutes  of  purchase  while minimizing costs based on demand. Here's why option B works best: 1. Newer Movies (Higher Demand): Stored in S3 Standard, which offers low-latency access and is cost-effective for frequently accessed data. This ensures fast streaming for high-demand content. 2. Older Movies (Lower Demand): Stored in S3 Standard-IA (Infrequent Access), which has lower storage costs than S3 Standard. Since older movies are accessed less often, this reduces storage costs without sacrificing performance. Retrieving older movies via standard retrieval (instant access) ensures the 5-minute streaming requirement is met. 3. Why Other Options Fail: - A: S3 Intelligent-Tiering automatically moves data between tiers, but retrieval costs for older tiers (like IA) might add up. Explicitly separating tiers (as in B) is more predictable and cost-efficient. - C: Using S3 Glacier Flexible Retrieval for older movies would require expedited retrieval (1 - 5 minutes) to meet the 5-minute requirement, which is expensive for low-demand content. - D: Glacier ��s bulk retrieval takes 5 - 12 hours, violating the 5-minute streaming requirement. Reference Links - [Amazon S3 Storage Classes](https://aws.amazon.com/s3/storage-classes/)                           -                           [S3                           Standard                           vs.
Standard-IA](https://docs.aws.amazon.com/AmazonS3/latest/user guide/storage-class-intro.html)

662.A solutions architect needs to design the architecture for an application that a vendor provides as a Docker container image. The container needs 50 GB of storage available for temporary files. The infrastructure must be serverless. Which solution meets these requirements with the LEAST operational overhead?

A ��Create an AWS Lambda function that uses the Docker container image with an Amazon S3 mounted volume that has more than 50 GB of space.

B �� Create an AWS Lambda function that uses the Docker container image with an Amazon Elastic Block Store (Amazon EBS) volume that has more than 50 GB of space.

C �� Create  an Amazon  Elastic  Container Service  (Amazon  ECS)  cluster  that  uses the AWS  Fargate  launch type.  Create  a task definition for the container image with an Amazon  Elastic  File System  (Amazon  EFS) volume. Create a service with that task
definition.

D ��Create an Amazon Elastic Container Service (Amazon ECS) cluster that uses the Amazon EC2 launch type with an Amazon Elastic Block Store (Amazon EBS) volume that has more than 50 GB of space. Create a task definition for the container image. Create a service with that task definition.

�𰸣�C

������ Correct Answer C Detailed Explanation The question requires a serverless solution with 50GB of temporary storage and minimal operational overhead. - Option A/B (Lambda with S3/EBS) are invalid because Lambda has a maximum /tmp storage limit of  10GB, and  Lambda cannot natively  mount  EBS/S3 as block storage. - Option  D (ECS  EC2) is  not serverless  because it requires managing EC2 instances, increasing operational effort. - Option C (ECS Fargate + EFS) is correct because: - AWS Fargate is serverless  (no  EC2  management).  -  Amazon  EFS  provides  scalable,  shared  file  storage  that  can  be  mounted  to  ECS  tasks.  It supports  dynamic  scaling  beyond  50GB,  meeting  the  temporary  storage  requirement.  -  EFS  is  fully  managed,  reducing operational work.  EFS is ideal here because it ��s a serverless file system that  integrates seamlessly with  Fargate, eliminating infrastructure management while providing flexible storage. Reference Links - [AWS Fargate](https://aws.amazon.com/fargate/) - [Amazon EFS with ECS](https://docs.aws.amazon.com/Amazon ecs/latest/developer guide/efs-volumes.html)

663.A company needs to use its on-premises LDAP directory service to authenticate its users to the AWS Management Console.

The  directory  service  is  not  compatible  with  Security  Assertion  Markup  Language  (SAML).  Which  solution  meets  these requirements?

A ��Enable AWS IAM Identity Center (AWS Single Sign-On) between AWS and the on-premises LDAP.

B ��Create an IAM policy that uses AWS credentials, and integrate the policy into LDAP.

C ��Set up a process that rotates the IAM credentials whenever LDAP credentials are updated.

D ��Develop an on-premises custom identity broker application or process that uses AWS Security Token Service (AWS STS) to get short-lived credentials.

�𰸣�D

�� �� �� Correct Answer:  D  Detailed  Explanation: The  correct  solution  is to  develop a custom  identity  broker that acts as a middleman  between  the  on-premises  LDAP  and  AWS.  Here's  why:  1.  LDAP  Isn't  SAML-Compatible:  Since  t he  LDAP  service doesn't support SAML (required for direct federation with AWS SSO/IAM Identity Center), you can't use options like AWS IAM Identity Center (Option A) directly. 2. How the Custom Broker Works: - Users authenticate first with the on-premises LDAP (their existing  credentials).  -  The  custom  broker  validates  t he  LDAP  credentials.  -  Once  authenticated,  the  broker  uses  AWS  STS (Security Token Service) to request short-term AWS credentials (like temporary IAM roles). - These temporary credentials allow users to access the AWS  Management Console without  needing  long-term  IAM  users  or SAML. 3. Why Other Options  Fail: - Option A: AWS  IAM  Identity Center (SSO)  requires SAML or a supported identity source (like AWS Managed AD).  LDAP alone won't work here. - Option B: IAM policies control permissions, not authentication. You can't integrate LDAP credentials into IAM policies directly. - Option C: Rotating IAM credentials manually or via scripts is error-prone and doesn't solve the core problem of LDAP-based authentication. Key Benefits of Option D: - No SAML required. - Uses temporary credentials (secure, no long-term keys).     -       Integrates      seamlessly      with      existing      LDAP      user       management.      Reference      Links:      -      [AWS      STS Documentation](https://docs.aws.amazon.com/STS/latest/Api reference/welcome.html)        -        [Custom        Identity        Broker Architecture](https://docs.aws.amazon.com/IAM/latest/User guide/id_roles_providers_enable-console-custom-url.html)

664.A company stores  multiple Amazon  Machine  Images (AMIs) in an AWS account to launch its Amazon  EC2 instances. The AMIs  contain  critical  data  and  configurations  that  are  necessary  for  the  company �� s  operations.  The  company  wants  to implement  a  solution  that  will   recover  accidentally  deleted  AMIs  quickly  and  efficiently.  Which  solution  will   meet  these requirements with the LEAST operational overhead?

A ��Create Amazon Elastic Block Store (Amazon EBS) snapshots of the AMIs. Store the snapshots in a separate AWS account.

B ��Copy all AMIs to another AWS account periodically.

C ��Create a retention rule in Recycle Bin.

D ��Upload the AMIs to an Amazon S3 bucket that has Cross-Region Replication.

�𰸣�C

������ Correct Answer: C. Create a retention rule in Recycle Bin. Detailed Explanation: The simplest and most efficient solution is to use AWS Recycle Bin. When enabled, Recycle Bin retains deleted AMIs for a configurable period (e.g., 7 days). This requires minimal effort: 1. Enable Recycle Bin once. 2. Create a retention rule to specify how long deleted AMIs should be kept. 3. If an AMI is accidentally deleted, restore it directly from the Recycle Bin with a few clicks. Other options involve higher operational overhead:  -  A   (EBS  snapshots   in  another  account):   Requires   manual  snapshot  creation,  cross-account   management,  and

rebuilding AMIs from snapshots. - B (Copy AMIs to another account): Needs periodic copying and cross-account coordination. - D (S3 with  cross-region  replication): AMIs  aren ��t stored  in S3; this  approach  is  invalid.  Recycle  Bin  automates  retention  and recovery,    aligning    with    the    least    operational    overhead     requirement.    Reference    Links:    -     [AWS    Recycle    Bin    for AMIs](https://docs.aws.amazon.com/AWSEC2/latest/User guide/recycle-bin.html)

665.A company has 150 TB of archived image data stored on-premises that needs to be moved to the AWS Cloud within the next month. The company ��s current network connection allows up to 100 Mbps uploads for this purpose during the night only. What is the MOST cost-effective mechanism to move this data and meet the migration deadline?

A ��Use AWS Snowmobile to ship the data to AWS.

B ��Order multiple AWS Snowball devices to ship the data to AWS.

C ��Enable Amazon S3 Transfer Acceleration and securely upload the data.

D ��Create an Amazon S3 VPC endpoint and establish a VPN to upload the data.

�𰸣�B

���� �� Correct Answer: B. Order multiple AWS Snowball devices to ship the data to AWS. Detailed Explanation: The company needs to move 150 TB of data within a month. Let's analyze the options: 1. Network Speed Limitation: - Upload speed = 100 Mbps (12.5 MB/s). - Night-only uploads (assume 8 hours/night): Daily upload = 12.5 MB/s   �� 28,800  seconds =  360 GB/day. Monthly upload = 360 GB/day   �� 30  days = 10.8 TB/ month. - 150 TB would take ~14 months��far exceeding the deadline. - Eliminates C (Transfer Acceleration) and D (VPN) since both rely on network uploads. 2. AWS Snowball: - Each Snowball device holds up to 80 TB. - 150 TB requires 2 Snowball devices. - Process: Copy data to Snowball  �� ship to AWS   �� AWS uploads to S3. - Total time: ~1-2 weeks (shipping + data transfer), meeting the 1-month deadline. - Cost-effective for large datasets (avoids slow, expensive network transfers). 3. AWS Snowmobile (A): - Designed for exabyte-scale data (1,000+ TB). - Overkill for 150 TB (high cost,  logistical complexity). Conclusion: Snowball (B)  is the fastest and  most cost-effective solution for  150 TB within a month. Network-based methods (C/D) are too slow, and Snowmobile (A) is impractical for this scale.  Reference Links: -  [AWS Snowball](https://aws.amazon.com/snowball/)   -   [AWS   Snowmobile](https://aws.amazon.com/snowmobile/)   -    [Amazon   S3 Transfer Acceleration](https://aws.amazon.com/s3/transfer-acceleration/)

666.A company wants to migrate its three-tier application from on premises to AWS. The web tier and the application tier are running on third-party virtual  machines  (VMs).  The  database tier  is  running  on  MySQL.  The  company  needs  to  migrate  the application by making the fewest possible changes to the architecture. The company also needs a database solution that can restore data to a specific point in time. Which solution will meet these requirements with the LEAST operational overhead?

A��Migrate the web tier and the application tier to Amazon EC2 instances in private subnets. Migrate the database tier to Amazon RDS for MySQL in private subnets.

B �� Migrate the web tier to Amazon EC2 instances in  public subnets.  Migrate the application tier to  EC2 instances in  private subnets. Migrate the database tier to Amazon Aurora MySQL in private subnets.

C �� Migrate the web tier to Amazon  EC2 instances  in public subnets.  Migrate the application tier to EC2 instances in  private subnets. Migrate the database tier to Amazon RDS for MySQL in private subnets.

D ��Migrate the web tier and the application tier to Amazon EC2 instances in public subnets. Migrate the database tier to Amazon Aurora MySQL in public subnets.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution is B because it optimizes architecture with minimal changes and reduces operational overhead: 1. Web Tier in Public Subnet: Front-end servers need internet access (e.g., user traffic). Public subnets allow direct internet connectivity via EC2 instances with Elastic IPs or public IPs (no unnecessary architectural changes).
2. App Tier in Private Subnet: Middle-tier servers shouldn��t be directly exposed to the internet. Private subnets enhance security while allowing communication via the web tier (preserves existing tiered design). 3. Aurora MySQL for Database: - Point-in-Time Recovery: Aurora automatically backs up data to S3 and retains restore capability to any second within the backup retention period (1 - 35 days), meeting the recovery requirement. - Lower Operational Overhead: Aurora handles automatic storage scaling, patching, backups, and replication. Compared to RDS MySQL, Aurora offers better performance and cost - efficiency at scale with minimal management. - MySQL Compatibility: Aurora is fully compatible with MySQL, so no code/database schema changes are needed (minimizes migration effort). Why Not Other Options: - A: Web tier in private subnet breaks internet accessibility unless a NAT/load balancer is added (adds complexity). - C: RDS MySQL works but requires more manual storage management vs. Aurora.
-     D:     Placing     the     database     in     a     public     subnet     is     a     security     risk.     Reference     Links:     -      [Amazon     Aurora Features](https://aws.amazon.com/rds/aurora/features/)                                   -                                    [AWS                                    Subnet Basics](https://docs.aws.amazon.com/vpc/latest/user guide/VPC_Subnets.html)

667.A development team is collaborating with another company to create an integrated product. The other company needs to access an Amazon Simple Queue Service (Amazon SQS) queue that is contained in the development team's account. The other company wants to poll the queue without giving  up its own account permissions to do so.  How should a solutions architect provide access to the SQS queue?

A ��Create an instance profile that provides the other company access to the SQS queue.

B ��Create an IAM policy that provides the other company access to the SQS queue.

C ��Create an SQS access policy that provides the other company access to the SQS queue.

D ��Create an Amazon Simple Notification Service (Amazon SNS) access policy that provides the other company access to the SQS queue.

�𰸣�C

���� �� Correct Answer C �� Create an SQS access policy that provides the other company access to the SQS queue. Detailed Explanation To allow cross-account access to an Amazon SQS  queue without  requiring the other company to  use their  own account permissions, you should use an SQS queue access policy (a resource-based policy). Here's why: 1. Resource-Based Policy:
- SQS queues support resource-based policies (attached directly to the queue). These policies specify which AWS accounts or users (even from other accounts) can perform actions like Send message, Receive message, or Delete message on the queue. - The development team can write a policy allowing the other company ��s AWS account ID to access the queue. The other company �� s applications can then use their own IAM credentials (e.g.,  EC2 instance roles) to poll the queue, as long as their account is explicitly allowed in the SQS policy. 2. Why Not IAM Policies (Option B)? - IAM policies are identity-based (attached to users/roles  in your own account). Since the other company isn ��t using your account ��s IAM roles, this won ��t work for cross-account access.
3. Why Not SNS (Option D)? - SNS access policies are unrelated unless the SQS queue is subscribed to an SNS topic. The question doesn ��t mention SNS, so this is irrelevant. 4. Instance Profile (Option A)? - Instance profiles are IAM roles for EC2 instances. The other company isn ��t running resources in your account, so this is not applicable. Example SQS Access Policy: ```json { Version: 2012-10-17,   Statement:    [{    Effect:   Allow,    Principal:    {   AWS:    Other company account id    },   Action:    [sqs:Receive message, sqs:Delete message],  Resource:  arn:aws:sqs:region:your-account-id:queue-name  }]  }  ```   Key  Take away:   SQS   resource-based policies are the standard way to grant cross-account access without sharing  IAM  credentials or  requiring the  other  party to modify        their         own          account         permissions.         Reference         Links         -         [Amazon         SQS          Resource-Based

Policies](https://docs.aws.amazon.com/Aws simple queue service/latest/Sqs developer guide/sqs-resource-based-policies.html) - [Cross-Account Access with SQS](https://aws.amazon.com/blogs/aws/amazon-sqs-cross-account-access/)

668.A company ��s developers want a secure way to gain SSH access on the company's Amazon EC2 instances that run the latest version of Amazon Linux. The developers work remotely and in the corporate office. The company wants to use AWS services as a part of the solution. The EC2 instances are hosted in a VPC private subnet and access the internet through a NAT gateway that is deployed in a public subnet. What should a solutions architect do to meet these requirements MOST cost-effectively?

A ��Create a bastion host in the same subnet as the EC2 instances. Grant the ec2:Create vpn connection IAM permission to the developers. Install EC2 Instance Connect so that the developers can connect to the EC2 instances.

B ��Create an AWS Site-to-Site VPN connection between the corporate network and the VPC. Instruct the developers to use the Site-to-Site  VPN  connection  to  access  the  EC2  instances  when  the  developers  are  on  the  corporate  network.  Instruct  the developers to set up another VPN connection for access when they work remotely.

C ��Create a bastion host in the public subnet of the Vp configure the security groups and SSH keys of t he bastion host to only allow connections and SSH authentication from the developers �� corporate and remote networks. Instruct the developers to connect through the bastion host by using SSH to reach the EC2 instances.

D��Attach the Amazon ssm managed instance core IAM policy to an IAM role that is associated with the EC2 instances. Instruct the developers to use AWS Systems Manager Session Manager to access the EC2 instances.

�𰸣�D

������Correct Answer D Detailed Explanation The best solution is to use AWS Systems Manager (SSM) Session Manager because it  provides  secure,  no-cost SSH access without  needing  public  IPs,  bastion  hosts,  or  VPNs.  Here's  why:  1.  Security: - Session Manager uses AWS IAM for authentication and encrypts all traffic, eliminating the need to open SSH ports (port 22) in security groups. - EC2 instances in private subnets can communicate with SSM via the NAT gateway. 2. Cost-Effectiveness: - No need to pay for bastion hosts, VPNs, or additional infrastructure. - Only requires attaching the Amazon ssm managed instance core IAM policy to the EC2 instances. 3. Simplicity: - Developers connect via the AWS CLI or Console with one command (e.g., aws ssm start-session  --target  [instance-id]).  -  Works  for  both  remote  and  corporate  office  users  without  extra  setup.  4.  Why  Other Options Are Less Ideal: - A: Requires a bastion host (costly) and risky VPN permissions. - B: Managing two VPNs (Site-to-Site + remote  VPN)  is  complex  and  expensive.  -  C:  Bastion  hosts  in  public  subnets  expose  attack  surfaces  and  add  maintenance. Reference                                               Links                                              -                                               [AWS                                               Session
Manager](https://docs.aws.amazon.com/systems-manager/latest/user guide/session-manager.html)  -   [SSM   Managed  Instance Core
Policy](https://docs.aws.amazon.com/systems-manager/latest/user guide/security-iam-aws man pol.html#aws-managed-policy-a mazon ssm managed instance core)

669.A  pharmaceutical  company  is  developing  a  new  drug.  The  volume   of  data  that  the   company  generates   has  grown exponentially  over  the  past  few  months.  The  company's  researchers  regularly  require  a  subset  of  the  entire  dataset  to  be immediately available with minimal lag. However, the entire dataset does not need to be accessed on a daily basis. All the data currently  resides  in  on-premises  storage arrays,  and the company wants to  reduce  ongoing capital expenses. Which storage solution should a solutions architect recommend to meet these requirements?

A ��Run AWS Data sync as a scheduled cron job to migrate the data to an Amazon S3 bucket on an ongoing basis.

B��Deploy an AWS Storage Gateway file gateway with an Amazon S3 bucket as the target storage. Migrate the data to the Storage Gateway appliance.

C �� Deploy an AWS Storage Gateway volume gateway with cached volumes with an Amazon S3 bucket as the target storage. Migrate the data to the Storage Gateway appliance.

D ��Configure an AWS Site-to-Site VPN connection from the on-premises environment to AWS. Migrate data to an Amazon Elastic File System (Amazon EFS) file system.

�𰸣�C

������ Correct Answer: C. Deploy an AWS Storage Gateway volume gateway with cached volumes with an Amazon S3 bucket as the target  storage.  Migrate  the  data to the  Storage  Gateway  appliance.  Detailed  Explanation:  The  pharmaceutical  company needs immediate access to a subset of data (low latency) while storing the full dataset cost-effectively. Here ��s why Volume Gateway with cached volumes (Option C) is the best fit: 1. Cached Volumes: - This mode stores the entire dataset in Amazon S3 (low-cost cloud storage) but keeps frequently accessed data cached locally on the Storage Gateway appliance. - Researchers get minimal lag when accessing the subset they need (cached locally), while infrequently accessed data stays in S3. - This reduces on-premises storage costs (no need to keep the full dataset locally). 2. Capital Expense Reduction: - The company can replace expensive on-premises  storage arrays with  cheaper  cloud storage  (S3). - AWS Storage Gateway operates  on a  pay-as-you-go model, shifting costs from  upfront capital expenses  (CapEx) to operational expenses  (OpEx).  3. Why  Not Other Options? - A (Data sync + S3): S3 alone  isn ��t  ideal for  low-latency access.  Data sync  is for  batch  transfers,  not  real-time  access. -  B  (File Gateway): File Gateway is for file-based access (e.g., NFS/SMB), but cached volumes (block storage) offer better performance for applications expecting  disk-like access. -  D  (EFS  over VPN):  EFS  is  a  cloud-only file  system. Accessing  it  over VPN  introduces latency,   which   contradicts   the     �� minimal   lag   ��  requirement.   Reference    Links:   -   [AWS   Storage   Gateway   Volume Gateway](https://docs.aws.amazon.com/storage gateway/latest/user guide/Storage gateway concepts.html#volume-gateway-con cepts) - [Cached Volumes vs. Stored Volumes](https://aws.amazon.com/storage gateway/volume-gateway/faqs/)

670.A company has a business-critical application that runs on Amazon EC2 instances. The application stores data in an Amazon Dynamo db table. The company must be able to revert the table to any point within the last 24 hours. Which solution meets these requirements with the LEAST operational overhead?

A ��Configure point-in-time recovery for the table.

B ��Use AWS Backup for the table.

C ��Use an AWS Lambda function to make an on-demand backup of the table every hour.

D ��Turn on streams on the table to capture a log of all changes to the table in the last 24 hours. Store a copy of the stream in an Amazon S3 bucket.

�𰸣�A

�� �� �� Correct Answer: A.  Configure  point-in-time  recovery  for  the  table.  Detailed  Explanation:  Dynamo db  Point-in-Time Recovery  (PITR)  is  the  simplest  and  most  efficient  solution  for  this  scenario.  When  enabled,  PITR  automatically  maintains incremental backups of t he table for the last 35 days, allowing you to restore the table to any second within that period. This meets the requirement of reverting to any point within the last 24 hours. - Operational Overhead: PITR requires minimal setup�� just enabling it via the AWS console, CLI, or SDK. No scripting, scheduling, or manual intervention is needed. - Precision: Unlike hourly  backups  (Option  C)  or  AWS  Backup  schedules  (Option  B),  PITR  supports  sub-minute  recovery  granularity.  -  Streams (Option D) require custom logic to replay changes from S3, adding complexity. Other options involve more manual steps (e.g., Lambda functions) or less precise recovery times. PITR is fully managed by AWS, aligning with the least operational overhead requirement.                         Reference                         Link:                         [Amazon                         Dynamo db                         Point-in-Time

Recovery](https://docs.aws.amazon.com/amazon dynamo db/latest/developer guide/Point in time recovery.html)

671.A company hosts an application used to upload files to an Amazon S3  bucket. Once uploaded, the files are processed to extract metadata, which takes less than 5 seconds. The volume and frequency of the uploads varies from a few files each hour to hundreds of concurrent uploads. The company has asked a solutions architect to design a cost-effective architecture that will meet these requirements. What should the solutions architect recommend?

A ��Configure AWS Cloud trail trails to log S3 API calls. Use AWS AppSync to process the files.

B ��Configure an object-created event notification within the S3 bucket to invoke an AWS Lambda function to process the files.

C ��Configure Amazon Kinesis Data Streams to process and send data to Amazon S3. Invoke an AWS Lambda function to process the files.

D ��Configure an Amazon Simple Notification Service (Amazon SNS) topic to process the files uploaded to Amazon S3. Invoke an AWS Lambda function to process the files.

�𰸣�B

������ Correct Answer B Detailed Explanation The best solution is Option B because it uses Amazon S3 event notifications to directly  trigger  an  AWS  Lambda  function  when  files  are  uploaded.  Here �� s  why  this  works  well  for  the  requirements:  1. Cost-Effective: - AWS Lambda charges based on the number of requests and execution time. Since metadata extraction takes less than 5 seconds, Lambda is inexpensive for this workload. - No need to provision or pay for idle servers (unlike options involving Kinesis or EC2). 2. Automatic Scaling: - Lambda automatically scales to handle hundreds of concurrent uploads without manual intervention. This matches the requirement for variable upload volumes. 3. Simplicity: - S3 event notifications natively integrate with Lambda, avoiding extra complexity (e.g., SNS in Option D or Kinesis in Option C). Why Other Options Are Less Suitable: - Option A: Cloud trail logs API activity but isn ��t designed to trigger real-time processing. AppSync is for GraphQL APIs, not file processing. - Option C: Kinesis Data Streams adds unnecessary complexity and cost for a simple, short-lived task like metadata
extraction. - Option D: SNS can trigger Lambda, but S3 event notifications can do this directly without needing an SNS topic as a middle layer. Reference Links - [AWS Lambda Pricing](https://aws.amazon.com/lambda/pricing/) - [Using S3 Event Notifications with Lambda](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Notification how to.html)

672.A  company �� s  application  is  deployed  on  Amazon  EC2  instances  and  uses  AWS  Lambda  functions  for  an  event-driven architecture. The  company  uses  nonproduction  development environments  in  a  different AWS account to test  new features before the company deploys the features to production. The production instances show constant usage because of customers in different time zones. The company uses nonproduction instances only during business hours on weekdays. The company does not use the nonproduction instances on the weekends. The company wants to optimize the costs to run its application on AWS. Which solution will meet these requirements MOST cost-effectively?

A ��Use On-Demand Instances for the production instances. Use Dedicated Hosts for the nonproduction instances on weekends only.

B �� Use  Reserved  Instances  for  the  production  instances  and  the  nonproduction  instances.  Shut  down  the  nonproduction instances when not in use.

C �� Use Compute Savings Plans for the production instances. Use On-Demand Instances for the nonproduction instances. Shut down the nonproduction instances when not in use.

D ��Use Dedicated Hosts for the production instances. Use EC2 Instance Savings Plans for the nonproduction instances.

�𰸣�C

�� �� ��  Correct  Answer  C   Detailed  Explanation  The  company  needs  a  cost-effective  solution  for  their  24/7   production environment  and  part-time   nonproduction  environments.   1.  Production  Instances:  -  Compute  Savings   Plans  are   ideal  for consistent, long-term workloads (like production). They offer significant discounts (up to 66%) compared to On-Demand pricing and are more flexible than Reserved Instances (RIs) because they apply to any instance family/region within a compute service (EC2,  Lambda, etc.). 2.  Nonproduction Instances: - On-Demand  Instances are suitable for irregular, short-term workloads (like testing during business hours). By shutting them down when unused (evenings/weekends), the company avoids paying for idle time. Savings Plans or R Is would be wasteful here because nonproduction instances aren ��t running continuously. Why other options are  less cost-effective: - A:  Dedicated  Hosts  are expensive and  billed  per  host,  even  if instances are stopped. This  is overkill  for  part-time   nonproduction  use.  -   B:   Reserved   Instances  for  nonproduction  are  wasteful  because   RIs   require upfront/long-term  commitments  for  instances  that  run  only  part-time.  -  D:  Dedicated  Hosts  (production)  and  EC2  Instance Savings Plans (nonproduction) are inflexible. Savings Plans for nonproduction don ��t align with irregular usage, and Dedicated Hosts add  unnecessary  cost for  production  unless  specific  licensing/ placement  is  required.  Reference  Links  -  [AWS  Compute Savings                  Plans](https://aws.amazon.com/savings plans/compute-pricing/)                  -                   [EC2                   On-Demand
Pricing](https://aws.amazon.com/ec2/pricing/on-demand/)                                                     -                                                      [Stopping
Instances](https://docs.aws.amazon.com/AWSEC2/latest/User guide/Stop_Start.html)

673.A company stores data in an on-premises Oracle  relational database. The company needs to  make the data available in Amazon Aurora  Postgresql  for  analysis. The  company  uses  an  AWS  Site-to-Site VPN  connection to  connect  its  on-premises network to AWS. The company must capture the changes that occur to the source database during the  migration to Aurora Postgresql. Which solution will meet these requirements?

A ��Use the AWS Schema Conversion Tool (AWS SCT) to convert the Oracle schema to Aurora Postgresql schema. Use the AWS Database Migration Service (AWS DMS) full-load migration task to migrate the data.

B ��Use AWS Data sync to migrate the data to an Amazon S3 bucket. Import the S3 data to Aurora Postgresql by using the Aurora Postgresql aws_s3 extension.

C �� Use the AWS Schema Conversion Tool (AWS SCT) to convert the Oracle schema to Aurora  Postgresql schema.  Use AWS Database Migration Service (AWS DMS) to migrate the existing data and replicate the ongoing changes.

D ��Use an AWS Snowball device to migrate the data to an Amazon S3 bucket. Import the S3 data to Aurora Postgresql by using the Aurora Postgresql aws_s3 extension.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The correct solution uses AWS Schema Conversion Tool (AWS SCT) to convert the Oracle schema to Aurora  Postgresql format, as Oracle and  Postgresql  use different database engines. Then, AWS  Database Migration Service (AWS DMS) is used for two purposes: 1. Full-load migration: Transfers existing data from the source (Oracle) to the target (Aurora  Postgresql). 2. Change  Data Capture  (CDC): Continuously  replicates ongoing changes  made to the source database during and after the migration. This ensures no data changes are lost, meeting the requirement to capture changes. Why not other options? - A: Only performs a full load, so ongoing changes are not captured. - B   �� D: Using Amazon S3 as an intermediate step (via Data sync/Snowball) does not natively support real-time change replication from Oracle. These methods are      better       for       bulk      data       transfers,      not       live       database      changes.       Reference       Link:       -      [AWS       DMS Documentation](https://docs.aws.amazon.com/dms/latest/user guide/CHAP_Getting started.html)            -              [AWS            SCT Overview](https://aws.amazon.com/dms/schema-conversion-tool/)

674.An  ecommerce  company  is  running  a  seasonal  online  sale.  The  company  hosts  its  website  on  Amazon  EC2  instances spanning multiple Availability Zones. The company wants its website to manage sudden traffic increases during the sale. Which solution will meet these requirements MOST cost-effectively?

A��Create an Auto Scaling group that is large enough to handle peak traffic load. Stop half of the Amazon EC2 instances. Configure the Auto Scaling group to use the stopped instances to scale out when traffic increases.

B ��Create an Auto Scaling group for the website. Set the minimum size of the Auto Scaling group so that it can handle high traffic volumes without the need to scale out.

C �� Use Amazon Cloud front and Amazon Elastic ache to cache dynamic content with an Auto Scaling group set as the origin. Configure the Auto Scaling group with the instances necessary to populate Cloud front and Elastic ache. Scale in after the cache is fully populated.

D ��Configure an Auto Scaling group to scale out as traffic increases. Create a launch template to start new instances from a preconfigured Amazon Machine Image (AMI).

�𰸣�D

������ Correct Answer: D Detailed Explanation: The best solution is to use Auto Scaling with a launch template. Here ��s why: - Auto Scaling automatically adds/removes EC2 instances based on traffic, ensuring you only pay for what you need. This avoids overpaying for unused servers (unlike Option B, which keeps instances running 24/7). - Launch templates with a preconfigured AMI let AWS spin up new instances quickly during traffic spikes. This is faster and cheaper than stopping/starting old instances (Option A)  or  relying  on  caching  (Option  C, which  may  not  work for  non-cacheable  content).  -  Cost-effectiveness: You  scale precisely with demand, minimizing idle resources. Options A and C involve extra steps (managing stopped instances/caches) that add      complexity       and       cost.       Reference:        [AWS       Auto      Scaling](https://aws.amazon.com/auto scaling/)        [Launch Templates](https://docs.aws.amazon.com/AWSEC2/latest/User guide/ec2-launch-templates.html)

675.A solutions architect  must  provide an automated solution for a company's compliance  policy that states security groups cannot include a rule that allows SSH from 0.0.0.0/0 . The company needs to be notified if there is any breach in the policy. A solution  is  needed  as soon  as  possible. What should the  solutions architect  do to  meet these  requirements with  the  LEAST operational overhead?

A �� Write  an  AWS  Lambda  script  that  monitors  security  groups  for  SSH  being  open  to  0.0.0.0/0  addresses  and  creates  a notification every time it finds one.

B �� Enable  the  restricted-ssh AWS  Config  managed  rule  and  generate an Amazon Simple  Notification  Service  (Amazon  SNS) notification when a noncompliant rule is created.

C��Create an IAM role with permissions to globally open security groups and network ACLs. Create an Amazon Simple Notification Service (Amazon SNS) topic to generate a notification every time the role is assumed by a user.

D��Configure a service control policy (SCP) that prevents non-administrative users from creating or editing security groups. Create a notification in the ticketing system when a user requests a rule that needs administrator permissions.

�𰸣�B

�� �� �� Correct Answer:  B  Detailed  Explanation: AWS Config  is a service that continuously  monitors and  records your AWS resource configurations. It uses managed rules (pre-built by AWS) to check compliance with best practices. The restricted-ssh

managed rule specifically checks if any security group allows inbound SSH (port 22) traffic from 0.0.0.0/0 (public internet). By enabling this rule, AWS Config automatically evaluates your security groups in real-time. If a non-compliant rule is created or modified, AWS Config triggers an Amazon SNS notification. This requires minimal setup (no coding or infrastructure management) and ensures immediate alerts for policy violations. Why other options are less ideal: - A: Writing a custom Lambda script adds operational overhead (code maintenance, error handling, triggers). - C: IAM roles don ��t directly detect or notify security group violations. - D: SCPs block actions but don ��t detect existing violations or notify breaches. Reference Link: [AWS Config Managed Rules](https://docs.aws.amazon.com/config/latest/developer guide/managed-rules-by-aws-config.html)

676.Use  Amazon  Elastic  Ku bernet es  Service   (Amazon  EKS)  with  Amazon   EC2  worker   nodes.  A  company  has  deployed  an application  in  an  AWS  account.  The   application  consists  of   micro services  that   run  on  AWS   Lambda  and  Amazon   Elastic Ku bernet es Service (Amazon EKS). A separate team supports each micro service. The company has multiple AWS accounts and wants to give each team its own account for its micro services. A solutions architect needs to design a solution that will provide service-to-service communication over HTTPS (port 443). The solution also must provide a service registry for service discovery. Which solution will meet these requirements with the LEAST administrative overhead?

A ��Create an inspection VPC. Deploy an AWS Network Firewall firewall to the inspection VPC. Attach the inspection VPC to a new transit gateway. Route VPC-to-VPC traffic to the inspection VPC. Apply firewall rules to allow only HTTPS communication.

B��Create a VPC Lattice service network. Associate the micro services with the service network. Define HTTPS listeners for each service. Register micro service compute resources as targets. Identify VPCs that need to communicate with the services. Associate those VPCs with the service network.

C �� Create  a  Network  Load  Balancer  (NLB)  with  an  HTTPS  listener  and  target  groups  for  each  micro service.  Create  an  AWS Private link endpoint service for each micro service. Create an interface VPC endpoint in each VPC that needs to consume that micro service.

D ��Create peering connections between VPCs that contain micro services. Create a prefix list for each service that requires a connection to a client. Create route tables to route traffic to the appropriate VPC. Create security groups to allow only HTTPS communication.

�𰸣�B

������ Correct Answer: B Detailed Explanation: Amazon VPC Lattice is a fully managed service that simplifies service-to-service communication, security, and monitoring across AWS accounts and VPCs. It inherently supports HTTPS communication (port 443) and handles service discovery through automatic service registration. By creating a service network and associating the relevant VPCs,   teams   can   securely    communicate   between    micro services   without    manual   peering    connections,   load    balancer configurations, or complex firewall rules. This approach requires minimal administrative overhead compared to other options involving  Network  Firewalls  (A),  Private link  (C),  or  VPC   peering  (D),  which  all   demand  more  manual  setup  and  ongoing management. Reference Link: [Amazon VPC Lattice](https://aws.amazon.com/vpc/lattice/)

677.A company has a mobile game that reads most of its metadata from an Amazon RDS DB instance. As the game increased in popularity, developers noticed slowdowns related to the game's metadata load times. Performance metrics indicate that simply scaling  the  database  will   not  help.  A  solutions  architect   must  explore  all  options  that  include  capabilities  for  snapshots, replication, and sub-millisecond response times. What should the solutions architect recommend to solve these issues?

A ��Migrate the database to Amazon Aurora with Aurora Replicas.

B ��Migrate the database to Amazon Dynamo db with global tables.

C ��Add an Amazon Elastic ache for Redis layer in front of the database.

D ��Add an Amazon Elastic ache for Memcached layer in front of the database.

�𰸣�C

������ Correct Answer: C. Add an Amazon Elastic ache for Redis layer in front of the database. Explanation: The problem involves   slow  metadata  load  times  in  a  mobile  game  using  Amazon  RDS.  Since  scaling  the  database  isn't  resolving  the  issue,  the   slowdowns likely stem from frequent read operations overwhelming the database. A caching layer like Amazon Elastic ache can   reduce  direct  database  queries,  improving  performance.   Here's  why  Elastic ache  for  Redis  (Option  C)  is  the  best  choice:  -   Sub-millisecond  response  times:   Redis  caches  frequently  accessed  metadata,  drastically  reducing  latency  for  read-heavy  workloads. - Snapshots and replication: Redis supports data persistence via snapshots (RDB/AOF) and replication through read   replicas,  meeting the  problem's  requirements.  -  Minimal  changes: Adding  a caching  layer  requires  less  effort than  migrating  databases (Options A/B) and works alongside the existing RDS setup. Why not other options? - A (Aurora): While Aurora offers   read replicas and better performance, the problem states scaling the database isn't effective. Caching addresses the root cause   (repeated reads) more directly. - B (Dynamo db): Migrating to Dynamo db would require restructuring the data model and doesn �� t  inherently  solve  the  caching  need.  -  D  (Memcached):  Memcached  lacks  snapshot/replication  features,  which  are  critical   requirements  here.  Reference  Links:  -  [Amazon  Elastic ache  for  Redis  Features](https://aws.amazon.com/elastic ache/redis/)  -   [Caching Strategies with Redis](https://docs.aws.amazon.com/Amazon elastic ache/latest/red-ug/Strategies.html)

678.A data analytics company has 80 offices that are distributed globally. Each office hosts 1 PB of data and has between 1 and 2 Gbps of internet bandwidth. The company needs to perform a one-time migration of a large amount of data from its offices to Amazon S3. The company must complete the migration within 4 weeks. Which solution will  meet these  requirements  MOST cost-effectively?

A ��Establish a new 10 Gbps AWS Direct Connect connection to each office. Transfer the data to Amazon S3.

B ��Use multiple AWS Snowball Edge storage-optimized devices to store and transfer the data to Amazon S3. C ��Use an AWS Snowmobile to store and transfer the data to Amazon S3.

D ��Set up an AWS Storage Gateway Volume Gateway to transfer the data to Amazon S3.

�𰸣�B

������ Correct Answer: B. Use multiple AWS Snowball Edge storage-optimized devices to store and transfer the data to Amazon  S3. Detailed Explanation: When transferring large amounts of data (like 1 PB per office) over limited internet bandwidth (1 -2  Gb ps), using physical devices like AWS Snowball Edge is far more efficient and cost-effective than internet-based solutions. Here �� s why:  1.  Bandwidth  Limitations:  -  Transferring  1  PB  (1,000,000  GB)  over  1  Gb ps  would  take  ~100+  days  per  office  (due  to
network latency, real-world speeds, and potential congestion). Snowball Edge bypasses this bottleneck entirely. 2. Cost Efficiency:
- Direct Connect (Option A) requires expensive dedicated connections. - Snowmobile (Option C) is designed for exabyte-scale data (not 1 PB per office). - Storage Gateway (Option D) is for ongoing hybrid storage, not one-time bulk transfers. - Snowball Edge charges per device, avoiding recurring bandwidth costs. 3. Scalability   �� Speed: - AWS can ship Snowball Edge devices to all 80 offices in parallel, allowing simultaneous data transfers. Each office can load 1 PB into multiple devices (e.g., ~13 Snowball Edge Storage Optimized devices per office) and return them to AWS for upload to S3, completing the migration within 4 weeks. Reference    Links:   -    [AWS    Snowball    Edge](https://aws.amazon.com/snowball/edge/)    -    [AWS    Large-Scale    Data   Transfer Guide](https://aws.amazon.com/blogs/storage/migrating-petabytes-to-aws-using-aws-snowball/)

679.A company has an Amazon Elastic File System (Amazon EFS) file system that contains a reference dataset. The company has

applications on Amazon EC2 instances that need to read the dataset. However, the applications must not be able to change the dataset. The company wants to  use  IAM access control to  prevent the applications from  being able to  modify or delete the dataset. Which solution will meet these requirements?

A ��Mount the EFS file system in read-only mode from within the EC2 instances.

B ��Create a resource policy for the EFS file system that denies the elastic filesystem:Client write action to the IAM roles that are attached to the EC2 instances.

C ��Create an identity policy for the EFS file system that denies the elastic filesystem:Client write action on the EFS file system.

D ��Create an EFS access point for each application. Use Portable Operating System Interface (POSIX) file permissions to allow read-only access to files in the root directory.

�𰸣�B

������ Correct Answer: B Detailed Explanation: To prevent EC2 applications from modifying the EFS dataset using IAM controls, a resource-based   policy   on   the    EFS   file   system    is   required.   Option    B   uses    a   resource    policy   to   explicitly   deny   the elastic filesystem:Client write action for the IAM roles attached to the EC2 instances. This ensures applications can read data but cannot write, modify, or delete it. - Why not other options: - A: Mounting EFS in read-only mode uses OS-level file permissions (POSIX), not IAM. The question specifies using IAM. - C: Identity policies (attached to IAM roles/users) don ��t directly control EFS file system access; EFS requires resource policies. - D: EFS access points with POSIX permissions are file system-level controls, not IAM-based.  This  approach  aligns  with  AWS  best  practices  for  IAM-based  access  control  on  EFS.  Reference  Link:  [AWS  EFS Resource-Based Policies](https://docs.aws.amazon.com/efs/latest/ug/access-control-overview.html)

680.A company has hired an external vendor to perform work in the company ��s AWS account. The vendor uses an automated tool that is hosted in an AWS account that the vendor owns. The vendor does not have IAM access to the company ��s AWS account. The  company  needs to grant the vendor access to the company ��s AWS account. Which solution will  meet these requirements MOST securely?

A ��Create an IAM role in the company ��s account to delegate access to the vendor ��s IAM role. Attach the appropriate IAM policies to t he role for the permissions that the vendor requires.

B��Create an IAM user in the company ��s account with a password that meets the password complexity requirements. Attach the appropriate IAM policies to the user for the permissions that the vendor requires.

C ��Create an IAM group in the company ��s account. Add the automated tool ��s IAM user from the vendor account to the group. Attach the appropriate IAM policies to the group for the permissions that the vendor requires.

D ��Create an IAM user in the company ��s account that has a permission boundary that allows the vendor ��s account. Attach the appropriate IAM policies to the user for the permissions that the vendor requires.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution is to create an IAM role in the company's account that allows the vendor's AWS account to assume it. This is secure because: 1. No Long-Term Credentials: The vendor uses temporary security credentials (via role assumption) instead of permanent IAM user passwords/access keys, reducing the risk of credential leaks. 2. Cross-Account Access: The vendor ��s automated tool in their own AWS account can assume the role in the company ��s account without  needing  direct  IAM  access.  3.  Least  Privilege:  The  company  can  attach  IAM  policies  to  t he  role  to  grant  only  the

permissions the vendor needs. Other options are less secure: - B: IAM users require long-term credentials, which are riskier to share with external  parties. - C:  IAM  users/groups can ��t  be  shared  across AWS accounts, so this  is technically  invalid. -  D: Permission boundaries don ��t enable cross-account access and still rely on long-term credentials. Reference: [AWS Cross-Account IAM Roles](https://docs.aws.amazon.com/IAM/latest/User guide/tutorial_cross-account-with-roles.html)

681.A company wants to run its experimental workloads in the AWS Cloud. The company has a budget for cloud spending. The company's CFO is concerned about cloud spending accountability for each department. The CFO wants to receive notification when the spending threshold reaches 60% of the budget. Which solution will meet these requirements?

A ��Use cost allocation tags on AWS resources to label owners. Create usage budgets in AWS Budgets. Add an alert threshold to receive notification when spending exceeds 60% of the budget.

B ��Use AWS Cost Explorer forecasts to determine resource owners. Use AWS Cost Anomaly Detection to create alert threshold notifications when spending exceeds 60% of the budget.

C �� Use cost allocation tags on AWS resources to label owners. Use AWS Support API on AWS Trusted Advisor to create alert threshold notifications when spending exceeds 60% of the budget.

D��Use AWS Cost Explorer forecasts to determine resource owners. Create usage budgets in AWS Budgets. Add an alert threshold to receive notification when spending exceeds 60% of the budget.

�𰸣�A

���� �� Correct Answer: A Detailed Explanation: The correct solution is A because it uses two key AWS features to meet the requirements:  1.  Cost  Allocation  Tags:  These tags  help  track  spending  by  department.  By  tagging  resources  with  labels  like Department: Marketing or  Project:  Experiment x, AWS can organize costs by department, making accountability clear. 2. AWS Budgets with Alerts: AWS Budgets allows setting custom budgets (e.g., ,000/month) and configuring alerts at specific thresholds (like 60%). When spending crosses 60% of the budget, the CFO gets an automated notification via email or Amazon SNS. Why other  options  are   incorrect:  -   B   uses  Cost  Anomaly   Detection,  which  focuses  on  unexpected  spikes  in  spending,   not percentage-based thresholds tied to budgets. - C incorrectly relies on AWS Trusted Advisor (which gives optimization tips, not budget alerts) and the AWS Support API, which isn ��t designed for budget tracking. - D uses Cost Explorer forecasts to determine owners, but this isn ��t a reliable way to assign departmental accountability without proper tagging. Reference Links: - [AWS Cost Allocation     Tags](https://docs.aws.amazon.com/aws account billing/latest/aboutv2/cost-alloc-tags.html)       -      [AWS      Budgets Alerts](https://aws.amazon.com/aws-cost-management/aws-budgets/)

682.A company wants to deploy an internal web application on AWS. The web application must be accessible only from the company's office. The company needs to download security patches for the web application from the internet. The company has created a VPC and has configured an AWS Site-to-Site VPN connection to the company's office. A solutions architect must design a secure architecture for the web application. Which solution will meet these requirements?

A �� Deploy the web application on Amazon EC2 instances in public subnets behind a public Application Load Balancer (ALB). Attach an internet gateway to the VPC. Set the inbound source of the ALB's security group to 0.0.0.0/0.

B ��Deploy the web application on Amazon EC2 instances in private subnets behind an internal Application Load Balancer (ALB). Deploy NAT gateways in public subnets. Attach an internet gateway to the VPC. Set the inbound source of the ALB's security group to the company's office network CIDR block.

C ��Deploy the web application on Amazon EC2 instances in public subnets behind an internal Application Load Balancer (ALB). Deploy NAT gateways in private subnets. Attach an internet gateway to the VPSet the outbound destination of the ALB ��s security

group to the company's office network CIDR block.

D �� Deploy the web application on Amazon EC2 instances in private subnets behind a public Application Load Balancer (ALB). Attach an internet gateway to the VPC. Set the outbound destination of the ALB ��s security group to 0.0.0.0/0 .

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: The correct solution here is B because it meets all security and accessibility requirements: 1. Private Subnets: EC2 instances are placed in private subnets, which are not directly accessible from the internet. This ensures the web app is isolated from public exposure. 2. Internal ALB: An internal Application Load Balancer (ALB) routes traffic only within the VPC or connected networks (via VPN). This restricts access to the web app exclusively to the company ��s office network over the VPN. 3. Security Group Rules: The ALB ��s security group is configured to allow inbound traffic only from the company ��s office  network CIDR  block. This ensures  no public internet traffic can  reach the ALB. 4.  NAT Gateways:  NAT gateways in public subnets allow instances in private subnets to connect to the internet (e.g., to download security patches) while  blocking  unsolicited inbound traffic. This  maintains outbound internet access without exposing the EC2 instances. Why other options fail: - A and  D  use a  public ALB, exposing the app to the internet. - C  places  NAT gateways in  private subnets (incorrect; NAT must be in public subnets) and mis configures ALB/security groups. Reference Links: - [AWS VPC with Public and Private  Subnets](https://docs.aws.amazon.com/vpc/latest/user guide/VPC_Scenario2.html)  -   [Internal  vs.  Internet-facing  Load Balancers](https://docs.aws.amazon.com/elastic load balancing/latest/application/introduction.html#internet-facing-vs-internal) - [NAT Gateways for Outbound Internet Access](https://docs.aws.amazon.com/vpc/latest/user guide/vpc-nat-gateway.html)

683.A company maintains its accounting records in a custom application that runs on Amazon EC2 instances. The company needs to migrate the data to an AWS managed service for development and maintenance of the application data. The solution must require minimal operational support and provide immutable, cryptographically verifiable logs of data changes. Which solution will meet these requirements MOST cost-effectively?

A ��Copy the records from the application into an Amazon Redshift cluster.

B ��Copy the records from the application into an Amazon Neptune cluster.

C ��Copy the records from the application into an Amazon Time stream database.

D ��Copy the records from the application into an Amazon Quantum Ledger Database (Amazon QLDB) ledger.

�𰸣�D

������ Correct Answer D �� Copy the records from the application into an Amazon Quantum Ledger Database (Amazon QLDB) ledger.  Detailed  Explanation The  key  requirements  are:  1.  Minimal  operational  support   �� QLDB  is  fully  managed  by  AWS, reducing maintenance. 2. Immutable, verifiable logs  �� QLDB uses a cryptographically linked transaction ledger that cannot be altered.  Every  change  is  permanently  recorded  and  can  be  cryptographically  verified.  3.  Cost  -  effectiveness    ��  QLDB  is optimized for ledger/audit use cases like accounting, avoiding over provisioning costs. Why other options are less suitable: - A. Amazon  Redshift:  Designed for analytics,  not transactional  data.  Lacks  built - in  immutability. -  B. Amazon  Neptune: A graph database for  relationships  (e.g., social  networks),  not financial auditing. - C. Amazon Time stream: Time - series database for IoT/metrics, not transactional integrity. QLDB ��s ledger - style architecture directly aligns with accounting needs, ensuring trust and compliance without extra tooling. Reference Link [Amazon QLDB Overview](https://aws.amazon.com/qldb/)

684.A company's marketing data is uploaded from multiple sources to an Amazon S3 bucket. A series of data preparation jobs aggregate the data for reporting. The data preparation jobs need to run at regular intervals in parallel. A few jobs need to run in a specific  order  later.  The  company  wants  to  remove  the  operational  overhead  of  job  error  handling,  retry  logic,  and  state

management. Which solution will meet these requirements?

A �� Use an AWS Lambda function to process the data as soon as the data is uploaded to the S3 bucket. Invoke other Lambda functions at regularly scheduled intervals.

B ��Use Amazon Athena to process the data. Use Amazon Event bridge Scheduler to invoke Athena on a regular internal.

C ��Use AWS Glue Data brew to process the data. Use an AWS Step Functions state machine to run the Data brew data preparation jobs.

D ��Use AWS Data Pipeline to process the data. Schedule Data Pipeline to process the data once at midnight.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The best solution is to use AWS Glue Data brew for data processing and AWS Step Functions for orchestration. Here's why: 1. AWS Glue Data brew simplifies data preparation with a visual interface, handling data transformations  without  coding.  2.  AWS  Step  Functions  manages  workflows,  allowing  parallel  execution  of  most  jobs  and sequential execution for  dependent jobs.  It  automates  error  handling,  retries,  and  state  management,  reducing  operational overhead.  3. Alternatives  like  Lambda  (A)  or  Athena  (B)  require  manual  error  handling.  Data  Pipeline  (D)  is  less  flexible  for scheduling  and  dependencies.  Step   Functions  ensures  jobs  run  in  the  correct  order  when   needed,  while  Glue   Data brew efficiently processes data. Together, they minimize manual intervention, meeting all requirements. Reference Links: - [AWS Step Functions](https://aws.amazon.com/step-functions/) - [AWS Glue Data brew](https://aws.amazon.com/glue/features/data brew/)

685.A solutions architect  is designing  a  payment  processing  application that  runs  on AWS  Lambda  in  private  subnets  across multiple Availability Zones. The application uses multiple Lambda functions and processes millions of transactions each day. The architecture   must   ensure  that  the  application   does   not   process   duplicate   payments.   Which   solution  will   meet  these requirements?

A��Use Lambda to retrieve all due payments. Publish the due payments to an Amazon S3 bucket. Configure the S3 bucket with an event notification to invoke another Lambda function to process the due payments.

B��Use Lambda to retrieve all due payments. Publish the due payments to an Amazon Simple Queue Service (Amazon SQS) queue. Configure another Lambda function to poll the SQS queue and to process the due payments.

C ��Use Lambda to retrieve all due payments. Publish the due payments to an Amazon Simple Queue Service (Amazon SQS) FI FO queue. Configure another Lambda function to poll the FI FO queue and to process the due payments.

D ��Use Lambda to retrieve all due payments. Store the due payments in an Amazon Dynamo db table. Configure streams on the Dynamo db table to invoke another Lambda function to process the due payments.

�𰸣�C

�� �� �� Correct Answer  C  Detailed  Explanation The  requirement  is to  ensure the  payment  processing  application does  not process duplicate transactions. Here ��s why Option C is the best choice: 1. Amazon SQS FI FO Queues guarantee exactly-once processing and strict message ordering. - FI FO (First-In-First-Out) queues de duplicate messages using a Message De duplication ID, ensuring no duplicate payments are processed even if the same message is sent multiple times. - This is critical for payment systems where duplicates could result in financial errors. 2. Other options are less reliable for de duplication: - Option A (S3): S3 event notifications can trigger Lambda, but S3 lacks built - in de duplication. Multiple events for the same object could lead to duplicates. - Option B (Standard SQS): Standard queues offer at - least - once delivery, meaning messages might be delivered

more than once, risking duplicate processing. - Option D (Dynamo db Streams): Streams can invoke Lambda, but they use an at - least  -  once  delivery   model,  requiring  manual  de duplication   logic  in  the  application.   Reference   Link  -  [Amazon  SQS   FI FO queues](https://docs.aws.amazon.com/Aws simple queue service/latest/Sqs developer guide/FIFO - queues.html)

686.A company runs multiple workloads in its on-premises data center. The company's data center cannot scale fast enough to meet  the  company's  expanding   business   needs.  The   company  wants  to  collect   usage   and  configuration   data  about  the on-premises servers and workloads to p lan a migration to AWS. Which solution will meet these requirements?

A ��Set the home AWS Region in AWS Migration Hub. Use AWS Systems Manager to collect data about the on-premises servers.

B �� Set  the  home  AWS   Region  in  AWS  Migration  Hub.  Use  AWS  Application  Discovery  Service  to  collect  data  about  the on-premises servers.

C ��Use the AWS Schema Conversion Tool (AWS SCT) to create the relevant templates. Use AWS Trusted Advisor to collect data about the on-premises servers.

D ��Use the AWS Schema Conversion Tool (AWS SCT) to create the relevant templates. Use AWS Database Migration Service (AWS DMS) to collect data about the on-premises servers.

�𰸣�B

�� �� �� Correct Answer  B  Detailed  Explanation  The  correct  answer  is  B  because  the  company  needs  to  collect  usage  and configuration data about on - premises servers to p lan a migration to AWS. Here ��s why: 1. AWS Migration Hub provides a central location to track migrations across AWS services. Setting the home AWS Region in Migration Hub ensures the data is organized in the desired region. 2. AWS Application Discovery Service is specifically designed to collect detailed configuration and usage data (e.g., server specifications, running applications, dependencies) from on - premises servers. This data helps map workloads to AWS and  plan  migrations.  Other  options  are  incorrect:  -  A: AWS  Systems  Manager  is  for  managing  AWS  and  on  -  premises resources but isn ��t tailored for  migration - specific data collection. - C/D: The AWS Schema Conversion Tool (SCT) and AWS Database  Migration  Service   (DMS)  focus  on  database   migrations,   not  general  server/workload   discovery.  Trusted  Advisor optimizes AWS resources, not on - premises data collection. For a beginner: Think of Application Discovery Service as a scanner that maps your on - premises servers, while Migration Hub acts as a dashboard to track your migration progress. Reference Link [AWS    Application     Discovery     Service](https://aws.amazon.com/application    -     discovery     -    service/)     [AWS     Migration Hub](https://aws.amazon.com/migration - hub/)

687.A company has an organization in AWS Organizations that has all features enabled. The company requires that all API calls and logins in any existing or new AWS account must be audited. The company needs a managed solution to prevent additional work  and  to   minimize  costs.  The  company  also   needs  to   know  when  any  AWS  account  is  not  compliant  with  the  AWS Foundational Security Best Practices (FSBP) standard. Which solution will meet these requirements with the LEAST operational overhead?

A ��Deploy an AWS Control Tower environment in the Organizations management account. Enable AWS Security Hub and AWS Control Tower Account Factory in the environment.

B��Deploy an AWS Control Tower environment in a dedicated Organizations member account. Enable AWS Security Hub and AWS Control Tower Account Factory in the environment.

C �� Use AWS Managed Services (AMS) Accelerate to build a multi-account landing zone (MALZ). Submit an RFC to self-service provision Amazon Guard duty in the MALZ.

D �� Use AWS Managed Services (AMS) Accelerate to build a multi-account landing zone (MALZ). Submit an RFC to self-service provision AWS Security Hub in the MALZ.

�𰸣�A

�� �� �� Correct Answer: A  Detailed  Explanation:  The  best  solution  is  A  because  AWS  Control  Tower  provides  a  managed, automated way to set up and govern a multi-account AWS environment with minimal operational overhead. Here's why: 1. AWS Control Tower: - Deployed in the Organizations management account, it automatically configures foundational services like AWS Cloud trail (for auditing API calls/logins) and AWS Config across all existing and new accounts. - Account Factory ensures new accounts  are  pre-configured  with  compliance  guardrails  (e.g.,  enabling  Cloud trail  logging  by  default),  meeting  the  auditing requirement without manual effort. 2. AWS Security Hub: - When enabled, Security Hub automatically checks compliance with the  AWS  Foundational  Security  Best   Practices  (FSBP)  standard  across  all  accounts.  Non-compliant  resources  trigger  alerts, fulfilling the  monitoring  requirement.  3.  Why  not  other  options:  -  B:  Control  Tower  must  be  deployed  in  the  management account, not a member account. - C/D: AWS Managed Services (AMS) Accelerate involves additional steps like submitting RFCs (Request for Change), which increases operational overhead compared to Control Tower's fully automated setup. Reference Links:
-          [AWS           Control          Tower](https://aws.amazon.com/control tower/)           -           [AWS          Security           Hub           and FSBP](https://docs.aws.amazon.com/security hub/latest/user guide/security hub-standards-fsbp.html)

688.A company has stored 10 TB of log files in Apache Parquet format in an Amazon S3 bucket. The company occasionally needs to use SQL to analyze the log files. Which solution will meet these requirements MOST cost-effectively?

A �� Create  an  Amazon  Aurora  MySQL  database.  Migrate  the  data  from  the  S3  bucket  into  Aurora  by  using  AWS  Database Migration Service (AWS DMS). Issue SQL statements to the Aurora database.

B ��Create an Amazon Redshift cluster. Use Redshift Spectrum to run SQL statements directly on the data in the S3 bucket.

C �� Create  an  AWS Glue  crawler to store and  retrieve table  metadata from the  S3  bucket.  Use  Amazon  Athena to  run  SQL statements directly on the data in the S3 bucket.

D ��Create an Amazon EMR cluster. Use Apache Spark SQL to run SQL statements directly on the data in the S3 bucket.

�𰸣�C

������ Correct Answer C Detailed Explanation The most cost-effective solution is C (AWS Glue crawler + Amazon Athena). Here �� s why: 1. Amazon Athena is a serverless query service that lets you analyze data in S3 using standard SQL. You only pay per query	 (based on the amount of data scanned). Since the data is already in Parquet format (a columnar storage optimized for analytics),  Athena can scan it efficiently, reducing costs. 2. AWS Glue crawler automatically discovers the schema of the Parquet files in S3	 and creates a metadata table in the AWS Glue Data Catalog. This allows Athena to understand the structure of the data without manual setup. 3.  No infrastructure management or data migration is required. Options like Aurora (A) or  Redshift (B) involve moving  data  into a database/cluster, which  adds storage and operational costs.  EMR  (D)  requires  cluster  provisioning  and  is   overkill for occasional queries. 4. Cost optimization: Athena ��s pay-per-query model is ideal for occasional use. Redshift Spectrum	 (B) still requires a running Redshift cluster (expensive for sporadic use), and Aurora (A) or EMR (D) incur ongoing costs even when	 idle.          Reference           Links           -           [Amazon           Athena](https://aws.amazon.com/athena/)           -           [AWS          Glue	 Crawler](https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html)

689.A company needs a solution to prevent AWS Cloud formation stacks from deploying AWS Identity and Access Management (IAM) resources that include an inline policy or  ��* �� in the statement. The solution must also prohibit deployment of Amazon EC2 instances with public IP addresses. The company has AWS Control Tower enabled in its organization in AWS Organizations. Which solution will meet these requirements?

A ��Use AWS Control Tower proactive controls to block deployment of EC2 instances with public IP addresses and inline policies with elevated access or  ��* ��.

B ��Use AWS Control Tower detective controls to block deployment of EC2 instances with public IP addresses and inline policies with elevated access or  ��* ��.

C ��Use AWS Config to create rules for EC2 and IAM compliance. Configure the rules to run an AWS Systems Manager Session Manager automation to delete a resource when it is not compliant.

D �� Use  a  service  control   policy  (SCP)  to   block  actions  for  the   EC2  instances  and   IAM   resources  if  the  actions   lead  to noncompliance.

�𰸣�A

���� �� Correct Answer: A  Detailed  Explanation: AWS Control Tower  proactive controls are designed to enforce compliance before resources are deployed. They act as guardrails to prevent non-compliant resources (like EC2 instances with public IPs or IAM policies with wildcards *) from being created in the first place. This aligns with the requirement to block deployments of non-compliant resources via Cloud formation. - Option B (Detective Controls) only identify non-compliance after resources are created, which  doesn't  meet the  prevent  deployment  requirement.  -  Option C  (AWS Config + Systems  Manager)  is  reactive, meaning it deletes resources after they're created, which is less efficient and could cause operational disruptions. - Option D (SCPs) could technically block these actions, but SCPs are account-wide permissions boundaries and may not offer the granular, Cloud formation-specific controls provided by proactive controls. Control Tower proactive controls are a more targeted solution in              this               scenario.              Reference               Links:               -              [AWS               Control               Tower              Proactive Controls](https://docs.aws.amazon.com/control tower/latest/user guide/proactive-controls.html)    -    [Preventive    vs.    Detective
Controls](https://aws.amazon.com/blogs/mt/comparing-aws-config-rules-and-aws-control-tower-proactive-controls/)

690.A company has AWS Lambda functions that use environment variables. The company does not want its developers to see environment variables in plain text. Which solution will meet these requirements?

A ��Deploy code to Amazon EC2 instances instead of using Lambda functions.

B ��Configure SSL encryption on the Lambda functions to use AWS Cloud hsm to store and encrypt the environment variables.

C ��Create a certificate in AWS Certificate Manager (ACM). Configure the Lambda functions to use the certificate to encrypt the environment variables.

D ��Create an AWS Key Management Service (AWS KMS) key. Enable encryption helpers on the Lambda functions to use the KMS key to store and encrypt the environment variables.

�𰸣�D

������ Correct Answer: D Detailed Explanation: To securely store environment variables in AWS Lambda without exposing them in plain text, use AWS Key Management Service (KMS). Here ��s why: - Option D is correct because AWS Lambda natively supports encrypting environment variables using KMS keys. By enabling encryption helpers in Lambda, the service automatically encrypts the variables with a KMS key. Developers without decryption permissions (via KMS policies) can ��t view the values, ensuring security. - Why other options are wrong: - A: Moving to EC2 doesn ��t solve encryption; environment variables in EC2 (e.g., in user data) are still risky if not manually encrypted. - B: SSL/Cloud hsm protects data in transit, not at rest. Cloud hsm is for custom key storage, which is unnecessary here. - C: AWS Certificate Manager (ACM) handles TLS certificates for encrypting network traffic,

not        environment         variables         at         rest.         Reference          Link:         [AWS         Lambda         Environment         Variables Encryption](https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html#configuration-envvars-encryption)

691.An analytics company uses Amazon VPC to run its multi-tier services. The company wants to use RESTful APIs to offer a web analytics service to millions of users. Users must be verified by using an authentication service to access the APIs. Which solution will meet these requirements with the MOST operational efficiency?

A ��Configure an Amazon Cognito user pool for user authentication. Implement Amazon API Gateway REST APIs with a Cognito authorizer.

B��Configure an Amazon Cognito identity pool for user authentication. Implement Amazon API Gateway HTTP APIs with a Cognito authorizer.

C ��Configure an AWS Lambda function to handle user authentication. Implement Amazon API Gateway REST APIs with a Lambda authorizer.

D��Configure an IAM user to handle user authentication. Implement Amazon API Gateway HTTP APIs with an IAM authorizer.

�𰸣�A

���� �� Correct Answer: A Detailed Explanation: The  best solution is A  because it uses Amazon Cognito User  Pools for user authentication and API Gateway REST APIs with a Cognito authorizer. Here ��s why: 1. Amazon Cognito User Pools are designed for managing user sign-up/sign-in and authentication. They scale effortlessly for millions of users, which aligns with the requirement for serving a large user base. 2. API Gateway REST APIs with a Cognito authorizer simplify security by automatically validating user  tokens  (JWT)  issued  by  Cognito.  This  eliminates  the  need  for  custom  authentication  code,  ensuring  high  operational efficiency.  Other  options  are  less  optimal:  -   B  uses  Cognito  Identity  Pools  (for  granting  AWS   resource  access,   not  user authentication) and HTTP APIs (which are cheaper but don��t explicitly match the RESTful API requirement). - C requires custom Lambda code for authentication, increasing maintenance and reducing scalability. - D uses IAM users, which are impractical for millions     of      external      users     and      add      operational      overhead.     Reference      Links:      -      [Amazon      Cognito      User Pools](https://docs.aws.amazon.com/cognito/latest/developer guide/cognito-user-identity-pools.html)  -  [API  Gateway  Cognito Authorizer](https://docs.aws.amazon.com/api gateway/latest/developer guide/api gateway-integrate-with-cognito.html)

692.A company has a mobile app for customers. The app ��s data is sensitive and must be encrypted at rest. The company uses AWS Key Management Service (AWS KMS). The company needs a solution that prevents the accidental deletion of KMS keys. The solution must use Amazon Simple Notification Service (Amazon SNS) to send an email notification to administrators when a user attempts to delete a KMS key. Which solution will meet these requirements with the LEAST operational overhead?

A ��Create an Amazon Event bridge rule that reacts when a user tries to delete a KMS key. Configure an AWS Config rule that cancels any deletion of a KMS key. Add the AWS Config rule as a target of the Event bridge rule. Create an SNS topic that notifies the administrators.

B��Create an AWS Lambda function that has custom logic to prevent KMS key deletion. Create an Amazon Cloud watch alarm that is activated when a user tries to delete a KMS key. Create an Amazon Event bridge rule that invokes the Lambda function when the  Delete key  operation  is  performed.  Create an  SNS topic.  Configure the  Event bridge  rule  to  publish  an SNS  message that notifies the administrators.

C��Create an Amazon Event bridge rule that reacts when the KMS Delete key operation is performed. Configure the rule to initiate an AWS Systems Manager Automation runbook. Configure the runbook to cancel the deletion of the KMS key. Create an SNS topic. Configure the Event bridge rule to publish an SNS message that notifies the administrators.

D �� Create  an  AWS  Cloud trail  trail.  Configure  the  trail  to  deliver  logs  to  a  new  Amazon  Cloud watch  log  group.  Create  a Cloud watch alarm based on the metric filter for the Cloud watch log group. Configure the alarm to use Amazon SNS to notify the administrators when the KMS Delete key operation is performed.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The best solution is Option C because it uses Amazon Event bridge to detect the KMS `Delete key` operation and triggers an AWS Systems Manager Automation runbook to cancel the deletion. This approach requires no custom code (unlike Option B) and directly prevents key deletion in real - time (unlike Options A and D, which either rely on AWS Config rules that don ��t block actions or use delayed Cloud watch alarms for alerts). Systems Manager Automation provides  pre  -  built,  managed  workflows,  minimizing  operational  overhead.  Event bridge  also  sends  an  SNS  notification  to administrators, fulfilling both requirements: preventing accidental deletion and alerting admins. Option D only sends alerts but doesn��t stop deletion. Option A incorrectly assumes AWS Config can block deletions (it can ��t). Option B requires custom Lambda code,   increasing   maintenance.   Option   C   is   fully   managed   and   effective.   Reference   Links:   -   [AWS   KMS    Key   Deletion Prevention](https://docs.aws.amazon.com/kms/latest/developer guide/deleting-keys.html) -  [Using  Event bridge with  AWS  KMS Events](https://docs.aws.amazon.com/event bridge/latest/user guide/eb-kms-events.html)    -    [Systems    Manager    Automation Run books](https://docs.aws.amazon.com/systems-manager/latest/user guide/automation-run books.html)

693.A company wants to analyze and generate reports to track the usage of its mobile app. The app is popular and has a global user base. The company uses a custom report building program to analyze application usage. The program generates multiple reports during the last week of each month. The program takes less than 10 minutes to produce each report. The company rarely uses the program to generate reports outside of the last week of each month The company wants to generate reports in the least amount of time when the reports are requested. Which solution will meet these requirements MOST cost-effectively?

A ��Run the program by using Amazon EC2 On-Demand Instances. Create an Amazon Event bridge rule to start the EC2 instances when reports are requested. Run the EC2 instances continuously during the last week of each month.

B ��Run the program in AWS Lambda. Create an Amazon Event bridge rule to run a Lambda function when reports are requested.

C ��Run the program in Amazon Elastic Container Service (Amazon ECS). Schedule Amazon ECS to run the program when reports are requested.

D ��Run the program by using Amazon EC2 Spot Instances. Create an Amazon Event bnd ge rule to start the EC2 instances when reports are requested. Run the EC2 instances continuously during the last week of each month.

�𰸣�B

������ Correct Answer: B. Run the program in AWS Lambda. Create an Amazon Event bridge rule to run a Lambda function when reports are requested. Detailed Explanation: The company's requirements are: 1. Short runtime: Each report takes  ��10 minutes (Lambda's  15-minute  timeout  is  sufficient).  2.  Infrequent  usage:  Only  active  during  the  last  week  of  each  month  (Lambda's pay-per-use model avoids idle costs). 3. Cost-effectiveness: Lambda charges only for execution time (per 1ms) + requests, making it cheaper than always-on servers like EC2/ECS. 4. Fast scaling: Lambda automatically handles parallel requests without capacity planning. Why other options are less ideal: - A/D (EC2 On-Demand/Spot): Requires managing servers, incurs hourly billing even for short  runs,  and  needs  complex  start/stop  scheduling.  Spot  Instances  add  interruption  risks.  -  C  (ECS):  Involves  container management  overhead  and  higher  cost   per  execution  compared  to  serverless   Lambda.   Reference  Links:  -   [AWS  Lambda Pricing](https://aws.amazon.com/lambda/pricing/)   -    [Amazon   Event bridge](https://aws.amazon.com/event bridge/)    -    [AWS Lambda Use Cases](https://aws.amazon.com/lambda/use-cases/)

694.A  company  needs  a  solution  to  prevent  photos  with  unwanted  content  from  being  uploaded  to  the  company's  web application.  The   solution   must   not   involve   training   a   machine   learning   (ML)   model.   Which   solution   will   meet   these requirements?

A �� Create and deploy a model  by using Amazon Sage maker Autopilot. Create a real-time endpoint that the web application invokes when new photos are uploaded.

B ��Create an AWS Lambda function that uses Amazon Re k ognition to detect unwanted content. Create a Lambda function URL that the web application invokes when new photos are uploaded.

C ��Create an Amazon Cloud front function that uses Amazon Comprehend to detect unwanted content. Associate the function with the web application.

D ��Create an AWS Lambda function that uses Amazon Re k ognition Video to detect unwanted content. Create a Lambda function URL that the web application invokes when new photos are uploaded.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The correct solution is B because Amazon Re k ognition is a pre-trained service that  can  detect  inappropriate  or  unwanted  content  in  images  without  requiring  any  machine  learning  model  training.  AWS Lambda  allows you to  run  code without  managing  servers.  By  creating  a  Lambda  function that  uses  Re k ognition  to  analyze uploaded photos, the web application can invoke this function via a URL whenever a photo is uploaded. If unwanted content is detected, the upload can be blocked. - Why not A? Sage maker Autopilot trains custom ML models, which violates the no training requirement. -  Why  not  C?  Amazon  Comprehend  analyzes text,  not  images,  so  it  cannot  solve  this  problem.  -  Why  not  D? Re k ognition Video  is designed for video analysis,  not  static  images,  making  it  inefficient for this  use  case.  Reference  Links:  - [Amazon   Re k ognition   Content    Moderation](https://docs.aws.amazon.com/re k ognition/latest/dg/moderation.html)    -    [AWS Lambda Function URLs](https://docs.aws.amazon.com/lambda/latest/dg/lambda-urls.html)

695.A company uses AWS to run its ecommerce platform. The platform is critical to the company's operations and has a high volume of traffic and transactions. The company configures a multi-factor authentication (MFA) device to secure its AWS account root user credentials. The company wants to ensure that it will not lose access to the root user account if the MFA device is lost. Which solution will meet these requirements?

A ��Set up a backup administrator account that the company can use to log in if the company loses the MFA device.

B ��Add multiple MFA devices for the root user account to handle the disaster scenario.

C ��Create a new administrator account when the company cannot access the root account.

D ��Attach the administrator policy to another IAM user when the company cannot access the root account.

�𰸣�B

������ Correct Answer: B Detailed Explanation: AWS allows you to associate only one MFA device with the root user account. If the MFA device is lost, regaining access requires a complex account recovery process with AWS Support. However, option B is incorrect because AWS does  not support adding multiple  MFA devices for the root user. The correct answer should be A:  By setting  up  a  backup  administrator  IAM  user  with  full  permissions,  the  company  can  use this  account to  regain  access  (e.g., disable the root user ��s  MFA or request AWS Support assistance). This avoids relying solely on the root user ��s  MFA. Other options: - C/D:  Creating a  new  admin account or  modifying  IAM  users  requires  root  access,  which  is  unavailable  if  the  root

account is locked. - AWS ��s root account MFA cannot be managed by IAM users. Why the original answer (B) is wrong: AWS root users  can  only   have  one  active   MFA  device.  Adding   multiple  devices   is  not  supported.   Reference:  [AWS  Root  User  MFA Documentation](https://docs.aws.amazon.com/IAM/latest/User guide/id_credentials_mfa_enable_virtual.html#enable-virt-mfa- for-root)

696.A social media company is creating a rewards program website for its users. The company gives users points when users create and upload videos to the website. Users redeem their points for gifts or discounts from the company's affiliated partners. A  unique  ID  identifies  users. The  partners  refer to this  ID  to verify  user  eligibility for  rewards. The  partners  want to  receive notification of user IDs through an HTTP endpoint when the company gives users points. Hundreds of vendors are interested in becoming affiliated partners every day. The company wants to design an architecture that gives the website the ability to add partners rapidly in a scalable way. Which solution will meet these requirements with the LEAST implementation effort?

A ��Create an Amazon Time stream database to keep a list of affiliated partners. Implement an AWS Lambda function to read the list. Configure the Lambda function to send user IDs to each partner when the company gives users points.

B��Create an Amazon Simple Notification Service (Amazon SNS) topic. Choose an endpoint protocol. Subscribe the partners to the topic. Publish user IDs to the topic when the company gives users points.

C ��Create an AWS Step Functions state machine. Create a task for every affiliated partner. Invoke the state machine with user IDs as input when the company gives users points.

D��Create a data stream in Amazon Kinesis Data Streams. Implement producer and consumer applications. Store a list of affiliated partners in the data stream. Send user IDs when the company gives users points.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution is to use Amazon SNS (Simple Notification Service). Here's why in simple terms: 1. Publisher - Subscriber Model: SNS works like a newsletter. The company (publisher) sends notifications to a central topic. All partner endpoints (subscribers) automatically receive copies of these notifications. 2. Easy Scaling: When new partners join daily, you just add their HTTP endpoints as new subscribers to the SNS topic. No code changes needed - like adding email addresses to a mailing list. 3. Automatic Delivery: AWS handles all message delivery to partners' HTTP endpoints. You don't need to write code to  manage  retries  or  failures.  4.  Least  Effort:  Other  options  require  maintaining  databases  (A),  complex workflows (C), or building custom consumer apps (D). SNS requires just setting up a topic and adding subscribers. The other options  require  more  coding/ maintenance:  -  A  needs  database  management  +  custom  Lambda  code  -  C  requires  complex workflow management - D needs custom producer/consumer apps - B just uses ready - made SNS features Reference Links: - [Amazon                 SNS                 Features](https://aws.amazon.com/sns/)                 -                  [SNS                 HTTP                  Endpoint
Subscriptions](https://docs.aws.amazon.com/sns/latest/dg/sns-http-https-endpoint-as-subscriber.html)

697.A company needs to extract the  names of ingredients from recipe  records that are stored as text files  in an Amazon S3 bucket. A web application will use the ingredient names to query an Amazon Dynamo db table and determine a nutrition score. The application can handle non-food records and errors. The company does not have any employees who have machine learning knowledge to develop this solution. Which solution will meet these requirements MOST cost-effectively?

A ��Use S3 Event Notifications to invoke an AWS Lambda function when Put object requests occur. Program the Lambda function to analyze the object and extract the ingredient names by using Amazon Comprehend. Store the Amazon Comprehend output in the Dynamo db table.

B �� Use an Amazon Event bridge rule to invoke an AWS Lambda function when Put object requests occur. Program the Lambda function to  analyze the  object  by  using  Amazon  Forecast  to  extract the  ingredient  names.  Store  the  Forecast  output  in  the

Dynamo db table.

C �� Use S3 Event Notifications to invoke an AWS Lambda function when Put object requests occur. Use Amazon Polly to create audio recordings of the recipe records. Save the audio files in the S3 bucket. Use Amazon Simple Notification Service (Amazon SNS) to send a URL as a message to employees. Instruct the employees to listen to the audio files and calculate the nutrition score. Store the ingredient names in the Dynamo db table.

D ��Use an Amazon Event bridge rule to invoke an AWS Lambda function when a Put object request occurs. Program the Lambda function to analyze the object and extract the ingredient names by using Amazon Sage maker. Store the inference output from the Sage maker endpoint in the Dynamo db table.

�𰸣�A

���� �� Correct Answer: A Detailed Explanation: The correct answer is A because Amazon Comprehend is a managed natural language  processing  (NLP)  service  that  can  automatically  detect  entities  (like  ingredient   names)  in  text  without  requiring machine learning expertise. Here ��s why this works best: 1. S3 Event Notifications: When a new text file is uploaded to S3, it triggers a Lambda function automatically. This is cost-effective and serverless (no servers to manage). 2. Amazon Comprehend: The Lambda function sends the text to Comprehend, which extracts entities (e.g., flour, sugar) from the recipe. Comprehend is pre-trained, so no machine learning knowledge is needed. 3. Dynamo db: The extracted ingredient names are stored directly in Dynamo db for the web app to query later. Other options are less suitable: - B: Amazon Forecast is for time-series forecasting, not text analysis. - C: Amazon Polly converts text to speech, which doesn ��t solve the ingredient extraction problem. - D: Amazon Sage maker  requires  machine  learning  skills  to  train/deploy  models,  which  the  company  lacks.  Reference  Links:  -  [Amazon Comprehend](https://aws.amazon.com/comprehend/)                                       -                                        [S3                                       Event
Notifications](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Notification how to.html)

698.A company needs to create an AWS Lambda function that will run in a VPC in the company's primary AWS account. The Lambda function needs to access files that the company stores in an Amazon Elastic File System (Amazon EFS) file system. The EFS file system is located in a secondary AWS account. As the company adds files to the file system, the solution must scale to meet the demand. Which solution will meet these requirements MOST cost-effectively?

A ��Create a new EFS file system in the primary account. Use AWS Data sync to copy the contents of t he original EFS file system to the new EFS file system.

B ��Create a VPC peering connection between the VPCs that are in the primary account and the secondary account.

C ��Create a second Lambda function in the secondary account that has a mount that is configured for the file system. Use the primary account's Lambda function to invoke the secondary account's Lambda function.

D �� Move the contents of t he file system to a Lambda layer. Configure the Lambda layer's permissions to allow the company's secondary account to use the Lambda layer.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The most cost-effective solution is to create a VPC peering connection between the  primary and secondary accounts. This allows the  Lambda function  in the  primary  account to directly  access the  EFS file system in the secondary account without duplicating data or adding complex workflows. Here's why: 1. VPC Peering: Connects the two VPCs across accounts, enabling private network communication. Lambda in the primary account can mount the EFS file system in the secondary account as if it were local, avoiding data transfer costs between accounts (in the same region). 2. EFS Cross-Account Access: EFS supports cross-account access via VPC peering. You only need to: - Configure the EFS file system in the

secondary account to allow access from the primary account ��s VPC. - Ensure the Lambda function ��s security group permits inbound/outbound traffic to the  EFS security group. 3. Scalability:  EFS automatically scales storage capacity, so  no additional effort is needed as files grow. Other options are less optimal: - A (Data sync): Duplicates data and incurs ongoing sync costs. - C (Cross-account Lambda): Adds complexity, latency, and invocation costs. - D (Lambda Layer): Unsuitable for dynamic/large files due            to            size             limits            and            static             nature.            Reference             Links:            -             [AWS            VPC Peering](https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html)              -               [EFS               Cross-Account Access](https://docs.aws.amazon.com/efs/latest/ug/efs-access-cross-account.html)

699.A financial company needs to handle highly sensitive data. The company will store the data in an Amazon S3 bucket. The company  needs to ensure that the data  is encrypted  in transit and at  rest. The company  must  manage the encryption  keys outside the AWS Cloud. Which solution will meet these requirements?

A ��Encrypt the data in the S3 bucket with server-side encryption (SSE) that uses an AWS Key Management Service (AWS KMS) customer managed key.

B ��Encrypt the data in the S3 bucket with server-side encryption (SSE) that uses an AWS Key Management Service (AWS KMS) AWS managed key.

C ��Encrypt the data in the S3 bucket with the default server-side encryption (SSE).

D ��Encrypt the data at the company's data center before storing the data in the S3 bucket.

�𰸣�D

���� �� Correct Answer: D Detailed Explanation: The company must manage encryption keys outside AWS, which means they cannot use AWS-managed services like KMS (Options A, B, or C). By encrypting data client-side at their own data center before uploading to S3 (Option D), they retain full control over the encryption keys. This ensures: - Encryption at rest: Data is already encrypted before reaching S3. -  Encryption in transit: Achieved  by using  HTTPS  (SSL/TLS) when transferring data to S3. -  Key management: Keys never leave the company ��s control and are not stored in AWS. Options A/B/C rely on AWS-managed keys (KMS), which violates the requirement to manage keys outside AWS. Reference Links: - [Client-Side Encryption vs. Server-Side Encryption](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Using client side encryption.html)   -   [Protecting   Data   in Transit](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Using encryption.html)

700.A  company  wants  to  run  its  payment  application  on  AWS.  The  application  receives  payment  notifications  from  mobile devices.  Payment notifications  require a basic validation before they are sent for further  processing. The  backend processing application  is  long  running  and  requires  compute and  memory  to  be adjusted. The company does  not  want to  manage the infrastructure. Which solution will meet these requirements with the LEAST operational overhead?

A �� Create an Amazon Simple Queue Service (Amazon SQS) queue.  Integrate the queue with an Amazon Event bridge rule to receive  payment  notifications  from  mobile  devices.  Configure  the  rule  to  validate  payment  notifications  and  send  the notifications to the backend application. Deploy the backend application on Amazon Elastic Ku bernet es Service (Amazon EKS) Anywhere. Create a standalone cluster.

B �� Create  an  Amazon API  Gateway  API.  Integrate  the  API  with  an  AWS  Step  Functions  state  machine  to  receive  payment notifications from mobile devices. Invoke the state machine to validate payment notifications and send the notifications to the backend application.  Deploy the  backend application  on Amazon  Elastic  Ku bernet es  Service  (Amazon  EKS).  Configure  an  EKS cluster with self-managed nodes.

C �� Create an Amazon Simple Queue Service (Amazon SQS) queue.  Integrate the queue with an Amazon  Event bridge rule to

receive  payment  notifications  from  mobile  devices.  Configure  the  rule  to  validate  payment  notifications  and  send  the notifications to the backend application. Deploy the backend application on Amazon EC2 Spot Instances. Configure a Spot Fleet with a default allocation strategy.

D��Create an Amazon API Gateway API. Integrate the API with AWS Lambda to receive payment notifications from mobile devices. Invoke a Lambda function to validate payment notifications and send the notifications to the backend application. Deploy the backend application on Amazon Elastic Container Service (Amazon ECS). Configure Amazon ECS wit h an AWS Fargate launch type.

�𰸣�D

������ Correct Answer D Detailed Explanation The correct answer is D. Here's why: 1. Receiving Payments: Amazon API Gateway acts as the entry point for mobile devices to send payment notifications. It's scalable and handles HTTP requests efficiently. 2. Basic Validation: AWS  Lambda is serverless, so it automatically scales and requires  no infrastructure management. A  Lambda function can validate payment notifications quickly and cost-effectively. 3. Backend Processing: Amazon ECS with AWS Fargate is ideal for long-running applications. Fargate abstracts infrastructure management (no EC2 instances to manage) and automatically adjusts  compute/memory  based  on  workload  needs.  This  matches  the  requirement  for  no  infrastructure  management  and auto-scaling. Why Other Options Fail - A   �� C: Use SQS + Event bridge, which aren ��t designed for direct mobile device input. EKS Anywhere/EC2 Spot  Instances  require infrastructure management. -  B: Step  Functions adds unnecessary complexity for  basic validation.      Self-managed       EKS       nodes       require       infrastructure       oversight.       Reference       Links       -       [AWS       API Gateway](https://aws.amazon.com/api-gateway/)        -        [AWS         Lambda](https://aws.amazon.com/lambda/)        -         [AWS Fargate](https://aws.amazon.com/fargate/)

701.A  solutions  architect  is  designing  a  user  authentication  solution  for  a  company.  The  solution  must  invoke  two-factor authentication for users that log in from inconsistent geographical locations, IP addresses, or devices. The solution must also be able to scale up to accommodate millions of users. Which solution will meet these requirements?

A ��Configure Amazon Cognito user pools for user authentication. Enable the risk-based adaptive authentication feature with multifactor authentication (MFA).

B ��Configure Amazon Cognito identity pools for user authentication. Enable multi-factor authentication (MFA).

C ��Configure AWS Identity and Access Management (IAM) users for user authentication. Attach an IAM policy that allows the Allow manage own user mfa action.

D ��Configure AWS IAM Identity Center (AWS Single Sign-On) authentication for user authentication. Configure the permission sets to require multi-factor authentication (MFA).

�𰸣�A

�� �� ��  Correct  Answer:  A  Detailed  Explanation:  Amazon  Cognito  User  Pools  is  designed  for  scalable  user  authentication (supports millions of users) and offers built-in risk-based adaptive authentication. This feature automatically triggers MFA when it detects suspicious login attempts like inconsistent locations, new devices, or unfamiliar IP addresses. Other options either lack adaptive risk detection (B, D) or aren't designed for large-scale external user bases (C, D). Cognito User Pools uniquely combines both       security       automation       and       massive       scalability.       Reference       Links:       -       [Amazon       Cognito       Adaptive Authentication](https://docs.aws.amazon.com/cognito/latest/developer guide/cognito-user-pool-adaptive-authentication.html) - [Cognito User Pools Overview](https://aws.amazon.com/cognito/user-pools/)

702.A company has an Amazon S3 data lake. The company needs a solution that transforms the data from the data lake and loads the data into a data warehouse every day. The data warehouse must have massively parallel processing (MPP) capabilities.

Data analysts then need to create and train machine learning (ML) models by using SQL commands on the data. The solution must use serverless AWS services wherever possible. Which solution will meet these requirements?

A��Run a daily Amazon EMR job to transform the data and load the data into Amazon Redshift. Use Amazon Redshift ML to create and train the ML models.

B ��Run a daily Amazon EMR job to transform the data and load the data into Amazon Aurora Serverless. Use Amazon Aurora ML to create and train the ML models.

C ��Run a daily AWS Glue job to transform the data and load the data into Amazon Redshift Serverless. Use Amazon Redshift ML to create and train the ML models.

D �� Run a daily AWS Glue job to transform the data and load the data into Amazon Athena tables. Use Amazon Athena ML to create and train the ML models.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The question requires a serverless solution for transforming S3 data into a data warehouse with  MPP  capabilities and enabling  ML  model training  via SQL.  Here ��s why  Option C works  best:  1. AWS  Glue (Serverless ETL): - AWS Glue is a fully managed, serverless ETL service. It can transform data from S3 and load it into the data warehouse  without  managing  infrastructure.  2.  Amazon   Redshift  Serverless   (MPP  Data  Warehouse):  -   Redshift  Serverless provides MPP (massively parallel processing) for fast analytics and scales automatically, meeting the serverless requirement. 3. Amazon Redshift ML (SQL-based ML): - Redshift ML allows analysts to create/train ML models using SQL commands directly on Redshift data, aligning with the requirement. Why other options fail: - A: Uses Amazon EMR (not serverless) and regular Redshift (requires  manual scaling). -  B: Amazon Aurora Serverless  is a  relational database,  not an  MPP data warehouse. -  D: Amazon Athena is a query service for S3 (not a data warehouse) and lacks native ML training features like Redshift ML. Reference Links: - [AWS   Glue](https://aws.amazon.com/glue/)   -   [Amazon   Redshift   Serverless](https://aws.amazon.com/redshift/serverless/)   - [Amazon Redshift ML](https://docs.aws.amazon.com/redshift/latest/dg/machine_learning.html)

703.A company runs containers in a  Ku bernet es environment in the company's  local data center. The company wants to use Amazon Elastic Ku bernet es Service (Amazon EKS) and other AWS managed services. Data must remain locally in the company's data  center  and  cannot   be  stored  in  any  remote  site  or  cloud  to   maintain  compliance.  Which  solution  will   meet  these requirements?

A ��Deploy AWS Local Zones in the company's data center.

B ��Use an AWS Snowmobile in the company's data center.

C ��Install an AWS Outposts rack in the company's data center.

D ��Install an AWS Snowball Edge Storage Optimized node in the data center.

�𰸣�C

�� �� �� Correct Answer: C  Detailed  Explanation: AWS Outposts  is a fully  managed service that extends AWS infrastructure, services, APIs, and tools to customer data centers. By installing an AWS Outposts rack in the company's data center, the company can run Amazon EKS and other AWS managed services locally while keeping all data within their own data center. This ensures compliance with data residency requirements. Unlike Local Zones (A) or Snowmobile/Snowball (B/D), Outposts allows seamless integration      with       AWS       services       while       maintaining       full       local       data       control.       Reference       Link:       [AWS

Outposts](https://aws.amazon.com/outposts/)

704.A social  media company  has workloads that collect and  process data. The workloads store the data  in on-premises  NFS storage. The data store cannot scale fast enough to meet the company ��s expanding business needs. The company wants to migrate the current data store to AWS. Which solution will meet these requirements MOST cost-effectively?

A ��Set up an AWS Storage Gateway Volume Gateway. Use an Amazon S3 Lifecycle policy to transition the data to the appropriate storage class.

B ��Set up an AWS Storage Gateway Amazon S3 File Gateway. Use an Amazon S3 Lifecycle policy to transition the data to the appropriate storage class.

C �� Use  the  Amazon  Elastic  File  System  (Amazon  EFS)  Standard-Infrequent  Access  (Standard-IA)  storage  class.  Activate  the infrequent access lifecycle policy.

D �� Use the Amazon  Elastic  File  System  (Amazon  EFS) One Zone-Infrequent Access  (One Zone-IA) storage class. Activate the infrequent access lifecycle policy.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The company uses on-premises NFS storage and needs a cost-effective, scalable solution.  AWS  Storage  Gateway's  S3   File  Gateway  is   ideal  because  it   provides  an   NFS  interface  compatible  with  existing applications, allowing seamless migration without code changes. Data is stored in Amazon S3, which offers virtually unlimited scalability  and  lower  costs  compared  to  block  or  file  storage.  By  adding  an  S3  Lifecycle  policy,  infrequently  accessed  data automatically transitions to cheaper storage classes (e.g., S3 Standard-Infrequent Access or Glacier), optimizing costs. Options C and D use Amazon EFS, which is NFS-compatible but more expensive for large datasets. EFS Infrequent Access (IA) classes reduce costs but still cost more t han S3 IA. Options A (Volume Gateway) is designed for block storage (iSCSI), not file/NFS workloads, making it unsuitable here. Reference Links: - [AWS Storage Gateway File Gateway](https://aws.amazon.com/storage gateway/file/)
- [Amazon S3 Storage Classes](https://aws.amazon.com/s3/storage-classes/)

705.A  company  uses  high  concurrency  AWS  Lambda  functions  to  process  a  constantly  increasing  number  of  messages  in  a message queue during marketing events. The Lambda functions use CPU intensive code to process the messages. The company wants  to   reduce  the   compute  costs  and  to   maintain   service  latency  for   its  customers.  Which   solution  will   meet  these requirements?

A ��Configure reserved concurrency for the Lambda functions. Decrease the memory allocated to the Lambda functions.

B �� Configure  reserved  concurrency  for the  Lambda  functions.  Increase  the  memory  according  to  AWS  Compute  Optimizer recommendations.

C ��Configure provisioned concurrency for the Lambda functions. Decrease the memory allocated to the Lambda functions.

D ��Configure provisioned concurrency for the Lambda functions. Increase the memory according to AWS Compute Optimizer recommendations.

�𰸣�D

������Correct Answer D ��Configure provisioned concurrency for the Lambda functions. Increase the memory according to AWS Compute Optimizer recommendations. Detailed Explanation For a CPU-intensive Lambda function with high concurrency during

marketing events: 1. Provisioned Concurrency keeps pre-initialized instances ready, reducing cold-start delays. This maintains low latency even when message queues grow suddenly. 2. Increasing Memory (as recommended by AWS Compute Optimizer) gives Lambda  more CPU  power  proportionally.  Even though  higher  memory  costs  more  per  second,  CPU-heavy tasks finish faster, lowering total  execution time  and  overall  cost.  Reserved  Concurrency  (Options  A/B)  would  cap  scalability,  which  is  bad  for constantly increasing workloads. Reducing memory (Options A/C) would worsen CPU limitations, increasing runtime and costs. Reference                                 Links                                -                                  [AWS                                 Lambda                                 Provisioned
Concurrency](https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html)  -  [AWS  Compute  Optimizer  for Lambda](https://aws.amazon.com/compute-optimizer/)

706.A company runs its workloads on Amazon Elastic Container Service (Amazon ECS). The container images that the ECS task definition uses need to be scanned for Common Vulnerabilities and Exposures (CVEs). New container images that are created also need to be scanned. Which solution will meet these requirements with the FEWEST changes to the workloads?

A ��Use Amazon Elastic Container Registry (Amazon ECR) as a private image repository to store the container images. Specify scan on push filters for the ECR basic scan.

B ��Store the container images in an Amazon S3 bucket. Use Amazon Macie to scan the images. Use an S3 Event Notification to initiate a Macie scan for every event with an s3:Object created:Put event type.

C ��Deploy the workloads to Amazon Elastic Ku bernet es Service (Amazon EKS). Use Amazon Elastic Container Registry (Amazon ECR) as a private image repository. Specify scan on push filters for the ECR enhanced scan.

D �� Store the  container images  in an Amazon S3  bucket that  has versioning enabled.  Configure an S3  Event  Notification for s3:Object created:* events to invoke an AWS Lambda function. Configure the Lambda function to initiate an Amazon Inspector scan.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The correct answer is A because Amazon ECR (Elastic Container Registry) natively integrates with Amazon  ECS  and  provides  built-in  vulnerability  scanning. When you enable  scan  on  push  in  ECR,  every  new container image pushed to the repository is automatically scanned for CVEs using the basic scan feature. This requires minimal changes  to  existing  workflows �� only  updating  the  ECS  task  definitions  to  use  ECR  as  the  image  source  and  enabling  the scan-on-push setting. Other options involve unnecessary complexity (e.g., migrating to EKS in option C, using S3 buckets in B/D) or tools not designed for container image scanning (e.g., Macie in B, Inspector in D). ECR ��s native scanning is the simplest and most                 AWS-integrated                  solution.                 Reference                  Link:                  [Amazon                  ECR                  Image
Scanning](https://docs.aws.amazon.com/Amazon ecr/latest/user guide/image-scanning.html)

707.A company uses an AWS Batch job to run its end-of-day sales process. The company needs a serverless solution that will invoke a third-party  reporting application when the AWS  Batch job  is  successful. The  reporting  application  has  an  HTTP  API interface that uses username and password authentication. Which solution will meet these requirements?

A ��Configure an Amazon Event bridge rule to match incoming AWS Batch job Succeeded events. Configure the third-party API as an Event bridge API destination with a username and password. Set the API destination as the Event bridge rule target.

B ��Configure Amazon Event bridge Scheduler to match incoming AWS Batch job Succeeded events. Configure an AWS Lambda function to  invoke the third-party API  by  using a  username  and  password. Set the  Lambda function  as the  Event bridge  rule target.

C��Configure an AWS Batch job to publish job Succeeded events to an Amazon API Gateway REST API. Configure an HTTP proxy

integration on the API Gateway REST API to invoke the third-party API by using a username and password.

D �� Configure  an AWS  Batch job to  publish job  Succeeded events to an Amazon API  Gateway  REST API.  Configure a  proxy integration on the API Gateway REST API to an AWS Lambda function. Configure the Lambda function to invoke the third-party API by using a username and password.

�𰸣�A

���� �� Correct Answer A Explanation The best solution is A because it uses Amazon Event bridge's built-in capability to route events directly to an  external  HTTP  endpoint  (third-party API) via API  Destinations  with  username/password  authentication. Here's why: 1. Event-Driven   �� Serverless: AWS Batch job success events are automatically sent to Event bridge. An Event bridge rule can detect these events (e.g., Succeeded state) and trigger the  next step without  needing servers. 2. API  Destinations: Event bridge   API   Destinations   allow   direct   HTTP   calls   to   external   APIs   with   authentication   (e.g.,   Basic   Auth   using   a username/password).  This  avoids  extra  code  or  services  like  Lambda.  3.  Simpler  Architecture:  Option  A  skips  unnecessary components (Lambda, API Gateway) by leveraging native Event bridge features, making it cost-effective and easier to maintain. Why Not Other Options? - B: Event bridge Scheduler is for timed events, not event-driven triggers. Using Lambda adds complexity when direct API calls are possible. - C/D: API Gateway isn ��t needed here. AWS Batch jobs don ��t publish events directly to API Gateway;  Event bridge  already  captures  job   events.  Adding  Lambda  (D)  introduces  extra  steps.  Reference  Link   [Amazon Event bridge API Destinations](https://docs.aws.amazon.com/event bridge/latest/user guide/eb-api-destinations.html)

708.A company collects and processes data from a vendor. The vendor stores its data in an Amazon RDS for MySQL database in the vendor's own AWS account. The company ��s VPC does not have an internet gateway, an AWS Direct Connect connection, or an AWS Site-to-Site VPN connection. The company needs to access the data that is in the vendor database. Which solution will meet this requirement?

A �� Instruct the vendor to sign  up for the AWS Hosted Connection  Direct Connect  Program. Use VPC peering to connect the company's VPC and the vendor's VPC.

B �� Configure  a  client VPN connection  between the company's VPC  and the vendor's VPC.  Use VPC  peering  to connect the company's VPC and the vendor's VPC.

C ��Instruct the vendor to create a Network Load Balancer (NLB). Place the NLB in front of the Amazon RDS for MySQL database. Use AWS Private link to integrate the company's VPC and the vendor's VPC.

D ��Use AWS Transit Gateway to integrate the company's VPC and the vendor's VPC. Use VPC peering to connect the company ��s VPC and the vendor's VPC.

�𰸣�C

������ Correct Answer C Detailed Explanation The correct solution is C because AWS Private link allows the company to securely access the vendor's RDS database without requiring public internet, VPN, Direct Connect, or VPC peering. Here's how it works: 1. Vendor Setup: The vendor creates a Network Load Balancer (NLB) in front of their RDS database. This NLB acts as the entry point for the database. 2. Private link Service: The vendor exposes the NLB via AWS Private link (by creating a VPC Endpoint Service). This ensures the database is only accessible through private AWS networking. 3. Company Access: The company creates a VPC Endpoint in their own VPC to connect to the vendor ��s Private link service. This endpoint enables private, secure communication between the company ��s VPC and the vendor ��s RDS database. Why Other Options Fail: - A   �� D: Both require VPC peering or Transit Gateway, which are incompatible here. VPC peering requires overlapping route tables and cannot work if the company ��s VPC has no route to the vendor ��s network (no internet/VPN/Direct Connect). - B: A Client VPN would require public internet access        (contradicting        the         no        internet         gateway        constraint).         Reference        Link         [AWS         Private link

Documentation](https://docs.aws.amazon.com/white papers/latest/aws-vpc-connectivity-options/aws-private link.html)

709.A company wants to set up Amazon Managed Grafana as its visualization tool. The company wants to visualize data from its Amazon RDS database as one data source. The company needs a secure solution that will not expose the data over the internet. Which solution will meet these requirements?

A ��Create an Amazon Managed Grafana workspace without a VPC. Create a public endpoint for the RDS database. Configure the public endpoint as a data source in Amazon Managed Grafana.

B ��Create an Amazon Managed Grafana workspace in a VPC. Create a private endpoint for t he RDS database. Configure the private endpoint as a data source in Amazon Managed Grafana.

C ��Create an Amazon Managed Grafana workspace without a Vp create an AWS Private link endpoint to establish a connection between Amazon Managed Grafana and Amazon RDS. Set up Amazon RDS as a data source in Amazon Managed Grafana.

D ��Create an Amazon Managed Grafana workspace in a VPC. Create a public endpoint for the RDS database. Configure the public endpoint as a data source in Amazon Managed Grafana.

�𰸣�B

������ Correct Answer B Detailed Explanation The correct solution is to create an Amazon Managed Grafana workspace in a VPC and use a private endpoint for the RDS database. Here ��s why: 1. Secure Communication - By placing the Grafana workspace in a VPC, it operates within the company ��s isolated AWS network. This ensures that communication between Grafana and t he RDS database stays entirely within the VPC, avoiding exposure to the public internet. - RDS with a private endpoint is only accessible within the VPC, adding a layer of security by preventing external access. 2. Avoiding Public Exposure - Options A and D use a public RDS endpoint, which exposes the database to the internet and violates the security requirement. - Option C suggests AWS Private link, which is unnecessary here because Private link is typically used to connect to services outside your VPC (e.g., AWS services like S3) privately. Since RDS and Grafana are both in the same VPC, a simpler VPC - based setup (Option B) is sufficient. 3. Simpler Architecture - Option B avoids the complexity of configuring Private link (Option C) while still ensuring secure, private communication        between        Grafana         and        RDS.         Reference        Link         [Amazon        Managed         Grafana        VPC Configuration](https://docs.aws.amazon.com/grafana/latest/user guide/vpc - configuration.html)

710.A company hosts a data lake on Amazon S3. The data lake ingests data in Apache Parquet format from various data sources. The  company  uses   multiple  transformation  steps  to   prepare  the   ingested  data.  The  steps  include  filtering  of  anomalies, normalizing of data to standard date and time values, and generation of aggregates for analyses. The company must store the transformed data in S3 buckets that data analysts access. The company needs a prebuilt solution for data transformation that does  not  require  code.  The  solution  must  provide  data  lineage  and  data  profiling.  The  company  needs  to  share  the  data transformation steps with employees throughout the company. Which solution will meet these requirements?

A ��Configure an AWS Glue Studio visual canvas to transform the data. Share the transformation steps with employees by using AWS Glue jobs.

B �� Configure Amazon  EMR  Serverless to transform the data. Share the transformation steps with  employees  by  using  EMR Serverless jobs.

C �� Configure AWS Glue  Data brew to transform the data. Share the transformation steps with employees by using  Data brew recipes.

D ��Create Amazon Athena tables for the data. Write Athena SQL queries to transform the data. Share the Athena SQL queries

with employees.

�𰸣�C

������ Correct Answer: C. Configure AWS Glue Data brew to transform the data. Share the transformation steps with employees by using  Data brew  recipes.  Detailed  Explanation: AWS  Glue  Data brew  is a  no-code visual data  preparation tool  designed for users like data analysts to clean, normalize, and transform data without writing code. It provides data profiling (automatically analyzing data quality, statistics, and patterns) and data lineage (tracking how data is transformed from source to destination). Data brew  uses  recipes,  which   are  reusable  sets  of  transformation  steps  that  can   be  shared   across  teams,  fulfilling  the requirement to share workflows company-wide. Other options fall short: - A (Glue Studio): Requires some technical setup (e.g., jobs) and is better for complex ETL workflows, not fully no-code. - B (EMR Serverless): Requires coding (e.g., Spark/PySpark) and is  meant  for  large-scale  data  processing,  not  no-code.  -  D  (Athena):  Uses  SQL  queries  (code-based)  and  lacks  built-in  data lineage/profiling tools. Reference Link: [AWS Glue Data brew Overview](https://aws.amazon.com/glue/features/data brew/)

711.A solutions architect runs a web application on multiple Amazon EC2 instances that are in individual target groups behind an Application Load Balancer (ALB). Users can reach the application through a public website. The solutions architect wants to allow engineers to use a development version of the website to access one specific development EC2 instance to test new features for the application. The  solutions  architect wants to  use  an Amazon  Route  53  hosted  zone  to give the engineers  access to the development instance. The solution must automatically route to the development instance even if the development instance is replaced. Which solution will meet these requirements?

A ��Create an A Record for the development website that has the value set to the ALB. Create a listener rule on the ALB that forwards requests for the development website to the target group that contains the development instance.

B ��Recreate the development instance with a public IP address. Create an A Record for the development website that has the value set to the public IP address of the development instance.

C��Create an A Record for the development website that has the value set to the ALB. Create a listener rule on the ALB to redirect requests for the development website to the public IP address of the development instance.

D ��Place all the instances in the same target group. Create an A Record for the development website. Set the value to the ALB. Create a listener rule on the ALB that forwards requests for the development website to the target group.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The problem requires routing traffic to a specific development EC2 instance via Route  53  and  ALB,  even  if  the  instance  is  replaced.  Here's  why  Option  A  works  best:  1.  Route  53  A  Record:  Pointing  the development website's  DNS  record  (A  Record)  to  the  ALB  ensures  engineers  access the ALB first. This  avoids  hardcoding  IP addresses (which change if the instance is replaced), unlike Options B and C. 2. ALB Listener Rule: By creating a listener rule on the ALB that checks for the development website's host header (e.g., `dev.example.com`), traffic is forwarded to the dedicated target group for the development instance. This keeps the development environment isolated from other instances. 3. Automatic Recovery: Even if the development instance is replaced (e.g., due to failure or updates), the target group automatically registers the new instance ��s IP. This avoids manual updates, which Options B and C fail to address. Why other options fail: - Option B/C: Using a public IP (instead of ALB) breaks when the instance is replaced. - Option D: Grouping all instances into one target group removes    isolation     and     makes     routing     to     a     specific     instance     impossible.     Reference     Links:     -     [ALB     Listener Rules](https://docs.aws.amazon.com/elastic load balancing/latest/application/listener-update-rules.html)     -       [Route      53      A Records](https://docs.aws.amazon.com/Route53/latest/Developer guide/resource-record-sets-choosing-alias-non-alias.html)

712.A  company  runs  a  container  application  on  a   Ku bernet es  cluster  in  the  company's  data  center.  The  application  uses

Advanced Message Queuing Protocol (AMQP) to communicate with a message queue. The data center cannot scale fast enough to meet the company ��s expanding business needs. The company wants to migrate the workloads to AWS. Which solution will meet these requirements with the LEAST operational overhead?

A �� Migrate the container application to Amazon Elastic Container Service (Amazon ECS).  Use Amazon Simple Queue Service (Amazon SQS) to retrieve the messages.

B �� Migrate the  container application to Amazon  Elastic  Ku bernet es  Service  (Amazon  EKS).  Use  Amazon  MQ  to  retrieve the messages.
C ��Use highly available Amazon EC2 instances to run the application. Use Amazon MQ to retrieve the messages.

D��Use AWS Lambda functions to run the application. Use Amazon Simple Queue Service (Amazon SQS) to retrieve the messages.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The company's existing setup uses Ku bernet es and AMQP (commonly used with message  brokers  like  Rabbit mq).  To  minimize  operational overhead  during  migration, the  best  approach  is  to  use  managed services  that  require  the  least  changes.  -  Option  B  (Amazon  EKS  +  Amazon  MQ)  is  ideal  because:  1.  Amazon  EKS  is  AWS's managed  Ku bernet es service, allowing the company to retain their  Ku bernet es setup without managing the control plane. 2. Amazon MQ supports AMQP  natively (as it offers managed  Rabbit mq or Activemq). This avoids code changes to adapt to a different messaging protocol (e.g., SQS in Option A/D uses a proprietary API). 3. Both services are managed, reducing operational tasks like scaling, patching, and maintenance. - Why not other options? - A (ECS + SQS): SQS doesn ��t support AMQP, requiring code changes.  -  C  (EC2  +  Amazon  MQ):  Managing  EC2  instances  adds  operational  overhead  (scaling,  security,  updates).  -  D (Lambda + SQS): Rewriting a container app into serverless functions is time-consuming, and SQS lacks AMQP support. Reference Links: - [Amazon EKS](https://aws.amazon.com/eks/) - [Amazon MQ](https://aws.amazon.com/amazon-mq/)

713.An  online  gaming  company  hosts  its  platform  on  Amazon  EC2  instances  behind  Network  Load  Balancers  (NLBs)  across multiple AWS Regions. The NLBs can route requests to targets over the internet. The company wants to improve the customer playing experience by reducing end-to-end load time for its global customer base. Which solution will meet these requirements?

A ��Create Application Load Balancers (ALBs) in each Region to replace the existing NLBs. Register the existing EC2 instances as targets for the ALBs in each Region.

B ��Configure Amazon Route 53 to route equally weighted traffic to the NLBs in each Region.

C ��Create additional NLBs and EC2 instances in other Regions where the company has large customer bases.

D ��Create a standard accelerator in AWS Global Accelerator. Configure the existing NLBs as target endpoints.

�𰸣�D

�� �� �� Correct Answer:  D  Detailed  Explanation:  The  best  solution  is  to  use  AWS  Global  Accelerator.  Here's  why:  -  Global Accelerator uses AWS's private global network backbone and Anycast IP addresses to route user traffic to the nearest AWS edge location. This reduces latency because traffic travels through optimized network paths instead of the public internet. - Existing NLBs  can  stay  in   place  as  endpoints.  Global  Accelerator  intelligently   routes  traffic  to  the  closest   healthy   NLB,  improving performance for global users without needing to create new resources or change existing setups. - Why not other options? - A ALBs operate at Layer 7 (HTTP/HTTPS) and won��t reduce latency for non-HTTP traffic (common in gaming). - B Route 53 uses DNS-based routing, which depends on cached  DNS results and doesn ��t  optimize real-time  network paths. - C Adding more

regions is costly and doesn ��t guarantee optimized routing like Global Accelerator ��s backbone. Reference Links: [AWS Global
Accelerator](https://aws.amazon.com/global-accelerator/)                    [Global                   Accelerator                    vs.                    Route
53](https://docs.aws.amazon.com/global-accelerator/latest/dg/introduction-comparisons.html)

714.A company has an on-premises application that uses SFTP to collect financial data from multiple vendors. The company is migrating to the AWS Cloud. The company has created an application that uses Amazon S3 APIs to upload files from vendors. Some  vendors  run  their  systems  on  legacy  applications  that  do  not  support  S3  APIs.  The  vendors  want  to  continue to  use SFTP-based applications to upload data. The company wants to use  managed services for the needs of the vendors that use legacy applications. Which solution will meet these requirements with the LEAST operational overhead?

A ��Create an AWS Database Migration Service (AWS DMS) instance to replicate data from the storage of the vendors that use legacy applications to Amazon S3. Provide the vendors with the credentials to access the AWS DMS instance.

B ��Create an AWS Transfer Family endpoint for vendors that use legacy applications.

C ��Configure an Amazon EC2 instance to run an SFTP server. Instruct the vendors that use legacy applications to use the SFTP server to upload data.

D ��Configure an Amazon S3 File Gateway for vendors that use legacy applications to upload files to an SMB file share.

�𰸣�B

������ Correct Answer: B Detailed Explanation: AWS Transfer Family is a fully managed service that supports Secure File Transfer Protocol (SFTP), allowing vendors to upload files using their existing SFTP-based applications. The files are automatically stored in Amazon S3, matching the company ��s requirement to use S3 APIs for the new application. This solution eliminates the need to manage  servers  (unlike  Option  C)  and  avoids  compatibility  issues  with  protocols  like  SMB  (Option  D)  or  complex  database replication (Option A). It ��s the simplest, most cost-effective way to modernize file transfers without forcing vendors to change their systems. Reference Link: [AWS Transfer Family](https://aws.amazon.com/aws-transfer-family/)

715.A marketing team wants to build a campaign for an upcoming multi-sport event. The team has news reports from the past five years in PDF format. The team needs a solution to extract insights about the content and the sentiment of the news reports. The solution  must  use Amazon T extract to  process the  news  reports. Which solution will  meet these  requirements with the LEAST operational overhead?

A �� Provide the extracted  insights to Amazon Athena for analysis. Store the extracted  insights and analysis in an Amazon S3 bucket.

B ��Store the extracted insights in an Amazon Dynamo db table. Use Amazon Sage maker to build a sentiment model.

C ��Provide the extracted insights to Amazon Comprehend for analysis. Save the analysis to an Amazon S3 bucket.

D ��Store the extracted insights in an Amazon S3 bucket. Use Amazon Quick sight to visualize and analyze the data.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The correct answer is C because Amazon Comprehend is a fully managed natural language processing (NLP) service that can automatically analyze text for sentiment, entities, key phrases, and more. Since the team needs to extract insights and sentiment from the PDFs, using T extract to extract the text and then passing it directly to Comprehend for  analysis  minimizes  operational  overhead.  Comprehend  requires  no  custom  model  training  or  infrastructure

management,  making  it  the  simplest  solution.  Storing  the  results  in  Amazon  S3  aligns  with  serverless,  low  -  maintenance architecture. Other options add unnecessary complexity: - A (Athena) would require structuring the data for SQL queries and lacks built - in sentiment analysis. - B (Sage maker) involves building and managing a custom model, which increases operational work.  -  D  (Quick sight)  focuses  on  visualization   but  doesn  �� t   handle  sentiment  analysis   itself.  Reference  Links:   [Amazon Comprehend](https://aws.amazon.com/comprehend/) [Amazon T extract](https://aws.amazon.com/t extract/)

716.A company's application  runs on Amazon  EC2  instances that are  in  multiple Availability Zones. The application  needs to ingest real-time data from third-party applications. The company needs a data ingestion solution that places the ingested raw data in an Amazon S3 bucket. Which solution will meet these requirements?

A��Create Amazon Kinesis data streams for data ingestion. Create Amazon Kinesis Data Firehose delivery streams to consume the Kinesis data streams. Specify the S3 bucket as the destination of the delivery streams.

B �� Create database migration tasks in AWS  Database  Migration Service (AWS  DMS). Specify replication instances of t he EC2 instances as the source endpoints. Specify the S3 bucket as the target endpoint. Set the migration type to migrate existing data and replicate ongoing changes.

C ��Create and configure AWS Data sync agents on the EC2 instances. Configure Data sync tasks to transfer data from the EC2 instances to the S3 bucket.

D ��Create an AWS Direct Connect connection to the application for data ingestion. Create Amazon Kinesis Data Firehose delivery streams to consume direct PUT operations from the application. Specify the S3 bucket as the destination of the delivery streams.

�𰸣�A

���� �� Correct Answer: A  Detailed Explanation: The best solution is to use Amazon Kinesis Data Streams for real-time data ingestion and Amazon Kinesis Data Firehose to deliver the data to Amazon S3. Here's why: 1. Real-Time Ingestion: Kinesis Data Streams are designed to handle real-time data streams (like logs, metrics, or third-party app data) at scale, making them ideal for ingesting live data from  multiple sources. 2. Automatic  Delivery to S3:  Kinesis  Data  Firehose can directly consume data from Kinesis Data Streams and automatically batch, compress, and write it to an S3 bucket. This requires minimal code and handles failures/retries. 3. Alternatives Are Less Suitable: - Option B (DMS): AWS  DMS is for database migration, not general-purpose data ingestion. It requires a database as the source, not raw app data. - Option C (Data sync): Data sync is for scheduled bulk transfers (e.g., files), not real-time streaming. - Option D (Direct Connect):  Direct Connect is for private network connectivity, which    isn    �� t     required     here.     It     adds     unnecessary    cost/complexity.     Reference     Link:     [Amazon     Kinesis     Data Streams](https://aws.amazon.com/kinesis/data-streams/)                       |                       [Amazon                       Kinesis                       Data
Firehose](https://aws.amazon.com/kinesis/data-firehose/)

717.A company ��s application is receiving data from multiple data sources. The size of the data varies and is expected to increase over time. The current maximum size is 700 KB. The data volume and data size continue to grow as more data sources are added. The company decides to use Amazon  Dynamo db as the  primary database for the application. A solutions architect  needs to identify a solution that handles the large data sizes. Which solution will  meet these  requirements in the MOST operationally efficient way?

A ��Create an AWS Lambda function to filter the data that exceeds Dynamo db item size limits. Store the larger data in an Amazon Document db (with MongoDB compatibility) database.

B ��Store the large data as objects in an Amazon S3 bucket. In a Dynamo db table, create an item that has an attribute that points to the S3 URL of the data.

C ��Split all incoming large data into a collection of items that have the same partition key. Write the data to a Dynamo db table in a single operation by using the Batch write item API operation.

D ��Create an AWS Lambda function that uses gzip compression to compress the large objects as they are written to a Dynamo db table.

�𰸣�B

������ Correct Answer B Detailed Explanation Amazon Dynamo db has a 400 KB item size limit. Since the data already exceeds this limit (700 KB) and is expected to grow, storing the entire data directly in Dynamo db is impossible. Option B suggests using Amazon S3 to store large objects and storing only the S3 URL in Dynamo db. This is the most operationally efficient approach because:  1. S3 is designed for large objects  (up to 5 TB),  making it ideal for scaling with  increasing data sizes. 2.  Dynamo db remains  lightweight  by  storing  only  metadata  (e.g.,  S3  URLs),  avoiding  item  size  limits.  3.  No  complex  logic  (like  splitting, compression, or managing multiple databases) is required, reducing operational overhead. Other options are less efficient: - A and  D  require  extra  components  (Lambda,   Document db)  or   processing  (compression),  increasing  complexity.  -  C  violates Dynamo db ��s item size limit even after splitting, as the total data per partition key must still stay under 400 KB. Reference Link [AWS Dynamo db Limits](https://docs.aws.amazon.com/amazon dynamo db/latest/developer guide/Limits.html)

718.A company is migrating a legacy application from an on-premises data center to AWS. The application relies on hundreds of cron jobs that run between 1 and 20 minutes on different recurring schedules throughout the day. The company wants a solution to schedule and run the cron jobs on AWS with minimal refactoring. The solution must support running the cron jobs in response to an event in the future. Which solution will meet these requirements?

A ��Create a container image for the cron jobs. Use Amazon Event bridge Scheduler to create a recurring schedule. Run the cron job tasks as AWS Lambda functions.

B �� Create  a  container  image  for the  cron jobs.  Use  AWS  Batch  on  Amazon  Elastic  Container  Service  (Amazon  ECS)  with  a scheduling policy to run the cron jobs.

C ��Create a container image for the cron jobs. Use Amazon Event bridge Scheduler to create a recurring schedule. Run the cron job tasks on AWS Fargate.

D ��Create a container image for the cron jobs. Create a workflow in AWS Step Functions that uses a Wait state to run the cron jobs at a specified time. Use the RunTask action to run the cron job tasks on AWS Fargate.

�𰸣�C

���� �� Correct Answer C Detailed Explanation The best solution is C because it combines the required features with minimal refactoring:  1.  Amazon  Event bridge  Scheduler  allows  creating  precise  recurring  schedules  (like  cron  jobs)   and  supports event-driven triggering (e.g., future events). 2. AWS Fargate runs containerized tasks without time limits, accommodating jobs lasting  up to 20  minutes (unlike AWS  Lambda, which  has a  15-minute timeout). 3.  Containerization minimizes  refactoring  by packaging the  legacy  cron jobs  into  images,  avoiding  code  changes.  Why  Other  Options  Are  Less  Suitable:  -  A:  Lambda �� s 15-minute timeout makes it unsuitable for 20-minute jobs. - B: AWS Batch focuses on batch/compute-heavy workloads and lacks native event-driven scheduling. -  D: Step  Functions �� Wait  state  is  not designed for complex  recurring schedules and adds unnecessary               workflow                complexity.                Reference                Links               -                [Amazon                Event bridge Scheduler](https://aws.amazon.com/event bridge/scheduler/) - [AWS Fargate](https://aws.amazon.com/fargate/)

719.A company uses Sales force. The company needs to load existing data and ongoing data changes from Sales force to Amazon Redshift for analysis. The company does not want the data to travel over the public internet. Which solution will meet these

requirements with the LEAST development effort?

A ��Establish a VPN connection from the VPC to Sales force. Use AWS Glue Data brew to transfer data.

B ��Establish an AWS Direct Connect connection from the VPC to Sales force. Use AWS Glue Data brew to transfer data. C ��Create an AWS Private link connection in the VPC to Sales force. Use Amazon AppFlow to transfer data.

D ��Create a VPC peering connection to Sales force. Use Amazon AppFlow to transfer data.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The best solution is to use AWS Private link and Amazon AppFlow. Here's why: 1. AWS  Private link provides a secure  private connection between the VPC and Sales force, ensuring data never travels over the public  internet.  2.  Amazon  AppFlow   is  a  fully   managed  service   designed  specifically  for  transferring  data   between  SaaS applications (like Sales force) and AWS services (like Redshift). It natively supports Sales force and Private link, requiring minimal setup. Other options are less ideal: - A/B (VPN/Direct Connect + Glue Data brew): Glue Data brew focuses on data transformation, not direct SaaS - to - AWS integration, requiring extra development effort. Sales force typically doesn ��t support VPN/Direct Connect for customer connections. - D (VPC Peering): VPC peering only connects two VPCs, and Sales force isn ��t hosted in a customer��s VPC, making this impossible. Reference Links: - [Amazon AppFlow + Sales force](https://aws.amazon.com/appflow/) - [AWS Private link](https://aws.amazon.com/private link/)

720.A company recently migrated its application to AWS. The application runs on Amazon EC2 Linux instances in an Auto Scaling group across multiple Availability Zones. The application stores data in an Amazon Elastic File System (Amazon EFS) file system that uses EFS Standard-Infrequent Access storage. The application indexes the company's files. The index is stored in an Amazon RDS database. The company needs to optimize storage costs with some application and services changes. Which solution will meet these requirements MOST cost-effectively?

A �� Create  an Amazon S3  bucket that  uses  an  Intelligent-Tiering  lifecycle  policy.  Copy  all  files  to the  S3  bucket.  Update  the application to use Amazon S3 API to store and retrieve files.

B ��Deploy Amazon FSx for Windows File Server file shares. Update the application to use CIFS protocol to store and retrieve files.

C ��Deploy Amazon FSx for OpenZFS file system shares. Update the application to use the new mount point to store and retrieve files.

D ��Create an Amazon S3 bucket that uses S3 Glacier Flexible Retrieval. Copy all files to the S3 bucket. Update the application to use Amazon S3 API to store and retrieve files as standard retrievals.

�𰸣�A

���� �� Correct Answer A Detailed Explanation The most cost-effective solution is A because: 1. S3 Intelligent-Tiering vs. EFS Standard-IA: - Amazon EFS Standard-Infrequent Access (IA) is designed for file storage with occasional access, but it is still more expensive than Amazon  S3  (especially  for  large  datasets).  -  S3  Intelligent-Tiering  automatically  moves  objects  between  four access  tiers  (Frequent,   Infrequent,  Archive,  Deep  Archive)  based  on  usage  patterns,  with   no  retrieval  fees  or  operational overhead. This is ideal for unpredictable or changing access patterns, which aligns with the company ��s goal to optimize costs. 2. Cost Savings: - S3 Intelligent-Tiering is cheaper than EFS Standard-IA for most workloads. For example, S3 Intelligent-Tiering ��s Infrequent  Access  tier  costs  ~@.0125/GB/month  (us-east-1),  while  EFS  Standard-IA  costs  ~@.025/GB/month.  3.  Application Compatibility: - The application already indexes files in RDS, so replacing EFS wit h S3 (via S3 API) requires minimal changes. Using

S3 APIs  is  straightforward  for  developers  and  avoids the  complexity  of  managing  file  systems. Why  Other  Options Are  Less Optimal: - B (FSx for Windows): This is designed for Windows-based applications (CIFS protocol) and would require significant rework for a Linux app. FSx is also more expensive than S3. - C (FSx OpenZFS): While OpenZFS is Linux-compatible, it ��s still a file storage service with higher costs than S3 for most use cases. - D (S3 Glacier): Glacier Flexible Retrieval is for archival data with retrieval delays (minutes to hours). It ��s unsuitable for an application needing real-time access to files. Reference Links: - [Amazon S3 Pricing](https://aws.amazon.com/s3/pricing/) - [Amazon EFS Pricing](https://aws.amazon.com/efs/pricing/)

721.A  robotics  company  is  designing  a  solution for  medical  surgery. The  robots  will  use  advanced  sensors,  cameras,  and  AI algorithms to  perceive their environment and to complete surgeries. The company needs a  public  load  balancer  in the AWS Cloud that will ensure seamless communication with  backend services. The  load  balancer  must  be  capable of  routing traffic based  on  the  query  strings  to  different  target  groups.  The  traffic  must  also  be  encrypted.  Which  solution  will  meet  these requirements?

A ��Use a Network Load Balancer with a certificate attached from AWS Certificate Manager (ACM). Use query parameter-based routing.

B �� Use a Gateway Load  Balancer. Import a generated certificate in AWS  Identity and Access  Management (IAM). Attach the certificate to the load balancer. Use HTTP path-based routing.

C  �� Use   an   Application   Load   Balancer   with   a   certificate   attached   from   AWS   Certificate   Manager   (ACM).   Use   query parameter-based routing.

D �� Use a  Network Load  Balancer. Import a generated certificate in AWS Identity and Access  Management (IAM). Attach the certificate to the load balancer. Use query parameter-based routing.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The correct answer is C because an Application Load Balancer (ALB) is designed to handle HTTP/HTTPS traffic at the application layer (Layer 7). Here ��s why: 1. Query String Routing: ALB supports advanced routing rules based on query parameters, paths, or headers, which is critical for directing traffic to specific target groups based on the query string. 2.  Encryption: ALB  integrates seamlessly with AWS Certificate  Manager  (ACM) to provision and  manage SSL/TLS certificates, ensuring encrypted HTTPS traffic. 3. Layer 7 Features: Unlike Network Load Balancer (NLB) or Gateway Load Balancer (GLB), ALB operates at the application layer, making it the only AWS load balancer capable of inspecting and routing traffic based on query strings. Why other options fail: - A/D: NLB operates at Layer 4 (TCP/UDP) and cannot inspect HTTP query strings. - B: Gateway Load Balancer is for deploying third - party network appliances, not routing based on HTTP content. It also doesn ��t natively support query - based routing or ACM certificates. - D: NLB cannot use query parameters for routing, even with a certificate. Reference Link: [AWS Load Balancer Types](https://aws.amazon.com/elastic load balancing/features/)

722.A company has an application that runs on a single Amazon EC2 instance. The application uses a MySQL database that runs on the same EC2 instance. The company needs a highly available and automatically scalable solution to handle increased traffic. Which solution will meet these requirements?

A �� Deploy the application to EC2 instances that run in an Auto Scaling group behind an Application Load Balancer. Create an Amazon Redshift cluster that has multiple MySQL-compatible nodes.

B ��Deploy the application to EC2 instances that are configured as a target group behind an Application Load Balancer. Create an Amazon RDS for MySQL cluster that has multiple instances.

C �� Deploy the application to EC2 instances that run in an Auto Scaling group behind an Application Load Balancer. Create an

Amazon Aurora Serverless MySQL cluster for the database layer.

D ��Deploy the application to EC2 instances that are configured as a target group behind an Application Load Balancer. Create an Amazon Elastic ache for Redis cluster that uses the MySQL connector.

�𰸣�C

������ Correct Answer: C. Deploy the application to EC2 instances that run in an Auto Scaling group behind an Application Load Balancer. Create an Amazon Aurora Serverless MySQL cluster for the database layer. Detailed Explanation: 1. High Availability   & Auto-Scaling for the Application: - Auto Scaling Group automatically adds/removes EC2 instances based on traffic. This ensures the application scales  horizontally (adds more servers) during traffic spikes and reduces costs during low usage. - Application Load Balancer distributes traffic evenly across EC2 instances in the Auto Scaling group. It also provides redundancy by routing traffic only to  healthy  instances,  ensuring  high  availability.  2.  High  Availability   ��  Auto-Scaling for the  Database:  -  Amazon Aurora Serverless is a MySQL-compatible database that automatically scales compute and storage capacity based on demand. You don ��t need to manually adjust capacity, making it serverless. - Aurora inherently replicates data across 3 Availability Zones (AZs) for high availability. If the primary database instance fails, Aurora automatically fails over to a replica in another AZ within seconds. Why Other Options Are Incorrect: - Option A: Amazon Redshift is a data warehouse for analytics, not a replacement for MySQL. It ��s not designed for transactional workloads like a standard MySQL database. - Option B: While Amazon RDS for MySQL supports  high  availability  (via  Multi-AZ),  it  requires  manual  scaling.  The  EC2  setup  here  lacks  an  Auto  Scaling  group,  so  the application  layer can ��t  scale automatically. -  Option  D: Amazon  Elastic ache  is a caching service  (Redis/Memcached),  not  a database.      It      cannot       replace      MySQL      for       persistent      data      storage.      Reference       Links:      -       [Auto      Scaling Groups](https://docs.aws.amazon.com/auto scaling/ec2/user guide/Auto scaling group.html)                          -                           [Aurora
Serverless](https://aws.amazon.com/rds/aurora/serverless/)                               -                               [Application                               Load
Balancer](https://aws.amazon.com/elastic load balancing/application-load-balancer/)

723.A company is planning to migrate data to an Amazon S3 bucket. The data must be encrypted at rest within the S3 bucket. The  encryption  key  must  be  rotated  automatically  every  year.  Which  solution  will  meet  these  requirements  with  the  LEAST operational overhead?

A ��Migrate the data to the S3 bucket. Use server-side encryption with Amazon S3 managed keys (SSE-S3). Use the built-in key rotation behavior of SSE-S3 encryption keys.

B �� Create  an AWS  Key  Management  Service  (AWS  KMS)  customer  managed  key.  Enable  automatic  key  rotation.  Set  the S3 bucket's default encryption behavior to use the customer managed KMS key. Migrate the data to the S3 bucket.

C ��Create an AWS Key Management Service (AWS KMS) customer managed key. Set the S3 bucket's default encryption behavior to use the customer managed KMS key. Migrate the data to the S3 bucket. Manually rotate the KMS key every year.

D ��Use customer key material to encrypt the data. Migrate the data to the S3 bucket. Create an AWS Key Management Service (AWS KMS) key without key material. Import the customer key material into the KMS key. Enable automatic key rotation.

�𰸣�B

������ Correct Answer B Detailed Explanation The question requires encrypting data at rest in Amazon S3 with automatic annual key rotation and minimal operational overhead. Here's why Option B is the best choice: 1. AWS KMS Customer Managed Key: - By creating a customer - managed key in AWS Key Management Service (KMS), you retain control over the encryption key while leveraging AWS infrastructure. - Automatic Key Rotation: Enabling automatic rotation for a KMS customer - managed key ensures the  key  material  is  rotated  every  365  days   (annually),  aligning  with  the   requirement.  AWS   handles  the   rotation  process automatically, eliminating manual effort. 2. S3 Default Encryption: - Configuring the S3 bucket ��s default encryption to use the

KMS  key  ensures  all  objects  uploaded  to  the  bucket  are  automatically  encrypted  with  this  key.  This  minimizes  operational overhead since no additional steps are needed during data migration. 3. Operational Simplicity: - Options C and D require manual steps (e.g., rotating keys annually or importing external key material), which increase overhead. - Option A uses SSE - S3 (AWS - managed keys). While SSE - S3 encrypts data and AWS rotates keys internally, this rotation is not user - configurable or tied to a specific annual schedule. The question explicitly requires annual rotation, making KMS - managed keys (Option B) the correct choice. Reference Link [AWS KMS Key Rotation](https://docs.aws.amazon.com/kms/latest/developer guide/rotate - keys.html)

724.A company is migrating applications from an on-premises Microsoft Active Directory that the company manages to AWS. The company deploys the applications  in  multiple AWS accounts. The company  uses AWS Organizations to  manage the accounts centrally. The company's security team needs a single sign-on solution across all the company's AWS accounts. The company must  continue  to  manage  users  and  groups  that  are  in  the  on-premises  Active  Directory.  Which  solution  will  meet  these requirements?

A ��Create an Enterprise Edition Active Directory in AWS Directory Service for Microsoft Active Directory. Configure the Active Directory to be the identity source for AWS IAM Identity Center.

B ��Enable AWS IAM Identity Center. Configure a two-way forest trust relationship to connect the company's self-managed Active Directory with IAM Identity Center by using AWS Directory Service for Microsoft Active Directory.

C ��Use AWS Directory Service and create a two-way trust relationship with the company's self-managed Active Directory.

D ��Deploy an identity provider (IdP) on Amazon EC2. Link the IdP as an identity source within AWS IAM Identity Center.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The correct solution is to enable AWS IAM Identity Center (successor to AWS SSO) and set up a two-way forest trust between the company's existing on-premises Active Directory and AWS Managed Microsoft AD (via AWS Directory Service). Here ��s why: - Why B? - IAM Identity Center provides centralized SSO access to all AWS accounts under AWS Organizations. - The two-way trust allows seamless authentication: on-premises AD users/groups can access AWS resources via IAM Identity Center, while AWS Managed AD can trust the on-premises AD (if needed). - The company retains full control over their on-premises AD (no migration required). - Why not others? - A creates a new Enterprise Edition AD in AWS but doesn ��t integrate the existing on-premises AD. - C creates a trust but doesn ��t use IAM Identity Center, so no centralized SSO across accounts. - D adds unnecessary complexity (deploying a custom IdP on EC2). Reference Links: - [AWS IAM Identity Center integration                                                                                            with                                                                                             on-premises
AD](https://docs.aws.amazon.com/single sign on/latest/user guide/manage-your-identity-source-ad.html)  -   [Setting   up   a  trust relationship                                          with                                         AWS                                         Managed                                         Microsoft
AD](https://docs.aws.amazon.com/directory service/latest/admin-guide/ms_ad_setup_trust.html)

725.A company is planning to deploy its application on an Amazon Aurora Postgresql Serverless v2 cluster. The application will receive  large amounts of traffic. The company wants to optimize the storage  performance of the cluster as the  load on the application increases. Which solution will meet these requirements MOST cost-effectively?

A ��Configure the cluster to use the Aurora Standard storage configuration.

B ��Configure the cluster storage type as Provisioned IOPS.

C ��Configure the cluster storage type as General Purpose.

D ��Configure the cluster to use the Aurora I/O-Optimized storage configuration.

�𰸣�D

���� �� Correct Answer: D. Configure the cluster to use the Aurora I/O-Optimized storage configuration. Detailed Explanation: Aurora I/O-Optimized storage configuration is designed for I/O-intensive workloads and offers predictable pricing. For high-traffic applications with frequent database operations, this option optimizes both performance and cost: 1. Cost Efficiency: - Traditional Aurora Standard  (option A)  charges separately for storage AND  I/O  operations.  -  I/O-Optimized  (option  D)  uses  a  simplified model where I/O costs are included in the storage price. For workloads with heavy I/O (like high-traffic apps), this eliminates variable  I/O  charges  and  becomes  more  cost-effective  as  traffic  increases.  2.  Performance:  -  I/O-Optimized  provides  better throughput and lower latency compared to General Purpose (option C) or Standard storage. - Unlike Provisioned IOPS (option B), which requires manual capacity planning, I/O-Optimized automatically scales with Serverless v2's compute scaling. 3. Serverless Compatibility: - As the question specifies Aurora Serverless v2,  I/O-Optimized  is the  natural  pairing as  both are designed for auto-scaling      workloads       without       manual       provisioning.        Reference       Link:       [Amazon       Aurora       storage       and reliability](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/Aurora.Overview.Storage reliability.html)

726.A financial services company that runs on AWS has designed its security controls to meet industry standards. The industry standards  include  the  National  Institute  of  Standards  and  Technology  (NIST)  and  the  Payment  Card  Industry  Data  Security Standard (PCI DSS). The company's third-party auditors need proof that the designed controls have been implemented and are functioning correctly. The company has hundreds of AWS accounts in a single organization in AWS Organizations. The company needs to monitor the current state of the controls across accounts. Which solution will meet these requirements?

A �� Designate  one  account  as  the Amazon  Inspector  delegated  administrator  account  from  the  Organizations  management account.  Integrate  Inspector  with  Organizations  to  discover  and  scan  resources  across  all  AWS  accounts.  Enable  Inspector industry standards for NIST and PCI DSS.

B �� Designate  one account as the Amazon Guard duty delegated administrator account from the Organizations  management account.  In  the  designated  Guard duty  administrator  account,  enable  Guard duty  to   protect  all   member  accounts.   Enable Guard duty industry standards for NIST and PCI DSS.

C �� Configure an AWS Cloud trail  organization trail in the Organizations  management account.  Designate one account as the compliance account. Enable Cloud trail security standards for NIST and PCI DSS in the compliance account.

D �� Designate  one  account  as the AWS  Security  Hub  delegated  administrator  account  from the  Organizations  management account. In the designated Security Hub administrator account, enable Security Hub for all member accounts. Enable Security Hub standards for NIST and PCI DSS.

�𰸣�D

������ Correct Answer: D Detailed Explanation: AWS Security Hub is designed to aggregate and monitor security findings across multiple AWS accounts. By designating a Security Hub delegated administrator, the company can enable Security Hub across all member accounts in their AWS Organizations setup. Security Hub supports various industry standards, including NIST and PCI DSS, and provides continuous compliance checks. This allows the company to centrally monitor the status of controls required by these standards, generating reports that auditors can review. Other services like Inspector (vulnerability scanning), Guard duty (threat detection), or Cloud trail (logging) focus on specific areas but don ��t directly assess compliance against multiple industry standards  like  Security   Hub  does.   Reference  Links:  -  AWS  Security   Hub  overview:   https://aws.amazon.com/security-hub/  - Security Hub compliance standards: https://docs.aws.amazon.com/security hub/latest/user guide/security hub-standards.html

727.A company uses an Amazon S3 bucket as its data lake storage platform. The S3 bucket contains a massive amount of data that is accessed randomly by multiple teams and hundreds of applications. The company wants to reduce the S3 storage costs

and provide immediate availability for frequently accessed objects. What is the MOST operationally efficient solution that meets these requirements?

A ��Create an S3 Lifecycle rule to transition objects to the S3 Intelligent-Tiering storage class.

B ��Store objects in Amazon S3 Glacier. Use S3 Select to provide applications with access to the data.

C �� Use  data  from  S3  storage  class  analysis  to  create  S3  Lifecycle  rules  to  automatically  transition  objects  to  the  S3 Standard-Infrequent Access (S3 Standard-IA) storage class.

D ��Transition objects to the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Create an AWS Lambda function to transition objects to the S3 Standard storage class when they are accessed by an application.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution here is to use Amazon S3 Intelligent-Tiering. This storage class automatically moves objects between two access tiers (frequent and infrequent) based on changing access patterns. Since the company has unpredictable, random access patterns from multiple teams/applications, Intelligent-Tiering eliminates the need for  manual  analysis  or  complex  rules  (like  Options  C/D).   It  ensures  frequently  accessed  objects  stay  in  the  frequent  tier (immediately available) while less-accessed objects move to the infrequent tier (lower cost) - all automatically. Other options are less ideal: - Option C (manual analysis + lifecycle rules) requires ongoing effort and may not adapt to dynamic access patterns. - Option  D  (Lambda + Standard-IA) adds complexity and  risks extra fees due to S3 Standard-IA ��s  minimum storage duration charges.    -     Option    B     (Glacier)    violates     the     immediate    availability     requirement.    Reference     Links:     [Amazon    S3 Intelligent-Tiering](https://aws.amazon.com/s3/storage-classes/intelligent-tiering/)                                [S3                                Storage
Classes](https://aws.amazon.com/s3/storage-classes/)

728.A company has 5 TB of datasets. The datasets consist of 1 million user profiles and 10 million connections. The user profiles have connections as many-to-many relationships. The company needs a performance efficient way to find mutual connections up to five levels. Which solution will meet these requirements?

A ��Use an Amazon S3 bucket to store the datasets. Use Amazon Athena to perform SQL JOIN queries to find connections.

B ��Use Amazon Neptune to store the datasets with edges and vertices. Query the data to find connections. C ��Use an Amazon S3 bucket to store the datasets. Use Amazon Quick sight to visualize connections.

D ��Use Amazon RDS to store the datasets with multiple tables. Perform SQL JOIN queries to find connections.

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: The question involves efficiently finding mutual connections up to five levels deep in a large dataset with complex many-to-many relationships. Here ��s why Amazon Neptune (Option B) is the best choice: 1. Graph Database Strengths: - Neptune is designed for graph data (vertices for entities like users, edges for connections). - It excels at traversing relationships (e.g., friend of a friend) without costly JOIN operations, which are slow in relational databases (like RDS in Option  D) or SQL-on-S3 tools (Athena in Option A). 2.  Performance for  Deep Queries: -  Relational databases (RDS) or Athena  struggle  with   multi-level  JOINs  (e.g.,  5   layers)  due  to  computational  complexity.  -   Neptune  uses  optimized  graph algorithms to traverse connections in real time, even for deep queries. 3. Scalability: - Neptune handles large datasets (5 TB) and scales automatically, unlike RDS or Athena, which may require manual tuning. 4. Why Other Options Fail: - A (Athena): JOINs on S3 data are slow for deep relationships. - C (Quick sight): Visualization tools can ��t perform complex queries. - D (RDS): SQL JOINs

become       inefficient       with        5       layers       of        relationships.       Reference        Links:       -        [Amazon       Neptune        Use Cases](https://aws.amazon.com/neptune/use-cases/)                -                 [Graph                Databases                 vs.                 Relational Databases](https://aws.amazon.com/nosql/graph-vs-rdbms/)

729.A company needs a secure connection between its on-premises environment and AWS. This connection does not need high bandwidth and will handle a small amount of traffic. The connection should be set up quickly. What is the MOST cost-effective method to establish this type of connection?

A ��Implement a client VPN.

B ��Implement AWS Direct Connect.

C ��Implement a bastion host on Amazon EC2.

D ��Implement an AWS Site-to-Site VPN connection.

�𰸣�D

������ Correct Answer: D. Implement an AWS Site-to-Site VPN connection. Detailed Explanation: For a secure, low-bandwidth connection between on-premises and AWS that needs to be set up quickly and cost-effectively, an AWS Site-to-Site VPN is the best choice. Here's why: 1. Speed of Setup: A Site-to-Site VPN can be configured in hours using AWS-managed VPN endpoints (Virtual Private Gateway) and your on-premises VPN device. Direct Connect (Option B) requires physical cabling and third-party partnerships, which can take weeks. 2. Cost: Site-to-Site VPN uses your existing internet connection and only incurs minimal AWS data  transfer  costs.  Direct  Connect  has  upfront  port  fees  and  ongoing  costs  for  dedicated  network  links,  making  it  more expensive  for  small  traffic.  3.  Use  Case  Fit:  A  Site-to-Site  VPN  encrypts  traffic  over  the  public  internet,  which  is  ideal  for low-bandwidth needs. Client VPN (Option A) is for individual users, not entire networks. A bastion host (Option C) is for secure server              access,              not              network              connectivity.              Reference              Link:               [AWS              Site-to-Site VPN](https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html)

730.A company has an on-premises SFTP file transfer solution. The company  is  migrating to the AWS Cloud to scale the file transfer  solution  and  to  optimize  costs  by  using  Amazon  S3.  The  company's  employees  will  use  their  credentials  for  the on-premises Microsoft Active Directory (AD) to access the new solution. The company wants to keep the current authentication and file access mechanisms. Which solution will meet these requirements with the LEAST operational overhead?

A��Configure an S3 File Gateway. Create SMB file shares on the file gateway that use the existing Active Directory to authenticate.

B ��Configure an Auto Scaling group with Amazon EC2 instances to run an SFTP solution. Configure the group to scale up at 60% CPU utilization.

C ��Create an AWS Transfer Family server with SFTP endpoints. Choose the AWS Directory Service option as the identity provider. Use AD Connector to connect the on-premises Active Directory.

D ��Create an AWS Transfer Family SFTP endpoint. Configure the endpoint to use the AWS Directory Service option as the identity provider to connect to the existing Active Directory.

�𰸣�C

���� �� Correct Answer: C Detailed Explanation: The company needs to migrate their on-premises SFTP solution to AWS while retaining their  existing Active  Directory  (AD)  credentials  and  minimizing  operational  overhead.  AWS  Transfer  Family  natively

supports SFTP (matching their current protocol) and integrates with on-premises AD via AD Connector (part of AWS Directory Service). This setup allows authentication through the existing AD without copying data or managing a new directory service. - Option C uses AWS Transfer Family SFTP endpoints + AD Connector, which directly connects to the on-premises AD. This avoids the complexity of syncing users or maintaining a separate directory (e.g., Managed Microsoft AD), reducing operational work. - Option  D  is  incorrect  because  AWS  Directory  Service  alone  implies  a  fully  managed  Microsoft  AD  (which  requires  syncing on-premises AD data) or a trust relationship, adding complexity. - Option A uses SMB file shares (not SFTP), changing the access protocol. -  Option  B  requires  managing  EC2  instances  and  scaling,  increasing  operational  overhead.  Reference  Links:  -  [AWS Transfer Family + AD Connector](https://docs.aws.amazon.com/transfer/latest/user guide/service-integration-ad-connector.html)
- [AD Connector vs. Managed AD](https://aws.amazon.com/directory service/features/)

731.A company is designing an event-driven order  processing system.  Each order  requires  multiple validation steps after the order is created. An idempotent AWS Lambda function performs each validation step. Each validation step is independent from the other validation steps. Individual validation steps need only a subset of the order event information. The company wants to ensure that each validation step  Lambda function has access to only the information from the order event that the function requires. The components of the order processing system should be loosely coupled to accommodate future business changes. Which solution will meet these requirements?

A ��Create an Amazon Simple Queue Service (Amazon SQS) queue for each validation step. Create a new Lambda function to transform the order data to the format that each validation step requires and to publish the messages to the appropriate SQS queues. Subscribe each validation step Lambda function to its corresponding SQS queue.

B ��Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the validation step Lambda functions to the SNS topic. Use message body filtering to send only the required data to each subscribed Lambda function.

C ��Create an Amazon Event bridge event bus. Create an event rule for each validation step. Configure the input transformer to send only the required data to each target validation step Lambda function.

D ��Create an Amazon Simple Queue Service (Amazon SQS) queue. Create a new Lambda function to subscribe to the SQS queue and to transform the order data to the format that each validation step  requires.  Use the  new  Lambda function to  perform synchronous invocations of t he validation step Lambda functions in parallel on separate threads.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The best solution is Option C using Amazon Event bridge with input transformers. Here's why: 1. Data Minimization: Event bridge's input transformer allows modifying the event payload before sending it to each Lambda. This  ensures  each  validation function  receives  only  the  specific  data  subset  it  needs,  enhancing  security.  2.  Loose Coupling: - Validation Lambdas are completely independent. Adding/removing steps only requires creating/deleting Event bridge rules, without modifying existing components. - The order processing system doesn't need to know about validation steps, and validation steps don't communicate directly with each other. 3. Future Flexibility: - New validation steps can be added by creating new Event bridge rules, aligning with future business changes. - Input transformations can be modified independently for each rule.  Why  other  options  are  less  ideal:  -  Option  A  (SQS):  Requires  managing  multiple  queues  and  a  dedicated  transformer Lambda, adding complexity. SQS polling introduces latency compared to event-driven triggers. - Option B (SNS): SNS filtering only works with message attributes, not the message body. If data is in the body (as implied), this wouldn't filter content without custom  message  formatting.  -  Option  D  (Single  SQS):  The  transformer  Lambda  becomes  a  bottleneck  and  tightly  coupled component.  Synchronous  invocations  reduce  scalability  and  increase  error  handling  complexity.  Reference  Links:  -  [Amazon Event bridge  Input  Transformers](https://docs.aws.amazon.com/event bridge/latest/user guide/eb-transform-target-input.html)  - [Building Loosely Coupled Scenarios](https://aws.amazon.com/event-driven-architecture/)

732.A  company  is  migrating  a  three-tier  application  to  AWS.  The  application  requires  a  MySQL  database.  In  the  past,  the

application users reported poor application performance when creating new entries. These performance issues were caused by users  generating  different   real-time  reports  from  the  application  during  working   hours.  Which  solution  will   improve  the performance of the application when it is moved to AWS?

A ��Import the data into an Amazon Dynamo db table with provisioned capacity. Refactor the application to use Dynamo db for reports.
B �� Create the  database  on a compute optimized Amazon  EC2  instance.  Ensure  compute  resources  exceed the  on-premises database.

C ��Create an Amazon Aurora MySQL Multi-AZ DB cluster with multiple read replicas. Configure the application to use the reader endpoint for reports.

D��Create an Amazon Aurora MySQL Multi-AZ DB cluster. Configure the application to use the backup instance of the cluster as an endpoint for the reports.

�𰸣�C

�� �� �� Correct Answer:  C  Detailed  Explanation: The  main  performance  issue  arises  because  real-time  reports  (read-heavy operations) are competing with write operations (creating new entries) on the same database instance. Option C solves this by: 1. Using Aurora MySQL Multi-AZ DB cluster: This provides high availability by maintaining a primary instance and a standby replica in another Availability Zone. 2. Adding read replicas: These are separate database instances that handle only read operations (like reports). 3. Using the reader endpoint: This automatically distributes read queries across all available read replicas, freeing up the primary instance to focus on write operations (creating new entries). This separation of read and write workloads directly addresses the performance problem during peak hours. Other options either require risky refactoring (A), rely on vertical scaling limitations  (B),  or  misuse  backup  instances  not  designed  for  read  traffic  (D).  Reference  Links:  -  [Amazon  Aurora  Read Replicas](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/aurora-read-replicas.html)   -   [Aurora   Connection Management](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/Aurora.Overview.Endpoints.html)

733.A company is expanding a secure on-premises network to the AWS Cloud by using an AWS Direct Connect connection. The on-premises  network  has  no  direct  internet  access.  An  application  that  runs  on  the  on-premises  network  needs  to  use  an Amazon S3 bucket. Which solution will meet these requirements MOST cost-effectively?

A ��Create a public virtual interface (V IF). Route the AWS traffic over the public V IF.

B ��Create a VPC and a NAT gateway. Route the AWS traffic from the on-premises network to the NAT gateway.

C ��Create a VPC and an Amazon S3 interface endpoint. Route the AWS traffic from the on-premises network to the S3 interface endpoint.

D �� Create a VPC  peering connection between the on-premises  network and  Direct Connect.  Route the AWS traffic over the peering connection.

�𰸣�C

���� �� Correct Answer: C Detailed Explanation: The most cost-effective solution is to use an Amazon S3 interface endpoint. Here's why:  1.  No  Internet  Access  Required: The  on-premises  network  has  no  direct  internet  access, so solutions  relying  on public internet (like public VI Fs or NAT gateways) won't work or add unnecessary costs. 2. Private Connectivity: An S3 interface endpoint allows private connectivity to Amazon S3 via AWS Private link, which operates over the existing AWS Direct Connect

private connection. This avoids  public  internet traffic and  keeps data secure. 3. Cost Savings: -  No  Data Transfer  Fees: Traffic between the on-premises network and S3 via the interface endpoint uses AWS's private backbone, avoiding data transfer fees that would apply with public VI Fs or NAT gateways. - No NAT Gateway Costs: NAT gateways charge hourly and per-GB fees, while interface endpoints  have  minimal  hourly costs and  no data  processing fees. 4.  Simpler Architecture:  No  need  to set  up VPC peering  (which  isn't  directly  possible  between  on-premises  networks  and AWS) or  manage  public  routing tables. Why Other Options Are Less Optimal: - A (Public V IF): Public VI Fs route traffic over the public internet (contradicts the no internet access requirement) and incur higher data transfer costs. - B (NAT Gateway): Requires internet access (via NAT) and adds ongoing costs for NAT gateway usage. - D (VPC Peering): VPC peering connects two VPCs, not on-premises networks to AWS. Direct Connect already    provides     the     connection,     making     this     irrelevant.     Reference     Links:     -     [AWS     Private link     for    Amazon S3](https://docs.aws.amazon.com/AmazonS3/latest/user guide/private link-interface-endpoints.html)       -        [Direct        Connect Pricing](https://aws.amazon.com/directconnect/pricing/)

734.A company serves its website by using an Auto Scaling group of Amazon EC2 instances in a single AWS Region. The website does not require a database. The company is expanding, and the company's engineering team deploys the website to a second Region. The company wants to distribute traffic across both Regions to accommodate growth and for disaster recovery purposes. The solution  should  not serve traffic from a  Region  in  which the website  is  unhealthy. Which  policy  or  resource  should the company use to meet these requirements?

A ��An Amazon Route 53 simple routing policy

B ��An Amazon Route 53 multivalue answer routing policy

C ��An Application Load Balancer in one Region with a target group that specifies the EC2 instance IDs from both Regions

D ��An Application Load Balancer in one Region with a target group that specifies the IP addresses of the EC2 instances from both Regions

�𰸣�B

������ Correct Answer: B. An Amazon Route 53 multivalue answer routing policy Detailed Explanation: The company needs to distribute traffic across two AWS Regions while ensuring traffic is only routed to healthy Regions. Here's why Route 53 Multivalue Answer Routing is the correct choice: 1. Traffic Distribution: Multivalue Answer Routing allows you to return multiple healthy endpoints (e.g., ALBs in two Regions) in response to DNS queries. Clients receive a subset of these endpoints, enabling traffic distribution across Regions. 2. Health Checks: Route 53 automatically performs health checks on the configured endpoints (e.g., ALBs in each Region). If a Region becomes unhealthy (e.g., instances fail), Route 53 stops including that Region ��s endpoint in DNS responses.  3.  Disaster  Recovery:  By  excluding  unhealthy  Regions,  this  policy  ensures  users  are  only  directed  to  operational Regions, meeting the disaster recovery requirement. Why Other Options Fail: - A. Simple Routing Policy: Only returns a single endpoint and  lacks built - in  health checks for automatic failover. - C/D. Application  Load  Balancer  (ALB) with Cross -  Region Targets: ALB target groups cannot span multiple Regions. You cannot configure an ALB in one Region to route traffic to instances or            IPs             in            another             Region.             Reference            Links:             [Amazon             Route            53             Routing Policies](https://docs.aws.amazon.com/Route53/latest/Developer guide/routing       -         policy.html)        [Multivalue        Answer Routing](https://docs.aws.amazon.com/Route53/latest/Developer guide/routing - policy - multivalue.html)

735.A company runs its applications on Amazon EC2 instances that are backed by Amazon Elastic Block Store (Amazon EBS). The EC2  instances  run  the  most   recent  Amazon   Linux  release.  The  applications  are  experiencing  availability  issues  when  the company's employees store and retrieve files that are 25 GB or larger. The company needs a solution that does not require the company to transfer files  between  EC2  instances. The files  must  be available across  many  EC2  instances and across  multiple Availability Zones. Which solution will meet these requirements?

A ��Migrate all the files to an Amazon S3 bucket. Instruct the employees to access the files from the S3 bucket.

B ��Take a snapshot of the existing EBS volume. Mount the snapshot as an EBS volume across the EC2 instances. Instruct the employees to access the files from the EC2 instances.

C ��Mount an Amazon Elastic File System (Amazon EFS) file system across all the EC2 instances. Instruct the employees to access the files from the EC2 instances.

D ��Create an Amazon Machine Image (AMI) from the EC2 instances. Configure new EC2 instances from the AMI that use an instance store volume. Instruct the employees to access the files from the EC2 instances.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The company needs a solution where large files (25GB+) are accessible across multiple EC2 instances and Availability Zones without transferring files between instances. Here's why Amazon EFS (Option C) works best: 1. Shared Access: EFS acts like a network-attached hard drive for multiple EC2 instances simultaneously (like Google Drive for servers). All instances see the same files instantly, eliminating file transfers. 2. Multi-AZ Availability: EFS automatically stores copies of files in multiple Availability Zones (AZs). If one AZ has issues, instances in other AZs still access the files. 3. No Application Changes: Employees keep accessing files directly via EC2 instances (no need to rewrite apps for S3 APIs or manage snapshots). Why other options fail: - (A) S3: Requires apps to use S3 APIs (not file system access). Better for backups/archives, not active file sharing. - (B) EBS Snapshots: EBS volumes can ��t be shared live across instances. Snapshots create separate copies, causing version conflicts. - (D)  Instance Store: Data is deleted if the EC2 instance stops.  No cross-instance sharing.  Reference: [AWS EFS vs EBS vs S3](https://aws.amazon.com/efs/)

736.A  company  is  running  a  highly  sensitive  application  on  Amazon  EC2  backed  by  an  Amazon  RDS  database.  Compliance regulations  mandate that all  personally  identifiable  information  (P II)  be encrypted at  rest. Which solution should a solutions architect recommend to meet this requirement with the LEAST amount of changes to the infrastructure?

A ��Deploy AWS Certificate Manager to generate certificates. Use the certificates to encrypt the database volume.

B ��Deploy AWS Cloud hsm, generate encryption keys, and use the keys to encrypt database volumes.

C ��Configure SSL encryption using AWS Key Management Service (AWS KMS) keys to encrypt database volumes.

D �� Configure Amazon Elastic  Block Store (Amazon EBS) encryption and Amazon RDS encryption with AWS Key  Management Service (AWS KMS) keys to encrypt instance and database volumes.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The correct solution is to enable encryption for both Amazon EBS (used by EC2 instances)  and  Amazon  RDS  using  AWS  KMS  keys.  Here's  why:  1.  Least  Infrastructure  Changes:  -  EBS  encryption  and  RDS encryption are native AWS features requiring minimal configuration changes. You can enable them during resource creation or modify existing resources (though existing  EBS volumes/RDS instances  may require a snapshot/restore process). 2. AWS  KMS Integration:  -  Both  services  seamlessly  integrate  with  AWS  KMS  to  manage  encryption  keys.  No  need  for  external  tools  or complex certificate setups.  3.  Compliance  Coverage: -  EBS  encryption  protects  data  on  EC2  instance  storage  volumes. -  RDS encryption  safeguards  database  storage,  automated  backups,  and  read  replicas.  Other  options  are  suboptimal:  -  (A)  AWS Certificate Manager handles SSL/TLS certificates, not storage encryption. - (B) Cloud hsm adds unnecessary complexity for this use      case.      -       (C)      SSL      encrypts       data      in       transit,      not       at      rest.       Reference      Links:      -       [Amazon       EBS Encryption](https://docs.aws.amazon.com/AWSEC2/latest/User guide/Ebs encryption.html)              -               [Amazon               RDS

Encryption](https://docs.aws.amazon.com/Amazon rds/latest/User guide/Overview.Encryption.html)

737.A company runs an AWS  Lambda function in  private subnets in a VPC. The subnets  have a default  route to the internet through an Amazon EC2 NAT instance. The Lambda function processes input data and saves its output as an object to Amazon S3. Intermittently, the Lambda function times out while trying to upload the object because of saturated traffic on the NAT instance's network.  The   company  wants  to  access  Amazon  S3  without  traversing  the   internet.  Which   solution  will   meet  these requirements?

A ��Replace the EC2 NAT instance with an AWS managed NAT gateway.

B ��Increase the size of the EC2 NAT instance in the VPC to a network optimized instance type.

C ��Provision a gateway endpoint for Amazon S3 in the Vp update the route tables of t he subnets accordingly.

D ��Provision a transit gateway. Place transit gateway attachments in the private subnets where the Lambda function is running.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The Lambda function is in a private subnet and uses a NAT instance to access the internet, but this causes  network saturation. To resolve this, a VPC Gateway Endpoint for S3 (option C) allows direct,  private access to S3 without using the NAT or the public internet. This is done by adding a route in the subnet's route table that directs S3 traffic through the endpoint, bypassing the NAT entirely. Other options: - A/B still route S3 traffic through the NAT (public internet), which doesn ��t solve saturation. - D (Transit Gateway) connects VPCs or on - premises networks, unrelated to direct S3 access. Reference Links: - [AWS VPC Gateway Endpoints](https://docs.aws.amazon.com/vpc/latest/private link/vpc - endpoints - s3.html) - [Using Gateway Endpoints for S3](https://aws.amazon.com/blogs/aws/new - vpc - endpoint - for - amazon - s3/)

738.A  news  company that  has  reporters  all  over the world  is  hosting  its  broadcast  system  on  AWS. The  reporters  send  live broadcasts to the broadcast system. The  reporters use software on their phones to send live streams through the  Real Time Messaging Protocol (RTMP). A solutions architect must design a solution that gives the reporters the ability to send the highest quality streams. The solution must provide accelerated TCP connections back to the broadcast system. What should the solutions architect use to meet these requirements?

A ��Amazon Cloud front

B ��AWS Global Accelerator C ��AWS Client VPN

D ��Amazon EC2 instances and AWS Elastic IP addresses

�𰸣�B

���� �� Correct Answer B �� AWS Global Accelerator Detailed Explanation The best solution here is AWS Global Accelerator because it is specifically designed to improve the performance of TCP and  UDP applications by routing traffic through AWS's global  network  infrastructure.  Here's  why:  -  Accelerated  TCP  Connections:  Global  Accelerator  uses  AWS's  private  backbone network to reduce latency, jitter, and packet loss. This ensures reporters (located worldwide) can send high-quality live streams with stable, fast connections. - Static Entry Points: It provides static IP addresses that act as fixed entry points to the broadcast system, simplifying configuration for  reporters. - Optimized  Routing: Traffic  is automatically  routed to the  nearest AWS  edge location, then efficiently forwarded to the broadcast system (e.g., EC2 instances or other endpoints), avoiding unreliable public

internet paths. Other options are less suitable: - A. Amazon Cloud front: Primarily a CDN for caching/distributing content (e.g., videos) to viewers, not for accelerating uploads from reporters. - C. AWS Client VPN: Adds VPN overhead, which can degrade live - streaming performance. - D. EC2 + Elastic IPs: Direct connections over the public internet lack acceleration and reliability for global users. Reference Link [AWS Global Accelerator](https://aws.amazon.com/global - accelerator/)

739.A company uses Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) to run its self-managed database. The company has 350 TB of data spread across all EBS volumes. The company takes daily EBS snapshots and keeps the snapshots for 1 month. The daily change rate is 5% of t he EBS volumes. Because of new regulations, the company needs to keep the monthly snapshots for 7 years. The company needs to change its backup strategy to comply with the new regulations and to ensure that data is available with minimal administrative effort. Which solution will meet these requirements MOST cost-effectively?

A��Keep the daily snapshot in t he EBS snapshot standard tier for 1 month. Copy the monthly snapshot to Amazon S3 Glacier Deep Archive with a 7-year retention period.

B ��Continue with the current EBS snapshot policy. Add a new policy to move the monthly snapshot to Amazon EBS Snapshots Archive with a 7-year retention period.

C ��Keep the daily snapshot in t he EBS snapshot standard tier for 1 month. Keep the monthly snapshot in the standard tier for 7 years. Use incremental snapshots.

D ��Keep the daily snapshot in t he EBS snapshot standard tier. Use EBS direct APIs to take snapshots of all the EBS volumes every month. Store the snapshots in an Amazon S3 bucket in the Infrequent Access tier for 7 years.

�𰸣�B

������ Correct Answer: B Explanation The best solution is B because Amazon EBS Snapshots Archive is specifically designed for long-term retention (like 7 years) at a lower cost compared to the standard EBS snapshot tier. Here ��s why: - Cost-Effective: The Archive tier is cheaper than the standard tier for long-term storage. Storing 350 TB monthly snapshots for 7 years in the standard tier (as in Option C) would be extremely expensive. - Minimal Effort: You only need to update the existing policy to move monthly snapshots to the Archive tier.  No manual copying (Option A) or complex API workflows  (Option  D) are  required. -  Retention Compliance: The Archive tier supports retention policies, ensuring snapshots are automatically deleted after 7 years, avoiding regulatory  issues.  Other  options fail  because: - A:  Copying to S3  Glacier  Deep Archive  adds complexity  (requires extra steps beyond EBS snapshots). - C: Keeping snapshots in the standard tier for 7 years is cost-prohibitive. - D: S3 Infrequent Access (IA) is more expensive than EBS Snapshots Archive for this use case, and using APIs increases administrative work. Reference Links - [Amazon  EBS  Snapshots  Archive](https://docs.aws.amazon.com/AWSEC2/latest/User guide/ebs-snapshots-archive.html)  -  [EBS Snapshot Tiers Pricing](https://aws.amazon.com/ebs/pricing/)

740.A company runs an application on several Amazon EC2 instances that store persistent data on an Amazon Elastic File System (Amazon EFS) file system. The company needs to replicate the data to another AWS Region by using an AWS managed service solution. Which solution will meet these requirements MOST cost-effectively?

A ��Use the EFS-to-EFS backup solution to replicate the data to an EFS file system in another Region.

B ��Run a nightly script to copy data from the EFS file system to an Amazon S3 bucket. Enable S3 Cross-Region Replication on the S3 bucket.

C��Create a VPC in another Region. Establish a cross-Region VPC peer. Run a nightly rsync to copy data from the original Region to the new Region.

D ��Use AWS Backup to create a backup plan with a rule that takes a daily backup and replicates it to another Region. Assign the EFS file system resource to the backup plan.

�𰸣�D

������ Correct Answer D Detailed Explanation The most cost-effective and fully managed solution is AWS Backup. Here ��s why: - AWS Backup automates daily backups of t he EFS file system and replicates them to another Region. This  requires no manual scripting or infrastructure  management (unlike Options  B/C). -  EFS-to-EFS  replication  (Option A)  is  not a  native AWS service. Cross-Region  replication for  EFS typically  requires  custom  solutions  or third-party tools,  increasing  complexity and cost. - S3 Cross-Region Replication (Option B) would require nightly scripts to copy EFS data to S3, adding operational overhead. EFS and S3 have different data structures, making recovery cumbersome. - VPC peering + rsync (Option C) involves managing cross-Region networking,  EC2  instances  for  rsync,  and  ongoing  operational  costs,  which  is  neither  fully  managed  nor  cost-effective.  AWS Backup simplifies  cross-Region  replication  with  a  pay-as-you-go  model,  avoids  unnecessary  data  transfer  costs,  and  ensures compliance              with              minimal              effort.              Reference               Links              -              [AWS              Backup              for EFS](https://docs.aws.amazon.com/aws-backup/latest/dev guide/working-with-backups.html) -  [Cross-Region  Backup  with AWS Backup](https://aws.amazon.com/blogs/storage/automate-cross-region-backup-of-amazon-efs-file-systems-using-aws-backup/)

741.An ecommerce company is migrating its on-premises workload to the AWS Cloud. The workload currently consists of a web application  and  a  backend  Microsoft  SQL  database  for  storage.  The  company  expects  a  high  volume  of  customers  during  a promotional event. The  new infrastructure in the AWS Cloud must be highly available and scalable. Which solution will  meet these requirements with the LEAST administrative overhead?

A ��Migrate the web application to two Amazon EC2 instances across two Availability Zones behind an Application Load Balancer. Migrate the database to Amazon RDS for Microsoft SQL Server with read replicas in both Availability Zones.

B �� Migrate the web application to an Amazon EC2 instance that runs in an Auto Scaling group across two Availability Zones behind an Application Load Balancer. Migrate the database to two EC2 instances across separate AWS Regions with database replication.

C ��Migrate the web application to Amazon EC2 instances that run in an Auto Scaling group across two Availability Zones behind an Application Load Balancer. Migrate the database to Amazon RDS with Multi-AZ deployment.

D �� Migrate  the web  application to three Amazon  EC2  instances  across  three Availability Zones  behind  an Application  Load Balancer. Migrate the database to three EC2 instances across three Availability Zones.

�𰸣�C

������ Correct Answer C Detailed Explanation The best solution is Option C because it uses AWS managed services to minimize administrative effort while ensuring high availability and scalability. - Web Application: Using an Auto Scaling group across two Availability Zones (AZs) with an Application Load Balancer (ALB) ensures the web tier automatically scales up/down based on traffic  (scalability) and distributes traffic across AZs  (high  availability). Auto Scaling  reduces  manual  intervention. -  Database: Amazon RDS with Multi - AZ deployment automatically provisions a primary database in one AZ and a standby replica in another AZ. During outages, RDS fails over to the standby seamlessly (high availability). RDS handles maintenance, backups, and patching, minimizing  administrative  work.  Why  Other  Options  Are  Less  Optimal:  -  Option  A:  RDS  read  replicas  are  for  scaling  read operations, not high availability. Multi - AZ is required for automated failover. Also, using static EC2 instances (no Auto Scaling)
limits scalability. - Option B: Replicating a database across Regions (not AZs) adds complexity, latency, and management overhead. Multi - Region setups are overkill for typical high - availability needs. - Option D: Manual scaling (three fixed EC2 instances) and self - managed SQL Server on EC2 (instead of RDS) require significant administrative effort. Reference Links - [Amazon RDS Multi - AZ](https://aws.amazon.com/rds/features/multi - az/) - [Auto Scaling Groups](https://aws.amazon.com/ec2/auto scaling/)

742.A company has an on-premises business application that generates hundreds of files each day. These files are stored on an SMB   file   share   and   require   a    low-latency   connection   to   the   application   servers.   A   new   company   policy   states   all application-generated files  must be copied to AWS. There is already a VPN connection to AWS. The application development team does not have time to make the necessary code modifications to move the application to AWS. Which service should a solutions architect recommend to allow the application to copy files to AWS?

A ��Amazon Elastic File System (Amazon EFS)

B ��Amazon FSx for Windows File Server C ��AWS Snowball

D ��AWS Storage Gateway

�𰸣�D

������ Correct Answer D �� AWS Storage Gateway Detailed Explanation The best solution here is AWS Storage Gateway (File Gateway  mode).  Here's  why:  1.  SMB  Compatibility: The  application  uses  an SMB file  share. Storage Gateway's  File  Gateway provides an SMB interface, allowing the on-premises application to write files to it without code changes. 2. Low-Latency Access: Files are first written to the local Storage Gateway appliance (cached), ensuring low latency for the application. The gateway then automatically copies the files to Amazon S3 in the background over the existing VPN connection. 3. No Application Modification: Since the application continues to interact with the SMB share as usual, no code changes are needed. Why Other Options Are Less Suitable: - A. EFS: Only supports NFS (not SMB) and requires mounting in AWS, which isn ��t ideal for on-prem apps. - B. FSx for Windows: While it supports SMB, it ��s an AWS-hosted file share. Directly using it over VPN would introduce latency for the on-prem  app. - C. Snowball:  Designed  for one-time  bulk  data transfers,  not  ongoing  daily file  uploads.  Reference  Link  [AWS Storage Gateway Documentation](https://docs.aws.amazon.com/storage gateway/latest/user guide/What is storage gateway.html)

743.A company  has  15 employees. The company stores  employee start dates  in  an Amazon  Dynamo db table. The  company wants to send an email message to each employee on the day of the employee's work anniversary. Which solution will meet these requirements with the MOST operational efficiency?

A ��Create a script that scans the Dynamo db table and uses Amazon Simple Notification Service (Amazon SNS) to send email messages to employees when necessary. Use a cron job to run this script every day on an Amazon EC2 instance.

B��Create a script that scans the Dynamo db table and uses Amazon Simple Queue Service (Amazon SQS) to send email messages to employees when necessary. Use a cron job to run this script every day on an Amazon EC2 instance.

C ��Create an AWS Lambda function that scans the Dynamo db table and uses Amazon Simple Notification Service (Amazon SNS) to send email messages to employees when necessary. Schedule this Lambda function to run every day.

D ��Create an AWS Lambda function that scans the Dynamo db table and uses Amazon Simple Queue Service (Amazon SQS) to send email messages to employees when necessary. Schedule this Lambda function to run every day.

�𰸣�C

���� �� Correct Answer: C Detailed Explanation: The best solution is C because it uses serverless services (AWS Lambda and Amazon SNS), which minimize operational overhead. Here's why: 1. AWS Lambda: - No servers to manage (unlike EC2 in A/B). - Automatically scales (even though trivial here). - Pay-per-use (cost-effective for daily short tasks). 2. Amazon SNS: - Directly sends

emails/SMS (SQS is for queuing messages, not direct notifications). - Requires no extra processing (unlike SQS, which would need another service to read messages). 3. Scheduled Execution: - Lambda can be triggered daily via Cloud watch Events (no cron jobs on EC2 needed). Options A/B require maintaining an EC2 instance (costly/inefficient). Option D uses SQS incorrectly (can ��t send emails     directly).     C     is     fully      serverless,     automated,     and     AWS-managed.      Reference     Links:     -      AWS     Lambda: https://aws.amazon.com/lambda/           -            Amazon            SNS:            https://aws.amazon.com/sns/            -            Dynamo db: https://aws.amazon.com/dynamo db/

744.A company ��s application is running on Amazon EC2 instances within an Auto Scaling group behind an Elastic Load Balancing (ELB) load balancer. Based on the application's history, the company anticipates a spike in traffic during a holiday each year. A solutions architect must design a strategy to ensure that the Auto Scaling group proactively increases capacity to minimize any performance impact on application users. Which solution will meet these requirements?

A ��Create an Amazon Cloud watch alarm to scale up the EC2 instances when CPU utilization exceeds 90%.

B ��Create a recurring scheduled action to scale up the Auto Scaling group before the expected period of peak demand.

C ��Increase the minimum and maximum number of EC2 instances in the Auto Scaling group during the peak demand period.

D  �� Configure   an    Amazon   Simple    Notification   Service    (Amazon   SNS)    notification   to    send   alerts    when   there    are auto scaling:EC2_Instance_LAUNCH events.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The question asks for a proactive approach to handle anticipated traffic spikes. Option  B  uses  a  recurring scheduled action  in Auto Scaling, which allows scaling  up  in advance of  known  peak  periods  (like annual holidays). This ensures additional  EC2  instances are  ready  before traffic  increases, avoiding delays caused  by  reactive scaling  (like waiting for  high  CPU  in  Option  A).  Options  C  and  D  don ��t  automate  scaling  proactively �� C  requires  manual parameter changes, and  D only sends alerts without scaling. Scheduled actions  (B) are the  best fit for  predictable,  recurring traffic                patterns.                 Reference                Links:                 -                [AWS                 Auto                Scaling                Scheduled Scaling](https://docs.aws.amazon.com/auto scaling/ec2/user guide/schedule_time.html)  -  [AWS  Scheduled  Actions  vs  Dynamic
Scaling](https://aws.amazon.com/blogs/aws/auto-scaling-update-scheduled-scaling/)

745.A company uses Amazon RDS for Postgresql databases for its data tier. The company must implement password rotation for the databases. Which solution meets this requirement with the LEAST operational overhead?

A ��Store the password in AWS Secrets Manager. Enable automatic rotation on the secret.

B ��Store the password in AWS Systems Manager Parameter Store. Enable automatic rotation on the parameter.

C ��Store the password in AWS Systems Manager Parameter Store. Write an AWS Lambda function that rotates the password.

D ��Store the password in AWS Key Management Service (AWS KMS). Enable automatic rotation on the AWS KMS key.

�𰸣�A

���� �� Correct Answer: A. Store the  password in AWS Secrets  Manager. Enable automatic rotation on the secret.  Detailed Explanation: AWS Secrets Manager is specifically designed for managing sensitive credentials like database passwords. It offers built - in automatic rotation for supported services, including Amazon RDS for Postgresql. When you enable automatic rotation, Secrets Manager handles the entire process: 1. Generates a new password 2. Updates t he RDS database password 3. Updates

the secret value 4. Handles retries and error recovery automatically This requires minimal operational overhead because: - No custom code needed (unlike Option C) - No manual intervention required - Built - in integration with RDS - Automatic scheduling and execution Other options are less optimal: - B/C (Parameter Store) requires custom  Lambda code for rotation (C) or lacks native rotation support (B) - D (KMS) handles encryption key rotation, not database credentials Reference Links: - [AWS Secrets Manager RDS Rotation](https://docs.aws.amazon.com/secrets manager/latest/user guide/rotating - secrets - rds.html) - [Secrets Manager vs. Parameter Store](https://aws.amazon.com/blogs/security/aws - secrets - manager - vs - aws - systems - manager - parameter - store/)

746.A company runs its application on Oracle Database Enterprise Edition. The company needs to migrate the application and the database to AWS. The company can use the Bring Your Own License (BYOL) model while migrating to AWS. The application uses third-party database features that require privileged access. A solutions architect must design a solution for the database migration. Which solution will meet these requirements MOST cost-effectively?

A ��Migrate the database to Amazon RDS for Oracle by using native tools. Replace the third-party features with AWS Lambda.

B��Migrate the database to Amazon RDS Custom for Oracle by using native tools. Customize the new database settings to support the third-party features.

C �� Migrate the  database to Amazon  Dynamo db  by  using  AWS  Database  Migration  Service  (AWS  DMS).  Customize  the  new database settings to support the third-party features.

D �� Migrate the database to Amazon RDS for  Postgresql  by using AWS  Database  Migration Service (AWS DMS).  Rewrite the application code to remove the dependency on third-party features.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best choice is Amazon RDS Custom for Oracle because it allows the company to maintain their existing Oracle Database Enterprise Edition licenses (BYOL) while still meeting the requirement for privileged access to support third-party database features. Here's why: 1. Why not A or D? - Options A and D suggest migrating to standard RDS (Oracle/Postgresql) or Dynamo db. However, standard RDS does not allow privileged access (like OS-level access or custom database configurations), which is required for third-party features. Rewriting the application (as in D) or replacing features with Lambda (as in A) would add unnecessary cost and complexity. 2. Why not C? - Migrating to Dynamo db (a NoSQL database) would require significant changes to the database  schema and application  logic,  making  it  cost-ineffective  and  risky for an existing Oracle-based  system.  3. Why  B?  -  RDS  Custom  for  Oracle  is  designed  for  databases  needing  privileged  access  (e.g.,  custom patches,  third-party  tools).   It  supports   BYOL  and  allows  full  control  over  the  database  environment,  making   it  the  most cost-effective   way   to    migrate   without    re architecting   the   application    or   database.    Reference   Links:    -   [Amazon    RDS Custom](https://aws.amazon.com/rds/custom/)                -                 [AWS                Bring                Your                 Own                 License (BYOL)](https://aws.amazon.com/licensing/bring-your-own-license/)

747.A large international university has deployed all of its compute services in the AWS Cloud. These services include Amazon EC2, Amazon RDS, and Amazon Dynamo db. The university currently relies on many custom scripts to back up its infrastructure. However, the university wants to centralize management and automate data backups as much as possible by using AWS native options. Which solution will meet these requirements?

A ��Use third-party backup software with an AWS Storage Gateway tape gateway virtual tape library.

B ��Use AWS Backup to configure and monitor all backups for the services in use.

C ��Use AWS Config to set lifecycle management to take snapshots of all data sources on a schedule.

D ��Use AWS Systems Manager State Manager to manage the configuration and monitoring of backup tasks.

�𰸣�B

������ Correct Answer: B. Use AWS Backup to configure and monitor all backups for the services in use. Detailed Explanation: AWS Backup is a fully managed, centralized backup service designed to automate and simplify data backups across multiple AWS services like EC2, RDS, and Dynamo db. Here ��s why it ��s the best choice: 1. Native AWS Service: AWS Backup is built specifically for AWS resources, eliminating the need for custom scripts or third - party tools (unlike Option A). 2. Centralized Management: It provides  a  single  interface  to  create  backup  policies,  schedule  backups,  and  monitor  compliance  for  all  supported  services (meeting the university ��s requirement). 3. Automation: AWS Backup automates lifecycle management (retention, deletion) of backups, unlike Option C (AWS Config), which focuses on configuration tracking, not backups. 4. Simpler than Systems Manager (Option D): While Systems Manager can run scripts, AWS Backup is purpose - built for backups and requires zero code, reducing
complexity. Reference Link: [AWS Backup Documentation](https://docs.aws.amazon.com/aws - backup/)

748.A company wants to build a map of its IT infrastructure to identify and enforce policies on resources that pose security risks. The company's security team must be able to query data in the IT infrastructure map and quickly identify security risks. Which solution will meet these requirements with the LEAST operational overhead?

A ��Use Amazon RDS to store the data. Use SQL to query the data to identify security risks.

B ��Use Amazon Neptune to store the data. Use SPARQL to query the data to identify security risks. C ��Use Amazon Redshift to store the data. Use SQL to query the data to identify security risks.

D ��Use Amazon Dynamo db to store the data. Use PartiQL to query the data to identify security risks.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best choice is Amazon Neptune because it's a graph database designed to handle highly connected data, which is ideal for mapping IT infrastructure (like servers, networks, and dependencies). Security teams  can  use  SPARQL  (a  query   language  for  graph  data)  to  efficiently  traverse   relationships  and  identify   risks,  such  as misconfigured  resources  or  unauthorized  access  paths.  Other  options  like  RDS  (A),  Redshift  (C),  or  Dynamo db  (D)  are  less optimal  because they �� re  built for  structured tables or analytics,  not complex  relationships.  Neptune  minimizes operational overhead  by  natively  supporting  graph  queries  without  requiring  complex  joins  or  manual  data  modeling.  Reference  Link: [Amazon Neptune Use Cases](https://aws.amazon.com/neptune/use-cases/)

749.A large company wants to provide its globally located developers separate, limited size, managed Postgresql databases for development  purposes. The  databases will  be  low  volume. The  developers  need the  databases  only when they are actively working. Which solution will meet these requirements MOST cost-effectively?

A ��Give the developers the ability to launch separate Amazon Aurora instances. Set up a process to shut down Aurora instances at the end of the workday and to start Aurora instances at the beginning of the next workday.

B �� Develop an AWS Service Catalog  product that enforces size  restrictions for launching Amazon Aurora instances. Give the developers access to launch the product when they need a development database.

C ��Create an Amazon Aurora Serverless cluster. Develop an AWS Service Catalog product to launch databases in the cluster with the default capacity settings. Grant the developers access to the product.

D �� Monitor AWS Trusted  Advisor  checks for  idle Amazon  RDS  databases.  Create  a  process to terminate  identified  idle  RDS databases.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The best solution is C (Create an Amazon Aurora Serverless cluster and use AWS Service  Catalog)  because  it  directly  addresses  all  requirements  in  a  cost-effective  way:  1.  Cost-Efficiency:  Aurora  Serverless automatically scales to 0 ACUs (Aurora Capacity Units) when idle, meaning developers only pay for storage when databases are inactive. This eliminates costs for compute resources during non-working hours, unlike traditional RDS/Aurora instances (Options A/B/D). 2. Managed and Limited Size: Aurora Serverless automatically handles scaling within configured capacity limits, ensuring databases stay within budget. AWS Service Catalog enforces standardized configurations, preventing oversized deployments. 3. Global Availability and Simplicity: Developers can launch pre-approved databases on-demand via Service Catalog, regardless of their location. Aurora Serverless  requires no manual start/stop (unlike Option A) or delayed termination (Option D), reducing operational overhead. Why Other Options Are Less Ideal: - A: Manually stopping Aurora instances still incurs storage costs and complicates  global  scheduling.  -   B:  Traditional  Aurora  instances   run  24/7   unless  manually  stopped,  increasing  costs.  -   D: Terminating  idle  databases  risks  data  loss  and  delays  developer  workflows.  Reference   Link:  [Amazon  Aurora  Serverless Pricing](https://aws.amazon.com/rds/aurora/serverless/)

750.A company is building a web application that serves a content management system. The content management system runs on Amazon  EC2 instances  behind an Application  Load  Balancer  (ALB). The  EC2 instances  run  in an Auto Scaling group across multiple  Availability  Zones.  Users  are  constantly  adding  and  updating  files,  blogs,  and  other  website  assets  in  the  content management system. A solutions architect must implement a solution in which all the EC2 instances share up-to-date website content with the least possible lag time. Which solution meets these requirements?

A ��Update the EC2 user data in the Auto Scaling group lifecycle policy to copy the website assets from the EC2 instance that was launched most recently. Configure the ALB to make changes to the website assets only in the newest EC2 instance.

B ��Copy the website assets to an Amazon Elastic File System (Amazon EFS) file system. Configure each EC2 instance to mount the EFS file system locally. Configure the website hosting application to reference the website assets that are stored in the EFS file system.

C ��Copy the website assets to an Amazon S3 bucket. Ensure that each EC2 instance downloads the website assets from the S3 bucket to the attached Amazon Elastic Block Store (Amazon EBS) volume. Run the S3 sync command once each hour to keep files up to date.

D �� Restore  an  Amazon  Elastic  Block  Store  (Amazon  EBS)  snapshot  with  the  website  assets.  Attach  the  EBS  snapshot  as  a secondary EBS volume when a new EC2 instance is launched. Configure the website hosting application to reference the website assets that are stored in the secondary EBS volume.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution is to use Amazon EFS because it provides a shared file system that all EC2 instances can access simultaneously. When users update files, the changes are immediately available to all instances via the mounted EFS volume, ensuring minimal lag. This eliminates the need for manual syncing (like hourly S3 sync in Option C) or relying on outdated snapshots (Option D). Option A ��s approach would create inconsistencies since updates are limited to the newest instance, breaking load balancing. EFS ��s real - time shared storage aligns perfectly with the requirement for up - to - date content  across  all  instances  with   low   latency.  Reference  Links:  -  [Amazon  EFS  Overview](https://aws.amazon.com/efs/)  - [Mounting EFS on EC2 Instances](https://docs.aws.amazon.com/efs/latest/ug/mounting-fs.html)

751.A company's web application consists of multiple Amazon EC2 instances that run behind an Application Load Balancer in a VPC. An Amazon  RDS for  MySQL  DB  instance  contains the  data. The  company  needs the ability to automatically detect  and respond  to  suspicious  or  unexpected  behavior  in  its  AWS  environment.  The  company  already  has  added  AWS  WAF  to  its architecture. What should a solutions architect do next to protect against threats?

A ��Use Amazon Guard duty to perform threat detection. Configure Amazon Event bridge to filter for Guard duty findings and to invoke an AWS Lambda function to adjust the AWS WAF rules.

B ��Use AWS Firewall Manager to perform threat detection. Configure Amazon Event bridge to filter for Firewall Manager findings and to invoke an AWS Lambda function to adjust the AWS WAF web ACL.

C �� Use Amazon Inspector to perform threat detection and to update the AWS WAF rules. Create a VPC network ACL to limit access to the web application.

D ��Use Amazon Macie to perform threat detection and to update the AWS WAF rules. Create a VPC network ACL to limit access to the web application.

�𰸣�A

������ Correct Answer: A Detailed Explanation: Amazon Guard duty is a threat detection service that continuously monitors AWS accounts and workloads for malicious activity using machine learning and threat intelligence. Since the company already uses AWS  WAF  (which  protects  against  web  exploits),  adding  Guard duty  would  detect  broader  threats  like  unauthorized  access, compromised  instances, or  reconnaissance attacks.  Here ��s  how  it works:  1.  Guard duty  identifies suspicious  behavior  (e.g., unusual API calls, compromised EC2 instances). 2. Amazon Event bridge routes these findings to an AWS Lambda function. 3. The Lambda function automatically adjusts AWS WAF rules to block malicious IPs or patterns. Other options are incorrect: - Option B: AWS  Firewall  Manager  manages WAF  rules across accounts  but isn ��t designed for threat detection. - Option C/D: Amazon Inspector scans for vulnerabilities (not real-time threats), and Amazon Macie protects sensitive data (irrelevant here). Reference Links:   -   [AWS   Guard duty](https://aws.amazon.com/guard duty/)    -   [AWS   WAF](https://aws.amazon.com/waf/)    -   [Amazon Event bridge](https://aws.amazon.com/event bridge/)

752.A company is planning to run a group of Amazon EC2 instances that connect to an Amazon Aurora database. The company has built an AWS Cloud formation template to deploy the EC2 instances and the Aurora DB cluster. The company wants to allow the  instances  to  authenticate  to  the  database  in  a  secure  way.  The  company  does  not  want  to  maintain  static  database credentials. Which solution meets these requirements with the LEAST operational effort?

A ��Create a database user with a user name and password. Add parameters for the database user name and password to the Cloud formation template. Pass the parameters to the EC2 instances when the instances are launched.

B �� Create  a  database  user  with  a  user  name  and  password.  Store  the  user  name  and  password  in  AWS  Systems  Manager Parameter Store. Configure the EC2 instances to retrieve the database credentials from Parameter Store.

C �� Configure the  DB  cluster  to  use  IAM  database  authentication.  Create  a  database  user  to  use  with  IAM  authentication. Associate a role with the EC2 instances to allow applications on the instances to access the database.

D ��Configure the DB cluster to use IAM database authentication with an IAM user. Create a database user that has a name t hat matches the  IAM  user.  Associate the  IAM  user  with  the  EC2  instances  to  allow  applications  on  the  instances  to  access  the database.

�𰸣�C

������ Correct Answer C Detailed Explanation The best solution is Option C, which uses IAM database authentication. Here ��s why: - IAM database authentication allows EC2 instances to authenticate to Aurora using temporary IAM credentials instead of static passwords. This eliminates the need to manage or rotate static credentials manually. - An IAM role is attached to the EC2 instances. Applications on the instances can use this role to generate a short - lived authentication token (valid for 15 minutes) to connect to the database. - This approach is secure (no passwords stored), requires no operational effort (credentials are auto - rotated), and aligns with AWS best practices for credential management. Why other options are incorrect: - A   �� B: Both rely on static credentials (username/password), which violate the requirement to avoid maintaining static credentials. - D: Using an IAM user (instead of a role) still involves long - term credentials, which are less secure and require manual management. Reference Link                                 [AWS                                IAM                                 Database                                 Authentication                                 for
Aurora](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/Using with rds.I am db auth.html)

753.A company wants to configure its Amazon Cloud front distribution to use SSL/TLS certificates. The company does not want to use  the  default  domain   name  for  the  distribution.   Instead,  the  company  wants  to  use  a  different  domain  name  for  the distribution. Which solution will deploy the certificate without incurring any additional costs?

A ��Request an Amazon issued private certificate from AWS Certificate Manager (ACM) in the us-east-1 Region.

B ��Request an Amazon issued private certificate from AWS Certificate Manager (ACM) in the us-west-1 Region. C ��Request an Amazon issued public certificate from AWS Certificate Manager (ACM) in the us-east-1 Region. D ��Request an Amazon issued public certificate from AWS Certificate Manager (ACM) in the us-west-1 Region.

�𰸣�C

������ Correct Answer: C Detailed Explanation: To use a custom domain name with HTTPS in Amazon Cloud front, you must use a public SSL/TLS certificate from AWS Certificate Manager (ACM) in the us - east - 1 (N. Virginia) region. Here's why: 1. Region Requirement: Cloud front is a global service, but it only accepts ACM certificates provisioned in us - east - 1. Certificates from other regions (like us - west - 1 in options B/D) won't work, even if valid. 2. Public Certificate: A public certificate (option C) is required for internet - facing domains. Private certificates (options A/B) are for internal resources (e.g., VPCs or private APIs), not public CDN endpoints. 3. No Extra Cost: ACM public certificates are free. Using a custom domain with ACM avoids paying for third - party certificates (e.g., 0/year for a basic cert from some providers). Example: If your custom domain is `example.com`, request a public ACM certificate for `example.com` in us - east - 1, then attach it to your Cloud front distribution. Reference Links:
-  [AWS  Docs:  Using  SSL/TLS  with  Cloud front](https://docs.aws.amazon.com/Amazon cloud front/latest/Developer guide/cdn  - https.html) - [ACM Certificate Regions](https://docs.aws.amazon.com/acm/latest/user guide/acm - regions.html)

754.A company creates operations data and stores the data in an Amazon S3 bucket. For the company's annual audit, an external consultant needs to access an annual report that is stored in the S3 bucket. The external consultant needs to access the report for 7 days. The company must implement a solution to allow the external consultant access to only the report. Which solution will meet these requirements with the MOST operational efficiency?

A ��Create a new S3 bucket that is configured to host a public static website. Migrate the operations data to the new S3 bucket. Share the S3 website URL with the external consultant.

B ��Enable public access to the S3 bucket for 7 days. Remove access to the S3 bucket when the external consultant completes the audit.

C ��Create a new IAM user that has access to the report in the S3 bucket. Provide the access keys to the external consultant. Revoke the access keys after 7 days.

D��Generate a pre signed URL that has the required access to the location of the report on the S3 bucket. Share the pre signed URL with the external consultant.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The best solution is to generate a pre signed URL (Option D). A pre signed URL grants temporary access to a specific S3 object (the report) for a limited time (7 days) without exposing the entire bucket or requiring complex user management. Here's why: 1. Security: The URL only allows access to the specific report, not the whole bucket or other files. 2. Simplicity: No need to create IAM users (Option C), modify bucket policies (Option B), or migrate data (Option A). Just generate the  URL and share it. 3. Automatic  Expiry: The access automatically stops after 7 days, eliminating manual cleanup. Other options are risky (Options A/B expose too much data) or operationally heavy (Option C requires user/key management). Pre signed URLs are purpose-built for temporary, secure access to private S3 objects. Reference Link: [Amazon S3 Pre signed URLs](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Share object pre signed url.html)

755.A company plans to run a high performance computing (HPC) workload on Amazon EC2 Instances. The workload requires low-latency  network  performance  and  high  network  throughput  with  tightly  coupled  node-to-node  communication.  Which solution will meet these requirements?

A ��Configure the EC2 instances to be part of a cluster placement group.

B ��Launch the EC2 instances with Dedicated Instance tenancy.

C ��Launch the EC2 instances as Spot Instances.

D ��Configure an On-Demand Capacity Reservation when the EC2 instances are launched.

�𰸣�A

������ Correct Answer A �� Configure the EC2 instances to be part of a cluster placement group. Detailed Explanation For tightly coupled workloads like High Performance Computing (HPC), cluster placement groups are designed to minimize network latency and maximize throughput by placing instances physically close to one another within the same Availability Zone. This ensures fast, low - latency communication between nodes, which is critical for HPC tasks (e.g., simulations, rendering). - Option B (Dedicated Instance tenancy) only isolates hardware for compliance/security but does not optimize network performance. - Option C (Spot Instances)  prioritizes  cost  savings  over  reliability/performance  and  risks  interruption.  -  Option  D  (Capacity  Reservations) guarantees    instance    availability     but    does     not    address     network    requirements.     Reference    Link     [AWS    Placement Groups](https://docs.aws.amazon.com/AWSEC2/latest/User guide/placement-groups.html)

756.A company has primary and secondary data centers that are 500 miles (804.7 km) apart and interconnected with high-speed fiber-optic cable. The company needs a highly available and secure network connection between its data centers and a VPC on AWS for a mission-critical workload. A solutions architect must choose a connection solution that provides maximum resiliency. Which solution meets these requirements?

A ��Two AWS  Direct Connect connections from the  primary data center terminating at two  Direct Connect  locations on two separate devices

B ��A single AWS Direct Connect connection from each of the primary and secondary data centers terminating at one Direct

Connect location on the same device

C��Two AWS Direct Connect connections from each of the primary and secondary data centers terminating at two Direct Connect locations on two separate devices

D ��A single AWS Direct Connect connection from each of the primary and secondary data centers terminating at one Direct Connect location on two separate devices

�𰸣�C

������ Correct Answer: C Detailed Explanation: To achieve maximum resiliency between two on-premises data centers and AWS, the solution must eliminate single  points of failure at every layer. Here's why Option C is the best choice:  1. Per  Data Center Redundancy: - Two Direct Connect connections from each data center ensure redundancy if one connection fails at either the primary or secondary site. 2. Cross-Location Redundancy: - Terminating connections at two separate Direct Connect locations (physical AWS data centers) protects against outages at a single location. 3. Device Redundancy: - Using two separate devices (routers/switches) at each Direct Connect location prevents failures caused by hardware issues. Why Other Options Fail: - A: Only the  primary  data  center  has  redundancy; the  secondary  has  none.  -  B/D:  A  single  Direct  Connect  location  or  shared  device creates        a         single         point        of         failure.         Reference        Links:         -         [AWS         Direct        Connect         Resiliency Recommendations](https://docs.aws.amazon.com/directconnect/latest/User guide/reliability.html)        -        [Highly        Available Connections with AWS Direct Connect](https://aws.amazon.com/directconnect/features/#High_Availability)

757.A solutions architect is creating an application. The application will run on Amazon EC2 instances in private subnets across multiple Availability Zones in a VPC. The EC2 instances will frequently access  large files that contain confidential information. These files are stored in Amazon S3 buckets for processing. The solutions architect must optimize the network architecture to minimize data transfer costs. What should the solutions architect do to meet these requirements?

A��Create a gateway endpoint for Amazon S3 in the VPC. In the route tables for the private subnets, add an entry for the gateway endpoint.

B ��Create a single NAT gateway in a public subnet. In the route tables for the private subnets, add a default route that points to the NAT gateway.

C ��Create an AWS Private link interface endpoint for Amazon S3 in the VPIn the route tables for the private subnets, add an entry for the interface endpoint.

D ��Create one NAT gateway for each Availability Zone in public subnets. In each of the route tables for the private subnets, add a default route that points to the NAT gateway in the same Availability Zone.

�𰸣�A

������ Correct Answer: A Detailed Explanation: To minimize data transfer costs for EC2 instances in private subnets accessing S3, the  best solution  is to use a VPC Gateway  Endpoint for S3.  Here's why:  1. Gateway  Endpoint vs.  NAT Gateway:  - A Gateway Endpoint (Option A) allows direct, private connectivity between the VPC and S3 without using the public internet. This avoids data transfer costs associated with NAT gateways (Options B/D), which route traffic through the public internet. - NAT gateways (Options B/D) incur costs for data processing and internet data transfer, which is expensive for large files. 2. Gateway Endpoint vs. Interface Endpoint: - While an Interface Endpoint (Option C, powered by AWS Private link) also provides private connectivity, it is designed for services that don ��t support Gateway Endpoints (e.g., Dynamo db uses Gateway, but most other AWS services use Interface). - S3 supports Gateway Endpoints, which are free to use (you only pay for S3 requests). Interface Endpoints, on the other hand, incur hourly costs per endpoint and per GB of data processed. 3. Network Architecture: - Gateway Endpoints are

simple to set up. You create the endpoint in the VPC and add a route to it in the private subnet ��s route table. Traffic to S3 automatically uses the endpoint, keeping it within the AWS network. - NAT gateways require public subnets and complex routing, increasing costs and complexity. Why not other options? - NAT Gateways (B/D): Incur unnecessary internet data transfer costs and NAT gateway fees. -  Interface Endpoint (C):  More expensive and unnecessary for S3, which supports Gateway Endpoints.
Reference                            Links:                            -                             [AWS                            VPC                            Endpoints                            for
S3](https://docs.aws.amazon.com/AmazonS3/latest/user guide/private link-interface-endpoints.html)   -   [Gateway   vs.    Interface Endpoints](https://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/)

758.A  company  wants  to  relocate  its  on-premises  MySQL  database  to  AWS.  The  database  accepts  regular  imports  from  a client-facing application, which causes a high volume of write operations. The company is concerned that the amount of traffic might be causing performance issues within the application. How should a solutions architect design the architecture on AWS?

A��Provision an Amazon RDS for MySQL DB instance with Provisioned IOPS SSD storage. Monitor write operation metrics by using Amazon Cloud watch. Adjust the provisioned IOPS if necessary.

B ��Provision an Amazon RDS for MySQL DB instance with General Purpose SSD storage. Place an Amazon Elastic ache cluster in front of the DB instance. Configure the application to query Elastic ache instead.

C ��Provision an Amazon Document db (with MongoDB compatibility) instance with a memory optimized instance type. Monitor Amazon Cloud watch for performance-related issues. Change the instance class if necessary.

D ��Provision an Amazon Elastic File System (Amazon EFS) file system in General Purpose performance mode. Monitor Amazon Cloud watch for IOPS bottlenecks. Change to Provisioned Throughput performance mode if necessary.

�𰸣�A

�� �� �� Correct Answer: A  Detailed  Explanation: The  company's  main  issue  is  high write operations affecting  performance. Amazon RDS for MySQL with Provisioned IOPS SSD storage is designed for I/O-intensive workloads (like frequent writes) because it allows precise control over input/output operations per second (IOPS). Monitoring with Cloud watch helps track performance and adjust IOPS if needed. Option B uses Elastic ache (a caching service), which primarily optimizes read operations, not writes. Option C suggests Document db (a NoSQL database), which isn ��t compatible with  MySQL. Option  D uses  EFS (a file storage service),     which      isn     ��  t      suitable     for      database      workloads.      Reference      Links:     -      [Amazon      RDS      Storage Types](https://docs.aws.amazon.com/Amazon rds/latest/User guide/CHAP_Storage.html)                       -                         [Provisioned
IOPS](https://aws.amazon.com/rds/provisioned-iops/)

759.A  company  runs  an  application  in  the  AWS  Cloud  that  generates  sensitive  archival  data  files.  The  company  wants  to re architect the application's data storage. The company wants to encrypt the data files and to ensure that third parties do not have access to the data before the data is encrypted and sent to AWS. The company has already created an Amazon S3 bucket. Which solution will meet these requirements?

A ��Configure the S3 bucket to use client-side encryption with an Amazon S3 managed encryption key. Configure the application to use the S3 bucket to store the archival files.

B ��Configure the S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Configure the application to use the S3 bucket to store the archival files.

C ��Configure the S3 bucket to use dual-layer server-side encryption with AWS KMS keys (SSE-KMS). Configure the application to use the S3 bucket to store the archival files.

D �� Configure  the application to  use  client-side  encryption with  a  key  stored  in AWS  Key  Management  Service  (AWS  KMS). Configure the application to store the archival files in the S3 bucket.

�𰸣�D

������Correct Answer: D Detailed Explanation: The key requirements here are that the data must be encrypted before it reaches AWS, and third parties shouldn ��t have access to unencrypted data at any point. Here's why option D works best: 1. Client-Side Encryption: - With client-side encryption, the application encrypts the data  locally  (on the client side)  before uploading it to Amazon S3. This ensures that the data is already encrypted during transit and storage, so no third party (including AWS) can access  the   unencrypted  version.  2.  AWS   KMS  for   Key  Management:  -   Using  AWS   Key  Management  Service   (KMS)  to store/manage encryption keys adds security. The keys are never exposed to the application or third parties, and KMS provides auditing and access controls for the  keys. Why other options fail: - A/B/C: These options use server-side encryption  (SSE-S3, SSE-KMS) or client-side encryption with S3-managed keys. Server-side encryption means data is encrypted after it reaches S3, so unencrypted data could be intercepted during upload. Client-side encryption with S3-managed keys (option A) is less secure than KMS-managed keys (option D). Simple analogy: Imagine sending a locked safe (encrypted data) to AWS. Only you hold the key (managed by KMS). Options A/B/C are like sending an open safe to AWS and letting AWS lock it afterward, which leaves the data exposed            during            transit.             Reference            Links:            -             [Client-Side            Encryption             with            AWS KMS](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Using client side encryption.html)            -             [AWS            KMS Overview](https://aws.amazon.com/kms/)

760.A company uses Amazon RDS with default backup settings for its database tier. The company needs to make a daily backup of the database to meet regulatory requirements. The company must retain the backups for 30 days. Which solution will meet these requirements with the LEAST operational overhead?

A ��Write an AWS Lambda function to create an RDS snapshot every day.

B ��Modify t he RDS database to have a retention period of 30 days for automated backups.

C ��Use AWS Systems Manager Maintenance Windows to modify the RDS backup retention period.

D ��Create a manual snapshot every day by using the AWS CLI. Modify the RDS backup retention period.

�𰸣�B

���� �� Correct Answer B Detailed Explanation When using Amazon RDS with default backup settings, automated backups are enabled with a retention period of 7 days. To meet the requirement of retaining backups for 30 days, you simply need to modify the backup retention period to 30 days. This is a built - in RDS feature that requires no additional scripting, manual processes, or external tools. - Why B is correct: - RDS automated backups handle daily backups automatically. - Adjusting the retention period to 30 days ensures backups are retained for the required duration. - No operational overhead (no Lambda functions, CLI scripts, or manual steps). - Why other options are incorrect: - A: Creating snapshots via  Lambda adds unnecessary complexity (code maintenance, permissions, error handling). - C: Systems Manager isn't needed to adjust RDS backup settings��this can be done directly via t he RDS console/API. - D: Manual snapshots require daily human/script intervention, increasing operational effort. Reference                                   Links                                  -                                   [Amazon                                   RDS                                    Backup
Retention](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_Working with automated backups.html)

761.A company that runs its application on AWS uses an Amazon Aurora DB cluster as its database. During peak usage hours when multiple users access and read the data, the monitoring system shows degradation of database performance for the write queries. The company wants to increase the scalability of the application to meet peak usage demands. Which solution will meet these requirements MOST cost-effectively?

A �� Create a second Aurora DB cluster. Configure a copy job to replicate the users �� data to the new database.  Update the application to use the second database to read the data.

B ��Create an Amazon Dynamo db Accelerator (DAX) cluster in front of the existing Aurora DB cluster. Update the application to use the DAX cluster for read-only queries. Write data directly to the Aurora DB cluster.

C �� Create  an Aurora  read  replica  in  the  existing  Aurora  DB  cluster.  Update  the  application  to  use  the  replica  endpoint  for read-only queries and to use the cluster endpoint for write queries.

D ��Create an Amazon Redshift cluster. Copy the users' data to the Redshift cluster. Update the application to connect to the Redshift cluster and to perform read-only queries on the Redshift cluster.

�𰸣�C

������ Correct Answer: C Detailed Explanation: When an Amazon Aurora database experiences performance issues with write queries during peak read traffic, the most cost - effective solution is to offload read operations to an Aurora Read Replica. Aurora Read Replicas are specialized copies of the primary database instance that handle read - only queries, reducing the load on the primary  instance.  This  allows  the  primary  instance  to  focus  on  write  operations,  improving  overall  performance.  Option  C achieves this  by:  1. Creating a  Read  Replica within  the existing Aurora cluster  (no additional  cluster costs). 2.  Directing  read queries to the replica's endpoint using Aurora's built - in replication (no manual data copying). 3. Using the cluster endpoint for writes  (no  application  changes  beyond  query  routing).  Other  options  are  less  optimal:  -  A:  Duplicating  the  entire  cluster  is expensive and requires complex data synchronization. - B: DAX is designed for Dynamo db, not Aurora, and isn't compatible here.
-    D:     Redshift     is    for    analytics,     not     real    -     time    application     queries.    Reference     Link:     [Amazon    Aurora     Read Replicas](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/aurora-read-replicas.html)

762.A company runs a web application on multiple Amazon EC2 instances in a VPC. The application needs to write sensitive data to an Amazon S3 bucket. The data cannot be sent over the public internet. Which solution will meet these requirements?

A ��Create a gateway VPC endpoint for Amazon S3. Create a route in the VPC route table to the endpoint.

B ��Create an internal Network Load Balancer that has the S3 bucket as the target.

C ��Deploy the S3 bucket inside the Vp create a route in the VPC route table to the bucket.

D ��Create an AWS Direct Connect connection between the VPC and an S3 regional endpoint.

�𰸣�A

���� �� Correct Answer: A Detailed Explanation: To securely connect EC2 instances in a VPC to Amazon S3 without using the public  internet, a Gateway VPC  Endpoint  is the correct  solution.  Here ��s why: - A  Gateway VPC  Endpoint creates  a  private connection between the VPC and S3, allowing  EC2 instances to access S3 buckets directly through AWS ��s  internal network, avoiding public internet exposure. - You add a route to the VPC route table pointing S3 traffic to this endpoint. This ensures all S3-bound traffic stays within AWS ��s secure network. Other options are incorrect: - B: Network Load Balancers (NLB) can ��t directly target S3  buckets. - C: S3  buckets are  regional services and can ��t  be deployed  inside a VPC. -  D:  Direct Connect  is unnecessary here. While it provides dedicated network links, it ��s costlier and more complex than a Gateway VPC  Endpoint. Reference:                                            [AWS                                             VPC                                             Endpoints                                            for
S3](https://docs.aws.amazon.com/AmazonS3/latest/user guide/private link-interface-endpoints.html)

763.A company runs its production workload on Amazon EC2 instances with Amazon Elastic Block Store (Amazon EBS) volumes. A solutions architect  needs to analyze the current  EBS volume cost and to  recommend optimizations. The  recommendations need to include estimated monthly saving opportunities. Which solution will meet these requirements?

A ��Use Amazon Inspector reporting to generate EBS volume recommendations for optimization.

B ��Use AWS Systems Manager reporting to determine EBS volume recommendations for optimization.

C ��Use Amazon Cloud watch metrics reporting to determine EBS volume recommendations for optimization.

D ��Use AWS Compute Optimizer to generate EBS volume recommendations for optimization.

�𰸣�D

������ Correct Answer: D Detailed Explanation: AWS Compute Optimizer is the correct service to analyze EBS volume costs and recommend optimizations with estimated  monthly savings.  It  uses  machine  learning to  analyze  historical usage  patterns and provides actionable recommendations, such as adjusting volume types (e.g., switching from gp2 to gp3) or resizing underutilized volumes. - Amazon Inspector (A) focuses on security vulnerability assessments, not cost optimization. - AWS Systems Manager (B) manages operational tasks like patching or inventory tracking but doesn ��t specialize in cost-saving recommendations. - Amazon Cloud watch  (C)  monitors  performance  metrics  but doesn ��t generate optimization  suggestions. Compute Optimizer directly addresses cost-efficiency for  EBS volumes  by  analyzing  usage  and  providing  savings  estimates,  making  it  the  best fit for the scenario. Reference Link: [AWS Compute Optimizer Features](https://aws.amazon.com/compute-optimizer/)

764.A global company runs its workloads on AWS. The company's application uses Amazon S3 buckets across AWS Regions for sensitive data storage and analysis. The company stores millions of objects in multiple S3 buckets daily. The company wants to identify all S3 buckets that are not versioning-enabled. Which solution will meet these requirements?

A ��Use Amazon S3 Storage Lens to identify all S3 buckets that are not versioning-enabled across Regions.

B ��Enable IAM Access Analyzer for S3 to identify all S3 buckets that are not versioning-enabled across Regions.

C ��Create an S3 Multi-Region Access Point to identify all S3 buckets that are not versioning-enabled across Regions.

�𰸣�B

������ Correct Answer: A Detailed Explanation: The correct solution is to use Amazon S3 Storage Lens. S3 Storage Lens provides organization-wide  visibility  into  storage  usage  and  activity  metrics,  including  advanced  metrics  like  bucket  properties  (e.g., versioning  status,  encryption,  replication).  It  can  analyze  buckets  across  all  AWS  Regions  and  accounts  in  the  organization, making  it the  right tool to identify  buckets without versioning enabled. Why not the other options? - Option  B  (IAM Access Analyzer for S3): This service identifies S3 buckets with policies that grant external/public access, not bucket configurations like versioning. - Option C (Multi-Region Access Point): This  routes requests to buckets across regions but does not report bucket settings. - AWS Config (not listed): While AWS Config can track bucket configurations, it ��s not an option here. Among the given choices,   S3   Storage   Lens   (A)   is   the   only   service   that   meets   the   requirement.   Reference   Link:    [Amazon   S3   Storage Lens](https://docs.aws.amazon.com/AmazonS3/latest/user guide/storage-lens.html)

765.A company wants to enhance its ecommerce order-processing application that is deployed on AWS. The application must process each order exactly once without affecting the customer experience during unpredictable traffic surges. Which solution will meet these requirements?

A ��Create an Amazon Simple Queue Service (Amazon SQS) FI FO queue. Put all the orders in the SQS queue. Configure an AWS Lambda function as the target to process the orders.

B ��Create an Amazon Simple Notification Service (Amazon SNS) standard topic. Publish all the orders to the SNS standard topic. Configure the application as a notification target.

C ��Create a flow by using Amazon AppFlow. Send the orders to the flow. Configure an AWS Lambda function as the target to process the orders.

D ��Configure AWS X-Ray in the application to track the order requests. Configure the application to process the orders by pulling the orders from Amazon Cloud watch.

�𰸣�A

������ Correct Answer: A Detailed Explanation: To meet the requirements of processing each order exactly once and handling unpredictable traffic surges, Amazon SQS FI FO (First-In-First-Out) queues paired with AWS Lambda are the best solution. Here ��s why: 1. Exactly-Once Processing: SQS F IFO queues ensure messages (orders) are processed exactly once by using de duplication IDs.  This  prevents  duplicate  order  processing,  which  is  critical  for  ecommerce  applications.  2.  Scalability:  AWS  Lambda automatically  scales  with  the  number  of  messages  in  the  queue.  During  traffic  surges,  Lambda  spins  up  more  instances  to process orders quickly, ensuring the customer experience remains smooth. 3. Order Integrity: FI FO queues preserve the order of messages, ensuring orders are processed in the sequence they are received (important for tasks like inventory updates). Other options fail  because:  -  B  (SNS):  SNS  topics  deliver  messages  with  at-least-once  semantics,  risking  duplicates.  -  C  (AppFlow): Amazon AppFlow is designed for data integration (e.g., SaaS to AWS), not real-time order processing. - D (X-Ray/Cloud watch): These  are  monitoring/tracing  tools,   not  designed  for  queuing  or  processing  orders.   Reference  Links:  -   [Amazon  SQS   FI FO Queues](https://aws.amazon.com/sqs/features/fifo/)                          -                          [AWS                           Lambda                          with
SQS](https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html)

766.A  company  has  two  AWS  accounts:  Production  and   Development.  The  company  needs  to  push  code  changes  in  the Development account to the Production account. In the alpha phase, only two senior developers on the development team need access to the Production account. In the beta phase, more developers will need access to perform testing. Which solution will meet these requirements?

A ��Create two policy documents by using the AWS Management Console in each account. Assign the policy to developers who need access.

B ��Create an IAM role in the Development account. Grant the IAM role access to the Production account. Allow developers to assume the role.

C ��Create an IAM role in the Production account. Define a trust policy that specifies the Development account. Allow developers to assume the role.

D ��Create an IAM group in the Production account. Add the group as a principal in a trust policy that specifies the Production account. Add developers to the group.

�𰸣�C

������ Correct Answer: C Detailed Explanation: The best solution is to create an IAM role in the Production account (the target account) and configure its trust policy to allow users from the Development account (the source account) to assume this role. Here's why this works: 1. Cross-Account Access: IAM roles with trust policies are the standard way to grant cross-account access

in AWS. The Production account ��s IAM role defines who can access it (via the trust policy) and what permissions they get (via the role ��s permissions policy). 2. Trust Policy: By specifying the Development account ��s ID in the trust policy, you allow any IAM user/role in the Development account (with the right permissions) to assume this role. This avoids sharing credentials between accounts. 3. Scalability: In the alpha phase, only 2 developers need access. In the beta phase, you simply grant the Assume role permission to more developers in the Development account (via IAM policies there). No changes are needed in the Production account ��s role, making it easy to scale. Why Other Options Fail: - A: Creating policies in both accounts manually doesn ��t scale and  requires  per-user  management.  -  B:  Roles  must  be  created  in  the  target  account  (Production),  not  the  source  account (Development). - D: IAM groups can ��t be principals in trust policies; trust policies specify AWS accounts or IAM entities, not groups.             Reference              Links:              -             [IAM              Tutorial:              Delegate             Access              Across             AWS Accounts](https://docs.aws.amazon.com/IAM/latest/User guide/tutorial_cross-account-with-roles.html)   -   [How   to   Use   Trust Policies with IAM Roles](https://aws.amazon.com/blogs/security/how-to-use-trust-policies-with-iam-roles/)

767.A company wants to restrict access to the content of its web application. The company needs to protect the content by using authorization  techniques  that  are  available  on  AWS.  The  company  also  wants  to  implement  a  serverless  architecture  for authorization and authentication that has low login latency. The solution must integrate with the web application and serve web content globally. The application currently has a small user base, but the company expects the application's user base to increase. Which solution will meet these requirements?

A ��Configure Amazon Cognito for authentication. Implement Lambda@Edge for authorization. Configure Amazon Cloud front to serve the web application globally.

B ��Configure AWS Directory Service for Microsoft Active Directory for authentication. Implement AWS Lambda for authorization. Use an Application Load Balancer to serve the web application globally.

C �� Configure  Amazon  Cognito  for  authentication.  Implement  AWS  Lambda  for  authorization.  Use  Amazon  S3  Transfer Acceleration to serve the web application globally.

D  �� Configure  AWS   Directory  Service  for   Microsoft  Active   Directory  for  authentication.   Implement   Lambda@Edge  for authorization. Use AWS Elastic Beanstalk to serve the web application globally.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The question requires a serverless solution for authentication/authorization with low latency and global content delivery. Here's why Option A is the best fit:  1. Amazon Cognito: - A fully managed serverless service  for  user  authentication  (sign-up/sign-in).  -  Scales  automatically  as  the  user   base  grows.  -   Integrates  directly  with web/mobile  apps.  2.  Lambda@Edge:  -  Runs  serverless  code  at  AWS  Cloud front  edge  locations  (closer  to  users).  -  Enables low-latency authorization checks globally  by processing  requests at the edge. -  Regular AWS  Lambda (Options B/C)  runs in a single region, adding latency for distant users. 3. Amazon Cloud front: - A global Content Delivery Network (CDN) to serve web content  from  edge  locations.  -  Ensures  fast  content  delivery  worldwide.  -  Integrates  with   Lambda@Edge  for  real-time authorization at the edge. Why Other  Options  Fail:  - Option  B/D  (Directory  Service): Tied to  Microsoft  Active  Directory,  not serverless.  -  Option  C  (S3  Transfer  Acceleration):  Optimized  for  file  uploads,  not  global  web  content  delivery.  -  Option  B/D (ALB/Elastic Beanstalk): Regional services, not global. Reference Links: - [Amazon Cognito](https://aws.amazon.com/cognito/) - [Lambda@Edge](https://aws.amazon.com/lambda/edge/) - [Amazon Cloud front](https://aws.amazon.com/cloud front/)

768.A  development  team   uses  multiple  AWS  accounts  for   its  development,  staging,  and   production  environments.  Team members  have  been  launching  large  Amazon  EC2  instances  that  are  underutilized.  A  solutions  architect  must  prevent  large instances from being launched in all accounts. How can the solutions architect meet this requirement with the LEAST operational overhead?

A ��Update the IAM policies to deny the launch of large EC2 instances. Apply the policies to all users.

B ��Define a resource in AWS Resource Access Manager that prevents the launch of large EC2 instances.

C ��Create an IAM role in each account that denies the launch of large EC2 instances. Grant the developers IAM group access to the role.

D ��Create an organization in AWS Organizations in the management account with the default policy. Create a service control policy (SCP) that denies the launch of large EC2 instances, and apply it to the AWS accounts.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The best solution is to use AWS Organizations with a Service Control Policy (SCP). SCPs allow centralized  control over  permissions  across  multiple  AWS  accounts  in  an  organization.  By  creating  an  SCP  in the management  account  and  applying  it  to  all  member  accounts,  you  can  universally  block  the  launch  of  large  EC2  instances without manually configuring each account. This reduces operational overhead compared to options like updating IAM policies per  user  (A)  or  creating  roles  per  account  (C).  AWS  Resource  Access  Manager  (B)  is  unrelated  to  permission  restrictions. Reference                                 Links:                                  -                                 [AWS                                  Organizations                                 and
SCPs](https://docs.aws.amazon.com/organizations/latest/user guide/orgs_manage_policies_scps.html)      -       [Controlling      EC2 Instance                                                                                                     Types                                                                                                      with
SCPs](https://aws.amazon.com/blogs/security/control-access-to-aws-services-using-service-control-policies/)

769.A company has migrated a fleet of hundreds of on-premises virtual machines (VMs) to Amazon EC2 instances. The instances run a diverse fleet of Windows Server versions along with several Linux distributions. The company wants a solution that will automate inventory and updates of t he operating systems. The company also needs a summary of common vulnerabilities of each instance for regular monthly reviews. What should a solutions architect recommend to meet these requirements?

A �� Set  up AWS  Systems  Manager  Patch  Manager to  manage all the  EC2  instances.  Configure AWS Security  Hub to  produce monthly reports.

B �� Set  up AWS Systems  Manager  Patch  Manager to  manage all the  EC2  instances.  Deploy Amazon  Inspector,  and configure monthly reports.

C ��Set up AWS Shield Advanced, and configure monthly reports. Deploy AWS Config to automate patch installations on the EC2 instances.

D ��Set up Amazon Guard duty in the account to monitor all EC2 instances. Deploy AWS Config to automate patch installations on the EC2 instances.

�𰸣�B

������Correct Answer: B Detailed Explanation: The best solution here is B because it uses AWS Systems Manager Patch Manager for automated patching and Amazon Inspector for vulnerability assessments with monthly reports. Here's why: 1. AWS Systems Manager Patch Manager: - Automates OS Updates: It handles patch management for both Windows and Linux instances, which is crucial for the company's diverse OS environment. - Centralized Control: It lets you define patch policies (e.g., which patches to install, when to install them) across all EC2 instances, saving time compared to manual updates. 2. Amazon Inspector: - Scans for Vulnerabilities:   It    automatically   checks    EC2    instances   for   security    issues    like   missing    patches,    exposed   secrets,    or misconfigurations. - Generates Detailed Reports: Amazon Inspector provides a prioritized list of vulnerabilities, including severity levels and remediation steps. This aligns with the requirement for monthly vulnerability summaries. Why Other Options Fail: -

Option A: Security Hub aggregates findings from multiple AWS services (including Inspector) but isn ��t designed to generate detailed vulnerability reports itself. It ��s better for a high - level dashboard, not monthly instance - specific reviews. - Option C: AWS Shield Advanced protects against DDoS attacks, which isn��t related to patching or vulnerability reporting. AWS Config tracks resource configurations  but doesn ��t  automate patching. - Option  D: Guard duty detects threats like unauthorized access or malware but doesn ��t scan for OS vulnerabilities. AWS Config can ��t install patches automatically. References: - [AWS Systems Manager  Patch  Manager](https://docs.aws.amazon.com/systems-manager/latest/user guide/systems-manager  -  patch.html)  - [Amazon Inspector](https://docs.aws.amazon.com/inspector/latest/user/what - is - inspector.html)

770.A company hosts its application in the AWS Cloud. The application runs on Amazon EC2 instances in an Auto Scaling group behind an  Elastic  Load  Balancing  (ELB)  load  balancer.  The  application  connects to  an Amazon  Dynamo db table.  For  disaster recovery (DR) purposes, the company wants to ensure that the application is available from another AWS Region with minimal downtime. Which solution will meet these requirements with the LEAST downtime?

A ��Create an Auto Scaling group and an ELB in the DR Region. Configure the Dynamo db table as a global table. Configure DNS failover to point to the new DR Region's ELB.

B��Create an AWS Cloud formation template to create EC2 instances, ELBs, and Dynamo db tables to be launched when necessary. Configure DNS failover to point to the new DR Region's ELB.

C ��Create an AWS Cloud formation template to create EC2 instances and an ELB to be launched when necessary. Configure the Dynamo db table as a global table. Configure DNS failover to point to the new DR Region's ELB.

D��Create an Auto Scaling group and an ELB in the DR Region. Configure the Dynamo db table as a global table. Create an Amazon Cloud watch alarm with an evaluation period of 10 minutes to invoke an AWS Lambda function that updates Amazon Route 53 to point to the DR Region's ELB.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution is to set up a standby environment in the Disaster Recovery (DR) Region before a disaster occurs. Here's why Option A works best: 1. Pre-configured resources: An Auto Scaling group and ELB already exist in the DR Region, enabling immediate scaling and traffic routing. 2. Global Dynamo db Table: Dynamo db Global Tables automatically replicate data across regions, ensuring your DR Region has real-time data access. 3. Fast DNS failover: Route 53 DNS failover can switch traffic in seconds (not minutes/hours) if the primary Region fails. Other options are worse because: - Options B/C require time-consuming resource creation during a disaster via Cloud formation. - Option D adds a 10-minute delay from    Cloud watch    alarms,     while    Route     53    health     checks    work    faster.     Reference    Links:     -    [Dynamo db     Global Tables](https://docs.aws.amazon.com/amazon dynamo db/latest/developer guide/Global tables.html)      -        [Route      53        DNS Failover](https://docs.aws.amazon.com/Route53/latest/Developer guide/dns-failover.html)

771.A company runs an application on Amazon EC2 instances in a private subnet. The application needs to store and retrieve data in Amazon S3  buckets. According to regulatory requirements, the data must not travel across the public internet. What should a solutions architect do to meet these requirements MOST cost-effectively?

A ��Deploy a NAT gateway to access the S3 buckets.

B ��Deploy AWS Storage Gateway to access the S3 buckets.

C ��Deploy an S3 interface endpoint to access the S3 buckets.

D ��Deploy an S3 gateway endpoint to access the S3 buckets.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The most cost-effective solution is to use an S3 Gateway Endpoint. This is a free service provided by AWS that allows EC2 instances in a private subnet to securely access S3 buckets without using the public internet. Here's why: 1. How it works: - A Gateway Endpoint is a virtual device you add to your VPC. - It modifies the VPC route tables to direct S3 traffic through AWS's private network instead of the public internet. 2. Why it's better than other options: - NAT Gateway (A): Charges apply for NAT Gateways + data processing. Traffic still exits the VPC. - Storage Gateway (B): Designed for hybrid cloud (connecting on - premises to S3), unnecessary here. - Interface Endpoint (C): Uses AWS Private link ($$$$$ per hour) and requires security groups. Gateway Endpoint is free and simpler for S3. 3. Compliance: - Data stays within the AWS network,     meeting      the      no      public      internet      requirement.      Reference      Links:      -      [AWS      VPC      Endpoints      for S3](https://docs.aws.amazon.com/vpc/latest/private link/vpce         -          gateway.html)         -          [Gateway         vs          Interface Endpoints](https://aws.amazon.com/blogs/aws/new - vpc - endpoint - for - amazon - s3/)

772.A company  uses Amazon S3 to  host  its static website. The company wants to add a contact form to the webpage. The contact form will have dynamic server-side components for users to input their name, email address, phone number, and user message. The company expects fewer than 100 site visits each month. The contact form must notify the company by email when a customer fills out the form. Which solution will meet these requirements MOST cost-effectively?

A �� Host the dynamic  contact form  in Amazon  Elastic Container Service  (Amazon  ECS). Set  up Amazon Simple  Email Service (Amazon SES) to connect to a third-party email provider.

B ��Create an Amazon API Gateway endpoint that returns the contact form from an AWS Lambda function. Configure another Lambda function on the API Gateway to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic.

C ��Host the website by using AWS Amplify Hosting for static content and dynamic content. Use server-side scripting to build the contact form. Configure Amazon Simple Queue Service (Amazon SQS) to deliver the message to the company.

D ��Migrate the website from Amazon S3 to Amazon EC2 instances that run Windows Server. Use Internet Information Services (IIS) for Windows Server to host the webpage. Use client-side scripting to build the contact form. Integrate the form with Amazon Work mail.

�𰸣�B

������Correct Answer: B Detailed Explanation: The question requires a cost-effective solution for a low-traffic website hosted on Amazon S3 that needs a dynamic contact form with email notifications. Here ��s why option B is best: 1. Serverless Architecture: - AWS  Lambda  handles  the  server-side  logic  for  the  contact  form.  You  only  pay  for  the  milliseconds  of  compute  time  used (extremely cheap for  low traffic). - Amazon API Gateway triggers the  Lambda function when the form  is submitted.  It  has a pay-per-request pricing model, ideal for infrequent usage. 2. Email Notifications: - The second Lambda function sends the form data to Amazon SNS (Simple Notification Service), which can email the company. SNS is cost-effective for low message volumes (you pay per notification sent). 3. No Servers to Manage: Unlike options A (ECS), C (Amplify + SQS), and D (EC2), this solution avoids paying for idle servers or complex setups. Serverless services scale to zero when unused, minimizing costs. Other options are less optimal: - A (ECS + SES): Running containers (ECS) incurs costs even with no traffic. - C (Amplify + SQS): Amplify is great for static sites, but SQS adds unnecessary complexity for simple email  notifications. -  D (EC2 + Work mail):  EC2  requires 24/7 instance     costs,      and      Work mail      is      overkill      for      basic      email      alerts.      Reference      Links:      -      [AWS      Lambda Pricing](https://aws.amazon.com/lambda/pricing/)                         -                          [Amazon                          API                         Gateway
Pricing](https://aws.amazon.com/api-gateway/pricing/) - [Amazon SNS Pricing](https://aws.amazon.com/sns/pricing/)

773.A company creates dedicated AWS accounts in AWS Organizations for its business units. Recently, an important notification

was sent to the root user email address of a business unit account instead of the assigned account owner. The company wants to ensure that all future notifications can be sent to different employees based on the notification categories of billing, operations, or security. Which solution will meet these requirements MOST securely?

A ��Configure each AWS account to use a single email address that the company manages. Ensure that all account owners can access  the  email  account  to  receive  notifications.  Configure  alternate  contacts  for  each  AWS  account  with  corresponding distribution lists for the billing team, the security team, and the operations team for each business unit.

B �� Configure each AWS account to  use a different email distribution list for each  business  unit that the company manages. Configure each distribution list with administrator email addresses that can respond to alerts. Configure alternate contacts for each AWS account with corresponding distribution lists for the billing team, the security team, and the operations team for each business unit.

C ��Configure each AWS account root user email address to be the individual company managed email address of one person from each business unit. Configure alternate contacts for each AWS account with corresponding distribution lists for the billing team, the security team, and the operations team for each business unit.

D ��Configure each AWS account root user to use email aliases that go to a centralized mailbox. Configure alternate contacts for each account by using a single  business  managed email distribution list each for the billing team, the security team, and the operations team.

�𰸣�D

������ Correct Answer D Detailed Explanation The best solution is D because it centralizes root user email management while securely routing notifications to specific teams. - Root User Email: Configuring root user email aliases to a centralized mailbox ensures the  root account (which  has full AWS access)  is  not tied to an  individual ��s email. This  reduces security  risks  (e.g., compromised   personal  accounts)  and  simplifies   management.  -  Alternate  Contacts:  AWS  allows  setting  separate  email distribution  lists for  billing,  operations, and security  notifications.  By  using  dedicated  distribution  lists for  each  category, the company ensures the right teams receive relevant alerts without exposing root user credentials. Other options are less secure: - A/C risk root user email being shared or tied to individuals. - B uses distribution lists for root users, which is unnecessary and less centralized.                             Reference                              Link                              [AWS                              Alternate                             Contacts
Documentation](https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-alternate-contact.html)

774.A company that uses AWS Organizations runs 150 applications across 30 different AWS accounts. The company used AWS Cost and Usage Report to create a new report in the management account. The report is delivered to an Amazon S3 bucket that is replicated to a bucket in the data collection account. The company ��s senior leadership wants to view a custom dashboard that provides  NAT  gateway  costs   each  day  starting  at  the   beginning  of  the   current  month.  Which   solution  will   meet  these requirements?

A��Share an Amazon Quick sight dashboard that includes the requested table visual. Configure Quick sight to use AWS Data sync to query the new report.

B ��Share an Amazon Quick sight dashboard that includes the requested table visual. Configure Quick sight to use Amazon Athena to query the new report.

C �� Share  an  Amazon  Cloud watch  dashboard  that  includes  the  requested  table  visual.  Configure  Cloud watch  to  use  AWS Data sync to query the new report.

D ��Share an Amazon Cloud watch dashboard that includes the requested table visual. Configure Cloud watch to use Amazon

Athena to query the new report.

�𰸣�B

���� �� Correct Answer: B Detailed Explanation: The correct solution is B because Amazon Quick sight is designed for creating interactive dashboards and visualizing data, which aligns with the requirement for a custom dashboard. The AWS Cost and Usage Report (CUR) is stored in Amazon S3, and Amazon Athena is the service used to query data directly from S3 using standard SQL. Quick sight integrates seamlessly with Athena to pull and visualize the CUR data, including filtering for NAT gateway costs. - Why not A/C: AWS Data sync is for transferring data between storage services, not querying. Since the CUR is already in S3 (replicated), Data sync isn't needed here. - Why not C/D: Cloud watch focuses on monitoring metrics/logs, not cost analysis. It cannot directly query CUR data stored in S3. - Athena + Quick sight: Athena parses the CUR data in S3, and Quick sight builds the dashboard. This combination meets the requirement for daily NAT gateway cost tracking. Reference Links: - [Analyze AWS Cost and Usage Reports using                                                                                                                                                                                                                          Amazon
Quick sight](https://aws.amazon.com/blogs/big-data/analyze-aws-cost-and-usage-reports-using-amazon-quick sight/)      -       [Use Athena to Query Cost and Usage Reports](https://docs.aws.amazon.com/cur/latest/user guide/cur-query-athena.html)

775.A company runs its application by using Amazon EC2 instances and AWS Lambda functions. The EC2 instances run in private subnets  of a VPC. The  Lambda  functions  need  direct  network  access  to  the  EC2  instances  for  the  application to work. The application will run for 1 year. The number of Lambda functions that the application uses will increase during the 1-year period. The company must minimize costs on all application resources. Which solution will meet these requirements?

A ��Purchase an EC2 Instance Savings Plan. Connect the Lambda functions to the private subnets that contain the EC2 instances.

B ��Purchase an EC2 Instance Savings Plan. Connect the Lambda functions to new public subnets in the same VPC where the EC2 instances run.

C ��Purchase a Compute Savings Plan. Connect the Lambda functions to the private subnets that contain the EC2 instances.

D ��Purchase a Compute Savings Plan. Keep the Lambda functions in the Lambda service VPC.

�𰸣�C

������ Correct Answer C Detailed Explanation The correct solution is C because: 1. Compute Savings Plan: This plan covers both EC2  instances  and  Lambda  functions,  providing  cost  savings  for  both  services.  Since  the  number  of  Lambda  functions  will increase over time, a Compute Savings Plan is more cost - effective than an EC2 Instance Savings Plan (which only applies to EC2).
2.  Lambda  in  Private  Subnets:  To  enable  direct  network  access  between  Lambda  and  EC2  instances  in  private  subnets,  the Lambda functions must be configured to connect to the same VPC and private subnets as the EC2 instances. This ensures secure communication without exposing the EC2 instances to the public internet. Other options fail because: - A/B: EC2 Instance Savings Plans only apply to EC2, not Lambda, so they won ��t minimize costs for the growing Lambda usage. - D: Keeping Lambda in the Lambda service VPC (default) prevents direct access to EC2 instances in the private subnets. - B: Public subnets are unnecessary and less secure for this use case. Reference Links - [AWS Compute Savings Plans](https://aws.amazon.com/savings plans/compute - pricing/) - [Configuring Lambda for VPC Access](https://docs.aws.amazon.com/lambda/latest/dg/configuration - vpc.html)

776.A  company  has  deployed  a   multi-account  strategy  on  AWS  by  using  AWS  Control  Tower.  The  company  has  provided individual AWS accounts to each of its developers. The company wants to implement controls to limit AWS resource costs that the developers incur. Which solution will meet these requirements with the LEAST operational overhead?

A ��Instruct each developer to tag all their resources with a tag t hat has a key of Cost center and a value of the developer's name. Use the required-tags AWS Config managed rule to check for the tag. Create an AWS Lambda function to terminate resources

that do not have the tag. Configure AWS Cost Explorer to send a daily report to each developer to monitor their spending.

B��Use AWS Budgets to establish budgets for each developer account. Set up budget alerts for actual and forecast values to notify developers when they exceed or expect to exceed their assigned budget. Use AWS Budgets actions to apply a DenyAll policy to the developer's IAM role to prevent additional resources from being launched when the assigned budget is reached.

C ��Use AWS Cost Explorer to monitor and report on costs for each developer account. Configure Cost Explorer to send a daily report to  each  developer  to  monitor  their  spending.  Use  AWS  Cost  Anomaly  Detection  to  detect  anomalous  spending  and provide alerts.

D��Use AWS Service Catalog to allow developers to launch resources within a limited cost range. Create AWS Lambda functions in each AWS  account to stop  running  resources  at  the  end  of  each  work  day. Configure the  Lambda  functions  to  resume  the resources at the start of each work day.

�𰸣�B

������ Correct Answer: B Detailed Explanation: AWS Budgets allows setting custom budgets and triggering actions (like applying a restrictive IAM policy) when thresholds are met. This automates cost control without manual intervention. Why B? 1. Budget Alerts  +  Actions:  Automatically  blocks  new  resource  creation  via  IAM  policies  when  budgets  are  exceeded,  preventing  cost overruns. 2. Low Overhead: No need for tagging enforcement (A), manual anomaly reviews (C), or daily Lambda maintenance (D).
3. Native Integration: Works seamlessly with AWS Control Tower ��s  multi-account structure. Why Not Others? - A:  Relies on reactive resource termination, which doesn ��t prevent costs upfront. - C: Only monitors costs but doesn ��t enforce limits. - D: Requires      managing      Lambda      functions      across      accounts,       increasing      overhead.      Reference      Links:      -       [AWS Budgets](https://aws.amazon.com/aws-cost-management/aws-budgets/)                         -                         [AWS                          Budgets Actions](https://docs.aws.amazon.com/cost-management/latest/user guide/budgets-actions.html)

777.A company plans to rehost an application to Amazon EC2 instances that use Amazon Elastic Block Store (Amazon EBS) as the attached  storage.  A  solutions  architect  must  design  a  solution  to  ensure  that  all  newly  created  Amazon  EBS  volumes  are encrypted by default. The solution must also prevent the creation of unencrypted EBS volumes. Which solution will meet these requirements?

A ��Configure the EC2 account attributes to always encrypt new EBS volumes.

B��Use AWS Config. Configure the encrypted-volumes identifier. Apply the default AWS Key Management Service (AWS KMS) key.

C ��Configure AWS Systems Manager to create encrypted copies of t he EBS volumes. Reconfigure the EC2 instances to use the encrypted volumes.

D ��Create a customer managed key in AWS Key Management Service (AWS KMS). Configure AWS Migration Hub to use the key when the company migrates workloads.

�𰸣�A

������ Correct Answer: B Detailed Explanation: The correct solution is to use AWS Config with the encrypted-volumes identifier and apply the default AWS KMS key. Here ��s why: 1. AWS Config allows you to define rules to check resource configurations continuously. By creating a rule that identifies unencrypted EBS volumes, you can enforce compliance. 2. The encrypted-volumes identifier is a managed AWS Config rule that checks if EBS volumes are encrypted. If unencrypted volumes are detected, AWS Config can trigger remediation actions or alerts. 3. AWS KMS provides encryption keys. Using the default KMS key ensures that all new EBS volumes are encrypted by default without requiring manual key selection. Why not other options? - Option A: There is

no direct  EC2 account attribute to enforce encryption  by default. While AWS allows setting a default encryption  key for  EBS volumes at the account level, this does not prevent users from explicitly creating unencrypted volumes (they can override the default). - Option C: AWS Systems Manager (SSM) can automate tasks but cannot enforce encryption during EBS volume creation. This approach adds unnecessary complexity. - Option D: AWS Migration Hub is for tracking migrations, not enforcing encryption policies.  Key  Take away:  AWS  Config  +   KMS  provides  a  scalable,  automated  way  to  enforce  encryption  policies  and  block non-compliant  resources.  This  aligns  with  AWS  best  practices  for  security  and  compliance.  Reference  Links:  -  [AWS  Config Managed      Rules](https://docs.aws.amazon.com/config/latest/developer guide/managed-rules-by-aws-config.html)       -       [EBS Encryption by Default](https://docs.aws.amazon.com/AWSEC2/latest/User guide/Ebs encryption.html#encryption-by-default)

778.An  ecommerce  company wants to  collect  user  clickstream  data  from the  company's website for  real-time  analysis.  The website experiences fluctuating traffic patterns throughout the day. The company needs a scalable solution that can adapt to varying levels of traffic. Which solution will meet these requirements?

A ��Use a data stream in Amazon Kinesis Data Streams in on-demand mode to capture the clickstream data. Use AWS Lambda to process the data in real time.

B ��Use Amazon Kinesis Data Firehose to capture the clickstream data. Use AWS Glue to process the data in real time. C ��Use Amazon Kinesis Video Streams to capture the clickstream data. Use AWS Glue to process the data in real time.

D �� Use  Amazon  Managed  Service  for  Apache  Flink  (previously  known  as  Amazon  Kinesis  Data  Analytics)  to  capture  the clickstream data. Use AWS Lambda to process the data in real time.

�𰸣�A

���� �� Correct Answer: A Detailed Explanation: The best solution is A because: 1. Amazon Kinesis Data Streams (on-demand mode) automatically scales to handle fluctuating traffic, making it ideal for unpredictable workloads like clickstream data. You pay per usage, avoiding over-provisioning. 2. AWS Lambda processes data in real time. It ��s serverless, so it scales automatically with the incoming data stream. Other options are less suitable: - B: Kinesis Data Firehose is for near-real-time (not real-time) delivery to destinations like S3/Redshift. AWS Glue is for batch ETL, not real-time. - C: Kinesis Video Streams is for video/audio data,  not  clickstream.  -  D:  Managed  Service  for  Apache  Flink  (Kinesis  Data  Analytics)  is  for  processing  streaming  data,  not capturing                it.                Reference                Links:                -                [Kinesis                 Data               Streams                 On-Demand Mode](https://docs.aws.amazon.com/streams/latest/dev/kinesis-on-demand.html)            -             [AWS            Lambda            with Kinesis](https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html)

779.A global company runs its workloads on AWS. The company's application uses Amazon S3 buckets across AWS Regions for sensitive data storage and analysis. The company stores millions of objects in multiple S3 buckets daily. The company wants to identify all S3 buckets that are not versioning-enabled. Which solution will meet these requirements?

A ��Set up an AWS Cloud trail event that has a rule to identify all S3 buckets that are not versioning-enabled across Regions.

B ��Use Amazon S3 Storage Lens to identify all S3 buckets that are not versioning-enabled across Regions.

C ��Enable IAM Access Analyzer for S3 to identify all S3 buckets that are not versioning-enabled across Regions.

D ��Create an S3 Multi-Region Access Point to identify all S3 buckets that are not versioning-enabled across Regions.

�𰸣�B

������ Correct Answer: B Detailed Explanation: Amazon S3 Storage Lens is a centralized storage analytics solution that provides organization-wide visibility  into object storage  usage  and activity.  It  includes  predefined  metrics  such  as  bucket-level  details, including whether versioning  is enabled.  Unlike other options: - A (Cloud trail):  Focuses on API activity  logging,  not  real-time bucket  configuration  checks.  -  C  (Access  Analyzer):  Identifies  unintended  resource  exposure,  not  versioning  status.  -  D (Multi-Region Access  Point): Simplifies  multi-region  access  but  doesn ��t  audit configurations. S3 Storage  Lens  automatically aggregates data across all regions and accounts, making it the most efficient way to identify buckets without versioning enabled. It also offers a dashboard for easy monitoring, which  is  ideal for  large-scale operations.  Reference  Link:  [Amazon S3  Storage Lens](https://docs.aws.amazon.com/AmazonS3/latest/user guide/storage-lens.html)

780.A  company  needs  to  optimize  its Amazon  S3  storage  costs for  an  application that generates  many  files  that  cannot  be recreated. Each file is approximately 5 MB and is stored in Amazon S3 Standard storage. The company must store the files for 4 years before the files can be deleted. The files must be immediately accessible. The files are frequently accessed in the first 30 days of object creation, but they are rarely accessed after the first 30 days. Which solution will meet these requirements MOST cost-effectively?

A ��Create an S3 Lifecycle policy to move the files to S3 Glacier Instant Retrieval 30 days after object creation. Delete the files 4 years after object creation.

B��Create an S3 Lifecycle policy to move the files to S3 One Zone-Infrequent Access (S3 One Zone-IA) 30 days after object creation. Delete the files 4 years after object creation.

C��Create an S3 Lifecycle policy to move the files to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after object creation. Delete the files 4 years after object creation.

D��Create an S3 Lifecycle policy to move the files to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after object creation. Move the files to S3 Glacier Flexible Retrieval 4 years after object creation.

�𰸣�A

������ Correct Answer: B Detailed Explanation: The best option is B. Here's why in simple terms: - S3 One Zone-IA is cheaper than both Standard-IA (Option C) and Glacier Instant Retrieval (Option A) for storage. - The files are rarely accessed after 30 days but still need to be immediately accessible. Glacier Instant Retrieval (Option A) is an archive tier designed for very rare access, and while it allows instant retrieval, its retrieval fees are higher than S3 One Zone-IA. - S3 One Zone-IA is optimized for infrequent access with lower storage costs and no extra retrieval fees compared to Glacier. Since the files are rarely accessed after 30 days but must remain available instantly, this balances cost and accessibility. - Option C (Standard-IA) is more expensive than One Zone-IA.  -  Option  D  uses  Glacier  Flexible  Retrieval,  which  has  retrieval  delays  (minutes  to  hours),  violating  the  immediately accessible requirement. Why not A? Glacier Instant Retrieval has higher retrieval costs if accessed, and the files here are rarely accessed �� not  almost  never  accessed.  One  Zone-IA  is  more  cost-effective  for  this  scenario.  Reference  Links:  -  [Amazon  S3 Storage Classes](https://aws.amazon.com/s3/storage-classes/) - [S3 Pricing](https://aws.amazon.com/s3/pricing/)

781.A company runs its critical storage application in the AWS Cloud. The application uses Amazon S3 in two AWS Regions. The company wants the application to send  remote  user  data to the  nearest S3  bucket  with  no  public  network  congestion. The company also wants the application to fail over with the least amount of management of Amazon S3. Which solution will meet these requirements?

A �� Implement an active-active design between the two  Regions. Configure the application to use the regional S3 endpoints closest to the user.

B ��Use an active-passive configuration with S3 Multi-Region Access Points. Create a global endpoint for each of the Regions.

C ��Send user data to the regional S3 endpoints closest to the user. Configure an S3 cross-account replication rule to keep the S3 buckets synchronized.

D ��Set up Amazon S3 to use Multi-Region Access Points in an active-active configuration with a single global endpoint. Configure S3 Cross-Region Replication.

�𰸣�D

���� �� Correct Answer: D Detailed Explanation: The best solution is D because it uses Amazon S3 Multi-Region Access Points
(MRAP) in an active-active configuration with a single global endpoint. Here ��s why this works: 1. Nearest S3 Bucket Routing: MRAPs automatically  route user  requests to the  nearest AWS  Region  based on network latency, ensuring data is sent to the closest S3 bucket without public internet congestion (traffic stays on the AWS backbone network). 2. Automatic Failover: If one Region becomes unavailable, MRAPs instantly redirect traffic to the other Region with no manual intervention, meeting the least management requirement. 3. Data Synchronization: S3 Cross-Region Replication (CRR) keeps the buckets in sync, ensuring data consistency during normal operations and failover. Other options fail because: - A uses regional endpoints but lacks automated failover. - B uses an active-passive setup, which isn ��t fully active-active. - C relies on manual replication and routing. Reference Links:                                              -                                              [S3                                              Multi-Region                                              Access
Points](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Multiregion access points.html)        -         [S3        Cross-Region Replication](https://docs.aws.amazon.com/AmazonS3/latest/user guide/replication.html)

782.A company is migrating a data center from its on-premises location to AWS. The company has several legacy applications that are hosted on individual virtual servers. Changes to the application designs cannot be made. Each individual virtual server currently runs as its own EC2 instance. A solutions architect needs to ensure that the applications are reliable and fault tolerant after migration to AWS. The applications will run on Amazon EC2 instances. Which solution will meet these requirements?

A ��Create an Auto Scaling group that has a minimum of one and a maximum of one. Create an Amazon Machine Image (AMI) of each application instance. Use the AMI to create EC2 instances in the Auto Scaling group Configure an Application Load Balancer in front of the Auto Scaling group.

B��Use AWS Backup to create an hourly backup of the EC2 instance that hosts each application. Store the backup in Amazon S3 in a separate Availability Zone. Configure a disaster recovery process to restore the EC2 instance for each application from its most recent backup.

C��Create an Amazon Machine Image (AMI) of each application instance. Launch two new EC2 instances from the AMI. Place each EC2 instance in a separate Availability Zone. Configure a Network Load Balancer that has the EC2 instances as targets.

D��Use AWS Mitigation Hub Refactor Spaces to migrate each application off the EC2 instance. Break down functionality from each application into individual components. Host each application on Amazon Elastic Container Service (Amazon ECS) with an AWS Fargate launch type.

�𰸣�A

������ ���޽���

783.A company wants to isolate its workloads by creating an AWS account for each workload. The company needs a solution that centrally manages networking components for the workloads. The solution also must create accounts with automatic security controls (guardrails). Which solution will meet these requirements with the LEAST operational overhead?

A �� Use AWS Control Tower to deploy accounts. Create a networking account that has a VPC with private subnets and public subnets. Use AWS Resource Access Manager (AWS RAM) to share the subnets with the workload accounts.

B �� Use AWS Organizations to deploy accounts. Create a networking account that has a VPC with private subnets and public subnets. Use AWS Resource Access Manager (AWS RAM) to share the subnets with the workload accounts.

C ��Use AWS Control Tower to deploy accounts. Deploy a VPC in each workload account. Configure each VPC to route through an inspection VPC by using a transit gateway attachment.

D ��Use AWS Organizations to deploy accounts. Deploy a VPC in each workload account. Configure each VPC to route through an inspection VPC by using a transit gateway attachment.

�𰸣�A

���� �� Correct Answer: A Detailed Explanation: The question asks for a solution that allows a company to isolate workloads using separate AWS accounts, centrally  manage  networking,  and apply automatic security controls with  minimal operational overhead. Here's why Option A is the best choice: 1. AWS Control Tower: - Automates the setup of a multi-account environment (via AWS Organizations) and enforces preconfigured guardrails (security controls like Disallow Public S3 Buckets). This reduces manual work compared to using AWS Organizations alone (Options B/D). - Provides a centralized dashboard for governance and compliance, lowering operational overhead. 2. Centralized Networking: - A dedicated networking account hosts a shared VPC with  public/private  subnets.   -  AWS   RAM  (Resource  Access   Manager)   allows  workload   accounts  to  share  these   subnets, eliminating the need to recreate VPCs in each account (unlike Options C/D). This simplifies network management (e.g., firewall rules, monitoring). 3. Why Other Options Are Less Ideal: - Option B: Uses AWS Organizations but lacks Control Tower's guardrails, requiring  manual security setup  (higher  overhead). - Options C/D:  Deploying  a VPC  per  account  increases complexity. While Transit Gateway enables centralized routing, it adds cost and setup steps compared to subnet sharing via AWS RAM. Reference Links:          -          [AWS          Control          Tower](https://aws.amazon.com/control tower/)           -          [AWS           RAM          Shared Subnets](https://docs.aws.amazon.com/ram/latest/user guide/shareable-aws-resources.html)

784.A company hosts a website on Amazon EC2 instances behind an Application Load Balancer (ALB). The website serves static content. Website traffic is increasing. The company wants to minimize the website hosting costs. Which solution will meet these requirements?

A ��Move the website to an Amazon S3 bucket. Configure an Amazon Cloud front distribution for the S3 bucket.

B ��Move the website to an Amazon S3 bucket. Configure an Amazon Elastic ache cluster for the S3 bucket. C ��Move the website to AWS Amplify. Configure an ALB to resolve to the Amplify website.

D ��Move the website to AWS Amplify. Configure EC2 instances to cache the website.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution is to move the static website to Amazon S3 and use Amazon Cloud front.  Here ��s why:  1.  Cost  Efficiency: -  Hosting static content directly on Amazon S3  is far cheaper than  running  EC2 instances,  as you  only  pay for  storage and data transfer  (no  compute  costs).  -  Cloud front  (a  CDN)  reduces  costs  further  by caching  content   at  edge  locations,  lowering  data  transfer  fees  from  S3   and   improving  performance  for  global   users.   2. Performance: - Cloud front caches static content closer to  users,  reducing  latency and  offloading traffic from the origin  (S3), which is ideal for high traffic. 3. Simpler Architecture: - This setup eliminates the need for EC2 instances and an ALB, reducing complexity and maintenance. Why other options are incorrect: - Option B: Elastic ache is for caching dynamic database queries,

not static content. - Option C/D: Amplify is a valid static hosting option, but using it with an ALB or EC2 reintroduces unnecessary costs              and              complexity.              Reference              Links:              -              [Amazon              S3               Static              Website Hosting](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Website hosting.html)    -    [Amazon    Cloud front    with    S3 Origin](https://docs.aws.amazon.com/Amazon cloud front/latest/Developer guide/Getting started.Simple distribution.html)

785.A company is implementing a shared storage solution for a media application that the company hosts on AWS. The company needs  the  ability  to  use  SMB  clients  to  access  stored  data.  Which  solution  will  meet  these  requirements  with  the  LEAST administrative overhead?

A ��Create an AWS Storage Gateway Volume Gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.

B ��Create an AWS Storage Gateway Tape Gateway. Configure tapes to use Amazon S3. Connect the application server to the Tape Gateway.

C �� Create  an Amazon  EC2  Windows  instance.  Install  and configure a Windows file share  role on the  instance.  Connect the application server to the file share.

D ��Create an Amazon FSx for Windows File Server file system. Connect the application server to the file system.

�𰸣�D

������ Correct Answer D Detailed Explanation The question asks for a shared storage solution that supports SMB clients with minimal administrative overhead. Here ��s why Amazon FSx for Windows File Server (Option D) is the best choice: 1. Native SMB Support:  Amazon  FSx  for  Windows   File  Server   is  explicitly  designed  to  support  the  SMB   protocol,  which   is  required  for Windows-based applications and clients. 2. Fully Managed: AWS handles maintenance, patching, backups, and scaling, reducing administrative work. 3. Direct Integration: The media application can directly connect to the FSx file system without additional configuration or gateway setups. Why other options are incorrect: - A  (Storage Gateway Volume Gateway): Volume Gateway provides block storage (iSCSI), not file storage (SMB), so it ��s unsuitable for this use case. - B (Tape Gateway): Tape Gateway is for backup/archiving, not real-time file sharing via SMB. - C (EC2 Windows Instance): Self-managed EC2 instances require ongoing maintenance  (updates,  scaling,  backups),  increasing  administrative  overhead.  Reference  Link  [Amazon  FSx  for  Windows  File Server](https://aws.amazon.com/fsx/windows/)

786.A company is designing its production application's disaster recovery (DR) strategy. The application is backed by a MySQL database on an Amazon Aurora cluster in the us-east-1 Region. The company has chosen the us-west-1 Region as its DR Region. The company's target recovery point objective (RPO) is 5 minutes and the target recovery time objective (RTO) is 20 minutes. The company wants to minimize configuration changes. Which solution will  meet these  requirements with the MOST operational efficiency?

A��Create an Aurora read replica in us-west-1 similar in size to the production application's Aurora MySQL cluster writer instance.

B ��Convert the Aurora cluster to an Aurora global database. Configure managed failover.

C ��Create a new Aurora cluster in us-west-1 that has Cross-Region Replication.

D ��Create a new Aurora cluster in us-west-1 . Use AWS Database Migration Service (AWS DMS) to sync both clusters.

�𰸣�B

���� �� Correct Answer:  B  Detailed  Explanation: Aurora Global  Database  is designed for disaster  recovery across  regions.  It replicates data to a secondary region with typical latency under 1 second, easily meeting the 5-minute RPO. Managed failover automates regional failure recovery within minutes, satisfying the 20-minute RTO. Converting an existing Aurora cluster to Global Database requires minimal configuration changes compared to setting up cross-region replicas or DMS synchronization manually. Other options either require manual intervention (affecting RTO) or have higher replication latency (risking RPO). Reference Links: [Aurora     Global      Database](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/aurora-global-database.html) [Managed                                                                                                                                                                                                                 planned
failover](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/aurora-global-database-disaster-recovery.html#aur ora-global-database-disaster-recovery.managed-failover)

787.A company runs a critical data analysis job each week before the first day of the work week. The job requires at least 1 hour to complete the analysis. The job is stateful and cannot tolerate interruptions. The company needs a solution to run the job on AWS. Which solution will meet these requirements?

A �� Create a  container for the job. Schedule the job to  run as an AWS  Fargate task on an Amazon  Elastic Container Service (Amazon ECS) cluster by using Amazon Event bridge Scheduler.

B ��Configure the job to run in an AWS Lambda function. Create a scheduled rule in Amazon Event bridge to invoke the  Lambda function.

C �� Configure an Auto Scaling group of Amazon EC2 Spot Instances that run Amazon Linux. Configure a crontab entry on the instances to run the analysis.

D ��Configure an AWS Data sync task to run the job. Configure a cron expression to run the task on a schedule.

�𰸣�A

������ Correct Answer A Detailed Explanation The job requires at least 1 hour to complete and cannot tolerate interruptions. Here ��s why A is the best choice: - AWS Fargate is a serverless compute engine for containers. It handles long-running tasks (no time  limit)  and  ensures  uninterrupted  execution,  perfect  for  stateful  jobs.  -  Amazon   ECS  +   Event bridge  Scheduler  allows scheduling the job to run weekly without managing servers. Other options fail because: - B (Lambda): Lambda has a 15-minute runtime limit, insufficient for a 1-hour job. - C (Spot Instances): Spot Instances can be interrupted, violating the no interruptions requirement.   -    D   (Data sync):    Data sync    is   for    data   transfers,    not   running    compute   jobs.    Reference    Links   -    [AWS Fargate](https://aws.amazon.com/fargate/)         -           [Amazon          ECS](https://aws.amazon.com/ecs/)          -          [Event bridge Scheduler](https://docs.aws.amazon.com/scheduler/)

788.A company runs workloads in the AWS Cloud. The company wants to centrally collect security data to assess security across the  entire  company  and  to   improve  workload   protection.  Which  solution  will   meet  these   requirements  with  the   LEAST development effort?

A ��Configure a data lake in AWS Lake Formation. Use AWS Glue crawlers to ingest the security data into the data lake.

B ��Configure an AWS Lambda function to collect the security data in .csv format. Upload the data to an Amazon S3 bucket. C ��Configure a data lake in Amazon Security Lake to collect the security data. Upload the data to an Amazon S3 bucket.

D ��Configure an AWS Database Migration Service (AWS DMS) replication instance to load the security data into an Amazon RDS

�𰸣�C

�� �� �� Correct Answer:  C  Detailed  Explanation:  Amazon  Security  Lake  is  a  fully  managed  service  specifically  designed  to centralize security data from AWS services, third - party tools, and custom sources. It automatically collects, normalizes (using the Open Cybersecurity Schema  Framework - OCSF), and stores security data in an Amazon S3 bucket. This  requires  minimal development effort compared to other options: - Option A (Lake Formation + Glue) requires manual data ingestion and schema management.  -  Option  B  (Lambda  +  CSV)  involves  custom  code  for  data  collection/formatting.  -  Option  D  (DMS  +  RDS)  is unsuitable for unstructured security logs and requires database schema design. Security Lake simplifies compliance and threat detection by providing pre - built integrations and automated normalization, making it the most efficient choice. Reference Link: [Amazon Security Lake Overview](https://aws.amazon.com/security-lake/)

789.A company is migrating five on-premises applications to VPCs in the AWS Cloud. Each application is currently deployed in isolated virtual  networks on  premises and should  be deployed similarly  in the AWS Cloud. The applications  need to  reach a shared  services  VPC.  All the  applications  must  be  able  to  communicate  with  each  other.  If  the  migration  is  successful,  the company will repeat the migration process for more than 100 applications. Which solution will meet these requirements with the LEAST administrative overhead?

A ��Deploy software VPN tunnels between the application VPCs and the shared services VPC. Add routes between the application VPCs in their subnets to the shared services VPC.

B �� Deploy VPC  peering  connections  between  the  application  VPCs  and  the  shared  services  VPC.  Add  routes  between  the application VPCs in their subnets to the shared services VPC through the peering connection.

C �� Deploy an AWS Direct Connect connection between the application VPCs and the shared services VPAdd routes from the application VPCs in their subnets to the shared services VPC and the applications VPCs. Add routes from the shared services VPC subnets to the applications VPCs.

D��Deploy a transit gateway with associations between the transit gateway and the application VPCs and the shared services VPC. Add  routes  between the application VPCs  in their subnets and the application VPCs to the shared services VPC through the transit gateway.

�𰸣�D

������ Correct Answer: D Detailed Explanation: The best solution is to use an AWS Transit Gateway. A Transit Gateway acts like a
central hub that connects multiple VPCs (including the shared services VPC). Here ��s why this works best: 1. Simplified Scaling: With 5 VPCs now and 100+ later, managing individual connections (like VPNs or VPC peering) would become extremely complex. Transit Gateway lets you connect all VPCs to a single hub, avoiding the need for manual connections between every pair of VPCs.
2. Centralized Routing: You define routes once in the Transit Gateway route table. For example, you can set a rule like  ��all traffic from Application VPCs goes to the Shared Services VPC. �� No need to update every VPC ��s route table individually. 3. Lower Overhead: Adding a new VPC later only requires attaching it to the Transit Gateway and updating the central route table. With VPNs or peering (Options A/B), you ��d have to create and manage hundreds of separate connections, which is error-prone and time-consuming. Why Other Options Fail: - A (VPNs): Manually configuring VPNs between every VPC is unsustainable at scale. - B (VPC Peering): Peering doesn ��t support transitive routing (e.g., VPC 1 can ��t talk to VPC 3 via VPC 2), and managing 100+ peering s is  impractical.  -  C   (Direct  Connect):   Direct  Connect   is  for  connecting  on-premises   networks  to  AWS,   not  for  VPC-to-VPC communication  in  the  cloud.  Reference  Links:  -  [AWS  Transit  Gateway](https://aws.amazon.com/transit-gateway/)  -  [Transit Gateway                                                                                                           vs.                                                                                                           VPC
Peering](https://docs.aws.amazon.com/white papers/latest/building-scalable-secure-multi-vpc-network-infrastructure/transit-ga teway.html)

790.A  company  is  migrating  its  workloads  to  AWS.  The  company  has  sensitive  and  critical  data  in  on-premises  relational databases  that  run  on  SQL  Server  instances.  The  company  wants  to  use  the  AWS  Cloud  to  increase  security  and  reduce operational overhead for the databases. Which solution will meet these requirements?

A �� Migrate the databases to Amazon EC2 instances. Use an AWS Key Management Service (AWS KMS) AWS managed key for encryption.

B��Migrate the databases to a Multi-AZ Amazon RDS for SQL Server DB instance. Use an AWS Key Management Service (AWS KMS) AWS managed key for encryption.

C ��Migrate the data to an Amazon S3 bucket. Use Amazon Macie to ensure data security.

D ��Migrate the databases to an Amazon Dynamo db table. Use Amazon Cloud watch Logs to ensure data security.

�𰸣�B

������ Correct Answer: B Detailed Explanation: The best solution is to migrate the databases to a Multi-AZ Amazon RDS for SQL Server DB instance using AWS KMS encryption. Here's why: 1. Managed Service: Amazon RDS handles database administration tasks  like  backups,  patching,  and  hardware  provisioning.  This  reduces  operational  overhead  compared  to  self-managed  EC2 instances  (Option  A).  2.  High  Availability:  Multi-AZ  deployment  provides  automatic  failover  to  a  standby  replica  in  another Availability Zone, ensuring continuous availability for critical databases. 3. Security: AWS KMS encryption protects data at rest, meeting security requirements. AWS-managed keys simplify key management compared to customer-managed alternatives. 4. Compatibility: Since the  existing  databases  are on SQL  Server,  RDS for  SQL  Server  maintains  compatibility without  requiring application redesign (unlike Options C/D which involve non-relational services). Option A requires manual database management (increasing overhead). Options C/D force migration to non-relational services (S3/Dynamo db), which aren't suitable for existing SQL Server workloads. Option B directly addresses both security and operational efficiency needs. Reference Links: - [Amazon RDS                                       Features](https://aws.amazon.com/rds/features/)                                        -                                        [Multi-AZ
Deployments](https://docs.aws.amazon.com/Amazon rds/latest/User guide/Concepts.MultiAZ.html)   -    [RDS    Encryption   with KMS](https://docs.aws.amazon.com/Amazon rds/latest/User guide/Overview.Encryption.html)

791.A company wants to migrate an application to AWS. The company wants to increase the application's current availability. The company wants to use AWS WAF in the application's architecture. Which solution will meet these requirements?

A ��Create an Auto Scaling group that contains multiple Amazon EC2 instances that host the application across two Availability Zones. Configure an Application Load Balancer (ALB) and set the Auto Scaling group as the target. Connect a WAF to the ALB.

B �� Create  a  cluster  placement group that  contains  multiple Amazon  EC2  instances that  hosts the  application.  Configure  an Application Load Balancer and set the EC2 instances as the targets. Connect a WAF to the placement group.

C ��Create two Amazon EC2 instances that host the application across two Availability Zones. Configure the EC2 instances as the targets of an Application Load Balancer (ALB). Connect a WAF to the ALB.

D ��Create an Auto Scaling group that contains multiple Amazon EC2 instances that host the application across two Availability Zones. Configure an Application Load Balancer (ALB) and set the Auto Scaling group as the target. Connect a WAF to the Auto Scaling group.

�𰸣�A

������ Correct Answer A Explanation To meet the requirements of increasing availability and using AWS WAF: 1. Auto Scaling

ensures the  application  automatically  adjusts  the  number  of  EC2  instances  based  on  demand,  maintaining  availability  even during traffic spikes or instance failures. 2. Deploying instances across two Availability Zones (AZs) provides redundancy��if one AZ fails, the other  keeps the app running. 3. An Application  Load  Balancer (ALB) distributes traffic evenly across  instances  in multiple AZs,  improving  fault  tolerance.  4. AWS WAF  is  correctly  attached  to  the ALB,  protecting  the  application from  web exploits at the load balancer layer (WAF cannot be directly connected to Auto Scaling groups or placement groups). Option B is wrong because placement groups optimize low-latency clustering, not availability, and WAF can ��t attach to placement groups. Option C lacks Auto Scaling, making it less reliable. Option D incorrectly connects WAF to the Auto Scaling group instead of the ALB.                      Reference                       Links                      -                      [AWS                       WAF                      Integration                      with
ALB](https://docs.aws.amazon.com/waf/latest/developer guide/alb-classic.html)              -              [Auto               Scaling              Best Practices](https://docs.aws.amazon.com/auto scaling/ec2/user guide/auto-scaling-best-practices.html)

792.A company manages a data lake in an Amazon S3 bucket that numerous applications access. The S3 bucket contains a unique prefix for each application. The company wants to restrict each application to its specific prefix and to have granular control of the objects under each prefix. Which solution will meet these requirements with the LEAST operational overhead?

A ��Create dedicated S3 access points and access point policies for each application.

B ��Create an S3 Batch Operations job to set the ACL permissions for each object in the S3 bucket.

C ��Replicate the objects in the S3 bucket to new S3 buckets for each application. Create replication rules by prefix.

D �� Replicate the objects in the S3 bucket to new S3 buckets for each application. Create dedicated S3 access points for each application.

�𰸣�A

������ Correct Answer: A Detailed Explanation: The best solution is to use Amazon S3 Access Points with access point policies.    Here ��s why: -  S3 Access  Points simplify  permissions  management  by  allowing you to create  unique access  points for each     application.  Each  access  point  can  enforce  a  policy  that  restricts  access  to  a  specific  prefix  (e.g.,  `my-bucket/app1/*`).  This     ensures apps only access their designated prefixes. - Operational Overhead: Access points are easy to set up and update. You don �� t need to duplicate data (unlike replication in C/D) or manually adjust permissions for every new object (unlike ACLs in B). Policies     are applied automatically, reducing maintenance. - Granular Control: Access point policies support IAM-style permissions (e.g.,      read/write limits), providing precise control over object-level actions. Other options add complexity: - B requires repetitive Batch    Operations jobs for new objects. - C/D involve replicating data, doubling storage costs and management work. Reference Link:     [Amazon S3 Access Points](https://docs.aws.amazon.com/AmazonS3/latest/user guide/access-points.html)

793.A company  has an application that customers  use to  upload  images to an Amazon S3  bucket.  Each  night,  the  company launches an Amazon EC2 Spot Fleet that processes all the images that the company received that day. The processing for each image takes 2 minutes and requires 512 MB of memory. A solutions architect needs to change the application to process the images when the images are uploaded. Which change will meet these requirements MOST cost-effectively?

A ��Use S3 Event Notifications to write a message with image details to an Amazon Simple Queue Service (Amazon SQS) queue. Configure an AWS Lambda function to read the messages from the queue and to process the images.

B ��Use S3 Event Notifications to write a message with image details to an Amazon Simple Queue Service (Amazon SQS) queue. Configure an EC2 Reserved Instance to read the messages from the queue and to process the images.

C ��Use S3 Event Notifications to publish a message with image details to an Amazon Simple Notification Service (Amazon SNS) topic. Configure a container instance in Amazon Elastic Container Service (Amazon ECS) to subscribe to the topic and to process

the images.

D ��Use S3 Event Notifications to publish a message with image details to an Amazon Simple Notification Service (Amazon SNS) topic. Configure an AWS Elastic Beanstalk application to subscribe to the topic and to process the images.

�𰸣�A

���� �� Correct Answer: A Detailed Explanation: The best solution is A because it uses AWS Lambda, which is serverless and charges  you  only  when  the  function   runs  (per  request).  Here's  why:  -   Problem:  The  company  wants  to   process   images immediately when uploaded, not wait for nightly batch processing. The old method (EC2 Spot Fleet) is cheap for nightly jobs but inefficient for real - time processing. - Why Lambda? Each image takes 2 minutes to process, and Lambda allows up to 15 minutes per execution. Lambda automatically scales: if 10 images are uploaded at once, 10 Lambda functions run in parallel. You pay only for the time each function runs (512  MB memory   �� 2  minutes).  No  servers to manage, no idle costs. - Why SQS? S3  Event Notifications send a  message to SQS when  a  new  image  is  uploaded. SQS acts  as a  buffer, ensuring  no  messages  are  lost  if Lambda temporarily can ��t process them (e.g., during a spike in uploads). Lambda polls SQS and processes messages as they arrive. - Other Options Are Worse: - B (EC2 Reserved Instance): Reserved Instances are prepaid and meant for steady workloads. Real - time processing might have idle time, wasting money. - C/D (ECS/Elastic Beanstalk): These require running containers or EC2 instances 24/7, which costs more than serverless Lambda. Cost Example: If 1,000 images are uploaded daily: - Lambda cost = 1,000  requests   �� @.0000002  per  request  +  1,000    �� (2  minutes   �� 512  MB)    �� @.0000166667  per  GB  - second   �� @.11/day. - EC2 (even a small instance) would cost ~@.01/hour   �� 24  hours = @.24/day + management overhead. Reference Links:      -       [AWS       Lambda      Pricing](https://aws.amazon.com/lambda/pricing/)       -       [S3       Event      Notifications       with SQS](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Notification how to.html)

794.A company runs a self-managed Microsoft SQL Server on Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS). Daily snapshots are taken of t he EBS volumes. Recently, all the company ��s EBS snapshots were accidentally deleted while running a snapshot cleaning script that deletes all expired EBS snapshots. A solutions architect needs to update the architecture to prevent data loss without retaining EBS snapshots indefinitely. Which solution will meet these requirements with the LEAST development effort?

A ��Change the IAM policy of the user to deny EBS snapshot deletion.

B ��Copy the EBS snapshots to another AWS Region after completing the snapshots daily.

C ��Create a 7-day EBS snapshot retention rule in Recycle Bin and apply the rule for all snapshots.

D ��Copy EBS snapshots to Amazon S3 Standard-Infrequent Access (S3 Standard-IA).

�𰸣�C

������ Correct Answer C Detailed Explanation The best solution is C because using AWS Recycle Bin with a 7 - day retention rule directly addresses the accidental deletion problem while minimizing effort. Here ��s why: 1. Recycle Bin acts like a safety net for deleted snapshots. If a script accidentally deletes snapshots, they ��re moved to the Recycle Bin instead of being permanently erased. 2. Retention Rule: The 7 - day rule ensures snapshots stay in the Recycle Bin for a week, giving the team time to recover them if needed. After 7 days, they ��re automatically deleted, avoiding indefinite retention. 3. Least Effort: Creating retention rules  in   Recycle  Bin  requires   no  custom  scripting  or  complex  permissions  changes.   It  �� s  a  simple  configuration   in  AWS Management Console or CLI. Other options are less optimal: - A (IAM policy changes) might break legitimate cleanup workflows.
- B (cross - region copies) adds complexity and cost. - D (S3 copies) is redundant (EBS snapshots are already stored in S3) and adds                      extra                      steps.                       Reference                       Link                      [AWS                       Recycle                      Bin
Documentation](https://docs.aws.amazon.com/AWSEC2/latest/User guide/recycle - bin.html)

795.A company wants to use an AWS Cloud formation stack for its application in a test environment. The company stores the Cloud formation template in an Amazon S3 bucket that blocks public access. The company wants to grant Cloud formation access to the template  in the  S3  bucket  based  on  specific  user  requests  to  create  the  test  environment. The  solution  must  follow security best practices. Which solution will meet these requirements?

A ��Create a gateway VPC endpoint for Amazon S3. Configure the Cloud formation stack to use the S3 object URL.

B ��Create an Amazon API Gateway REST API that has the S3 bucket as the target. Configure the Cloud formation stack to use the API Gateway URL.

C ��Create a pre signed URL for the template object. Configure the Cloud formation stack to use the pre signed URL.

D ��Allow public access to the template object in the S3 bucket. Block the public access after the test environment is created.

�𰸣�C

�� �� �� Correct Answer: C  Detailed  Explanation:  When a  user  requests  to create the test environment, a  pre signed  URL  is generated for the Cloud formation template stored in the private S3 bucket. This URL grants temporary, time-limited access to the template only for the duration  of the stack  creation, without  exposing the  bucket to  public access. This  aligns with the principle of least privilege and avoids permanent permissions or risky public access (as in Option D). Options A and B involve unnecessary complexity (VPC endpoints, API Gateway) and don ��t directly tie permissions to user requests. Option D violates security     best     practices     by     temporarily     allowing     public     access.     Reference     Link:      [Using     Pre signed     URLs     for S3](https://docs.aws.amazon.com/AmazonS3/latest/user guide/using-pre signed-url.html)

796.A company has applications that run in an organization in AWS Organizations. The company outsources operational support of the applications. The company needs to provide access for the external support engineers without compromising security. The external support engineers need access to the AWS Management Console. The external support engineers also need operating system access to the company ��s fleet of amazon EC2 instances that run Amazon Linux in private subnets. Which solution will meet these requirements MOST securely?

A �� Confirm that AWS  Systems  Manager Agent  (SSM Agent)  is  installed  on all  instances. Assign  an  instance  profile with the necessary  policy  to  connect  to  Systems  Manager.  Use  AWS  IAM  Identity  Center  to  provide  the  external  support  engineers console access. Use Systems Manager Session Manager to assign the required permissions.

B �� Confirm that AWS  Systems  Manager  Agent  (SSM Agent)  is  installed  on all  instances. Assign  an  instance  profile with the necessary policy to connect to Systems Manager. Use Systems Manager Session Manager to provide local IAM user credentials in each AWS account to the external support engineers for console access.

C ��Confirm that all instances have a security group that allows SSH access only from the external support engineers �� source IP address ranges. Provide local IAM user credentials in each AWS account to the external support engineers for console access. Provide each external support engineer an SSH key pair to log in to the application instances.

D �� Create  a  bastion  host  in a  public subnet. Set  up the  bastion  host  security group to allow access from only the external engineers �� IP address ranges. Ensure that all instances have a security group that allows SSH access from the bastion host. Provide each external support engineer an SSH key pair to log in to the application instances. Provide local account IAM user credentials to the engineers for console access.

�𰸣�A

������ Correct Answer A Detailed Explanation The most secure solution is A because it uses AWS best practices and managed services to  minimize  risks:  1.  AWS  Systems  Manager  Session  Manager  -  Provides  secure  OS-level  access  to  EC2  instances  in private subnets without opening  SSH  ports  or  managing  SSH  keys.  - Sessions  are encrypted,  logged,  and controlled via  IAM policies. 2. IAM Identity Center (AWS SSO) - Centralizes console access management across AWS accounts in AWS Organizations.
-  Avoids  insecure  practices  like  creating  local  IAM  users  in  every  account  (as  in  options  B/C/D).  3.  No  Publicly  Accessible Resources - Unlike options C/D (SSH ports) and D (bastion host), this solution avoids exposing infrastructure to the internet. 4. Auditability - All actions (console access + OS sessions) can be tracked via AWS Cloud trail and Session Manager logs. Why Other Options Are Less Secure - B/C/ D use SSH keys or local IAM users, which are harder to manage securely at scale. - C/D expose SSH ports or bastion hosts, increasing attack surface. -  B/D  require credentials  per account, violating the least  privilege  principle. Reference                          Links                         -                          [AWS                          Systems                          Manager                          Session
Manager](https://docs.aws.amazon.com/systems-manager/latest/user guide/session-manager.html) - [IAM Identity Center (AWS SSO)](https://aws.amazon.com/iam/features/sso/)

797.A company uses Amazon RDS for Postgresql to run its applications in the us-east-1 Region. The company also uses machine learning (ML) models to forecast annual revenue based on near real-time reports. The reports are generated by using the same RDS for Postgresql database. The database performance slows during business hours. The company needs to improve database performance. Which solution will meet these requirements MOST cost-effectively?

A ��Create a cross-Region read replica. Configure the reports to be generated from the read replica.

B ��Activate Multi-AZ DB instance deployment for RDS for Postgresql. Configure the reports to be generated from the standby database.

C �� Use  AWS  Data  Migration  Service  (AWS  DMS)  to  logically  replicate  data  to  a  new  database.  Configure  the  reports  to  be generated from the new database.

D ��Create a read replica in us-east-1. Configure the reports to be generated from the read replica.

�𰸣�D

���� �� Correct Answer D �� Create a read replica in us-east-1 . Configure the reports to be generated from the read replica. Detailed Explanation The problem is caused by heavy read workload (ML reports) competing with transactional workloads on the main database. Here ��s why option D works best: 1. Read Replica: Creates a copy of the database in the same region (us-east-1) that handles read-only queries. This offloads the ML/reporting workload from the main database, improving  performance. 2. Cost-Effective: Read replicas are cheaper than running a second full database (like Option C) or cross-region replication (Option A). 3. Multi-AZ (Option B) only provides disaster recovery    - the standby database cannot  be used for read queries. 4. AWS DMS (Option C) adds complexity and costs for ongoing replication, unlike RDS ��s built-in read replica feature. Analogy: Imagine a busy coffee shop with one barista (main database). Adding a second barista (read replica) to handle takeout orders (reports) lets the    first    barista    focus    on    in-store    customers     (application    transactions).    Reference    Links    -     [Amazon    RDS    Read Replicas](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_Read repl.html)     -       [RDS      Read      Replica      vs Multi-AZ](https://aws.amazon.com/rds/features/multi-az/)

798.A company hosts its multi-tier, public web application in the AWS Cloud. The web application runs on Amazon EC2 instances, and its database runs on Amazon RDS. The company is anticipating a large increase in sales during an upcoming holiday weekend. A solutions architect needs to build a solution to analyze the performance of the web application with a granularity of no more than 2 minutes. What should the solutions architect do to meet this requirement?

A ��Send Amazon Cloud watch logs to Amazon Redshift. Use Amazon QuickS ght to perform further analysis.

B ��Enable detailed monitoring on all EC2 instances. Use Amazon Cloud watch metrics to perform further analysis.

C �� Create  an AWS  Lambda  function to fetch  EC2  logs  from Amazon  Cloud watch  Logs.  Use  Amazon  Cloud watch  metrics  to perform further analysis.

D ��Send EC2 logs to Amazon S3. Use Amazon Redshift to fetch logs from the S3 bucket to process raw data for further analysis with Amazon Quick sight.

�𰸣�B

������ Correct Answer B Detailed Explanation The correct answer is B because enabling detailed monitoring on EC2 instances reduces the metric collection interval from the default 5 minutes (basic monitoring) to 1 minute. This meets the requirement of analyzing performance with a granularity of no more than 2 minutes. Amazon Cloud watch Metrics natively supports this level of granularity, allowing  real-time analysis without additional data  processing. Other options are  less suitable: - A and  D  involve sending   logs   to   Amazon   Redshift   and   analyzing   with   Amazon   Quick sight.   These   solutions   add   latency   (due   to   data ingestion/processing) and are better for historical analysis, not near-real-time monitoring. - C uses Lambda to fetch logs, which introduces unnecessary complexity. Cloud watch Metrics already provides the required granularity with detailed monitoring. For a beginner: Think of detailed monitoring like checking your car ��s speedometer every 1 minute instead of every 5 minutes. If you need to detect sudden  speed changes  (performance  issues) quickly,  more frequent checks  (1-minute granularity) are  better. Cloud watch Metrics does this automatically, making option B the simplest and most effective solution. Reference Link [Amazon Cloud watch Monitoring](https://docs.aws.amazon.com/AWSEC2/latest/User guide/using-cloud watch-new.html)

799.A company runs an application that stores and shares photos. Users upload the photos to an Amazon S3 bucket. Every day, users upload approximately 150 photos. The company wants to design a solution that creates a thumbnail of each new photo and stores the thumbnail in a second S3 bucket. Which solution will meet these requirements MOST cost-effectively?

A ��Configure an Amazon Event bridge scheduled rule to invoke a script every minute on a long-running Amazon EMR cluster. Configure the script to generate thumbnails for the  photos that do  not  have thumbnails.  Configure the script to  upload the thumbnails to the second S3 bucket.

B �� Configure  an Amazon  Event bridge  scheduled  rule  to  invoke  a  script  every  minute  on  a  memory-optimized  Amazon  EC2 instance that is always on. Configure the script to generate thumbnails for the photos that do not have thumbnails. Configure the script to upload the thumbnails to the second S3 bucket.

C��Configure an S3 event notification to invoke an AWS Lambda function each time a user uploads a new photo to the application. Configure the Lambda function to generate a thumbnail and to upload the thumbnail to the second S3 bucket.

D �� Configure S3 Storage Lens to invoke an AWS  Lambda function each time a user uploads a new photo to the application. Configure the Lambda function to generate a thumbnail and to upload the thumbnail to a second S3 bucket.

�𰸣�C

���� �� Correct Answer: C Detailed Explanation: The most cost-effective solution is option C. Here's why in simple terms for someone new to AWS: When a user uploads a photo to an S3 bucket, S3 can automatically trigger an AWS Lambda function using S3 event notifications. Lambda is serverless, meaning you only pay for the milliseconds of compute time used to create each thumbnail. Since there's only ~150  photos daily  (about 6  photos  per  hour), this  keeps costs  minimal. Other options are  less efficient: - A (EMR) and B (EC2) use always-running servers, which you pay for 24/7 even when they're idle - D incorrectly uses S3 Storage Lens (a monitoring tool) instead of proper event notifications - Scheduled checks (options A/B) waste resources checking

empty buckets most of the time Lambda automatically scales to zero when not in use, making it perfect for sporadic workloads like this thumbnail generation task. You only pay for actual processing time without any idle costs. Reference Links: - S3 Event Notifications:    https://docs.aws.amazon.com/AmazonS3/latest/user guide/Notification how to.html    -    AWS     Lambda    Pricing: https://aws.amazon.com/lambda/pricing/

800.A company has stored millions of objects across multiple prefixes in an Amazon S3 bucket by using the Amazon S3 Glacier Deep Archive storage class. The company needs to delete all data older than 3 years except for a subset of data that must be retained. The  company  has  identified  the data that  must  be  retained  and  wants to  implement  a  serverless  solution.  Which solution will meet these requirements?

A �� Use S3 Inventory to list all objects. Use the AWS CLI to create a script that runs on an Amazon EC2 instance that deletes objects from the inventory list.

B ��Use AWS Batch to delete objects older than 3 years except for the data that must be retained.

C �� Provision an AWS Glue crawler to query objects older than 3 years. Save the manifest file of old objects. Create a script to delete objects in the manifest.

D ��Enable S3 Inventory. Create an AWS Lambda function to filter and delete objects. Invoke the Lambda function with S3 Batch Operations to delete objects by using the inventory reports.

�𰸣�D

������ Correct Answer D Detailed Explanation The correct solution is D because it uses serverless services (AWS Lambda and S3 Batch Operations) to delete objects efficiently and at scale, while ensuring only the required data is retained. Here's why: 1. S3 Inventory: - S3 Inventory generates a report listing all objects in the bucket. This is critical for identifying objects older than 3 years without manually scanning the entire bucket (which is slow and expensive for millions of objects). 2. AWS Lambda: - A Lambda function can filter objects from the inventory report, excluding the subset of data that must be retained. This ensures only eligible objects are deleted. 3. S3  Batch Operations: - S3  Batch Operations can  process  millions of objects  in parallel  by invoking the  Lambda  function  for  each  object  in  the  inventory  report.  It  handles  retries,  progress  tracking,  and  completion reports, making it ideal for large-scale deletions. Why Other Options Are Incorrect: - A: Using an EC2 instance and CLI script is not serverless and requires  managing  infrastructure.  It ��s  inefficient for  millions  of objects. -  B: AWS  Batch  requires configuring compute environments (EC2/Fargate), which adds complexity and isn ��t fully serverless. - C: AWS Glue crawlers are designed for metadata discovery (e.g., for analytics), not bulk deletions. This approach is unnecessarily complicated and slower. Key Take away S3 Batch Operations + Lambda is the most scalable, cost-effective, and serverless way to delete massive amounts of S3 objects while         applying          custom          logic          (e.g.,          retaining          specific          data).          Reference          Links         -          [S3 Inventory](https://docs.aws.amazon.com/AmazonS3/latest/user guide/storage-inventory.html)               -                [S3                Batch Operations](https://docs.aws.amazon.com/AmazonS3/latest/user guide/batch-ops.html)       -       [Using       Lambda       with       S3 Batch](https://docs.aws.amazon.com/AmazonS3/latest/user guide/batch-ops-invoke-lambda.html)


1.A company is  building an application on AWS. The application uses  multiple AWS  L ambda functions to  retrieve sensitive data from a single Amazon S3  bucket for  proce ssing. The company must ensure that only authorized  Lambda functions can access th

e data. The solution must comply with the  principle of least  privilege. Which solution will meet these  requirements?
A��Grant full S3  bucket access to all  Lambda functions through a  shared IAM  role.

B��Configure the  Lambda functions to  run within  a VPC. Configure a  bucket  policy to grant access  based on the  Lambda functions' VPC endpoint IP  addresses.

C��Create individual IAM  roles for each  Lambda function. Grant the IAM  roles  access t o the S3  bucket. Assign each IAM  role as the  Lambda  execution  role for  its corresp o nding  Lambda function.
D��Configure a  bucket  policy granting access to the  Lambda functions  based  on their function ARNs.
�� ��C



���� ��Correct Answer: C  Detailed  Explanation: To securely allow  multiple AWS  Lambda	functions to access a sensitive S3 bucket while following the principle of least  privile ge, the best approach  is to create individual IAM  roles for each  Lambda function.  Her e's why:  1. Individual IAM  Roles:  Each  Lambda  function  is assigned  its own IAM exec ution role. This role contains only the permissions  needed for that specific function to		interact with the S3 bucket (e.g.,  read-only access  if the function only  retrieves  data). This ensures no  Lambda  has unnecessary  permissions. 2.  Least  Privilege Compliance: By granting each role only the  permissions its corresponding  Lambda  requires, you  m ini mize the  r isk of accidental or malicious data exposure.  For example, a  Lambda that only  reads data won��t have write/delete  permissions. 3. Scalability and  Maintenance: While creating separate  roles  may seem tedious,  it��s scalable. AWS IAM allows you t

o  reuse  policies across  roles, and tools  like AWS Cloud Formation  or Terraform autom ate this  process. Why Other Options  Fail:  - A: A shared  role with full  S3 access violat es  least  privilege. If one  Lambda  is  compromised, all data  is at  risk.  -  B:  Restricting  a ccess via VPC endpoint IPs  is unreliable. IP addresses can change, and this  method d oesn��t authenticate the  Lambda function  itself.  -  D:  S3  bucket  policies cannot directl y  reference  Lambda ARNs as  principals.  Permissions  must  be  granted via IAM  roles as sumed by the  Lambda functions.  Reference  Links:  -  [AWS  Lambda  Execution  Role](htt ps://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html) -  [IAM  B est  Practices](https://docs.aws.amazon.com/IAM/latest/User guide/best-practices.html#gra nt-least-privilege)


2.A company has developed a  non-production application that  is composed of multipl e  micro services for each of the company's  business  units. A single development team maintains all the micro services. The current architecture uses a static web front end a nd a Java-based backend that contains the application logic. The architecture also use s a  MySQL database that the company  hosts  on an Amazon  EC2  instance. The compa ny needs to ensure that the application is secure and available globally. Which sol uti on will meet these  requirements with the  LEAST operational overhead?

A�� Use Amazon Cloud Front and AWS Amplify to  host the static web front end.  Refact or the micro services to  use AWS  Lambda functions that the  micro services access  by  u sing Amazon API Gateway.  Migrate the  MySQL  database to an Amazon  EC2  Reserved	Instance.

B��Use Amazon Cloud Front and Amazon S3 to  host the static web front end.  Refactor the micro services to use AWS  Lambda functions that the  micro services access  by  usin g Amazon API Gateway.  Migrate the  MySQL  database to Amazon  RDS for  MySQL.
C��Use Amazon Cloud Front and Amazon S3 to  host the static web front end.  Refactor the micro services to  use AWS  Lambda functions that are  in a target group  behind a Network  Load  Balancer.  Migrate the  MySQL  database to Amazon  RDS  for  MySQL.
D��Use Amazon S3 to  host the static web front end.  Refactor the  micro services to  use AWS  Lambda functions that are  in a target group  behind an Application  Load  Balanc er.  Migrate the  MySQL database to an Amazon  EC2  Reserved  Instance.
�� ��B



���� ��Correct Answer:  B  Detailed  Explanation: The  correct answer  is  B  because  it  uses fully managed AWS services that minimize operational overhead while ensuring securi ty and global availability. - Static Web  Front end:  Hosting on Amazon  S3 with Cloud Fr ont provides a secure,  highly available, and globally distributed solution. S3 is simple to manage, and Cloud Front accelerates content delivery worldwide. -  Backend  Microse rvices:  Refactoring to AWS  Lambda  and API Gateway eliminates server  management.  L ambda automatically scales, and API Gateway  handles security  (e.g., authentication,  rat e  limiting) and global access.  -  Database:  Migrating to Amazon  RDS  for  MySQL  reduc es operational tasks  like  backups,  patching, and  scaling.  RDS supports  Multi-AZ  deplo yments for  high availability and encryption for security. Other options add  un necessar y complexity (e.g., using  Load  Balancers with  Lambda)  or  retain  EC2  management  (e. g.,  Reserved Instances),  increasing operational effort.  Reference  Links:  -  [Amazon  S3  St

atic Website  Hosting](https://aws.amazon.com/s3/static-web-hosting/)  -  [AWS  Lambda &  API  Gateway](https://aws.amazon.com/api-gateway/)  -  [Amazon  RDS for  MySQL](htt ps://aws.amazon.com/rds/mysql/)


3.A video game company is deploying a  new gaming application to  its global users.  The company  requires a solution that will  provide  near  real-time  reviews and  rankings of the players. A solutions architect  must design a solution to  provide fast access to	the data. The solution  must also ensure the data  persists on disks  in the event that the company  restarts the application. Which solution will meet these  requirements wit h the  LEAST operational overhead?
A��Configure an Amazon Cloud Front distribution with an Amazon S3 bucket as the or igin. Store the  player data  in the  S3  bucket.

B��Create Amazon  EC2  instances  in  multiple AWS  Regions.  Store the  player data on t he  EC2  instances. Configure Amazon  Route  53 with  geolocation  records to direct user s to the closest  EC2  instance.

C�� Deploy an Amazon  Elastic ache for  Redis  duster. Store the  player data  in t he  Elasti Cache cluster.
D��Deploy an Amazon  Elastic ache for  Memcached duster.  Store the  player data  in the Elastic ache cluster.
�� ��C



���� ��Correct Answer: C  Detailed  Explanation: The  best solution is to  use Amazon  Elas tiCache for  Redis  (Option C)  because:  1.  Real-time  Performance:  Redis  is  an  in-memor

y data store designed for  low-latency access,  making  it  ideal for  near  real-time  leader boards and  player  reviews. 2.  Data  Persistence:  Unlike  Memcached  (Option  D),  Redis  s upports disk persistence. This ensures player rankings/reviews survive application resta rts. 3. Global Scalability: While the question doesn't explicitly mention global deploym ent,  Redis  replication can  help  scale  read operations across  regions  if  needed  later. 4. Managed Service:  Elastic ache  handles maintenance tasks automatically, minimizing op erational overhead compared to self-managed  EC2  instances (Option  B) or static  S3 st orage (Option A). Why other options fail: - A (Cloud Front+S3): S3 isn't designed for r eal-time writes/updates and  has  higher  latency than in-memory stores.  -  B  (Multi-regi on  EC2): Instance store volumes are ephemeral, and  managing global data consist enc y across  EC2  instances would create  high operational  complexity. -  D  (Memcached):  L acks  built-in  persistence features,  making  it unsuitable for data that  must survive  rest arts.  Reference  Link:  [Amazon  Elastic ache  for  Redis  Features](https://aws.amazon.com/e last i cache/redis/)

4.A company is designing an application on AWS that  processes sensitive data. The a pplication stores and  processes financial data for multiple customers. To  meet compli ance  requirements, the data for each customer  must  be encrypted separately at  rest by using a secure, centralized key  management solution. The company wants to  use AWS  Key  Management  Service  (AWS  KMS) to  implement encryption. Which solution will meet these  requirements with the  LEAST operational overhead?

A��Generate a unique encryption  key for each customer. Store the  keys  in an Amazon S3 bucket.  Enable server-side encryption.

B��Deploy a  hardware security appliance in the AWS environment that securely stores customer-provided encryption keys. Integrate the security appliance with AWS  KMS to encrypt the sensitive data in the application.
C��Create a single AWS  KMS  key to  encrypt all sensitive data across the application.

D��Create separate AWS  KMS  keys for each  customer's data that  have granular access control and  logging enabled.
�� ��D



���� �� Correct Answer:  D  Detailed  Explanation: The  requirement is to encrypt each cus tomer's data separately using AWS  KMS with  minimal operational effort.  - Option A  i s insecure and inefficient because storing  keys  in S3 is  risky, and  manual  key  manage ment increases overhead. - Option  B  introduces complexity  by  relying on external  har dware, which contradicts AWS-managed services like  KMS.  - Option C fails compli anc e since a single key encrypts all data, violating  per-customer encryption  needs.  - Opti on  D  uses AWS  KMS to  create  unique  keys  per customer.  KMS  automates  key  manag ement,  provides granular access control via  key  policies, and  logs  usage via Cloud trail ��all while  minimizing operational work. AWS  handles security and scalability,  making this the  best fit.  Reference  Links:  -  [AWS  KMS  Key  Management  Best  Practices](https:/ /docs.aws.amazon.com/kms/latest/developer guide/key-management-best-practices.html)
 -  [Using  Multi-Tenant  Keys  in AWS  KMS](https://aws.amazon.com/blogs/security/how- to-use-multi-tenant-customer-master-keys-with-aws-kms/)

5.A company needs to design a  resilient web application to  process customer orders. The web application must automatically  handle increases in web traffic and applicatio n usage without affecting the customer experience or  losing customer orders. Which solution will meet these  requirements?

A��Use a  NAT gateway to  manage web traffic.  Use Amazon  EC2 Auto  Scaling  groups to  receive,  process, and store  processed customer orders.  Use  an AWS  Lambda functi on to capture and store unprocessed orders.

B�� Use a  Network  Load  Balancer  (NLB)  to  manage web traffic.  Use  an Application  Loa
d  Balancer to  receive  customer orders from the  NLUse Amazon  Redshift with  a  Multi- AZ deployment to store unprocessed and  processed customer orders.

C��Use a Gateway  Load  Balancer  (GWLB) to  manage web traffic.  Use  Amazon  Elastic  C ontai ner Service (Amazon  ECS) to  receive and  process  customer orders.  Use the GWL B to capture and store unprocessed orders.  Use Amazon  Dynamo db to  store  process ed customer orders.
D��Use an Application  Load  Balancer to  manage web  traffic.  Use Amazon  EC2 Auto  Sc aling groups to  receive and  process customer orders.  Use Amazon  Simple Queue Serv ice (Amazon SQS) to store unprocessed orders.  Use Amazon  RDS with  a  Multi-AZ  de ployment to store  processed customer orders.
�� ��D



���� �� Correct Answer  D  Detailed  Explanation The correct answer  is  D  because  it  effec t ively combines scalable,  resilient AWS services to  handle varying traffic and ensure  n o order  loss:  1. Application  Load  Balancer  (ALB):  Distributes  incoming web traffic  acro

ss  multiple  EC2  instances,  improving availability and fault tolerance. 2.  EC2 Auto  Scali ng: Automatically adjusts the  number of  EC2 instances  based on traffic, ensuring the  application scales up/down without downtime. 3. Amazon SQS: Stores unprocessed or ders in a queue, ensuring orders are  not  lost even during traffic spikes or temporary processing failures. 4. Amazon  RDS  Multi-AZ:  Provides  a  highly available database for processed orders, with automatic failover to a standby instance if the primary fails. W hy other options are incorrect: - A:  NAT Gateway  is for outbound  internet access,  not	traffic  management.  Lambda  isn��t designed to  reliably store  unprocessed orders duri ng traffic spikes. -  B:  Network  Load  Balancer  (NLB)  is  less  suited  for  HTTP-based web	apps compared to ALB. Amazon  Redshift  is a data warehouse,  not ideal for transact i onal order storage.  - C: Gateway  Load  Balancer  (GWLB)  is for  deploying third-party  n etwork appliances (e.g., firewalls),  not general web traffic.  Reference  Links  -  [Applicatio n  Load  Balancer](https://aws.amazon.com/elastic load balancing/application-load-balancer /) -  [EC2 Auto  Scaling](https://aws.amazon.com/ec2/auto scaling/) -  [Amazon  SQS](https: //aws.amazon.com/sqs/) -  [Amazon  RDS  Multi-AZ](https://aws.amazon.com/rds/details/ multi-az/)


6.A company is using AWS  Data sync to  migrate  millions of files from an  on-premises	system to AWS. The files are  10  KB  in  size on average. The company wants to  use Amazon S3 for file storage.  For the first year after the  migration, the files will  be acc essed once or twice and  must  be  immediately available. After  1 year, the files  must  b e archived for at  least 7 years. Which solution will  meet these requirements  MOST co st-effectively?

A��Use an archive tool to group the files into  large objects.  Use  Data sync  to  migrate the objects. Store the objects in S3 Glacier Instant  Retrieval for the first year.  Use  a  li fecycle configuration to transition the files to S3 Glacier  Deep Archive after  1 year wit h a  retention  period of 7 years.
B��Use an archive tool to group the files into  large objects.  Use  Data sync  to copy the objects to S3 Standard-Infrequent Access (S3 Standard-IA). Use a lifecycle configura ti on to transition the files to S3 Glacier Instant  Retrieval after  1 year with a  retention  period of 7 years.
C��Configure the destination storage class for the files as S3 Glacier Instant  Retrieval. Use a  lifecycle  policy to transition the files to  S3 Glacier  Flexible  Retrieval  after  1 year with a  retention period of 7 years.
D��Configure a  Data sync task to transfer the files to  S3 Standard-Infrequent Access  (S 3 Standard-IA).  Use a  lifecycle configuration to transition the files to  S3  Deep Archive after 1 year with a retention period of 7 years.
�� ��D



���� �� Correct Answer:  D  Detailed  Explanation: The  most cost-effective solution is to  u se S3 Standard-Infrequent Access (S3 Standard-IA) for the first year and transition to S3 Glacier  Deep Archive  after  1 year.  Here��s why:  1.  Initial  Storage  (First Year):  - The files  need to  be  immediately available  but are accessed only once or twice.  - S3 Stan dard-IA is cheaper than S3 Standard and designed for infrequently accessed data that	requires rapid  retrieval. This fits the first-year  requirement.  - AWS  Data sync  directly s upports transferring files to S3 Standard-IA, avoiding complex workarounds (e.g., grou

ping files into  large archives). 2.  Long-Term Archiving  (After  1 Year):  - The files  must be archived for 7 + years at the  lowest cost.  - S3 Glacier  Deep Archive  is t he cheapes t storage class for long-term archiving (e.g.,  regulatory backups) with  retrieval times o f 12�C48 hours. 3. Why Other Options Are  Less Optimal:  - Option A/C: Glacier Instant Retrieval is  more expensive than S3 Standard-IA for infrequent access.  - Option  B: Tra nsi tioning to Glacier Instant  Retrieval after  1 year  is  unnecessary and costly for archiv ed data.  - Grouping small files (Options A/B) adds complexity and  makes  retrieving in dividual files harder.  Reference  Links:  -  [S3  Storage  Classes](https://aws.amazon.com/s3 /storage-classes/) -  [AWS  Data sync  S3 Storage Class  Support](https://docs.aws.amazon. com/data sync/latest/user guide/create-s3-location.html#create-s3-location-storage-class)


7.A company recently performed a  lift and shift  migration of its on-premises Oracle d atabase workload to  run on an Amazon  EC2  memory  optimized  Linux  instance. The  E C2  Linux  instance  uses a  1 TB  Provisioned IOPS  SSD  (io1)  EBS volume with  64,000 IO PS. The database storage  performance after the  migration  is slower than the  perform ance of the on-premises database. Which solution will improve storage  performance?

A��Add  more  Provisioned IOPS  SSD  (io1)  EBS volumes.  Use  OS  commands to create a Logical Volume  Management  (LVM) stripe.

B��Increase the  Provisioned IOPS SSD  (io1)  EBS volume to  more than  64,000 IOPS. C��Increase the size of the  Provisioned IOPS SSD  (io1)  EBS volume to 2 TB.

D��Change the  EC2  Linux  instance to  a storage optimized instance type.  Do  not  chan ge t he  Provisioned IOPS  SSD (io1)  EBS volume.
�� ��A

���� �� Correct Answer A  Detailed  Explanation The issue arises  because a single  Provisi oned IOPS SSD (io1)  EBS volume  has a  maximum of 64,000 IOPS when the volume s i ze  is  1 TB or  larger.  Even though the  migrated  setup  matches this  limit, the  perform a nce is still worse than on-premises.  Here��s why each option works or doesn��t:  - Op t ion A:  By adding  multiple  io1 volumes and striping them  using  Logical Volume  Mana gement (LVM), the total aggregated IOPS and throughput increase.  For example, two 1 TB io1 volumes (each with 64,000 IOPS) striped together  provide 128,000 IOPS. This bypasses the per-volume IOPS limit, directly addressing the performance bottleneck.
- Option  B: Increasing IOPS  beyond  64,000 on a single  1 TB  io1 volume is  impossible. The maximum IOPS for a single io1 volume is 64,000 unless the volume size is incre ased (e.g., 2 TB allows up to  128,000 IOPS).  - Option C: Increasing the volume size t
o 2 TB alone doesn��t improve  performance. While a 2 TB  io1 volume supports  up to
 128,000 IOPS, the user  must explicitly  provision the  higher IOPS  (which this option d oesn��t  mention). Without increasing IOPS, the  performance  remains  unchanged.  - Opt ion  D: Storage-optimized instances  (e.g., i3)  prioritize instance-store volumes (NVMe), which are ephemeral.  However, the workload uses  EBS, so  switching  instance types d oesn��t  resolve  EBS  performance  limits. Instead, an  EBS-optimized  instance with  suffici ent network  bandwidth (e.g.,  larger  memory-optimized  instances)  might  help,  but the question doesn��t  indicate a  network  bottleneck.  Key Take away When a  single  EBS vol ume��s IOPS  limit  is  reached, striping  multiple volumes via  LVM  is  a  proven  method t o scale  performance  linearly. This approach is cost-effective and aligns with AWS  best practices for high-performance database workloads.  Reference  Links  -  [Amazon  EBS V olume Types](https://docs.aws.amazon.com/AWSEC2/latest/User guide/ebs-volume-types.

html) -  [RAID Configuration on  Linux](https://docs.aws.amazon.com/AWSEC2/latest/User Guide/raid-config.html)


8.A company is  migrating from a  monolithic architecture for a web application that  is hosted on Amazon  EC2 to a serverless micro services architecture. The company want s to use AWS services that support an event-driven,  loosely coupled architecture. The company wants to use the publish/subscribe (pub/sub) pattern. Which solution will meet these  requirements  MOST cost-effectively?

A��Configure an Amazon API Gateway  REST API to  invoke an AWS  Lambda function t hat  publishes events to an Amazon Simple Queue Service (Amazon SQS) queue. Confi gure one or  more subscribers to  read events from the  SQS queue.

B��Configure an Amazon API Gateway  REST API to  invoke an AWS  Lambda function t hat  publishes events to an Amazon Simple  Notification Service (Amazon SNS) topic. C on figure one or more subscribers to  receive events from the SNS topic.

C��Configure an Amazon API Gateway Web Socket API to write to a data stream  in A mazon  Kinesis  Data  Streams with enhanced fan-out. Configure one or  more subscribe rs to  receive events from the data stream.
D��Configure an Amazon API Gateway  HTTP API to  invoke an AWS  Lambda function t hat  publishes events to an Amazon Simple  Notification Service (Amazon SNS) topic. C on figure one or more subscribers to  receive events from the topic.

���� �� Correct Answer:  D  Detailed  Explanation: The question asks for a cost-effective s erver less solution using an event-driven,  pub/sub  pattern.  Here's why  Option  D  is the best choice:  1. API Gateway  HTTP API:  -  HTTP APIs are  simpler and cheaper than  RE ST APIs  (used in Option  B). They are  optimized for serverless workloads and  provide  l ower latency and cost for basic  integrations. This directly improves cost-effectiveness.
2. Amazon SNS (Pub/Sub Service): - SNS  natively supports the  pub/sub  pattern. Whe n a  message  is  published to an  SNS topic,  it  is automatically  broadcast to all subscri bers (e.g.,  Lambda,  SQS, email, etc.). This ensures  loose coupling between  publishers  and subscribers. - SQS (Option A)  is a queue service designed for  point-to-point  mes saging (one consumer  per message),  making  it  unsuitable for  pub/sub.  Kinesis  (Optio n C)  is for  real-time data streaming, which  is overkill and  more expensive for simple pub/sub. 3.  Lambda Integration:  - While  using  Lambda  adds a small cost, it allows fle x ibility to  process/transform events  before  publishing to  SNS. If  no  processing  is  need ed, API Gateway could integrate directly with SNS via service integrations (no  Lambd a),  but the question doesn��t  rule out  Lambda as a valid serverless component. Why Other Options Are  Less  Optimal: - Option  B:  Uses  REST API, which  is  more  expensive	than  HTTP API.  - Option C: Web socket API and  Kinesis  are unnecessary for  pub/sub and increase complexity/cost. - Option A: SQS doesn��t support pub/sub  natively and		requires polling, which is inefficient for fan-out.  References:  -  [AWS  SNS vs  SQS](http s://aws.amazon.com/pub-sub-messaging/) -  [API  Gateway  HTTP vs  REST APIs](https://a ws.amazon.com/api-gateway/pricing/)


9.A company recently migrated a  monolithic application to an Amazon  EC2  instance a nd Amazon  RDS. The application  has tightly coupled  modules. The existing design of

the application gives the application the ability to  run on only a single  EC2  instance. The company  has  noticed  high CPU  utilization on the  EC2  instance  during  peak  usag e times. The  high CPU  utilization corresponds to degraded  performance on Amazon  R DS for  read  requests. The company wants to  reduce the  high CPU  utilization and  imp rove  read  request  performance. Which solution will  meet these  requirements?
A�� Resize the  EC2  instance to an  EC2  instance type that  has  more  CPU  capacity. Conf igure an Auto Scaling group with a  minimum and  maximum size of  1. Configure an  RDS  read  replica for  read  requests.

B��Resize the  EC2  instance to an  EC2  instance  type that  has  more  CPU capacity. Confi gure an Auto Scaling group with a  minimum and  maximum  size of  1. Add an  RDS  re ad  replica and  redirect all  read/write traffic to the  replica.

C��Configure an Auto Scaling group with a  minimum size of 1 and  maximum size of 2.  Resize t he  RDS  DB  instance  to an  instance type that  has  more  CPU capacity.
D��Resize the  EC2  instance to an  EC2  instance type that  has  more  CPU capacity.  Conf igure an Auto Scaling group with a  minimum and  maximum size of  1.  Resize the  RD S  DB  instance to an  instance type that  has  more  CPU capacity.
�� ��A



���� ��Correct Answer A.  Resize the  EC2  instance to an  EC2  instance type that  has  mo re CPU capacity. Configure an Auto Scaling group with a  minimum and  maximum s iz e of 1. Configure an  RDS  read  replica  for  read  requests.  Detailed  Explanation  The  pro blem  has two  main  issues:  1.  High  CPU  usage on the  EC2  instance: The  application is currently running on a single  EC2 instance, and vertical scaling (resizing to a  larger i

nstance type) is the simplest way to  handle this since the application is tightly couple d and cannot  run on  multiple  instances. 2.  Degraded  RDS  read  performance:  Creating an  RDS  read  replica separates  read traffic from write traffic.  Read  requests  can  be  re directed to the  replica,  reducing  load on the  primary database and  improving  read  p erformance. Why other options are wrong: - Option  B:  Redirecting all traffic  (read/writ e) to a  read  replica  is incorrect  because write operations  must  go to the  primary dat abase. - Option C: Auto Scaling (horizontal scaling) won't work  here  because the appl ication is tightly coupled and designed to  run on a single instance. Also,  resizing the RDS  instance doesn't address  read-heavy traffic. - Option  D:  Resizing t he  RDS  instanc e  might  help temporarily,  but  it doesn't optimize  read traffic. A  read  replica  is  a  mor e scalable and cost-effective solution.  Key Concepts:  - Vertical Scaling  (EC2):  Upgrad in g to a  larger instance type  (e.g., from `t3.medium` to `t3.large`) adds  more CPU/mem ory. -  RDS  Read  Replica:  A  read-only copy of the  primary database. It offloads  read q ueries, improving  performance.  Reference  Links  -  [Amazon  EC2  Instance Types](https:// aws.amazon.com/ec2/instance-types/) -  [Amazon  RDS  Read  Replicas](https://docs.aws.a mazon.com/Amazon rds/latest/User guide/USER_Read repl.html)


10.A company needs to grant a team of developers access to the company's AWS  re sources. The company must  maintain a  high  level of security for the  resources. The c ompany requires an access control solution that will prevent unauthorized access to t he sensitive data. Which solution will meet these requirements?
A��Share the IAM user credentials for each development team  member with the rest of the team to simplify access  management and to streamline development workflow s.

B��Define IAM  roles that  have fine-grained  permissions  based  on the  principle of  least privilege. Assign an IAM  role to each developer.
C��Create IAM access  keys to grant  programmatic access to AWS  resources. Allow onl y developers to interact with AWS  resources through API calls  by  using the access  ke ys.
D��Create an AWS Cognito user  pool. Grant developers access to AWS  resources  by  u sing the  user  pool.
�� ��B



���� �� Correct Answer:  B  Detailed  Explanation: The  best approach  is to  use IAM  roles with fine-grained permissions  based on the  principle of least  privilege.  Here's why:  1. IAM  Roles vs.  Shared Credentials  (Option A):  - Sharing IAM  user credentials  (like  pass words or access  keys)  is extremely  insecure. If credentials are  leaked, anyone can  mis use them.  Roles avoid this  by  providing temporary, short  - term  permissions  instead  of long - term credentials. 2.  Least  Privilege  (Option  B):  - Assigning  roles  ensures eac h developer only gets  permissions strictly  necessary for their tasks.  For  example, a de veloper might  need  read  - only access to an  S3  bucket,  not full admin  rights. This  mi ni mizes damage  if an account  is compromised. 3. Access  Keys  (Option  C):  - Access  ke ys are  long  - term credentials and  riskier than  roles. If a  key  is exposed, attackers  ca n misuse it  until  manually  revoked.  Roles automatically  rotate  credentials. 4. AWS Cog nito (Option  D):  -  Cognito  manages end  -  user  identities  (e.g., app  users),  not AWS  r esource access for developers. IAM is the correct service for this scenario.  Reference  L

ink:  [AWS IAM  Roles  Documentation](https://docs.aws.amazon.com/IAM/latest/User guid e/id_roles.html)


11.A company runs all its  business applications in the AWS Cloud. The company uses AWS Organizations to manage multiple AWS accounts. A solutions architect needs to	review all permissions that are granted to IAM  users to determine which IAM  users have  more permissions than  required. Which solution will  meet these  requirements wi th the  LEAST administrative overhead?
A�� Use  Network Access Analyzer to  review all access  permissions  in the  company's A WS accounts.
B��Create an AWS Cloud watch alarm that activates when an IAM user creates or  mod ifies  resources in an AWS account.
C��Use AWS Identity and Access  Management  (IAM) Access Analyzer to  review  all the company��s resources and accounts.
D�� Use Amazon Inspector to find vulnerabilities in existing IAM  policies.

�� ��C



���� ��Correct Answer: C  Explanation: The AWS Identity and Access  Management (IAM) Access Analyzer is specifically designed to help identify permissions granted to IAM users,  roles, or  resources that exceed what  is  necessary  (a  principle called  least  privile ge).  Here's why this  is the  best choice:  1. Automated Analysis: IAM Access Analyzer a u tomatically  reviews policies across all accounts in AWS Organizations, checks for ove rly permissive  permissions, and generates findings. This  reduces manual effort. 2. Cros

s-Account Support: Since the company uses AWS Organizations, IAM Access Analyzer can  be enabled once and applied to all  member accounts, streamlining the  review  pr ocess. 3.  Focus on  Least  Privilege:  It  identifies  policies that grant  unintended access  (e. g.,  public access to  resources) or excessive  permissions, directly addressing the  proble m. 4. Alternatives:  - A (Network Access Analyzer):  Focuses on  network configurations   (e.g., VPCs, security groups),  not IAM  policies.  -  B  (Cloud watch Alarm):  Monitors  actio ns in  real-time  but doesn��t audit existing  permissions.  -  D  (Amazon  Inspector):  Scans  for vulnerabilities in compute  resources  (EC2, containers),  not IAM  policies.  Reference  Link:  [AWS IAM Access Analyzer  Documentation](https://docs.aws.amazon.com/IAM/late st/User guide/what-is-access-analyzer.html)


12.A company needs to  implement a  new data  retention  policy for  regulatory  complia nce. As  part of this  policy, sensitive documents that are stored  in an Amazon  S3  buc ket  must  be  protected from deletion or  modification for a fixed period of time. Whic h solution will meet these requirements?

A��Activate S3 Object  Lock on the  required objects  and enable governance  mode.

B��Activate S3 Object  Lock on the  required objects and enable compliance  mode.

C�� Enable versioning on the S3  bucket. Set a  lifecycle  policy to delete the objects aft er a specified period.
D��Configure an S3  Lifecycle  policy to transition objects to  S3 Glacier  Flexible  Re tri eva l for the retention duration.
�� ��B

���� �� Correct Answer  B. Activate S3 Object  Lock on the  required objects and enable compliance mode.  Detailed  Explanation To  meet the  requirement  of protecting sensiti ve documents from deletion/modification for a fixed  period, S3 Object  Lock  in compli ance  mode  is the correct solution.  Here's why:  1.  S3 Object  Lock  provides Write-Once -Read-Many (WORM)  protection:  -  Prevents object deletion/modification during a  rete ntion period - Works at the object version  level 2. Compliance  Mode vs Governance Mode: - Compliance  Mode  (Correct choice):  -  Even  root AWS  account  users cannot o verride  retention settings -  Retention  period  cannot  be shortened  -  Meets  strict  regul atory  requirements  - Governance  Mode  (Option A):  - Allows override  by  users with  sp ecific IAM  permissions  -  Less  strict than compliance  mode 3. Why other options are  i n correct: - Option C: Versioning  +  Lifecycle  policy  only  manages versions  and de letio n schedules,  but doesn't enforce WORM  protection  - Option  D: Glacier transition cha nges storage class  but doesn't  prevent deletion/modification - Option A: Governance mode doesn't  provide the strict  protection  required for compliance  Reference  Link  [A mazon S3 Object  Lock](https://docs.aws.amazon.com/AmazonS3/latest/user guide/object -lock.html)


13.A company runs its customer-facing web application on containers. The workload  uses Amazon  Elastic Container Service  (Amazon  ECS)  on AWS  Fargate. The web applic ation is  resource intensive. The web application  needs to  be available 24  hours a  day, 7 days a week for customers. The company expects the application to experience s h ort  bursts of high traffic. The workload must  be  highly available. Which solution will meet these  requirements  MOST cost-effectively?

A��Configure an  ECS capacity  provider with  Fargate.  Conduct  load testing  by  using a third-party tool.  Right size the  Fargate tasks  in Amazon Cloud watch.

B��Configure an  ECS  capacity  provider with  Fargate  for steady state and  Fargate  Spot for burst traffic.

C��Configure an  ECS  capacity  provider with  Fargate  Spot for  steady  state and  Fargate for burst traffic.
D��Configure an  ECS capacity  provider with  Fargate.  Use  AWS Compute Optimizer to right size the  Fargate task.
�� ��D



���� �� Correct Answer  D. Configure an  ECS capacity  provider with  Fargate.  Use AWS  C ompute Optimizer to  right size the  Fargate task.  Detailed  Explanation The  question em pha sizes cost  - effectiveness while  maintaining  high availability for a  resource  -  intens ive,  burst  -  prone web app.  Here's why  Option  D  is the  best  choice:  1.  Rightsizing wi th AWS Compute Optimizer: - The app is  resource  -  intensive,  meaning  it  likely  uses more CPU/memory than  needed. Over provisioning  resources in  Fargate tasks wastes
money. - AWS Compute Optimizer analyzes  historical usage and  recommends optimal CPU/memory settings for  Fargate tasks. This reduces costs without sacrificing  perfor mance. 2. Why  not  Fargate  Spot  (Options  B/C)?  -  Fargate  Spot  is  cheaper  but can  be	interrupted (not ideal for 24/7 availability). While it could  handle  bursts,  interruptions during traffic spikes  might hurt customer experience. - Steady - state workloads sho uld prioritize  reliability  (standard  Fargate).  Burst  handling  is  better  managed  via autos caling (already  part of  Fargate) combined with  rightsizing.  3. Cost vs. Complexity: - O

ptions  B/C  require  managing two capacity  providers  (Spot  +  standard).  This adds com plexity and risks  interruptions during bursts.  - Option  D  simplifies architecture  (standa rd  Fargate) while directly tackling the  root cause of cost:  over provisioned resources.  R eference  Links  -  [AWS Compute Optimizer for  Fargate](https://docs.aws.amazon.com/c ompute-optimizer/latest/ug/container-recommendations.html) -  [Fargate  Rightsizing  Bes t  Practices](https://aws.amazon.com/blogs/containers/how-to-right-size-your-amazon-ecs -fargate-services/)


14.A company is  building an application in the AWS Cloud. The application is hosted on Amazon  EC2  instances  behind an Application  Load  Balancer  (ALB). The  company  u ses Amazon  Route  53 for the  DNS. The  company  needs a  managed solution with  pro active engagement to detect against  DDoS attacks. Which solution will  meet these  re quirements?

A��Enable AWS Config. Configure an AWS Config  managed  rule that detects  DDoS att acks.

B��Enable AWS WAF on t he Al create an AWS WAF web ACL with  rules to detect and prevent  DDoS attacks. Associate the web ACL with the ALB.

C��Store the ALB access  logs in an Amazon  S3  bucket. Configure Amazon Guard Duty to detect and take automated  preventative actions for  DDoS attacks.
D��Subscribe to AWS Shield Advanced. Configure  hosted zones  in  Route  53. Add ALB resources as  protected  resources.
�� ��D

���� �� Correct Answer:  D  Detailed  Explanation: The correct solution is AWS Shield Adv anced because it  provides comprehensive,  managed  DDoS  protection with  proactive
monitoring and direct engagement from AWS��s  DDoS  Response Team  (DRT).  Here��s why: - AWS Shield Advanced is specifically designed to  protect against  DDoS attacks	(layers 3/4 and 7) and includes 24/7 support,  real-time attack visibility, and automati c  mitigation. It also covers cost overages from scaling during attacks.  -  Proactive  Eng agement: Shield Advanced includes access to the AWS Shield  Response Team  (SRT),
who proactively monitor and assist during attacks,  meeting the  requirement for  proac tive engagement. - Integration with ALB and  Route  53: Shield Advanced allows you t o  protect  resources  like ALBs and  Route  53  hosted zones, which  are explicitly  mentio ned in the scenario. Other options fall short: - AWS WAF (B) only  handles  layer 7  (H TTP/HTTPS) attacks and  lacks  proactive support.  - Guard duty  (C) detects threats  but  doesn��t auto-mitigate  DDoS attacks.  - AWS  Config (A) audits configurations  but does n��t block attacks.  Reference:  [AWS  Shield Advanced](https://aws.amazon.com/shield/ad vanced/)

15.A company hosts a video streaming web application in a VPC. The company uses a  Network  Load  Balancer  (NLB) to  handle  TCP traffic for  real-time data  processing. Th ere have  been  unauthorized attempts to access the application. The company wants t o improve application security with minimal architectural change to prevent un author i zed attempts to access the application. Which solution will meet these  requirements?
A��Implement a series of AWS WAF  rules directly on the  NLB to filter out  un authorize
d traffic.

B�� Recreate the  NLB with a  security group to allow only trusted IP addresses.

C��Deploy a second  NLB  in  parallel with the  existing  NLB  configured with a strict IP a ddress allow list.
D�� Use AWS Shield Advanced to  provide enhanced  DDoS  protection and  prevent  una u thorized access attempts.
�� ��B



���� ��Correct Answer  B  Explanation The correct answer  is  B:  Recreate the  NLB  with a security group to allow only trusted IP addresses.  Here��s why:  -  Network  Load  Balanc er (NLB) operates at the TCP  layer (Layer 4) and  is designed for  high-performance,  lo w-latency traffic.  Unlike Application  Load  Balancers  (ALB),  NLBs  do  not support  securit y groups natively.  However, the confusion  here arises from the question's  phrasing.  - Security Groups are typically attached to  backend targets  (e.g.,  EC2  instances)  behind the  NLB. To  restrict access, you would configure these security groups to allow traffic only from the  NLB��s IP addresses or specific trusted client IPs. - The question assu mes  minimal architectural changes.  Recreating the  NLB  isn��t  necessary,  but  t he  inten ded solution involves updating the security groups of the  backend instances (not the NLB itself) to whitelist trusted IPs. This aligns with the idea of recreating the  NLB wit h a security group in the answer, though t he wording  is technically inaccurate.  - Othe r options are invalid: - A: AWS WAF cannot be attached directly to  NLBs  (only ALB/Cl oud Front). - C:  Deploying a  second  NLB adds complexity, violating  minimal change.  - D: AWS Shield Advanced protects against  DDoS  but doesn��t  block  specific un author i zed IPs. Why  B  is Accepted While  NLBs  don��t  use security groups directly, the answe r implies configuring security groups on backend  resources  (e.g.,  EC2) to  restrict traffi c to trusted IPs. This is the simplest way to  meet the  requirement without  major  infr

a structure changes.  Reference  Link  [AWS  NLB  Documentation](https://docs.aws.amazon. com/elastic load balancing/latest/network/introduction.html)


16.A company has an employee web portal.  Employees  log  in to the  portal to view  p ayroll details. The company is developing a  new system to give employees the ability	to upload scanned documents for reimbursement. The company  runs a  program to  extract text-based data from the documents and attach the extracted information to  each employee��s  reimbursement IDs for  processing. The employee web  portal  require s  100% uptime. The document extract  program  runs  infrequently throughout the day on an on-demand basis. The company wants to  build a scalable and cost-effective  ne w system that will require minimal changes to the existing web  portal. The company does not want to  make any code changes. Which solution will  meet these  require me nts with the  LEAST  implementation effort?

A�� Run Amazon  EC2 On-Demand Instances  in an Auto Scaling group for the web por tal.  Use an AWS  Lambda function to  run the  document extract  program. Invoke the  L ambda function when an employee uploads a new  reimbursement document.

B��Run Amazon  EC2  Spot Instances  in  an Auto Scaling group for the web  portal.  Run the document extract  program on  EC2  Spot Instances. Start document extract  progra
m instances when an employee uploads a  new  reimbursement document.

C��Purchase a Savings  Plan to  run the web  portal and the  document extract  program. Run the web portal and the document extract  program  in an Auto Scaling group.
D��Create an Amazon S3  bucket to  host the web  portal.  Use Amazon API  Gateway an d an AWS  Lambda function for the existing functionalities.  Use the  Lambda function t

o run the document extract  program. Invoke the  Lambda function when the API that is associated with a  new document  upload  is called.
�� ��A



���� �� Correct Answer A  Detailed  Explanation The requirements are:  1.  100%  uptime f or the web  portal (critical for employee access). 2. Infrequent, on - demand  process in g for document extraction (cost  - effective scaling  needed).  3.  Minimal changes to exi sting code/system. Why Option A Works  Best:  - Web  Portal  (EC2  +  Auto  Scaling):  Usi ng  EC2 On  -  Demand Instances  in an Auto  Scaling group ensures  high availability an d automatic scaling for the existing web portal. This  maintains  100% uptime without  re architecting the system. -  Document  Extraction  (Lambda): AWS  Lambda  is  serverless,	so  it runs only when triggered (e.g., when a document is uploaded). This is cost - ef fective for sporadic usage and  requires  no code changes  if integrated via triggers  (e. g., S3 upload events).  -  Minimal  Effort: The  existing  EC2  -  based  portal  stays  unchang ed. Only the  new document  upload feature needs to trigger  Lambda, which can  be d one via AWS services  (e.g., S3 event  notifications) without  modifying the  portal��s cod e. Why Other Options  Fail:  -  B:  Spot Instances  risk  interruptions  (bad for  uptime).  Ma naging on - demand  EC2  instances for document  processing  is slower and  less cost  - efficient than  Lambda.  - C: Savings  Plans  lock  in costs for  1�C3 years, which is in effic i ent for infrequent workloads. Auto Scaling for  both systems  is overkill for the docum ent program.  -  D:  Hosting  a dynamic web  portal on S3  is  unrealistic  (requires  rear chit ecting).  Using  Lambda/API Gateway for existing functionalities would force code chan ges.  Reference  Links  -  [AWS  Lambda](https://aws.amazon.com/lambda/)  -  [EC2  Auto  Sc

aling](https://aws.amazon.com/ec2/auto scaling/) -  [S3  Event  Notifications](https://docs.a ws.amazon.com/AmazonS3/latest/user guide/Notification how to.html)


17.A company is  migrating an application from an on-premises  location to Amazon  El astic  Ku bernet es Service  (Amazon  EKS). The  company  must  use a custom subnet for  pods that are in the company's VPC to comply with requirements. The company also needs to ensure that the  pods can communicate securely within the  pods' VPC. Whic h solution will meet these requirements?

A��Configure AWS Transit Gateway to directly manage custom subnet configurations f or the  pods in Amazon  EKS.

B��Create an AWS  Direct Connect connection from the company's on-premises IP add ress  ranges to the  EKS  pods.

C�� Use the Amazon VPC CNI  plugin for  Ku bernet es.  Define  custom subnets  in the VP C cluster for the  pods to  use.
D��Implement a  Ku bernet es  network  policy that  has  pod anti-affinity  rules to  restrict pod placement to specific  nodes that are within custom sub nets.
�� ��C



���� �� Correct Answer C  Detailed  Explanation The correct answer is C  because the Am azon VPC CNI  (Container  Networking Interface)  plugin allows  Ku bernet es  pods to dire ctly use IP addresses from the VPC.  By configuring custom subnets  in the VPC for th e  EKS cluster,  pods will  be  assigned IPs from these designated sub nets, ensuring com pliance with the company��s subnet requirements. Since the pods are within the same

 VPC, communication between them is inherently secure via VPC  networking features like security groups and network ACLs.  - Option A is incorrect  because AWS Transit  Gateway is used for connecting  multiple VPCs or on-premises networks,  not for  mana ging pod sub nets. - Option  B  is wrong  because AWS  Direct  Connect establishes a de dicated  network connection to AWS  but doesn��t address  pod  subnet configuration.  - Option  D  is  incorrect  because  Ku bernet es  network  policies  control traffic flow  betwee n pods,  not subnet assignment.  Pod anti-affinity  rules  influence where  pods are  sched u led  (e.g., avoiding co-location on  nodes),  but  nodes and  pods  can still  use default s ub nets unless explicitly configured otherwise.  Reference  Link  [Amazon  EKS  Networking with the Amazon VPC CNI  Plugin](https://docs.aws.amazon.com/eks/latest/user guide/p od-networking.html)


18.A company hosts an ecommerce application that stores all data  in a single Amazo n  RDS for  MySQL  DB  instance that  is  fully  managed  by AWS. The company  needs to mitigate the risk of a single point of failure. Which solution will meet these require ments with the  LEAST  implementation effort?

A��Modify t he  RDS  DB  instance to  use  a  Multi-AZ  deployment. Apply the changes dur ing the next  maintenance window.

B�� Migrate the current database to a  new Amazon  Dynamo db  Multi-AZ  deployment. Use AWS  Database  Migration  Service  (AWS  DMS) with a  heterogeneous  migration  str ategy to  migrate the current  RDS  DB  instance to  Dynamo db  tables.

C��Create a  new  RDS  DB  instance  in  a  Multi-AZ  deployment.  Manually  restore  the dat a from the existing  RDS  DB  instance from the  most  recent  snapshot.

D��Configure the  DB  instance  in an Amazon  EC2 Auto  Scaling  group with  a  minimum group size of three. Use Amazon  Route  53  simple  routing to distribute  requests to all DB instances.
�� ��A



���� �� Correct Answer: A  Detailed  Explanation The easiest way to mitigate the risk of a single point of failure for an Amazon  RDS  MySQL  database  is to enable  Multi-AZ d eployment. This feature automatically creates a standby  replica of the database  in a d ifferent Availability Zone (AZ). If the  primary database fails, AWS automatically switche s to the standby  replica with  minimal downtime.  - Option A  requires only a configura tion change in t he  RDS  settings and can  be applied during the  next  maintenance win dow. This involves  no  manual data  migration, application code changes, or complex s etup. - Option  B  involves  migrating a  relational  database  (MySQL) to a  NoSQL databa se (Dynamo db), which is time-consuming,  requires schema redesign, and introduces c om patibility  risks.  - Option C  requires  manually  restoring data from a  snapshot, which	is error-prone and adds unnecessary effort compared to the automated  Multi-AZ set up. - Option  D  is  invalid  because  RDS  instances  cannot  be  managed  by  EC2  Auto  Sc aling or  Route  53  routing.  Multi-AZ  is  a  native AWS feature for  RDS,  making  it  the si mplest and  most  reliable solution.  Reference  Links  -  [Amazon  RDS  Multi-AZ  Deploy me nts](https://docs.aws.amazon.com/Amazon rds/latest/User guide/Concepts.MultiAZ.html)


19.A company has  multiple  Microsoft Windows  SMB file servers and  Linux  NFS file  se rvers for file sharing in an on-premises environment. As  part of the company's AWS migration plan, the company wants to consolidate the file servers in the AWS Cloud.

The company  needs a  managed AWS storage service that supports  both  NFS  and SM B access. The solution  must  be able to share  between  protocols. The solution  must  h ave  redundancy at the Availability Zone  level. Which solution will meet these  require ments?
A�� Use Amazon  FSx for  NetApp  ONTAP for storage. Configure  multi-protocol access.

B��Create two Amazon  EC2  instances.  Use one  EC2  instance for Windows  SMB file ser ver access and one  EC2  instance for  Linux  NFS file  server access.

C�� Use Amazon  FSx for  NetApp  ONTAP for  SMB access.  Use Amazon  FSx for  Lustre  f or  NFS access.
D�� Use Amazon S3 storage. Access Amazon S3 through an Amazon S3  File Gateway.

�� ��A



���� �� Correct Answer: A.  Use Amazon  FSx for  NetApp ONTAP for storage. Configure multi-protocol access.  Explanation: Amazon  FSx for  NetApp  ONTAP  is a fully  managed AWS storage service that supports multi-protocol access (both SMB for Windows an d  NFS for  Linux) on the same file  system, allowing seamless cross-protocol data shari ng. It also  provides  built-in  redundancy at the Availability Zone  (AZ)  level  by deploy in g across  multiple AZs in a  region, ensuring  high availability. Other  options fail  becaus e: -  B:  Using  EC2  instances  requires  manual  management  (not  fully  managed) and  lac ks  native cross-protocol sharing. - C:  FSx for  Lustre  only  supports  NFS,  not  SMB, and cannot share data with  FSx for  NetApp ONTAP.  -  D:  S3  File Gateway only  provides  li mited protocol support (either SMB or  NFS,  not  both) and  lacks  native  multi-protocol

 sharing.  Reference  Link:  [Amazon  FSx  for  NetApp  ONTAP  Documentation](https://doc s.aws.amazon.com/fsx/latest/On tap guide/what-is-fsx-ontap.html)


20.A company has an internal application that  runs on Amazon  EC2  instances  in an A uto Scaling group. The  EC2 instances are compute optimized and  use Amazon  Elastic Block Store (Amazon  EBS) volumes. The company wants to identify cost optimization s across the  EC2  instances, the Auto Scaling group, and the  EBS volumes. Which  solu tion will meet these  requirements with the  MOST operational efficiency?
A��Create a  new AWS Cost and  Usage  Report.  Search the  report for  cost  recommend ations for the  EC2  instances the Auto Scaling group, and the  EBS volumes.

B��Create  new Amazon Cloud watch  billing alerts. Check the alert statuses for cost  rec ommendations for the  EC2  instances, the Auto Scaling group, and the  EBS volumes.

C��Configure AWS Compute Optimizer for cost recommendations for the  EC2  instance s, the Auto Scaling group and the  EBS volumes.
D��Configure AWS Compute Optimizer for cost  recommendations for the  EC2  instance s. Create a  new AWS Cost and  Usage  Report.  Search the  report for  cost  recommend a tions for the Auto Scaling group and the  EBS volumes.
�� ��C



���� ��Correct Answer: C  Detailed  Explanation: The  best  solution  is C  because AWS Co mpute Optimizer is designed to automatically analyze your AWS  resources (like  EC2  i n stances, Auto Scaling groups, and  EBS volumes) and  provide cost-saving  recommend ations with  minimal effort.  - AWS Compute Optimizer uses  machine  learning to  revie

w  historical  usage data and suggests optimized configurations, such as  rightsizing  EC2		instances, adjusting Auto Scaling groups, or  recommending cost-effective  EBS volume	types (e.g., gp3  instead of gp2).  - Option A (Cost and  Usage  Report)  requires  manua l analysis of billing data, which is time-consuming. - Option  B  (Cloud watch  billing ale rts) only tracks spending thresholds,  not optimizations.  - Option  D splits the work  bet ween two tools,  making it  less efficient than  using Compute Optimizer alone.  Referen ce  Links:  -  [AWS Compute Optimizer](https://aws.amazon.com/compute-optimizer/)  -
[Cost Optimization for  EC2](https://docs.aws.amazon.com/white papers/latest/cost-optimi zation-compute-optimization/amazon-ec2-cost-optimization.html)


21.A company is  running a  media store across  multiple Amazon  EC2  instances  distrib uted across  multiple Availability Zones  in a single VPC. The company wants a  high-pe r forming solution to share data  between all the  EC2  instances,  and  prefers to  keep th e data within the VPC only. What should a solutions architect recommend?

A��Create an Amazon S3 bucket and call the service APIs from each instance's applica tion

B��Create an Amazon S3  bucket and configure all  instances to access  it as a  mounted volume

C��Configure an Amazon  Elastic  Block  Store  (Amazon  EBS) volume  and  mount  it acros s all  instances
D��Configure an Amazon  Elastic  File  System  (Amazon  EFS)  file  system and  mount  it ac ross all instances
�� ��D

���� �� Correct Answer  D. Configure an Amazon  Elastic  File  System  (Amazon  EFS) file  s ystem and mount it across all instances  Detailed  Explanation Amazon  EFS  is  designed	for exactly this scenario. It  provides a shared, scalable, fully  managed  network file sy stem (NFS) that  multiple  EC2 instances can access simultaneously across Availability Z ones (AZs) within a VPC.  Here��s why  it��s the  best choice:  1. Shared Access:  EFS  allo ws thousands of  EC2 instances to  read/write to the same file system at the same tim e, unlike  EBS  (Option  C), which can only attach to one instance at a time. 2.  High  Pe r formance:  EFS scales automatically and delivers  low-latency  performance for  media w ork loads. S3 (Options A/B) is object storage and  not optimized for frequent file opera tions like a file system. 3. VPC-Only Access:  EFS file systems  are created  inside a VPC, ensuring data  never  leaves the  private  network. S3 (Options A/B) is a  public service by default (though access can  be  restricted via VPC endpoints). 4.  Multi-AZ  Support:  EFS replicates data across AZs, ensuring  high availability. Options A/B are incorrect  be cause S3 is  not a file system and cannot  be  mounted  like a volume. Option  C  is  inva lid because  EBS volumes cannot  be shared across  multiple  instances.  Reference  Links  -  [Amazon  EFS  Overview](https://aws.amazon.com/efs/)  -  [Comparing  EFS,  EBS,  and  S3] (https://aws.amazon.com/compare/the-difference-between-amazon-efs-amazon-ebs-and- amazon-s3/)


22.A company uses an Amazon  RDS for  MySQL  instance. To  prepare for  end-of-year processing, the company added a  read  replica to accommodate extra  read-only queri es from the company's  reporting tool. The  read  replica CPU  usage was  60% and the primary instance CPU usage was 60%. After end-of-year activities are complete, the  re ad  replica  has a constant 25% CPU  usage. The  primary  instance still  has a constant 6

0% CPU usage. The company wants to  right size the database and still  provide enoug h performance for future growth. Which solution will meet these  requirements?
A�� Delete the  read  replica  Do  not  make  changes to the  primary  instance

B�� Resize the  read  replica to  a smaller  instance size  Do  not  make  changes to the  pri mary instance

C�� Resize the  read  replica to a  larger  instance  size  Resize  the  primary  instance to a s maller instance size
D�� Delete the  read  replica  Resize the  primary  instance  to a  larger  instance

�� ��B



���� �� Correct Answer  B.  Resize the  read  replica to  a smaller instance size.  Do  not  ma ke changes to the  primary  instance.  Detailed  Explanation The  read  replica��s  CPU  usa ge dropped from 60% to 25% after  peak season,  indicating it��s  now  underutilized.  D own sizing the  read  replica  reduces costs without  impacting  performance since  it��s  no	longer handling  heavy traffic. The  primary  instance  remains at 60% CPU  usage, whic h is within a safe operational  range  (AWS  recommends  monitoring thresholds  below  80%).  Keeping the  primary  instance unchanged  preserves its 40%  buffer for future gr owth. Scaling it down (as  in Option C)  risks  performance  issues, while deleting the  re plica (Options A/D)  removes  read-scaling flexibility. Option  B optimizes costs for the c urrent workload while maintaining capacity for gradual growth. If read-heavy workloa ds  return, the  replica can  be  resized  again.  Reference  Links  -  [Amazon  RDS  Read  Repl icas](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_Read repl.html) -

[RDS Instance  Resizing](https://docs.aws.amazon.com/Amazon rds/latest/User guide/Con cepts.Db instance class.html)


23.A company is  migrating its databases to Amazon  RDS for  Postgresql. The  compan y is migrating its applications to Amazon  EC2  instances. The company wants to optim ize costs for long-running workloads. Which solution will meet this requirement  MOS T cost-effectively?

A�� Use On-Demand Instances for the Amazon  RDS for  Postgresql workloads.  Purchas e a  1 year Compute Savings  Plan with the  No  Upfront  option for the  EC2  instances.

B��Purchase  Reserved Instances for a  1 year term with the  No  Upfront  option  for the Amazon  RDS for  Postgresql workloads.  Purchase  a  1 year  EC2 Instance  Savings  Plan with the  No  Upfront option for the  EC2  instances.

C��Purchase  Reserved Instances for a  1 year term with the  Partial  Upfront  option for t he Amazon  RDS for  Postgresql workloads.  Purchase  a  1 year  EC2 Instance  Savings  Pl an with the  Partial  Upfront  option for the  EC2  instances.
D��Purchase  Reserved Instances for a  3 year term with the All  Upfront option for the Amazon  RDS for  Postgresql workloads.  Purchase  a  3 year  EC2 Instance  Savings  Plan with the All  Upfront option for the  EC2  instances.
�� ��D



���� �� Correct Answer:  D  Detailed  Explanation: To optimize costs for  long-running wor kloads, AWS offers significant discounts for committing to  longer terms and  upfront  payments. - Amazon  RDS:  Purchasing  3-year  Reserved Instances with All  Upfront  prov

ides the  highest discount  (up to  ~60% compared to  On-Demand). This is  better than 1-year terms (e.g., Options  B/C) or On-Demand (Option A).  -  EC2: A  3-year  EC2 Inst ance Savings  Plan with All  Upfront also offers the deepest discounts  (up to  ~72% vs On-Demand). This  beats shorter-term  plans  (1-year) or partial/no  upfront options (Opt ions A/B/C). Why not other options? - Option A:  Uses On-Demand for  RDS  (expen siv e for  long-term workloads).  - Options  B/C:  1-year terms and  partial/no  upfront  payme nts  reduce discount  levels.  - Option  D  maximizes  savings  by  locking  in the  longest te rm (3 years) and upfront  payments, which AWS  rewards with the  lowest effective  hou rly rates.  References:  -  [AWS  RDS  Pricing](https://aws.amazon.com/rds/pricing/)  -  [AWS Savings  Plans](https://aws.amazon.com/savings plans/)


24.A company regularly uploads confidential data to Amazon S3 buckets for analysis. The company's security  policies  mandate that the objects  must  be encrypted at  rest. The company  must automatically rotate the encryption  key every year. The company must  be able to track  key  rotation  by  using AWS  Cloud trail. The company also  must minimize costs for the encryption key. Which solution will meet these  requirements?

A�� Use server-side encryption with customer-provided keys (SSE-C)

B�� Use server-side encryption with Amazon S3  managed  keys  (SSE-S3) C�� Use server-side encryption with AWS  KMS  keys  (SSE-KMS)

D�� Use server-side encryption with customer  managed AWS  KMS  keys

�� ��C

���� ��Correct Answer:  D.  Use  server-side encryption with customer  managed AWS  KM
S keys  Detailed  Explanation: The  requirements are:  1. Automatic  annual  key  rotation:  Customer-managed  KMS  keys  (option  D) allow you to enable automatic  rotation and set the interval to 365 days. AWS-managed  KMS  keys  (option C) are  rotated automati cally, but AWS controls the schedule  (typically every 3 years), so they don��t  meet the yearly requirement. 2. Track  rotation via Cloud trail:  KMS  logs  key  rotation events in Cloud trail only for customer-managed  keys (option  D). AWS-managed  keys  (C) are  m anaged internally by AWS, so  rotation events aren��t visible in your Cloud trail  logs.  3. Minimize costs: While AWS-managed keys (C)  have  no monthly fee, they fail the rot ation requirement. Customer-managed keys (D) cost /month  but ensure compliance w ith the  policy. Why the Given Answer  (C) is Incorrect: AWS-managed  KMS  keys  (SSE-K MS) do  not  let you control the  rotation  interval  (they  rotate  every 3 years) and do  n ot log  rotations  in your Cloud trail. This violates two critical  requirements.  Reference  L inks: -  [AWS  KMS  Key  Rotation](https://docs.aws.amazon.com/kms/latest/developer guid e/rotate-keys.html) -  [Cloud trail  Logging for  KMS](https://docs.aws.amazon.com/kms/la test/developer guide/logging-using-cloud trail.html)


25.A company has  migrated several applications to AWS  in the  past  3  months. The c ompany wants to  know the  breakdown of costs for each of these applications. The c ompany wants to  receive a  regular  report that  includes this  information. Which sol uti on will meet these  requirements  MOST cost-effectively?

A�� Use AWS  Budgets to download data for the  past  3  months  into a  .csv file.  Look  u p the desired information.

B�� Load AWS Cost and  Usage  Reports  into  an Amazon  RDS  DB  instance.  Run  SQL  qu eries to get the desired information.
C��Tag all the AWS  resources with a  key for cost and a value of the application's  na me. Activate cost allocation tags.  Use Cost  Explorer to  get the desired information.
D��Tag all the AWS resources with a  key for cost and a value of the application's  na me. Use the AWS  Billing and Cost  Management  console to download  bills for the  past 3 months.  Look up the desired information.
�� ��C



���� �� Correct Answer: C  Detailed  Explanation: To track costs  per application effective l y, tagging  resources with specific  keys (like Application) and enabling cost allocation t ags is crucial. AWS Cost  Explorer can then automatically categorize costs  based on th ese tags,  providing easy-to-generate  reports without extra  infrastructure. - Option A:  AWS  Budgets focuses on  budget alerts,  not  detailed cost  breakdowns  by tag.  Manual CSV analysis is inefficient for regular reporting. - Option  B:  Using  RDS adds  unneces sary costs and complexity. Cost  Explorer already offers SQL-like querying without a da tabase.  - Option  D: The  Billing  console��s  raw  bills  lack  automatic tag-based grouping,	requiring manual effort. - Option C: Tags  +  Cost  Explorer  automate cost tracking  by application, aligning with AWS��s native, cost-effective tools. Reference:  [AWS Cost Al location Tags](https://docs.aws.amazon.com/aws account billing/latest/aboutv2/cost-alloc- tags.html),  [AWS Cost  Explorer](https://aws.amazon.com/aws-cost-management/aws-cos t-explorer/).

26.An ecommerce company is  preparing to deploy a web application on AWS to ens ure continuous service for customers. The architecture includes a web application that	the company  hosts on Amazon  EC2  instances, a  relational database  in Amazon  RDS, and static assets that the company stores in Amazon S3. The company wants to desi gn a  robust and  resilient architecture for the application. Which solution will  meet th ese  requirements?

A�� Deploy Amazon  EC2  instances  in a single Availability Zone.  Deploy  an  RDS  DB  inst ance in the same Availability Zone.  Use Amazon S3 with versioning enabled to store  static assets.

B��Deploy Amazon  EC2  instances  in  an Auto Scaling group across  multiple Availability Zones.  Deploy a  Multi-AZ  RDS  DB  instance.  Use  Amazon  Cloud Front to distribute stat ic assets.

C�� Deploy Amazon  EC2  instances  in a single Availability Zone.  Deploy an  RDS  DB  inst ance in a second Availability Zone for cross-AZ  redundancy. Serve static assets directl y from the  EC2  instances.
D��Use AWS  Lambda functions to serve the web application.  Use Amazon Aurora  Serv erless v2 for the database. Store static assets  in Amazon  Elastic  File  System  (Amazon EFS) One Zone-Infrequent Access (One Zone-IA).
�� ��B



���� �� Correct Answer:  B  Detailed  Explanation: To  build a  robust and  resilient architect ure on AWS, all critical components  must avoid single  points of failure.  Here's why o ption  B works  best:  1.  EC2 with Auto  Scaling  across AZs  - Auto Scaling automatically

launches  EC2  instances in  multiple Availability Zones (AZs). If one AZ fails, instances i n other AZs  keep the web app  running.  - Auto Scaling also adjusts capacity  based o n traffic, improving fault tolerance. 2.  Multi-AZ  RDS  - A  Multi-AZ  RDS  database  has  a primary instance in one AZ and a standby replica in another AZ. If the primary AZ f ails,  RDS automatically fails over to the standby, ensuring database availability.  3. Clo ud Front for Static Assets  - Cloud Front  (a CDN) caches static assets  (images, CSS, etc.)	at edge  locations worldwide. This  reduces  latency and serves content even  if the ori gin (e.g., S3)  has issues.  - S3 alone isn��t enough for  resilience; Cloud Front adds glob al  redundancy. Why other options fail:  - A/C:  Use a single AZ for  EC2 or  RDS  �� sing le  point of failure.  -  D:  EFS  One Zone-IA  stores data  in one AZ ��  not  resilient. Auror a Serverless and  Lambda are serverless  (good for scaling)  but aren��t  inherently AZ-re dundant without  proper configuration.  Reference  Links:  -  [Auto  Scaling  Groups](https:/ /docs.aws.amazon.com/auto scaling/ec2/user guide/auto-scaling-groups.html) -  [Multi-AZ RDS](https://aws.amazon.com/rds/features/multi-az/) -  [Cloud front with S3](https://aws. amazon.com/cloud front/)

27.An ecommerce company runs several internal applications in multiple AWS account
s. The company uses AWS Organizations to manage its AWS accounts. A security app liance in the company's  networking account  must inspect interactions  between applica tions across AWS accounts. Which solution will meet these requirements?

A�� Deploy a  Network  Load  Balancer  (NLB)  in  the networking account to send traffic t
o the security appliance. Configure the application accounts to send traffic to the  NL B by using an interface VPC endpoint  in the application accounts.

B�� Deploy an Application  Load  Balancer  (ALB)  in the application accounts to  send traf fic directly to the security appliance.
C��Deploy a Gateway  Load  Balancer  (GWLB)  in  the  networking account to send traffic to the security appliance. Configure the application accounts to send traffic to the G WLB by using an  interface GWLB endpoint in the application accounts.
D�� Deploy an interface VPC endpoint in the application accounts to send traffic direct ly to the security appliance.
�� ��C



���� �� Correct Answer: C  Detailed  Explanation: The correct solution is to use a Gatewa y  Load  Balancer  (GWLB).  Here's why:  -  GWLB  is  designed for deploying third  -  party  security appliances (like firewalls, intrusion detection systems) in AWS. It  uses the GE NEVE  protocol to  preserve traffic  metadata.  - The  networking account  hosts the GWL B and security appliance, centralizing traffic inspection. - Application accounts use GW LB endpoints (a special VPC endpoint type) to route traffic to the GWLB in the netwo r king account. - This setup allows cross - account traffic inspection without exposing  the security appliance publicly or  managing complex  peering/VPN connections. Other options fail  because: - A  (NLB)  lacks  native cross  - account integration and  metadata preservation. -  B  (ALB) operates at  layer 7  (HTTP only) and can't  handle all traffic typ es. -  D  (Interface VPC)  is for accessing AWS services,  not custom appliances.  Referenc e:  [AWS Gateway  Load  Balancer](https://docs.aws.amazon.com/elastic load balancing/late st/gateway/introduction.html)  [GWLB  Endpoints](https://docs.aws.amazon.com/vpc/latest /private link/gateway-load-balancer-endpoints.html)

28.A company runs its  production workload on an Amazon Aurora  MySQL  DB  cluster that includes six Aurora  Replicas. The company wants  near-real-time  reporting queries	from one of its departments to  be automatically distributed across three of the Auro ra  Replicas. Those three  replicas  have a different compute and  memory specification f rom the  rest of t he  DB  cluster. Which solution  meets these  requirements?
A��Create and use a custom endpoint for the workload

B��Create a three-node cluster clone and use the reader endpoint C�� Use any of the instance endpoints for the selected three nodes

D�� Use the reader endpoint to automatically distribute the read-only workload

�� ��A



���� �� Correct Answer A  Detailed  Explanation The correct solution is to create a custo m endpoint.  Here's why:  - Custom endpoints  in Amazon Aurora allow you to group s pecific Aurora  Replicas  (in this case, the three  replicas with  unique compute/memory  specs) and define  load-balancing rules. This ensures that the department��s  reporting queries are automatically distributed only to those three replicas. - The  reader endpoi nt (Option  D) would  distribute queries across all six  replicas, which doesn��t meet the requirement to isolate traffic to three specific  replicas. - Instance endpoints (Option C) are for direct access to a single replica, not load balancing.  - A cluster clone (Optio n  B) creates a copy of the database, which  is unnecessary  here since the  requirement	is for  near-real-time queries on the same dataset. Custom endpoints are designed fo r this exact scenario: routing specific workloads to a subset of replicas with specific c

on figurations.  Reference  Link  [Amazon Aurora  Connection  Management](https://docs.a ws.amazon.com/Amazon rds/latest/Aurora user guide/Aurora.Overview.Endpoints.html)


29.A company runs a  Node js function on a server  in  its on-premises data center. Th e data center stores data  in a  Postgresql database. The company stores the cre denti als in a connection string in an environment variable on the server. The company wa nts to  migrate its application to AWS and to  replace the  Node.js application  server w ith AWS  Lambda. The company also wants to  migrate to Amazon  RDS for  Postgres q L and to ensure that the database credentials are securely  managed. Which solution will meet these  requirements with the  LEAST operational overhead?
A��Store the database credentials as a  parameter in AWS Systems  Manager  Parameter Store Configure  Parameter Store to automatically  rotate the secrets every 30 days.  U pdate the  Lambda function to  retrieve the credentials from the  parameter.
B��Store the database credentials as a secret in AWS Secrets  Manager. Configure  Secr ets  Manager to automatically  rotate the credentials every  30 days.  Update the  Lambd a function to  retrieve the credentials from the secret.
C��Store the database credentials as an encrypted  Lambda environment variable. Writ e a custom  Lambda function to  rotate the credentials.  Schedule the  Lambda function to  run every  30 days.
D��Store the database credentials as a  key  in AWS  Key  Management  Service  (AWS  K MS). Configure automatic rotation for the key.  Update the  Lambda function to  retnev e the credentials from the  KMS  key.
�� ��B

���� ��Correct Answer  B  Detailed  Explanation The  best  solution is  B  because AWS  Secr ets  Manager  is specifically designed for securely storing and  managing secrets  like da tabase credentials. It offers automatic  rotation of secrets, which  reduces operational o verhead compared to  manual  rotation.  Here's why:  - Secrets  Manager  natively  support s automatic  rotation for  RDS databases  (including  Postgresql). You can enable this w it h a few clicks, and AWS  handles the  rotation  process,  including  updating the creden tials in  both the database and the secret.  -  Parameter  Store  (Option A) can  store secr ets  but  lacks  built  -  in automatic  rotation for database credentials. Implementing  rota tion would  require additional custom code or workflows,  increasing operational compl exity. -  Lambda environment variables  (Option C) are  not  ideal for secrets,  even  if en crypted. Custom rotation  logic adds  maintenance work and  potential  security  risks.  -  KMS  (Option  D)  is for encryption  keys,  not  credential storage.  Rotating  a  KMS  key  do esn��t  rotate the actual database credentials.  Reference  Links  -  [AWS  Secrets  Manager] (https://aws.amazon.com/secrets-manager/) -  [Rotating Amazon  RDS credentials with S ecrets  Manager](https://docs.aws.amazon.com/secrets manager/latest/user guide/rotating- secrets-rds.html)

30.A company wants to  replicate existing and ongoing data changes from an on-pre mises Oracle database to Amazon  RDS for Oracle. The amount of data to  replicate va ries throughout each day. The company wants to use AWS  Database  Migration  Servic e (AWS  DMS) for data  replication. The  solution  must allocate only the capacity that t he replication instance  requires. Which solution will  meet these requirements?

A��Configure the AWS  DMS  replication  instance with a  Multi-AZ  deployment to  provis ion instances across  multiple Availability Zones.

B��Create an AWS  DMS  Serverless  replication task to analyze and  replicate the data w hile provisioning the  required capacity.
C��Use Amazon  EC2 Auto  Scaling to scale the size of the AWS  DMS  replication  instan ce up or down  based on the amount of data to replicate.
D��Provision AWS  DMS  replication  capacity  by  using Amazon  Elastic  Container  Service (Amazon  ECS) with an AWS  Fargate  launch  type to analyze and  replicate the data wh ile provisioning the required capacity.
�� ��B



���� ��Correct Answer:  B  Detailed  Explanation: The  best  solution  is to  use AWS  DMS  S erver less (Option  B). AWS  DMS  Serverless automatically scales the  replication capacity up or down based on the workload, which is perfect for varying data volumes. You
only  pay for the  resources used during  replication, avoiding over-provisioning.  - Optio  n A  (Multi-AZ) focuses on high availability,  not scaling,  by duplicating  instances acros    s AZs. This doesn��t adjust capacity dynamically. - Option C (EC2 Auto Scaling) doesn�� t work  because  DMS  replication  instances  aren��t  EC2  instances  and  can��t  be  resized    automatically. - Option  D  (ECS/Fargate)  is  unrelated to  DMS  replication tasks, which  u   se dedicated replication instances,  not containers.  Reference  Links:  [AWS  DMS  Server le   ss](https://docs.aws.amazon.com/dms/latest/user guide/CHAP_Replication instance.Server le   ss.html)  [AWS  DMS  Replication Instances](https://docs.aws.amazon.com/dms/latest/user   guide/CHAP_Replication instance.html)

31.A company has a  multi-tier web application. The application's internal service com ponents are deployed on Amazon  EC2  instances. The internal service components  nee
d to access third-party software as a service  (SaaS) APIs that are  hosted on AWS. The	company needs to  provide secure and  private connectivity from the application's  int ernal services to the third-party SaaS application. The company needs to ensure that  there is  minimal public internet exposure. Which solution will meet these requirement s?

A��Implement an AWS Site-to-Site VPN to establish a secure connection with the thir d-party SaaS provider.

B�� Deploy AWS Transit Gateway to  manage and  route traffic  between the application' s VPC and the third-party SaaS provider.

C��Configure AWS  Private link to allow only outbound traffic from the VPC without en abling the third-party SaaS provider to establish.
D��Use AWS  Private link to create a  private  connection  between the application's VPC and the third-party SaaS provider.
�� ��D



���� �� Correct Answer:  D  Detailed  Explanation: AWS  Private link  is the  best solution  he re. It allows the company's VPC to connect  privately to the third-party SaaS APIs  host ed on AWS without exposing traffic to the  public  internet.  Here's  how  it works:  1.  Pri vate link creates an internal AWS  network  path  between the company's VPC and the  SaaS  provider's service. 2. The  EC2  instances can access the  SaaS APIs through an  int erface endpoint (ENI)  in their VPC. 3.  No  public IPs,  internet  gateways, or  NAT device

s are  required. 4. The SaaS  provider can't  initiate connections  back to the company's VPC,  maintaining security. Other options explained: - A (VPN):  Requires  internet-facing	connectivity and manual configuration with the third  party.  -  B  (Transit Gateway):  Ma nages  routing  between  networks  but doesn't inherently  provide  private connectivity.  -
 C: While  Private link does use only outbound traffic, option  D  more accurately descri bes the complete  private connection setup.  Reference  link:  https://aws.amazon.com/pri vate link/



32.A solutions architect  needs to connect a company's corporate network to its VPC t
o allow on-premises access to its AWS  resources. The solution  must  provide encrypt io n of all traffic  between the corporate  network and the VPC at the  network  layer and the session  layer. The solution also  must  provide security controls to  prevent  un restric ted access  between AWS and the on-premises systems. Which solution meets these  r equirements?
A��Configure AWS  Direct Connect to connect to the VPC. Configure the VPC  route ta bles to allow and deny traffic  between AWS and on  premises as  required.

B��Create an IAM  policy to allow access to the AWS  Management Console  only from a defined set of corporate IP addresses.  Restrict  user access  based  on job  responsibili ty  by  using an IAM  policy  and  roles.

C��Configure AWS Site-to-Site VPN to connect to the Vp configure  route table entries to direct traffic from on  premises to the Vp configure  instance security groups and  ne twork ACLs to allow only  required traffic from on  premises.

D��Configure AWS Transit Gateway to connect to the VPC. Configure  route table entri es to direct traffic from on  premises to the VPC. Configure  instance security groups a nd network ACLs to allow only required traffic from on  premises.
�� ��C



���� �� Correct Answer: C  Detailed  Explanation: The question requires a secure connect i on between on -  premises and AWS VPC with encryption at  both  network and sessio n  layers,  plus  restricted access  controls.  - AWS Site  - to  -  Site VPN  uses IPsec  (netwo rk layer encryption) and can enforce TLS/HTTPS (session layer encryption via applicati ons). -  Route tables ensure traffic flows correctly  between on  -  premises  and VPC. -  Security Groups and  NACLs act as firewalls to  restrict traffic to  only  necessary  ports/I Ps. Options A (Direct Connect)  lacks  built  -  in encryption. Option  B  (IAM  policies)  doe sn��t  handle  network connectivity. Option  D  (Transit  Gateway)  is for complex  multi  - VPC setups  but doesn��t inherently add encryption  beyond what VPN  provides.  Refere nce:  [AWS Site  - to  - Site VPN](https://aws.amazon.com/vpn/)  [Security Groups ��  NA CLs](https://docs.aws.amazon.com/vpc/latest/user guide/vpc - security - groups.html)


33.A company has a custom application with embedded credentials that  retrieves info rmation from a database in an Amazon  RDS for  MySQL  DB  cluster. The  company nee ds to  make the application  more secure with  minimal  programming effort. The comp any  has created credentials on t he  RDS for  MySQL  database for the application  user. Which solution will meet these requirements?

A��Store the credentials in AWS  Key  Management  Service  (AWS  KMS).  Create  keys  in AWS  KMS. Configure the application to  load the database credentials from AWS  KMS.	Enable automatic  key  rotation
B��Store the credentials in encrypted local storage. Configure the application to  load t he database credentials from the local storage. Set up a credentials  rotation schedule by creating a cron job.
C��Store the credentials in AWS Secrets  Manager. Configure the application to  load th e database credentials from Secrets  Manager. Set up a credentials  rotation schedule  b y creating an AWS  Lambda function for Secrets  Manager.
D��Store the credentials in AWS Systems  Manager  Parameter  Store. Configure the app lication to  load the database credentials from  Parameter  Store. Set up a credentials  r otation schedule  in t he  RDS for  MySQL  database  by  using  Parameter  Store.
�� ��C



���� �� Correct Answer: C  Detailed  Explanation: The  best solution is C  because AWS Se crets  Manager  is specifically designed to securely store and  manage sensitive  in format ion  like database credentials.  Here's why  it fits  perfectly:  1.  Secure Storage: Secrets  M anager encrypts the credentials at  rest,  keeping them safe. 2. Automatic  Rotation:  Sec rets  Manager can automatically  rotate t he  RDS database credentials on a schedule. Y ou only  need to set  up a  Lambda function  (which  Secrets  Manager  helps automate), minimizing coding effort. 3. Simplified Integration: The application only needs to call  Secrets  Manager to  retrieve the  latest credentials, avoiding  hardcoded or  manually  rot ated  passwords. Other options are  less  ideal:  - A  (KMS):  KMS  manages  encryption  key

s  but doesn��t store or  rotate credentials.  -  B  (Local  storage):  Storing credentials  local ly, even encrypted, is  risky and  requires  manual  rotation.  -  D  (Parameter  Store): While Parameter Store can store secrets, it  lacks  built  - in  rotation for  RDS credentials,  req uiring more custom work.  Reference  Link:  [AWS  Secrets  Manager  Documentation](http s://docs.aws.amazon.com/secrets manager/latest/user guide/intro.html)


34.A company wants to  move its application to a serverless solution. The serverless s ol ution  needs to analyze existing data and  new data  by  using SQL. The company stor es the data  in an Amazon S3  bucket. The data  must  be encrypted  at  rest and  replica ted to a different AWS  Region. Which solution will  meet these  requirements with the	LEAST operational overhead?

A��Create a  new S3  bucket that  uses server-side encryption with AWS  KMS  multi-Regi on keys (SSE-KMS). Configure Cross-Region  Replication  (CRR).  Load the  data  into the new S3 bucket.  Use Amazon Athena to query the data.

B��Create a  new S3  bucket that  uses server-side encryption with Amazon S3  managed keys (SSE-S3). Configure Cross-Region  Replication  (CRR).  Load the  data  into the  new S3  bucket.  Use Amazon  RDS to  query the data.

C��Configure Cross-Region  Replication  (CRR) on the existing S3  bucket.  Use server-sid e encryption with Amazon S3 managed  keys  (SSE-S3).  Use Amazon Athena to query t he data.
D��Configure S3 Cross-Region  Replication  (CRR) on the existing S3  bucket.  Use  server -side encryption with AWS  KMS  multi-Region  keys  (SSE-KMS).  Use Amazon  RDS to  qu ery the data.

�� ��C



���� �� Correct Answer: A  Explanation: The question requires a serverless solution for a nal y zing data with SQL, encryption at  rest, cross-region  replication, and  minimal opera t ional overhead.  Let��s  break down the options:  - Option C  (Incorrect): Configuring Cr oss-Region  Replication (CRR) on the existing S3  bucket with SSE-S3 encryption and  u sing Athena  might seem viable.  However, SSE-S3 encryption cannot  be  retroactively a pplied to existing objects  in the bucket. If the existing data  isn��t already encrypted with SSE-S3, enabling encryption afterward would require re-uploading all objects (hig h operational effort). This violates the  least operational overhead  requirement.  - Optio n A  (Correct): Creating a  new S3  bucket with SSE-KMS  multi-Region  keys ensures enc ryption at  rest and seamless cross-region  replication.  Multi-Region  KMS  keys  allow en crypted data to be  replicated to another  region without  re-encrypting objects  (simpler	than managing separate  keys  per  region). Athena, a  serverless SQL query service for S3, fits the analysis  requirement. While  KMS adds  some management,  multi-Region  ke ys  reduce overhead compared to  re-uploading data  (as  in Option C). Why Other Opti ons  Fail:  - Option  B/D: Amazon  RDS  is  not  designed to  query  S3 data directly  (it��s a		relational database service). - Option C/D (existing  bucket):  Retroactively encrypting e xisting data  in S3 is operationally intensive.  Reference  Links:  -  [S3  Cross-Region  Replic ation](https://docs.aws.amazon.com/AmazonS3/latest/user guide/replication.html) -  [AWS KMS  Multi-Region  Keys](https://docs.aws.amazon.com/kms/latest/developer guide/multi -region-keys-overview.html) -  [Amazon Athena](https://aws.amazon.com/athena/)

35.A company has a web application that  has thousands of users. The application use s 8-10 user-uploaded images to generate AI images.  Users can download the generat ed AI  images once every 6  hours. The company also  has a  premium  user  option that gives users the ability to download the generated AI images anytime. The company uses the user-uploaded images to  run AI  model training twice a year. The company  n eeds a storage solution to store the images. Which storage solution meets these  req ui rements  MOST cost-effectively?

A��Move uploaded images to Amazon S3 Glacier  Deep Archive.  Move  premium  user-g ene rated AI images to S3 Standard.  Move  non-premium  user-generated AI  images to S3 Standard-Infrequent Access (S3 Standard-IA).

B��Move  uploaded images to Amazon S3 Glacier  Deep Archive  Move  all generated AI images to S3 Glacier  Flexible  Retrieval.

C��Move uploaded images to Amazon S3 One Zone-Infrequent Access  (S3 One Zone-I A).  Move  premium  user-generated AI  images to S3  Standard.  Move  non-premium  use r-generated AI images to S3 Standard-Infrequent Access (S3 Standard-IA).
D��Move uploaded images to Amazon S3 One Zone-Infrequent Access  (S3 One Zone-I A).  Move all generated AI  images to  S3 Glacier  Flexible  Retrieval.
�� ��A



���� �� Correct Answer: A  Detailed  Explanation: The best choice is A  because it opti miz es costs  based on access  patterns:  -  User-uploaded  images  are stored  in S3 Glacier  D eep Archive, the cheapest option for data accessed only twice a year.  Retrieval delays	(hours) are acceptable for  biannual training. -  Premium  users�� AI  images  use S3 Sta

ndard for instant, frequent access.  -  Non-premium  users�� AI images use S3 Standard- IA, which is cheaper for infrequent access  (every 6  hours fits IA��s  monthly access thr eshold). Other options fail due to: -  B/D: Glacier  Flexible  Retrieval  adds  retrieval delay s, violating premium users�� instant download  requirement.  -  C: S3 One Zone-IA  risks data  loss  (single AZ) for critical user-uploaded images, which  is unnecessary when Gla cier  Deep Archive offers  lower cost  and sufficient  retrieval times.  Reference  Links:  -  [A mazon S3 Storage Classes](https://aws.amazon.com/s3/storage-classes/) -  [Choosing th e  Right Storage Class](https://docs.aws.amazon.com/AmazonS3/latest/user guide/storage -class-intro.html)


36.A company is developing  machine  learning  (ML)  models on AWS. The company is developing the  ML  models as  independent  micro services. The  micro services fetch appr oxi mately  1 GB of model data from Amazon S3 at startup and  load the data  into  me mory.  Users access the  ML  models through an  asynchronous API.  Users can  send a  re quest or a  batch of requests. The company  provides the  ML  models to  hundreds  of  users. The  usage  patterns for the  models are  irregular.  Some  models are  not  used for days or weeks. Other models receive batches of thousands of requests at a time. W hich solution will  meet these  requirements?

A��Direct the  requests from the API to a  Network  Load  Balancer  (NLB).  Deploy  the  ML models as AWS  Lambda functions that the  NLB will  invoke.  Use auto scaling to scal e the  Lambda functions  based on the traffic that the  NLB  receives.

B��Direct the  requests from the API to an Application  Load  Balancer  (ALB).  Deploy  the
ML  models as Amazon  Elastic Container  Service (Amazon  ECS)  services that the ALB

will invoke.  Use auto scaling to scale the  ECS cluster  instances  based on the traffic th at the ALB  receives.
C��Direct the requests from the API  into an Amazon Simple Queue Service  (Amazon S QS) queue.  Deploy the  ML  models  as AWS  Lambda functions that  SQS events will  i nv oke.  Use auto scaling to increase the  number of vCPUs for the  Lambda functions  bas ed on the size of the SQS queue.
D�� Direct the  requests from the API into an Amazon Simple Queue Service  (Amazon  SQS) queue.  Deploy the  ML  models  as Amazon  Elastic Container Service  (Amazon  ECS)	services that  read from the queue.  Use auto scaling for Amazon  ECS to  scale  both t he cluster capacity and  number of the services  based on the size of the SQS queue.
�� ��D



���� �� Correct Answer  D  Explanation The correct solution uses Amazon SQS (to  handl e asynchronous requests and  buffer  irregular traffic)  + Amazon  ECS  (to  run  long-lived micro services that  load  1 GB  model data at startup)  + Auto  Scaling  (to  scale  ECS ta sks  based on SQS queue size). Why this works:  1. Asynchronous API:  SQS queues dec ouple the API from the  ML services, allowing  requests to wait  in the queue  until  reso urces are available. 2.  1 GB  model data at  startup:  ECS tasks  (containers)  can  persist  i n memory after  loading the  model, avoiding  repeated cold starts  (unlike AWS  Lambd a, which would  reload data on every invocation). 3. Irregular usage: Auto Scaling adju sts the  number of  ECS tasks  based on the SQS queue size.  For  unused  models, tasks	scale to zero, saving costs.  For sudden  batches, tasks  scale up to  process thousands of requests. Why other options fail: - A/B:  Lambda  (Option A) or ALB  +  ECS  (Option

B)  require  real-time scaling and are  less efficient for  irregular workloads.  Lambda wou ld  reload  1 GB data  on every cold start.  - C:  Lambda with  SQS  (Option C) cannot  ha ndle  1 GB  memory efficiently and  lacks  persistent  runtime for  batch  processing.  Refer ence  Links  -  [AWS  SQS  +  ECS  Auto Scaling](https://docs.aws.amazon.com/auto scaling/ ec2/user guide/as-using-sqs-queue.html) -  [ECS Service Auto Scaling](https://docs.aws.a mazon.com/Amazon ecs/latest/developer guide/service-auto-scaling.html)


37.A company runs a web application on Amazon  EC2  instances  in an Auto Scaling g roup behind an Application  Load  Balancer  (ALB). The application  stores data  in an A mazon Aurora  MySQL  DB  cluster. The company  needs to create a disaster  recovery
(DR) solution. The acceptable  recovery time for the  DR  solution  is up to  30  minutes. The  DR solution does  not  need to  support customer usage when the primary infra str ucture is  healthy. Which solution will meet these  requirements?
A�� Deploy the  DR  infrastructure  in a second AWS  Region with  an ALB and an Auto S caling group. Set the desired capacity and  maximum capacity of the Auto Scaling gro up to a  minimum value. Convert the Aurora  MySQL  DB  cluster to an Aurora global d atabase. Configure Amazon  Route  53 for an active-passive failover with ALB endpoint s.
B��Deploy the  DR  infrastructure  in a second AWS  Region with  an Al update the Auto Scaling group to  include  EC2  instances from the second  Region.  Use Amazon  Route  5 3 to configure active-active failover. Convert the Aurora  MySQL  DB  cluster to an Auro ra global database.

C��Back  up the Aurora  MySQL  DB  cluster  data  by  using AWS  Backup.  Deploy  the  DR infrastructure in a second AWS  Region with an ALB.  Update the Auto  Scaling group t o include  EC2  instances from the second  Region.  Use Amazon  Route  53  to configure active-active failover. Create an Aurora  MySQL  DB  cluster in the second  Region  Resto re the data from the  backup.
D��Back  up the infrastructure configuration  by  using AWS  Backup.  Use the  backup  to create the  required  infrastructure in a second AWS  Region.  Set the Auto Scaling grou p desired capacity to zero.  Use Amazon  Route  53 to configure active-passive failover. Convert the Aurora  MySQL  DB cluster to an Aurora global database.
�� ��A



���� ��Correct Answer: A  Detailed  Explanation: The best solution here is option A  beca use it uses Aurora Global  Database for fast cross-region database  replication and ena bles quick failover.  Here's why:  1. Aurora Global  Database: Converts the  existing data base into a global database, which continuously  replicates data to the secondary AW S  Region with typical  latency  under  1 second.  During a  disaster, the secondary datab ase can  be  promoted to  primary  in  minutes. 2.  Cost-Efficient  DR  Setup: The Auto Scal ing group in the  DR  region  has  minimum  capacity  (likely 0  instances), so you only  pa y for resources when activated. This matches the  requirement to  not support custome r traffic during  normal operations. 3.  Fast Activation:  Route  53's active-passive failover	can  redirect traffic to the  DR  region's ALB within  minutes when  health  checks detect an outage. Combined with Auto Scaling (which can  launch  EC2  instances  in  ~2  -  5 minutes), this  meets the 30-minute  RTO. Why other options are  less  ideal:  -  B: Active -active setup contradicts the  no customer usage during normal operations  require men

t. - C:  Restoring from  backups would take too  long  (Aurora cluster creation  +  data  re store typically exceeds 30 minutes).  -  D: Infrastructure  backups via AWS  Backup  aren't	real-time, risking data  loss. Setting ASG capacity to zero still  requires full  instance  la unch during failure.  Reference  Links:  -  [Aurora  Global  Database](https://docs.aws.amaz on.com/Amazon rds/latest/Aurora user guide/aurora-global-database.html) -  [Route  53  F ailover](https://docs.aws.amazon.com/Route53/latest/Developer guide/dns-failover.html) -		[Auto Scaling Groups](https://docs.aws.amazon.com/auto scaling/ec2/user guide/auto-sc aling-groups.html)


38.A company is  migrating its data  processing application to the AWS Cloud. The ap plication processes several short-lived batch jobs that cannot  be disrupted.  Data  is ge nerated after each  batch job is completed. The data  is accessed for 30 days and  retai ned for 2 years. The company wants to keep the cost of running the application in t he AWS Cloud as  low as  possible. Which solution will  meet these  requirements?

A�� Migrate the data  processing application to Amazon  EC2  Spot Instances. Store the data  in Amazon S3 Standard.  Move the data to Amazon  S3 Glacier Instant.  Retrieval after 30 days. Set an expiration to delete the data after 2 years.

B��Migrate the data  processing application to Amazon  EC2 On-Demand Instances.  Stor e the data  in Amazon S3 Glacier Instant  Retrieval.  Move the  data to  S3 Glacier  Deep Archive after 30 days. Set an expiration to delete the data after 2 years.

C�� Deploy Amazon  EC2 Spot Instances to  run the  batch jobs.  Store the data  in Amaz on S3 Standard.  Move the  data to Amazon S3 Glacier  Flexible  Retrieval  after  30 days. Set an expiration to delete the data after 2 years.

D�� Deploy Amazon  EC2 On-Demand Instances to  run the  batch jobs.  Store the data  i n Amazon S3 Standard.  Move the data to Amazon  S3 Glacier  Deep Archive after  30 days. Set an expiration to delete the data after 2 years.
�� ��D



���� ��Correct Answer:  D  Detailed  Explanation: The question involves optimizing costs f or a batch job application with specific requirements: no job disruption, data accesse d for 30 days,  retained for 2 years, and  minimal cost.  1.  Compute  (EC2):  - Spot Insta nces (Options A/C) are cheaper  but can  be  interrupted. Since the jobs cannot  be disr upted, On-Demand Instances  (Options  B/D) are  mandatory  here. This eliminates A an d C. - While Spot is cheaper, reliability takes  priority for  uninterrupted  processing. 2. Storage (S3):  - Initial Storage  (First  30  Days):  -  Data  is  actively  accessed, so S3 Standa rd (Options A/C/D) is appropriate. - Glacier Instant  Retrieval  (Option  B)  is  designed fo r archives  needing  millisecond access  but  has  higher storage  costs than  S3 Standard. Using it immediately  (as  i n  B) would  be  unnecessarily  expensive for active data.  -  Pos t-30  Days  (Archive):  - After  30 days, data  is  rarely/never accessed.  S3 Glacier  Deep Ar chive (D)  is the cheapest storage class for  long-term  retention (vs. Glacier Instant/Flex ible in A/C). - Glacier  Deep Archive  is  ~75%  cheaper than S3 Standard and optimized	for data accessed once a year or  less, aligning with the 2-year  retention. 3.  Lifecycle Policy: - All options correctly set a 2-year expiration, but only  D combines the  right compute (On-Demand) and storage  (S3 Standard �� Glacier  Deep Archive) choices for both reliability and cost-efficiency.  Reference  Links:  -  [Amazon  EC2  Pricing](https://aw s.amazon.com/ec2/pricing/) -  [Amazon  S3 Storage Classes](https://aws.amazon.com/s3/

storage-classes/) -  [S3  Lifecycle  Policies](https://docs.aws.amazon.com/AmazonS3/latest/ user guide/object-lifecycle-mgmt.html)


39.A global ecommerce company runs its critical workloads on AWS. The workloads u se an Amazon  RDS for  Postgresql  DB  instance that  is  configured for a  Multi-AZ  depl oyment. Customers have  reported application timeouts when the company undergoes database failover s. The company needs a resilient solution to reduce failover time. W hich solution will  meet these  requirements?
A��Create an Amazon  RDS  Proxy. Assign the  proxy to the  DB  instance.

B��Create a  read  replica for the  DB  instance.  Move  the  read  traffic to the  read  replica. C�� Enable  Performance Insights.  Monitor the CPU  load to  identify the timeouts.

D��Take  regular automatic snapshots. Copy the automatic snapshots to  multiple AWS Regions.
�� ��A



���� ��Correct Answer: A  Detailed  Explanation: When a database failover  happens in a Multi-AZ  RDS  setup, applications  need to  reconnect to the  new  primary database  inst ance. This  reconnection  process can cause delays and application timeouts. Amazon  R DS  Proxy solves this  by acting as a  middleman  between your  application and databas e. It  maintains a  pool of database connections and automatically  redirects traffic to th e standby instance during failover. This  means applications don't  have to  manually  re connect -  like  having a  backup  driver  ready to take the wheel  immediately  if the  mai n driver gets sick. Options  B/C/D address different aspects  (read scaling/monitoring/b

ackups)  but don't  reduce failover time  like  RDS  Proxy  does.  Reference  Link:  https://aw s.amazon.com/rds/proxy/


40.A company has  multiple Amazon  RDS  DB  instances that  run  in  a development AW S account. All the instances  have tags to identify them as development resources. Th e company needs the development  DB  instances to  run  on a schedule only during  b usi ness  hours. Which solution will meet these  requirements with the  LEAST operation a
l overhead?

A��Create an Amazon Cloud watch alarm to identify  RDS  instances that  need to  be  st opped. Create an AWS  Lambda function to  start and stop t he  RDS  instances.

B��Create an AWS Trusted Advisor  report to  identify  RDS  instances to  be  started  and stopped. Create an AWS  Lambda function to  start and stop t he  RDS  instances.

C��Create AWS Systems  Manager  State  Manager  associations to start and stop t he  RD S instances.
D��Create an Amazon  Event bridge  rule that  invokes AWS  Lambda functions to  start a nd stop t he  RDS  instances.
�� ��D



���� �� Correct Answer:  D  Detailed  Explanation: To schedule Amazon  RDS  instances to start/stop during  business  hours with  minimal effort, Amazon  Event bridge  is the  best choice.  Event bridge  lets you create  rules that trigger actions  (like starting/stopping  RD S) on a schedule (e.g., cron jobs). Combined with AWS  Lambda,  it can automatically  i dentify  RDS  instances  using tags and  manage their state.  - Why  not other options:  -

A/B: Cloud watch alarms/Trusted Advisor aren��t designed for scheduling and add extr a steps.  - C: Systems  Manager State  Manager  focuses on  maintaining instance config u rations, not scheduling. - Why  D:  Event bridge  +  Lambda  is  serverless,  fully automate d, and  requires  no  infrastructure  management.  Reference  Links:  -  [Scheduling  RDS  wit h  Event bridge](https://docs.aws.amazon.com/event bridge/latest/user guide/eb-rds-schedu le.html) -  [Lambda for  RDS start/stop](https://docs.aws.amazon.com/lambda/latest/dg/s ervices-rds.html)


41.A consumer survey company  has gathered data for several years from a specific g eo graphic  region. The company stores this data  in an Amazon S3  bucket in an AWS Region. The company has started to share this data with a  marketing firm  in a  new geographic  region. The company has granted the firm's AWS account access to the S 3 bucket. The company wants to  minimize the data transfer costs when the  market in g firm requests data from the S3 bucket. Which solution will  meet these  requirement s?

A��Configure the  Requester  Pays feature  on the company��s S3  bucket.

B��Configure S3 Cross-Region  Replication  (CRR) from the company��s S3  bucket to on e of the marketing firm��s S3  buckets.

C��Configure AWS  Resource Access  Manager to  share the S3  bucket with the  marketi ng firm AWS account.
D��Configure the company��s S3 bucket to use S3 Intelligent-Tiering Sync the S3 buc ket to one of the  marketing firm��s S3  buckets.
�� ��A

���� ��Correct Answer A. Configure the  Requester  Pays feature on the company��s S3 bucket.  Detailed  Explanation When data  is downloaded from an Amazon  S3  bucket, t he data transfer costs are typically charged to the bucket owner by default.  However, 	if the company enables the  Requester  Pays feature  on their S3  bucket, the market in g firm (requester) will bear the data transfer costs when accessing the  bucket. This di rectly  minimizes the company��s expenses  because the  marketing firm  pays for cross  -  region data transfers  instead of the company. Other options  like S3 Cross  -  Region Replication (CRR)  (Option  B) or syncing via S3 Intelligent  - Tiering  (Option  D) would  r equire the company to  pay for  replication and  storage  in the  marketing firm��s  region, which increases costs. Sharing the bucket via AWS  Resource Access  Manager  (Option C) does not address data transfer costs��it only simplifies permissions. Thus,  Request er  Pays  is the  simplest and  most cost  - effective  solution for the company��s goal.  Re ference  Link  [Amazon  S3  Requester  Pays](https://docs.aws.amazon.com/AmazonS3/lates t/user guide/Requester pays buckets.html)


42.A company uses AWS to  host its  public ecommerce website. The website uses an AWS Global Accelerator accelerator for traffic from the internet. The Global Accelera to r accelerator forwards the traffic to an Application  Load  Balancer  (ALB) that  is the ent ry  point for an Auto Scaling group. The company  recently identified a  DDoS attack o n the website. The company needs a solution to  mitigate future attacks. Which sol uti on will meet these  requirements with the  LEAST  implementation effort?

A��Configure an AWS WAF web ACL for the Global Accelerator accelerator to  block tr affic by using rate-based  rules

B��Configure an AWS  Lambda function to  read the ALB  metrics to  block  attacks  by  u pdating a VPC  network ACL
C��Configure an AWS WAF web ACL on the ALB to  block traffic  by  using  rate-based  r u les
D��Configure an Amazon Cloud Front distribution in front of the Global Accelerator ac celera tor
�� ��C



���� ��Correct Answer: C  Detailed  Explanation: The  scenario involves an e-commerce w ebsite using AWS Global Accelerator to  route traffic to an Application  Load  Balancer  (ALB), which is  part of an Auto Scaling group. After a  DDoS attack, the company  nee ds a simple solution to  block future attacks. Option Analysis:  - Option A: AWS WAF c annot be directly attached to AWS Global Accelerator. WAF operates at the applicatio n  layer (Layer 7) and integrates with services  like ALB, Cloud Front, or API Gateway. Gl o bal Accelerator operates at the network/transport  layer (Layers  3/4), so this option is		invalid. - Option  B:  Using  Lambda  to  monitor ALB  metrics  and update VPC  network ACLs would  require custom code, continuous  monitoring, and  managing IP  lists.  Netw ork ACLs are stateless and inefficient for dynamic  DDoS  mitigation,  making this sol uti on complex and  high - effort.  - Option C: AWS WAF can  be directly attached to the ALB.  Rate  -  based  rules  in WAF  automatically block IP addresses exceeding a  request	threshold, effectively mitigating volumetric  DDoS attacks. This  requires  minimal setup		(e.g., creating a web ACL and associating it with the ALB) and leverages existing infr a structure, making it the  least effort.  - Option  D: Adding Cloud Front  (with AWS Shiel

d Standard for  DDoS  protection) would  require  reconfiguring  DNS  settings,  updating t he origin to  point to Global Accelerator, and ensuring compatibility. While effective, t his involves  more steps than simply enabling WAF on t he ALB. Why Option C  is  Best:
 -  Minimal  Effort:  Enabling AWS WAF  on the ALB  requires  no  architectural changes.  R ate  -  based  rules are easy to configure and  manage.  - Targeted  Protection: WAF  rate	-  based  rules automatically  block excessive  requests, addressing the  DDoS  attack at t he application layer without disrupting legitimate traffic. - Cost -  Effective: AWS WAF  is a  managed service, eliminating the  need for custom code or infrastructure changes. Reference  Links: -  [AWS WAF  Rate  -  Based  Rules](https://docs.aws.amazon.com/waf/la test/developer guide/waf -  rate  -  based  -  rules.html)  -  [AWS WAF Integration with ALB] (https://docs.aws.amazon.com/waf/latest/developer guide/web - acl -  load -  balancer.ht ml)

43.A company uses an Amazon  Dynamo db table to store data that the company  rece ives from devices. The  Dynamo db table supports a customer-facing website to display	recent activity on customer devices. The company configured the table with pro visio ned throughput for writes and  reads. The company wants to calculate  performance  m etrics for customer device data on a daily basis. The solution  must  have  mini mal effe ct on the table's  provisioned read and write capacity. Which solution will meet these  requirements?

A��Use an Amazon Athena SQL query with the Amazon Athena  Dynamo db connector to calculate  performance  metrics on a  recurring schedule.

B��Use an AWS Glue job with the AWS Glue  Dynamo db export connector to calculate performance metrics on a  recurring schedule.

C�� Use an Amazon  Redshift COPY command to calculate  performance  metrics on  a  re curring schedule.
D��Use an Amazon  EMR job with an Apache  Hive external table to  calculate perform a nce  metrics on a  recurring schedule.
�� ��B



���� �� Correct Answer  B  Detailed  Explanation The  best  solution  is  B  because AWS  G lu e can export data from  Dynamo db to Amazon S3  using the  Dynamo db export conne ctor. This export  process does  not consume  provisioned  read/write capacity of t he  Dy namoDB table, as it uses a separate export feature (backed  by  Dynamo db's  point-in-t ime recovery/backup system). Once exported to S3, the Glue job can  process the dat a to calculate metrics without impacting the  live  Dynamo db table. Other options:  - A 	(Athena) and  D  (EMR/Hive) directly query the  Dynamo db table, consuming  read cap acity. - C (Redshift COPY) also  requires  reading data from  Dynamo db,  impacting  prov isioned capacity.  Reference  Links  [AWS  Glue  Dynamo db  Export  Connector](https://docs. aws.amazon.com/glue/latest/dg/aws-glue-programming-connectors-dynamo db-export.ht ml)  [Dynamo db  Export to S3](https://docs.aws.amazon.com/amazon dynamo db/latest/de velo per guide/Data export.html)


44.A solutions architect is designing the cloud architecture for a  new stateless applica tion that will be deployed on AWS. The solutions architect created an Amazon  Machi ne Image (AMI) and launch template for the application.  Based on the number of job s that  need to  be  processed, the  processing  must  run  in  parallel while  adding and  re moving application Amazon  EC2  instances as  needed. The application  must  be  loosely

 coupled. The job items  must  be durably stored. Which solution will meet these requi rements?
A��Create an Amazon Simple  Notification Service (Amazon SNS) topic to send the job s that  need to  be  processed. Create an Auto  Scaling group  by  using the  launch temp late with the scaling policy set to add and  remove  EC2  instances  based  on CPU  usag e.
B��Create an Amazon Simple Queue Service (Amazon SQS) queue to hold the jobs th at  need to  be  processed. Create  an Auto Scaling group  by  using the  launch template with the scaling policy set to add and  remove  EC2  instances based on network usag e.
C��Create an Amazon Simple Queue Service (Amazon SQS) queue to  hold the jobs th at  need to  be  processed. Create  an Auto Scaling group  by  using the  launch template with the scaling policy set to add and  remove  EC2 instances  based on the  number  of items in the SQS queue.
D��Create an Amazon Simple  Notification Service  (Amazon SNS) topic to send the job s that  need to  be  processed. Create an Auto  Scaling group  by  using the  launch temp late with the scaling policy set to add and  remove  EC2  instances  based  on the  numb er of messages  published to the SNS topic.
�� ��C



���� ��Correct Answer: C  Detailed  Explanation: The question requires a solution that  pr ocesses jobs in  parallel with auto-scaling,  loose coupling, and durable job storage.  He re's why option C is correct:  1.  Durable Job  Storage: Amazon SQS  (Simple Queue Ser

vice) is designed for durable  message storage.  Messages  stay  in the queue  until  proc essed, ensuring no data  loss even  if instances fail (unlike SNS  in options A/D, which  l acks  built-in storage). 2.  Loose Coupling:  SQS decouples job  producers  (e.g.,  users su b mitting jobs) from consumers (EC2 instances). Instances  pull jobs independently, allo wing parallel processing and independent scaling. 3. Auto-Scaling  Based on Workload: Using the number of items in the SQS queue as the scaling metric directly aligns sc aling with actual job demand. If the queue grows (more jobs),  EC2 instances scale  up;		if the queue shrinks (fewer jobs),  instances scale down.  Metrics  like CPU/network  usa ge (options A/B) are indirect and  may  not reflect true job  backlog. 4. Stateless Applic ation: Since the app is stateless, auto-scaled instances can  process jobs from the shar ed SQS queue without  needing local state. Why other options fail: - A/D: SNS topics don��t store  messages durably. If  no subscribers are available,  messages are  lost.  -  B: Scaling  based on network usage doesn��t correlate with job backlog. A queue might be empty even with  high  network traffic  (or vice versa).  Reference  Links:  -  [AWS  Auto	Scaling  Using SQS Queue  Metrics](https://docs.aws.amazon.com/auto scaling/ec2/userg uide/as-using-sqs-queue.html) -  [SQS vs.  SNS](https://aws.amazon.com/sqs/faqs/#:~:text =SQS%20is%20a%20message%20queueing%20service%20and%20SNS%20is%20a%20pub /sub%20service.)


45.A global ecommerce company uses a monolithic architecture. The company needs a solution to  manage the increasing volume of product data. The solution  must  be sc alable and  have a  modular service architecture. The company  needs to  maintain  its st ruc tured database schemas. The company also  needs a storage solution to store  prod

uct data and  product images. Which solution will  meet these  requirements with the  L EAST operational overhead?
A�� Use an Amazon  EC2  instance  in an Auto Scaling group to deploy a containerized application.  Use an Application  Load  Balancer to  distribute web traffic.  Use an Amazo n  RDS  DB  instance  to store  product data and  product  images.

B��Use AWS  Lambda functions to  manage the  existing  monolithic application.  Use Am azon  Dynamo db to store  product  data and  product  images.  Use Amazon Simple  Noti fication Service (Amazon SNS) for event-driven communication  between the  Lambda f unctions.

C�� Use Amazon  Elastic  Ku bernet es  Service  (Amazon  EKS) with an Amazon  EC2  deploy ment to deploy a containerized application.  Use an Amazon Aurora cluster to store th e  product data.  Use AWS  Step  Functions to  manage workflows.  Store the  product  im ages in Amazon S3 Glacier  Deep Archive.
D��Use Amazon  Elastic Container  Service (Amazon  ECS) with AWS  Fargate  to deploy a containerized application.  Use Amazon  RDS with a  Multi-AZ  deployment to store the product data. Store the product images in an Amazon S3  bucket.
�� ��D



���� ��Correct Answer  D  Detailed  Explanation The correct answer  is  D  because  it  best meets the  requirements with the  least operational overhead.  Here��s why:  1.  Modular and Scalable Architecture: Amazon  ECS with AWS  Fargate allows the company to depl oy a containerized application without managing servers (serverless containers). This s upports  modular services and auto-scaling. 2. Structured  Database: Amazon  RDS  (Mult

i-AZ)  provides a fully  managed relational database,  maintaining structured schemas w hile ensuring  high availability. 3. Image Storage: Amazon S3  is ideal for storing  produ ct images due to its scalability, durability, and cost-effectiveness. 4.  Least Operational Overhead:  Fargate  (serverless) and  RDS/S3  (managed services)  minimize  infrastructure management, unlike options involving  EC2  (A, C) or  Dynamo db  (B, which  isn��t  relatio nal). Other options fail  because:  - A:  Storing images in  RDS  is  inefficient  (use S3  inste ad).  Managing  EC2  adds  overhead.  -  B:  Dynamo db  isn��t  relational,  and  Lambda  isn��t	ideal for monolithic apps. - C:  EKS with  EC2  requires  node  management,  and Glacier	is unsuitable for frequently accessed images.  Reference  Links  -  [AWS  Fargate](https:// aws.amazon.com/fargate/) -  [Amazon  RDS](https://aws.amazon.com/rds/)  -  [Amazon  S3] (https://aws.amazon.com/s3/)


46.A company is migrating an application from an on-premises environment to AWS. The application will store sensitive data  in Amazon S3. The company  must encrypt th e data  before storing the data  in Amazon S3. Which solution will  meet these  require ments?

A�� Encrypt the data  by using client-side encryption with customer  managed  keys.

B�� Encrypt the data  by  using server-side encryption with AWS  KMS  keys  (SSE-KMS).

C�� Encrypt the data  by  using server-side encryption with customer-provided  keys  (SSE -C).
D�� Encrypt the data  by  using client-side encryption with Amazon S3  managed  keys.

�� ��A

���� ��Correct Answer: A.  Encrypt the data  by using client-side encryption with custom er  managed keys.  Detailed  Explanation: The  question  requires encrypting data  before  storing it in S3 (i.e.,  before it  reaches AWS).  Here's why  each option works or doesn't work:  1. Client-Side  Encryption  (A ��  D)  - This  means the data  is encrypted on the  u ser's side (on-premises)  before  being  uploaded to S3. This meets the encrypt before   storing requirement. - Customer  Managed  Keys  (A): The company fully controls the e n crypt ion keys  (e.g.,  using AWS  KMS  or t heir own  key  management system).  -  S3  Ma naged  Keys  (D): This doesn��t exist for client-side encryption. S3-managed  keys are  us ed in server-side encryption  (SSE-S3), not client-side. So  D  is  invalid. 2. Server-Side  En crypt ion (B �� C) -  Data  is encrypted after  it  reaches  S3  (i.e., AWS  handles encrypt io  n). This violates the encrypt  before storing  requirement. - SSE-KMS (B): AWS  KMS  ma nages  keys,  but encryption  happens on AWS servers.  -  SSE-C  (C): You  provide  keys to	S3, but AWS still performs encryption after  receiving the data. The data  is  un encrypt ed during upload. Why A is correct: Client-side encryption with customer-managed ke ys ensures the data  is already encrypted before it  leaves the company��s systems and gives them full control over the keys.  Reference  Links:  -  [Client-Side  Encryption](https: //docs.aws.amazon.com/AmazonS3/latest/user guide/Using client side encryption.html) -  [S erver-Side  Encryption](https://docs.aws.amazon.com/AmazonS3/latest/user guide/serv-sid e-encryption.html)


47.A company wants to create an Amazon  EMR cluster that  multiple teams will  use. T he company wants to ensure that each team��s  big data workloads can access only t he AWS services that each team  needs to  interact with. The company does  not want

the workloads to  have access to Instance  Metadata  Service Version 2  (IMDSv2) on th e cluster��s  underlying  EC2  instances. Which solution will  meet these requirements?
A��Configure interface VPC endpoints for each AWS service that the teams  need.  Use the required interface VPC endpoints to submit the  big data workloads.

B��Create  EMR  runtime  roles.  Configure the cluster to  use the  runtime  roles.  Use the runtime roles to submit the big data workloads.

C��Create an  EC2 IAM  instance  profile that  has the  required  permissions for  each tea m. Use the instance profile to submit the big data workloads.
D��Create an  EMR security configuration that  has the  Enable application scope diam rol e option set to false.  Use the security configuration to submit the  big data workload s.
�� ��B



���� �� Correct Answer  B  Detailed  Explanation The correct  answer is  B  because  EMR  ru ntime roles allow you to assign specific IAM  roles to  Spark or  Hadoop jobs  running  on the cluster. This ensures each team��s workloads use only the  permissions defined in their assigned  role, adhering to the  principle of least  privilege.  Here��s w hy the ot her options are incorrect:  - A: Interface VPC endpoints enable  private  network access to AWS services  but don��t enforce workload-specific  permissions.  - C:  EC2  instance  p rofiles apply  permissions to all workloads on the  EC2  instance, violating the  require m ent to  restrict access  per team.  -  D:  Disabling  `Enable application scope diam role` (setti ng it to `false`) forces workloads to use the  EC2  instance  profile, which exposes IMDS v2 and doesn��t isolate  permissions  per team.  Key  Points  -  Runtime  roles  isolate  per

missions at the job level, avoiding reliance on the  EC2  instance  profile.  -  Restricting I MDSv2 is  possible  because workloads  no  longer  need  instance  metadata for crede ntia ls.  Reference  Link  [AWS  EMR  Runtime  Roles  Documentation](https://docs.aws.amazon.c om/emr/latest/Management guide/emr-iam-roles.html)


48.A solutions architect is designing an application that  helps  users fill out and sub mi t  registration forms. The solutions architect  plans to  use a two-tier architecture that  i ncludes a web application server tier and a worker tier. The application needs to  proc ess submitted forms quickly. The application  needs to  process each form exactly once. The solution must ensure that  no data  is lost. Which solution will meet these requir ements?

A��Use an Amazon Simple Queue Service (Amazon SQS)  FI FO  queue  between the web application server tier and the worker tier to store and forward form data.

B��Use an Amazon API Gateway  HTTP API  between the web application  server tier an
d the worker tier to store and forward form data.

C��Use an Amazon Simple Queue Service (Amazon SQS) standard queue  between the web application server tier and the worker tier to store and forward form data.
D��Use an AWS Step  Functions workflow. Create a synchronous workflow  between the web application server tier and the worker tier that stores and forwards form data.
�� ��A



���� �� Correct Answer: A  Detailed  Explanation: The question requires a solution that e nsures each form is processed exactly once with  no data  loss.  Here's why option A  is

 correct: - SQS  FIFO Queues guarantee exactly-once  processing and  strict ordering of messages, which aligns with the requirement to process each form exactly once. - T hey also  provide durability  (messages are stored  redundantly  until  processed), ensurin g  no data  loss.  - Standard Queues (C) allow duplicates and don��t enforce order, viol ating the exactly once  requirement.  - API Gateway  (B)  is for  building APIs,  not queuin g. - Step  Functions  (D)  manage workflows  but  aren��t designed for direct  message qu euing with exactly-once guarantees.  Reference  Link:  [Amazon  SQS  FIFO  Queues](https: //aws.amazon.com/sqs/features/fifo/)


49.A finance company uses an on-premises search application to collect streaming da ta from various  producers. The application  provides  real-time updates to search and v isu alization features. The company is  planning to  migrate to AWS and wants to use a n AWS  native solution. Which solution will meet these requirements?

A�� Use Amazon  EC2  instances to  ingest and  process the data streams to Amazon  S3 buckets tor storage.  Use Amazon Athena to search the data.  Use Amazon  Managed  G rafana to create visualizations.

B�� Use Amazon  EMR to  ingest and  process the  data streams to Amazon  Redshift for storage.  Use Amazon  Redshift  Spectrum to search the data.  Use Amazon Quick sight t
o create visualizations.

C��Use Amazon  Elastic  Ku bernet es  Service  (Amazon  EKS)  to ingest and  process the da ta streams to Amazon  Dynamo db for storage.  Use Amazon Cloud watch to create  gra phical dashboards to search and visualize the data.

D��Use Amazon  Kinesis  Data  Streams to  ingest  and  process the data streams to Amaz on Open search Service.  Use Open search Service to search the data.  Use Amazon Qui ckSight to create visualizations.
�� ��D



���� ��Correct Answer:  D  Detailed  Explanation: The correct solution uses Amazon  Kines is  Data  Streams for  real-time data  ingestion and  processing, Amazon Open search  Ser vice for storage and fast search capabilities, and Amazon Quick sight for visualization. Here's why: 1.  Real-Time  Data  Streaming:  Kinesis  Data  Streams  is  designed for  real-ti me data  ingestion from multiple  producers,  matching the  need for streaming data.  2. Search  Functionality: Open search Service (formerly Amazon  Elastic search)  is optimized	for real-time search and analytics,  replacing the on-premises search application. 3. Vi su alization: Quick sight integrates seamlessly with Open search to create dashboards an d visualizations. Other options fail  because: - A:  EC2  +  S3  +  Athena  are  batch-oriente d,  not  real-time.  -  B:  Redshift  is  for analytics,  not  real-time  search.  -  C:  Dynamo db  la cks  built-in search features, and Cloud watch  is for  monitoring,  not  business visualiza ti ons.  Reference  Links:  -  [Amazon  Kinesis  Data  Streams](https://aws.amazon.com/kinesis/ data-streams/) -  [Amazon Open search Service](https://aws.amazon.com/open search-ser vice/) -  [Amazon Quick sight](https://aws.amazon.com/quick sight/)


50.A company currently  runs an on-premises application that  usesASP.NET on  Linux  m achines. The application is resource-intensive and serves customers directly. The comp any wants to  modernize the application to .NET. The company wants to  run t he appli cation on containers and to scale  based on Amazon Cloud watch  metrics. The compa

ny also wants to  reduce the time spent on operational  maintenance activities. Which solution will meet these  requirements with the  LEAST operational overhead?
A��Use AWS App2Container to container ize the application.  Use an AWS Cloud Formati on template to deploy the application to Amazon  Elastic Container  Service (Amazon  E CS) on AWS  Fargate.

B��Use AWS App2Container to container ize the application.  Use an AWS Cloud Formati on template to deploy the application to Amazon  Elastic Container  Service (Amazon  E CS) on Amazon  EC2  instances.

C�� Use AWS App  Runner to container ize the application.  Use App  Runner to  deploy t he application to Amazon  Elastic Container Service  (Amazon  ECS) on AWS  Fargate.
D�� Use AWS App  Runner to container ize the application.  Use App  Runner to  deploy t he application to Amazon  Elastic  Ku bernet es  Service  (Amazon  EKS)  on Amazon  EC2  in stances.
�� ��A



���� ��Correct Answer: A  Detailed  Explanation: The company  needs to container ize the i r existing ASP.NET application, scale using Cloud watch metrics, and  minimize operatio nal maintenance.  Here's why option A  is the  best:  1. Containerization: AWS App2Cont ainer is specifically designed to automate the containerization of existing applications (like their ASP.NET app), while AWS App  Runner does  not  handle containerization. Thi s  makes options C/D invalid. 2. Serverless Infrastructure:  Deploying to Amazon  ECS  on AWS  Fargate  (option A)  removes the  need to  manage servers  (EC2  instances),  unlike options B/D.  Fargate  handles server  provisioning, scaling, and  maintenance automatic

ally. 3. Scaling:  Both  ECS  Fargate  and  EC2  support auto scaling with Cloud watch  metri cs,  but  Fargate  requires  no  infrastructure  management. 4.  Operational Overhead:  Farg ate eliminates  EC2  patching, capacity  planning, and  cluster  management,  making  it th e  lowest-effort option.  Reference  Links:  -  [AWS App2Container](https://aws.amazon.co m/app2container/) -  [ECS  Fargate vs  EC2](https://aws.amazon.com/fargate/)  -  [AWS  Ap p  Runner vs  ECS](https://docs.aws.amazon.com/app runner/latest/dg/what-is-app runner. html)


51.A company is designing a  new internal web application in the AWS Cloud. The  ne w application  must securely  retrieve and store  multiple employee usernames and  pass words from an AWS managed service. Which solution will meet these requirements w ith the  LEAST operational overhead?

A��Store the employee credentials in AWS Systems  Manager  Parameter  Store.  Use AW S Cloud Formation and the  Batch get secret value API to  retrieve  usernames and  passwo rds from  Parameter  Store.

B��Store the employee credentials in AWS Secrets  Manager.  Use AWS  Cloud Formation and AWS  Batch with the  Batch get secret value API to  retrieve the  usernames and  pass words from Secrets  Manager.

C��Store the employee credentials in AWS Systems  Manager  Parameter  Store.  Use AW S Cloud Formation and AWS  Batch with the  Batch get secret value API to  retrieve the  us ernames and  passwords from  Parameter  Store.

D��Store the employee credentials in AWS Secrets  Manager.  Use AWS Cloud Formation and the  Batch get secret value API to  retrieve the usernames and passwords from Secr ets  Manager.
�� ��D



���� �� Correct Answer:  D  Detailed  Explanation: The  best  solution is  D  because  it  uses AWS Secrets  Manager to securely store credentials and integrates seamlessly with AW S Cloud Formation using the `Batch get secret value` API.  Here's why:  1. AWS  Secrets  M anager vs.  Parameter  Store:  - Secrets  Manager  is designed specifically for  managing s ensit ive data  like  passwords. It  provides  built  -  in features  like  automatic secret  rotati on, encryption  by default, and fine - grained access control via IAM  policies. This  red uces operational overhead compared to  Parameter Store, which  requires additional set up for secure storage (e.g.,  using Secure string  parameters and  manual  rotation). 2.  Ba tch get secret value API:  - This API  is  part of Secrets  Manager, allowing applications to  retrieve  multiple secrets  in a single call.  Parameter  Store does  not support this API;  in stead, it uses `Getparameter` or `Getparameters`, which are  less efficient for  bulk oper ations. 3. Avoiding AWS  Batch:  - Options  B  and C  include AWS  Batch,  a service for  r unning batch computing workloads.  However,  retrieving secrets during application de ployment does  not  require a  batch  processing  service.  Using AWS  Batch  here  adds  u n necessary complexity and operational overhead. 4. Cloud Formation Integration: - Clo ud Formation natively supports Secrets  Manager via dynamic  references (e.g., `{{resolve: secrets manager:My secret:Secret string}}`). This allows secrets to  be securely injected int o templates without  hardcoding credentials, simplifying infrastructure - as - code (IaC) workflows. In summary, Secrets  Manager  +  Cloud Formation provides a fully managed,

 secure, and  low  - overhead solution for storing and  retrieving credentials,  making  D  the correct choice.  Reference  Links:  -  [AWS  Secrets  Manager](https://aws.amazon.com/ secrets-manager/) -  [AWS Cloud formation ��  Secrets  Manager Integration](https://docs. aws.amazon.com/Aws cloud Formation/latest/User guide/dynamic-references.html#dynami c-references-secrets manager)


52.A company that  is in t he ap-northeast-1  Region  has a fleet of thousands of AWS Outposts servers. The company has deployed the servers at  remote  locations around  the world. All the servers  regularly download  new software versions that consist of 10 0 files. There is significant latency  before all servers  run the  new software versions. T he company must  reduce the deployment  latency for  new software versions. Which s ol ution will meet this requirement with the  LEAST operational overhead?

A��Create an Amazon S3  bucket  in ap-northeast-1. Set up an Amazon Cloud Front dist ri bution i n ap-northeast-1 that includes a Caching disabled cache  policy. Configure the S3 bucket as the origin.  Download the software  by  using signed URLs.

B��Create an Amazon S3  bucket  in ap-northeast-1. Create a second S3  bucket in the us-east-1  Region. Configure  replication  between the  buckets.  Set  up an Amazon C lou d Front distribution that uses ap-northeast-1 as the primary origin and us-east-1 as th e secondary origin.  Download the software  by  using signed  URLs.

C��Create an Amazon S3  bucket in ap-northeast-1. Configure Amazon S3 Transfer Acc el eration.  Download the software  by  using the S3 Transfer Acceleration endpoint.

D��Create an Amazon S3  bucket  in ap-northeast-1. Set  up an Amazon Cloud Front dist ri bution. Configure the S3  bucket as the origin.  Download the software  by  using sign ed  URLs.
�� ��D



���� �� Correct Answer:  D  Detailed  Explanation: The company's  main  issue  is  high  laten cy when thousands of globally distributed Outposts servers download software from a	single S3 bucket in ap-northeast-1. The goal is to  reduce latency without adding op erational complexity. ### Why Option  D works  best:  1. Amazon  Cloud Front CDN:  - Cl oud Front caches files at edge  locations worldwide. When the first server  in a  region  downloads a file, it gets stored at the  nearest edge  location. Subsequent downloads f rom nearby servers will use this cached copy, drastically  reducing  latency.  - Without c aching (like in Option A), every download would go  back to the origin S3  bucket in  Tokyo, causing delays for distant servers. 2. Signed URLs:  - Signed  URLs securely gran t access to download files without exposing the S3 bucket  publicly. This aligns with s ecurity best  practices.  3.  Least  Operational Overhead:  -  No  cross  -  region  replication   (unlike Option  B) or extra configurations. Simply set  up a Cloud Front distribution  poin ting to the S3 bucket, and caching works automatically. ### Why other options are  l ess optimal: - Option A:  Disabling caching  in Cloud Front defeats the  purpose of  usin g a CDN. All downloads go directly to Tokyo, worsening latency. - Option  B: Cross  - region replication and  multi  - origin Cloud Front add complexity.  Managing two  S3  bu ckets and replication  rules increases operational effort.  - Option C: S3 Transfer Acceler ation improves uploads/downloads for individual clients  but  lacks caching.  Repeated d ownloads of t he same files (by thousands of servers) would still  hit the origin S3  buc

ket, causing  latency.  Reference  Links:  -  [Amazon  Cloud front Overview](https://aws.ama zon.com/cloud front/) -  [S3 Signed  URLs](https://docs.aws.amazon.com/AmazonS3/latest /user guide/Share object pre signed url.html)


53.A company currently  runs an on-premises stock trading application  by using  Micro soft Windows Server. The company wants to migrate the application to the AWS C lou
d. The company  needs to design a  highly available solution that  provides  low-latency access to block storage across  multiple Availability Zones. Which solution will meet t hese requirements with the  LEAST  implementation effort?

A��Configure a Windows Server cluster that spans two Availability Zones on Amazon EC2 instances. Install the application on  both cluster  nodes.  Use Amazon  FSx for Win dows  File Server as shared storage  between the two cluster  nodes.

B��Configure a Windows Server cluster that spans two Availability Zones on Amazon  E C2 instances. Install the application on  both cluster  nodes.  Use Amazon  Elastic  Block  Store (Amazon  EBS) General  Purpose  SSD  (gp3) volumes as storage attached to the  E C2 instances. Set up application-level  replication to sync data from one  EBS volume  i n one Availability Zone to another  EBS volume  in the second Availability Zone.

C��Deploy the application on Amazon  EC2  instances in two Availability Zones. Configu re one  EC2  instance as active  and the second  EC2  instance  in standby  mode.  Use  an Amazon  FSx for  NetApp  ONTAP  Multi-AZ file  system to access the data  by  using Inte rnet Small Computer Systems Interface (iSCSI) protocol.
D��Deploy the application on Amazon  EC2  instances  in two Availability Zones. Configu re one  EC2  instance as active  and the second  EC2  instance  in standby  mode.  Use Am

azon  Elastic  Block  Store  (Amazon  EBS)  Provisioned  IOPS SSD (io2) volumes as storage attached to the  EC2 instances. Set  up Amazon  EBS  level  replication to sync data fro m one io2 volume in one Availability Zone to another io2 volume in the second Avail ability Zone.
�� ��C



���� ��Correct Answer: C  Detailed  Explanation: The  best solution is C  because Amazon FSx for  NetApp ONTAP  Multi-AZ file  systems are designed for  high availability and  lo w-latency block storage across Availability Zones (AZs).  Here��s why:  1.  High Availabil i ty:  FSx for ONTAP  Multi-AZ  automatically  replicates data across two AZs, ensuring  red undancy. If one AZ fails, the storage  remains accessible from the other AZ with  mini mal downtime. 2.  Low-Latency  Block  Storage:  Using the  i SCSI  protocol,  EC2  instances can directly access  block storage  (like a virtual  hard disk) from the  FSx file  system. Th is meets the  requirement for  low-latency  block storage.  3.  Least  Effort:  AWS fully  man ages the storage  replication and failover. The company only  needs to deploy  EC2  inst ances in active/standby  mode and connect them to the  FSx  Multi-AZ file  system.  No manual replication or complex cluster configuration is  required. Other options involve more effort: - A uses  FSx for Windows  File  Server  (file storage,  not  block  storage) an d  requires  manual cluster setup.  -  B  and  D  require  manual  application-level or  EBS  re plication, which adds complexity and  maintenance.  Reference  Links:  -  [Amazon  FSx  for NetApp ONTAP](https://aws.amazon.com/fsx/netapp-ontap/) -  [Multi-AZ  File Systems] (https://docs.aws.amazon.com/fsx/latest/On tap guide/high-availability-AZ.html)

54.A weather forecasting company collects temperature  readings from various sensors on a continuous basis. An existing data ingestion  process collects the readings and aggregates the  readings into  larger Apache  Parquet files. Then the  process encrypts t he files  by  using client-side encryption with  KMS  managed  keys  (CSE-KMS).  Finally, th e  process writes the files to an Amazon S3  bucket with separate  prefixes for each cal endar day. The company wants to  run occasional SQL queries on the data to take sa mple moving averages for a specific calendar day. Which solution will meet these  req ui rements  MOST cost-effectively?

A��Configure Amazon Athena to  read the encrypted files.  Run SQL queries on the dat a directly in Amazon S3.

B�� Use Amazon S3 Select to  run  SQL queries on the data directly in Amazon S3.

C��Configure Amazon  Redshift to  read the  encrypted files.  Use  Redshift  Spectrum  and Redshift query editor v2 to  run SQL queries on the data directly  in Amazon S3.
D��Configure Amazon  EMR Serverless to  read the encrypted files.  Use Apache  SparkS QL to  run  SQL queries on the data directly in Amazon S3.
�� ��A



���� �� Correct Answer: A  Detailed  Explanation: Amazon Athena is the  most cost-effecti
ve solution for occasional SQL queries on encrypted  Parquet files  in S3.  Here��s why:
1.  Native  Support for  Parquet and  Encryption: Athena  natively  supports querying Apac he  Parquet files and can decrypt client-side encrypted  (CSE-KMS) data  if proper IAM/ KMS  permissions are configured. S3 Select  (Option  B) also  supports  Parquet  but  has  l imited SQL capabilities (e.g.,  no window functions for  moving averages). 2. Serverless

and  Pay-per-Query: Athena charges  based on the amount of data scanned  per query. Since the data is already stored in  Parquet (columnar format), queries scan  less data,	reducing costs. Options  like  Redshift  Spectrum (C) or  EMR Serverless  (D) add comple xity or  higher costs for occasional  use. 3.  Partitioning: The data  is organized  in  S3 wit h daily  prefixes, which Athena can  leverage via  partitioning to  scan only the relevant day��s data, further optimizing cost and  performance.  S3 Select  (B)  is  less suitable for complex analytical queries  like  moving averages.  Redshift  Spectrum  (C)  requires a  Red shift cluster (costly for occasional  use), and  EMR  Serverless  (D)  is overkill for simple S QL queries.  Reference  Links:  -  [Athena  Querying  Encrypted  Data](https://docs.aws.amaz on.com/athena/latest/ug/encryption.html) -  [Athena vs S3 Select](https://aws.amazon.co m/blogs/big-data/amazon-athena-vs-amazon-s3-select-vs-amazon-redshift-spectrum-whi ch-to-use-and-why/)


55.A company is implementing a  new application on AWS. The company will  run the application on  multiple Amazon  EC2  instances across  multiple Availability Zones withi n multiple AWS  Regions. The application will  be available through the internet.  Users will access the application from around the world. The company wants to ensure that each user who accesses the application is sent to the  EC2 instances that are closest to the user��s  location. Which solution will meet these  requirements?

A��Implement an Amazon  Route  53 geolocation  routing  policy.  Use an  internet-facing Application Load  Balancer to distribute the traffic across all Availability Zones within t he same  Region.

B��Implement an Amazon  Route  53 geo proximity  routing  policy.  Use an  internet-facin g  Network  Load  Balancer to  distribute the traffic across all Availability Zones within t he same  Region.
C��Implement an Amazon  Route  53  multivalue answer  routing  policy.  Use  an  internet- facing Application  Load  Balancer to distribute the traffic across all Availability Zones
within the same  Region.

D��Implement an Amazon  Route  53 weighted  routing  policy.  Use  an  internet-facing  N etwork  Load  Balancer to distribute the traffic across all Availability Zones within the s ame  Region.
�� ��B



���� ��Correct Answer: A. Implement an Amazon  Route  53 geolocation  routing  policy. Use an internet-facing Application  Load  Balancer to distribute the traffic across all Av a il ability Zones within the same  Region.  Detailed  Explanation: The  question asks for a	solution to  route  users to the  nearest  EC2  instances across  multiple AWS  Regions.  H ere's why Option A  is the correct choice:  1.  Route  53  Geolocation  Routing  Policy:  -  G eo location routing directs traffic  based on the user's geographic  location (e.g., countr y or continent).  For  example:  -  Users  in  Europe  ��  Route to  the  EU  Region.  -  Users  in Asia ��  Route to the Asia-Pacific  Region. - This  policy  requires manually mapping ge o graphic zones to specific  Regions,  but  it ensures users are always sent to the closest predefined Region. This aligns with the requirement to  route users to the closest  EC 2 instances. 2. Why Geolocation (A)  is  better than Geo proximity  (B):  - Geo proximity c on siders  both user  location and  resource  location,  but  it  requires configuring a  bias t

o adjust traffic flow. It��s  more complex and better suited for  hybrid cloud scenarios or dynamic traffic shifting. - Geolocation is simpler for static  mappings  like  users  in X	country go to Y  Region, which fits the scenario  here.  3. Application  Load  Balancer  (A LB) vs.  Network  Load  Balancer  (NLB):  -  The question doesn��t specify TCP/UDP or  low -latency  needs, so ALB  (HTTP/HTTPS  layer 7  load  balancing)  is the default choice for web applications.  NLB  (layer 4)  is  used for extreme  performance or non-HTTP traffic, which isn��t  mentioned  here. 4. Why the original answer  (B)  is  incorrect:  - Geo pro xi mi ty adds  unnecessary complexity  (bias configuration). -  NLB  is  less suitable for generic web apps compared to ALB.  Reference  Links:  -  [Route  53  Routing  Policies](https://doc s.aws.amazon.com/Route53/latest/Developer guide/routing-policy.html) -  [ALB vs.  NLB](h ttps://aws.amazon.com/elastic load balancing/features/)


56.A financial services company  plans to  launch a  new application on AWS to  handle	sensitive financial transactions. The company will deploy the application on Amazon  EC2 instances. The company will use Amazon  RDS for  MySQL  as the database. The c ompany��s security  policies  mandate that data  must  be encrypted  at  rest and  in trans it. Which solution will meet these requirements with the  LEAST operational overhead?

A��Configure encryption at  rest for Amazon  RDS for  MySQL  by  using AWS  KMS  mana ged  keys. Configure AWS Certificate  Manager  (ACM)  SSL/TLS certificates for encrypt io n in transit.

B��Configure encryption at  rest for Amazon  RDS for  MySQL  by  using AWS  KMS  mana ged  keys. Configure IPsec tunnels for encryption in transit.

C��Implement third-party application-level data encryption before storing data in Ama zon  RDS for  MySQL.  Configure AWS Certificate  Manager  (ACM) SSL/TLS certificates fo r encryption in transit.
D��Configure encryption at  rest for Amazon  RDS  for  MySQL  by  using AWS  KMS  mana ged  keys. Configure a VPN connection to enable  private connectivity to encrypt data  in transit.
�� ��A



���� �� Correct Answer A  Explanation The correct answer is A because it uses AWS-ma naged services with  minimal setup:  1.  Encryption at  rest: Amazon  RDS  for  MySQL  nati vely supports encryption using AWS  KMS.  Enabling  it during database  creation requir es  no ongoing  management. 2.  Encryption  in transit: AWS Certificate  Manager  (ACM) provides free SSL/TLS certificates.  RDS  integrates with ACM to encrypt data  between t he application and the database, without manual certificate  handling. Other options a dd complexity: -  B  (IPsec):  Requires  configuring/managing VPN tunnels.  - C (third-part
y encryption): Adds application-layer work and  key  management.  -  D  (VPN):  Secures  n etwork traffic  but doesn��t replace SSL/TLS for database-specific encryption.  Reference Links -  [Amazon  RDS  Encryption](https://docs.aws.amazon.com/Amazon rds/latest/User Guide/Overview.Encryption.html) -  [ACM with  RDS](https://docs.aws.amazon.com/Amazo nRDS/latest/User guide/Using with rds.SSL.html)


57.A company is  migrating its on-premises Oracle database to an Amazon  RDS for Or acle database. The company needs to  retain data for 90 days to  meet  regulatory  req ui rements. The company must also  be able to  restore the database to a  specific  poin

t in time for up to  14 days. Which solution will  meet these  requirements with the  LE AST operational overhead?
A��Create Amazon  RDS automated  backups.  Set the  retention  period to 90 days.

B��Create an Amazon  RDS  manual  snapshot every day.  Delete  manual snapshots that are older than 90 days.

C��Use the Amazon Aurora Clone feature for Oracle to create a  point-in-time  restore. Delete clones that are older than 90 days.
D��Create a  backup  plan t hat  has  a  retention  period  of 90 days  by  using AWS  Back u p for Amazon  RDS.
�� ��D



���� �� Correct Answer  D  Detailed  Explanation The company  needs two things:  1.  Retai n  backups for 90 days (regulatory  requirement). 2.  Restore to any  point  in time withi n the  last  14 days  (operational  recovery).  -  Option A  is incorrect  because Amazon  RD S automated  backups  have  a  maximum  retention  period  of 35 days,  not 90  days.  - O ption  B involves  manual snapshots, which  require daily effort to  create and delete. Th is increases operational overhead and does not support  point-in-time recovery (PITR).
 - Option C is invalid because Amazon Aurora Clone is  not compatible with Amazon RDS for Oracle. - Option  D  uses AWS  Backup, which  automates daily snapshots and enforces a 90-day  retention  policy. AWS  Backup  reduces  operational overhead  by  ma naging backups automatically.  Meanwhile, Amazon  RDS automated  backups  (enabled by default)  provide  point-in-time  recovery for the  last  14  days  (retention can  be  set t o  14 days). AWS  Backup  handles  long-term  retention  effortlessly, while  RDS automate

d  backups  (with a shorter  retention  period)  handle  point-in-time  recovery. This combi nation  meets  both  requirements with  minimal effort.  Reference  Link  [AWS  Backup  for Amazon  RDS](https://docs.aws.amazon.com/aws-backup/latest/dev guide/working-with-b ackups.html)


58.A company is developing a  new application that uses a  relational database to stor e user data and application configurations. The company expects the application to  h ave steady  user growth. The company expects the database  usage to  be variable and	read-heavy, with occasional writes. The company wants to cost-optimize the databas e solution. The company wants to use an AWS  managed database solution that will  provide the  necessary  performance. Which solution will  meet these requirements  MOS T cost-effectively?
A�� Deploy the database on Amazon  RDS.  Use  Provisioned IOPS  SSD  storage to ensur e consistent  performance for  read and write operations.

B��Deploy the database on Amazon Aurora Serverless to automatically scale the datab ase capacity  based on actual  usage to accommodate the workload.

C�� Deploy the database on Amazon  Dynamo db.  Use on-demand capacity  mode to  au tomatically scale throughput to accommodate the workload.
D��Deploy the database on Amazon  RDS.  Use  magnetic  storage  and  use  read  replicas to accommodate the workload.
�� ��B

���� ��Correct Answer  B  Detailed  Explanation The  best solution  is Amazon Aurora Serv erless (Option  B)  because:  1. Cost Optimization: Aurora Serverless automatically scales database capacity up or down based on actual usage. This pay-per-use  model elimin ates the  need to over-provision  resources,  making it cost-effective for variable worklo ads. 2.  Read-Heavy Workloads: Aurora Serverless inherently supports  read-heavy workl oads  by scaling read  replicas automatically, ensuring  performance without  manual  inte r vention. 3.  Managed Service: As an AWS-managed service,  it  handles scaling,  patchin g, and  backups,  reducing operational overhead. Other options are  less optimal:  - Opti on A (RDS with  Provisioned IOPS):  Provisioned IOPS  is expensive for variable workloa ds, as you  pay for reserved capacity even when unused. - Option C (Dynamo db):  Dyn amoDB is a  NoSQL database, which doesn��t fit the  requirement for a  relational data base. - Option  D  (RDS with  Magnetic  Storage):  Magnetic  storage  has  poor  perform an ce, and  manual scaling with  read  replicas adds  complexity and  potential downtime.  R eference  Links  -  [Amazon Aurora Serverless](https://aws.amazon.com/rds/aurora/serverl ess/) -  [Amazon Aurora  Serverless  Use Cases](https://docs.aws.amazon.com/Amazon rd S/latest/Aurora user guide/aurora-serverless.html)

59.A company hosts its application on several Amazon  EC2  instances  inside a VPC. Th e company creates a dedicated Amazon S3 bucket for each customer to store their r e levant information in Amazon S3. The company wants to ensure that the application	running on  EC2  instances can  securely access only the S3  buckets that  belong to th e company��s AWS account. Which solution will meet these requirements with the  LE AST operational overhead?

A��Create a gateway endpoint for Amazon S3 that  is attached to the VPC.  Update the	IAM instance  profile  policy to  provide access to only the specific  buckets that the ap plication  needs.

B��Create a  NAT gateway  in a  public  subnet with a security group that allows access to only Amazon S3.  Update the  route tables to  use the  NAT  Gateway.

C��Create a gateway endpoint for Amazon S3 that is attached to the Vp update the I AM instance profile  policy with a  Deny action and the following condition  ke



y:

D��Create a  NAT  Gateway  in a  public subnet.  Update  route tables to  use the  NAT  Ga teway. Assign  bucket  policies for all  buckets with a  Deny  action and the following co
ndition  key:  �� ��C


���� ��Correct Answer: C  Explanation: The  best solution  is to  use a VPC Gateway  Endp o int for S3 combined with an IAM  policy that explicitly denies access to  buckets outsi de the company's AWS account.  Here's why:  1. VPC Gateway  Endpoint:  -  Provides  a s ecure private connection between the VPC and S3 (traffic stays within the AWS  netw ork). -  Eliminates the  need for a  NAT  Gateway/Internet  Gateway,  reducing cost and c om plexity. 2. IAM  Policy with  Deny  +  Condition:  -  The  policy  in Option C  uses a  Den y action with a condition like aws:Resource account (as shown in the image) to block

access to S3  buckets  not owned  by the  company's AWS account. - This ensures  EC2  i n stances can only access S3 buckets in the company's account, even if other per miss i ons exist. Why other options are worse: - A: While a gateway endpoint and IAM  poli cy work, Option A  lacks the explicit account-based denial,  making it  less secure.  -  B/
D:  Using a  NAT  Gateway adds  unnecessary cost/overhead and doesn't  restrict access   to specific AWS accounts. -  D:  Bucket  policies would  require  manual  updates for  ever y  bucket, increasing operational overhead.  Reference  Links:  [AWS  VPC  Endpoints](https: //docs.aws.amazon.com/vpc/latest/private link/vpc-endpoints.html)  [IAM  Policy Condition s](https://docs.aws.amazon.com/IAM/latest/User guide/reference_policies_condition-keys.h tml)


60.A company is  building a cloud-based application on AWS that will handle sensitive	customer data. The application uses Amazon  RDS for the database, Amazon  S3 for o bject storage, and S3  Event  Notifications that  invoke AWS  Lambda for serverless  proc essing. The company uses AWS IAM Identity Center to  manage user credentials. The development, testing, and operations teams need secure access to Amazon  RDS and Amazon S3 while ensuring the confidentiality of sensitive customer data. The solution must comply with the principle of least privilege. Which solution meets these require ments with the  LEAST operational overhead?

A��Use IAM  roles with  least  privilege to grant all the teams access. Assign IAM  roles t
o each team with customized IAM  policies defining specific  permission for Amazon  R DS and S3 object access  based on team  responsibilities.

B��Enable IAM Identity Center with an Identity Center directory. Create and configure permission sets with granular access to Amazon  RDS and Amazon S3. Assign all the t eams to groups that  have specific access with the  permission sets.
C��Create individual IAM users for each  member  in all the teams with  role-based  per missions. Assign the IAM  roles with  predefined  policies for  RDS  and S3 access to eac h user  based on user  needs. Implement IAM Access Analyzer for periodic credential e valuation.
D��Use AWS Organizations to create separate accounts for each team. Implement cros s-account IAM  roles with  least  privilege. Grant  specific  permission for  RDS  and S3 acc ess  based on team  roles and  responsibilities.
�� ��B



���� �� Correct Answer:  B  Detailed  Explanation: The  best  solution  is  B  because  it  uses AWS IAM Identity Center (formerly AWS Single Sign-On) to centrally manage access f or all teams with  minimal effort.  Here��s why:  1.  Permission  Sets: These define granul ar  permissions for AWS services  (like Amazon  RDS  and S3) and can  be  reused across accounts or teams. This aligns with the least privilege principle by granting only nec essary access. 2. Groups: Assigning teams to groups (e.g.,  Dev, Test, Ops) simplifies ac cess  management. When permissions are updated for a group, all  members inherit th e changes automatically, reducing operational overhead. 3. Identity Center  Directory: C entral izes user  management and integrates with existing directories (e.g., Active  Direct ory), avoiding the  need to create  individual IAM  users  (as  in Option C) or  manage cr oss-account  roles  (as  in Option  D).  Other options  have drawbacks:  - A:  Managing  IA

M  roles  per team  is error-prone and scales  poorly.  - C: Creating  individual IAM  users and roles for each  person is time-consuming. -  D:  Multi-account setups add complex ity unless strict  isolation is  required.  Reference  Links:  -  [AWS  IAM Identity  Center](http s://docs.aws.amazon.com/single sign on/latest/user guide/what-is.html) -  [Permission Sets] (https://docs.aws.amazon.com/single sign on/latest/user guide/permission sets.html)


61.A company has an Amazon S3  bucket that contains sensitive data files. The compa ny has an application that  runs on virtual  machines  in an on-premises data center. Th e company currently uses AWS IAM Identity Center. The application requires tempora r y access to files in the S3  bucket. The company wants to grant the application secure access to the files in the S3 bucket. Which solution will meet these requirements?

A��Create an S3  bucket  policy that  permits access to the  bucket from the  public IP  a ddress  range of the company��s on-premises data center.

B��Use IAM  Roles Anywhere to  obtain security credentials  in IAM Identity Center that grant access to the S3  bucket. Configure the virtual  machines to assume the  role  by using the AWS CLI.

C��Install the AWS CLI on the virtual  machine. Configure the AWS CLI with access  key s f rom an IAM  user that  has access to the  bucket.
D��Create an IAM user and  policy that grants access to the  bucket.  Store the access key and secret  key for the IAM  user  in AWS Secrets  Manager. Configure the applicati on to  retrieve the access  key  and secret  key at startup.
�� ��B

���� �� Correct Answer:  B  Detailed  Explanation: The  best  solution  is  B  because IAM  Rol es Anywhere allows applications outside AWS (like on-premises VMs) to securely assu me IAM  roles and obtain temporary credentials instead of using  long-term access  key s.  Here��s why:  - Temporary Access: The application gets short-lived credentials (via A WS Security Token Service) that expire automatically,  reducing security risks.  - Integrat ion with IAM Identity Center: Since the company already uses IAM Identity Center,  Ro les Anywhere can  leverage existing identities/policies without creating  new IAM  users.
 -  No  Hard-Coded  Keys:  Options  C and  D  use  long-term  access  keys, which  are  risky		if compromised. -  No  Public IP  Reliance:  Option A  depends on IP  ranges, which are  less secure and  harder to  manage than IAM-based access.  How  it works:  1.  Configure	IAM  Roles Anywhere with a trust anchor (e.g., certificates). 2. The on-premises applic ation uses AWS CLI to assume the  role, exchanging certificates for temporary cre denti als. 3. These credentials grant access to the S3  bucket  based on the  role��s  per missio ns.  Reference  Links:  [IAM  Roles Anywhere  Documentation](https://docs.aws.amazon.co m/roles anywhere/)  [Securing  Hybrid Workloads with IAM  Roles Anywhere](https://aws. amazon.com/blogs/security/aws-iam-roles-anywhere-how-to-securely-access-aws-resourc es-from-on-premises-servers/)


62.A company hosts its core network services,  including directory services and  DNS,  i n its on-premises data center. The data center is connected to the AWS Cloud using AWS  Direct Connect (DX). Additional AWS accounts are  planned that will  require quic k, cost-effective, and consistent access to these  network services. What should a sol ut ions architect implement to  meet these  requirements with the  LEAST amount of oper ational overhead?

A��Create a  DX connection  in each  new account.  Route the  network traffic to the  on- premises servers.

B��Configure VPC endpoints in t he  DX VPC for all  required  services.  Route the  networ k traffic to the on-premises servers.

C��Create a VPN connection between each  new account and the  DX VPRoute the  net work traffic to the on-premises servers.
D��Configure AWS Transit Gateway  between the accounts. Assign  DX to the transit ga teway and  route network traffic to the on-premises servers.
�� ��D



���� ��Correct Answer:  D  Detailed  Explanation: The  best  solution  is  D  (Configure AWS Transit Gateway  between the accounts. Assign  DX to the transit  gateway and  route  n etwork traffic to the on -  premises servers).  Here's why:  1. Transit  Gateway Simplifies Multi - Account  Networking: AWS Transit Gateway acts as a central  hub to connect
multiple AWS accounts/VPCs and on -  premises  networks  (via  Direct  Connect).  New a ccounts can easily attach their VPCs to the Transit Gateway, avoiding the  need to set up individual connections (like  DX or VPN) for each account. This reduces operation a l overhead significantly. 2.  Direct Connect  Reuse:  By  attaching the  Direct Connect con nection to the Transit Gateway, all connected accounts automatically gain access to t he on -  premises  network services  (directory services,  DNS,  etc.) without  needing sepa rate  DX connections or VPNs. This  is cost  - effective and ensures consistent  routing.
3. Avoid  Redundant Configurations:  Options A,  B, and C  require setting  up  new conn ections (DX, VPN, or VPC endpoints) for every  new account. This is time - consuming,

 costly, and  hard to  manage at scale. Transit Gateway centralizes  routing,  making  it t he least operational overhead choice.  Example for  Beginners: Think of Transit Gateway as a  big train station. The  Direct Connect connection is  like a dedicated train  line fr om this station to your on -  premises data center.  Every  new AWS  account  is  like a new passenger��they just  need to  hop onto the station  (Transit Gateway) to access t he train  line  (Direct Connect). Without Transit Gateway, each  passenger would  need t heir own train  line (DX/VPN), which is expensive and complicated to  build every time. Reference  Links: -  [AWS Transit Gateway](https://aws.amazon.com/transit  - gateway/)
-  [Direct Connect  + Transit  Gateway Integration](https://docs.aws.amazon.com/whitepa pers/latest/aws - vpc  - connectivity  - options/aws - direct - connect.html)


63.A company hosts its  main  public web application  in one AWS  Region across  multi ple Availability Zones. The application uses an Amazon  EC2 Auto  Scaling group and a n Application  Load  Balancer  (ALB). A web development team  needs a cost-optimized compute solution to improve the company��s ability to serve dynamic content globall y to  millions of customers. Which solution will  meet these  requirements?

A��Create an Amazon Cloud Front distribution. Configure the existing ALB as the origi

n.

B��Use Amazon  Route  53 to  serve traffic to the ALB and  EC2  instances  based  on the geographic  location of each customer.
C��Create an Amazon S3  bucket with  public  read access enabled.  Migrate the web ap plication to the S3 bucket. Configure the S3 bucket for website hosting.

D�� Use AWS  Direct Connect to directly serve content from the web application to the location of each customer.
�� ��A



���� �� Correct Answer: A. Create an Amazon Cloud Front distribution. Configure the exi sting ALB as the origin.  Detailed  Explanation: The  best  solution  is A  because Amazon Cloud Front is a Content  Delivery  Network  (CDN) designed to serve  content globally w ith  low latency and cost optimization.  Here��s why it works:  1.  Dynamic  Content Accel eration: Cloud Front doesn��t just cache static content��it also optimizes dynamic cont ent delivery.  For dynamic content  (e.g.,  personalized  pages, API  responses),  Cloud Fron t uses techniques like TCP and TLS optimizations and  persistent connections to the or igin (the ALB  in this case),  reducing  latency and  server  load. 2.  Global  Edge  Network: Cloud Front serves content from edge locations worldwide. Users connect to the near est edge  location, drastically  reducing  round  - trip time compared to directing all traf fic to a single AWS  Region.  3. Cost Savings: Cloud Front  reduces data transfer  costs  b ecause traffic from users to Cloud Front edge locations is cheaper than transferring th e same data directly from the ALB to global users. Additionally, fewer  requests to the ALB (due to optimized connections)  mean lower compute costs for  EC2  instances. W hy other options are  not  ideal:  -  B  (Route  53  Geo  Routing):  While  Route  53  can  rout e users  based on geography, it still directs traffic to the original ALB  in one AWS  Re gion. This doesn��t  reduce  latency or costs for users far from that  Region.  -  C (S3 St atic  Hosting): S3  is great for static websites,  but the question specifies dynamic conte nt, which  requires  backend  processing  (EC2  instances).  S3 can��t  replace this. -  D  (Dir ect Connect):  Direct Connect creates a  private  network connection  between AWS and

on -  premises infrastructure. It doesn��t  help with global  public web traffic and  is cos t -  prohibitive for this use case.  Reference  Link:  [Amazon  Cloud front  Features](https:// aws.amazon.com/cloud front/features/)

64.A company stores  user data in AWS. The data  is  used continuously with  peak  usa ge during  business  hours. Access  patterns vary, with  some data  not  being  used for  m onths at a time. A solutions architect  must choose a cost-effective solution that  maint ains the highest  level of durability while maintaining high availability. Which storage s ol ution meets these  requirements?

A��Amazon S3 Standard

B��Amazon S3 Intelligent-Tiering

C��Amazon S3 Glacier  Deep Archive

D��Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)

�� ��B



���� �� Correct Answer:  B. Amazon S3 Intelligent-Tiering  Detailed  Explanation: Amazon S3 Intelligent-Tiering is designed for data with unpredictable or varying access  patter ns.  Here's why  it fits  best:  1.  Cost  Efficiency:  - Automatically  moves objects  between  4 tiers (Frequent, Infrequent, Archive,  Deep Archive)  based on  last accessed time.  -  D ata unused for 30+ days  moves to cheaper tiers, saving  money without  manual effort.
 2.  Durability �� Availability: - All tiers (except One Zone-IA) offer 99.999999999% (11 9's) durability. -  Maintains  high availability with  milliseconds access for  Frequent/Inf re quent tiers. 3.  Problem-Specific  Fit:  -  Peak  usage  during  business  hours  ��  Frequently

accessed data stays  in the  Frequent tier.  -  Data  unused for  months �� Automatically t ransit ions to  low-cost Archive tiers.  -  No  retrieval  delays  (unlike Glacier options) for a ctive data. Why not others: - A  (Standard):  Expensive for  rarely  accessed data.  - C  (Gl acier):  Retrieval  delays (hours/days) conflict with continuous  use  requirement.  -  D  (On e Zone-IA):  Lower durability (99.5%) violates the  highest durability  requirement.  Refere nce  Links: -  [S3 Intelligent-Tiering](https://aws.amazon.com/s3/storage-classes/intellige n t-tiering/) -  [S3 Storage Classes Comparison](https://aws.amazon.com/s3/storage-classe s/)

65.A company is testing an application that  runs on an Amazon  EC2  Linux  instance. A	single  500 GB Amazon  Elastic  Block  Store  (Amazon  EBS)  General  Purpose  SSO  (gp2) volume is attached to the  EC2  instance. The company will deploy the application on multiple  EC2 instances  in an Auto Scaling group. All  instances  require access to the d ata that  is stored  in t he  EBS volume. The  company needs a  highly available and  resili ent solution that does not introduce significant changes to the application's code. Wh ich solution will meet these  requirements?

A�� Provision an  EC2  instance that  uses  NFS  server  software. Attach a single  500 GB g p2  EBS volume to the  instance.

B�� Provision an Amazon  FSx for Windows  File  Server file system. Configure the file sy stem as an SMB file store within a single Availability Zone.

C�� Provision an  EC2  instance with two 250 GB  Provisioned IOPS  SSD  EBS volumes.

D�� Provision an Amazon  Elastic  File  System  (Amazon  EFS) file  system. Configure the fi le system to  use General  Purpose  performance  mode.

�� ��D



���� �� Correct Answer  D  Detailed  Explanation The scenario  requires a  highly available and  resilient shared storage solution for  multiple  EC2  instances  in an Auto Scaling gr oup.  Here��s why Amazon  EFS  (Option  D)  is  the  best choice:  1.  Shared Access  - Amaz on  EFS  is a fully  managed  network file system  (NFS) that allows  multiple  EC2  instance s to access the same data simultaneously. This is critical for applications  running in a n Auto Scaling group where instances scale  up/down dynamically. - In contrast,  EBS v ol umes (Options A, C) are  block storage devices that can only  be attached to one  EC 2 instance at a time,  making them unsuitable for shared access. 2.  High Availability
&  Durability  -  EFS  automatically  replicates  data  across  multiple Availability Zones  (AZs) within a region, ensuring high availability and protection against AZ failures. - Optio ns like  FSx for Windows  (B) or a  single-instance  NFS  server  (A)  are  limited to a  single AZ or introduce a single  point of failure. 3.  No Code Changes  -  EFS  uses the standa rd  NFS  protocol, which  is  natively supported  by  Linux. The  application can access the 	shared file system just  like a  local directory,  requiring  no significant code changes.  - Options like  FSx for Windows  (B) rely on SMB/CIFS  protocols, which are  not ideal fo r  Linux workloads and  might  require configuration adjustments. 4. Cost-Effective  Scali ng -  EFS  scales automatically as data grows  (no  manual  provisioning), and  its General Purpose performance  mode (used in Option  D)  is optimized for  most  Linux-based w ork loads. Why Other Options  Fail  - Option A: A single  EC2  instance with  an  EBS vol u me as an  NFS  server  is  not  highly available and  introduces a single  point  of failure.  - Option  B:  FSx for Windows is designed for Windows environments and uses SMB, w hich is not suitable for  Linux-based applications.  - Option C:  Using  multiple  EBS vol u

mes on a single  EC2  instance does not solve the shared storage  requirement for Aut o Scaling.  Reference  Link  [Amazon  EFS  Overview](https://aws.amazon.com/efs/)


66.A company runs database workloads on AWS that are the backend for the compa ny's customer  portals. The company  runs a  Multi-AZ  database cluster on Amazon  RD S for  Postgresql. The company  needs to  implement a  30-day  backup  retention  policy. The company currently has both automated  RDS  backups and  manual  RDS  backups. The company wants to  maintain  both types of existing  RDS  backups that  are  less tha n 30 days old. Which solution will meet these  requirements  MOST cost-effectively?

A��Configure t he  RDS  backup  retention  policy to  30  days for automated  backups  by using AWS  Backup.  Manually  delete  manual  backups that are older than  30 days.

B�� Disable  RDS automated  backups.  Delete  automated  backups and  manual  backups t hat are older than 30 days. Configure t he  RDS  backup  retention  policy  to  30 days for automated backups.

C��Configure t he  RDS  backup  retention  policy  to 30 days for automated  backups.  Ma nually delete manual  backups that are older than 30 days.
D��Disable  RDS automated  backups.  Delete  automated  backups and  manual  backups t hat are older than 30 days automatically  by  using AWS Cloud Formation. Configure th e  RDS  backup  retention  policy to  30  days for automated  backups.
�� ��C



���� �� Correct Answer: C  Detailed  Explanation: The  best solution is to set t he  RDS aut omated backup  retention  policy to  30 days and  manually delete old  manual  backups.

 Here's why: 1. Automated  Backups:  RDS automatically  manages these when you set t he  retention  period  (1  -  35 days).  Backups  older than  30  days will  be auto  - deleted by AWS at  no extra effort/cost. 2.  Manual  Snapshots: These don't expire automatically. The company must manually delete those older than 30 days to comply with the po licy. Other options are  less optimal:  - Option A unnecessarily  involves AWS  Backup  (e xtra complexity/cost for simple  RDS  backups).  -  Options  B ��  D  mistakenly  disable au tomated backups, which would stop  new  backups from  being created and  might de le te existing ones  prematurely. - Option  D adds Cloud Formation for deletion tasks that 	can  be done  manually  (over  -  engineering).  Reference  Links:  -  [Amazon  RDS  Backup Documentation](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_Worki ng with automated backups.html) -  [Manual Snapshot  Management](https://docs.aws.am azon.com/Amazon rds/latest/User guide/USER_Delete snapshot.html)


67.A company is  planning to  migrate a  legacy application to AWS. The application cu rrently uses  NFS to communicate to an on-premises storage  solution to store applicat ion data. The application cannot  be  modified to  use any other communication  protoc ols other than  NFS for this  purpose. Which storage solution should a solutions archit ect  recommend for use after the  migration?

A��AWS  Data sync

B��Amazon  Elastic  Block  Store  (Amazon  EBS) C��Amazon  Elastic  File  System  (Amazon  EFS)

D��Amazon  EMR  File  System  (Amazon  EMRFS)

�� ��C

���� ��Correct Answer: C. Amazon  Elastic  File  System (Amazon  EFS)  Detailed  Explana tio n: Amazon  EFS  is a fully  managed  NFS  (Network  File  System)-compatible  storage serv ice. Since the  legacy application  requires  NFS  protocol  and cannot  be  modified,  EFS  i s the  best choice. It  provides shared file storage accessible  by  multiple  EC2  instances or on-premises servers via AWS  Direct Connect/VPN,  making  it ideal for lift-and-shift migrations. Other options  like  EBS  (block storage) or  Data sync  (data transfer tool) do n��t  natively support  NFS.  EMRFS  is  specific to  Hadoop  and  unrelated to  NFS.  Referen ce  Link:  [Amazon  EFS  Features](https://aws.amazon.com/efs/)


68.A company uses GPS trackers to document the migration patterns of thousands of	sea turtles. The trackers check every  5  minutes to  see  if a turtle  has  moved  more th an  100 yards  (91.4  meters). If a turtle  has  moved,  its tracker  sends the  new coordina t es to a web application running on three Amazon  EC2  instances that are  in  multiple Availability Zones in one AWS  Region.  Recently, the web application was overwhelm e
d while processing an unexpected volume of tracker data.  Data was  lost with  no way	to replay the events. A solutions architect  must  prevent this  problem from  happen in g again and needs a solution with the least operational overhead. What should the s ol utions architect do to  meet these  requirements?

A��Create an Amazon S3 bucket to store the data. Configure the application to scan f or  new data  in the  bucket for  processing.

B��Create an Amazon API Gateway endpoint to  handle transmitted  location coordinate
s.  Use an AWS  Lambda function to  process  each  item concurrently.

C��Create an Amazon Simple Queue Service (Amazon SQS) queue to store the incomi ng data. Configure the application to  poll for  new  messages for  processing.
D��Create an Amazon  Dynamo db table to store transmitted  location coordinates. Conf igure the application to query the table for new data for  processing.  Use TTL to  rem ove data that  has  been  processed.
�� ��C



���� ��Correct Answer: C  Detailed  Explanation: The  best solution is to use Amazon SQS		(Simple Queue Service)  because it acts as a  buffer  between the GPS trackers and th e web application.  Here��s why:  -  Problem: The web application  (EC2  instances) was o ver whelmed by sudden spikes in data, causing data  loss.  Directly  handling  incoming  r equests can overload servers.  - Solution: SQS decouples data  producers  (trackers) and	consumers (EC2 instances). Trackers send messages to the queue, and  EC2  instances poll the queue at their own  pace. This  prevents overload.  -  Key  Benefits:  -  No  Data  L oss: SQS  retains  messages  (up to  14 days)  until  processed,  allowing  replay  if process i ng fails. - Auto-Scaling:  EC2  instances can scale  based on the queue  length  (using A WS Auto Scaling), ensuring they  handle the workload efficiently. -  Minimal Operation a l Overhead: SQS is fully managed��no servers to  maintain.  - Cost-Effective:  Pay only f or what you use, with  no  upfront costs. Why  not other options?  - A  (S3):  S3  isn��t  de signed for real-time event  processing. Scanning for  new data adds  latency and compl exity. -  B  (API Gateway  +  Lambda):  While  serverless  (Lambda) scales well,  refactoring  the existing  EC2-based app to  use  Lambda would  require  significant code changes, in creasing operational effort. -  D  (Dynamo db): Querying  Dynamo db for  new  data  is  les s efficient than using a queue. TTL (time-to-live) deletes data automatically, risking da

ta  loss  if processing is delayed.  Reference  Links:  -  [Amazon  SQS  Features](https://aws. amazon.com/sqs/features/) -  [Decoupling Applications with SQS](https://docs.aws.amaz on.com/white papers/latest/serverless-migration-aws-sqs/decoupling-components-with-a mazon-sqs.html)


69.A company's software development team  needs an Amazon  RDS  Multi-AZ  cluster.  T he  RDS  cluster will serve as a  backend for a desktop client that  is deployed on  pre mises. The desktop client  requires direct connectivity to t he  RDS cluster. The compan y  must give the development team the ability to connect to the cluster  by  using the client when the team is in the office. Which solution provides the  required connect iv it y  MOST securely?

A��Create a VPC and two  public  sub nets. Create t he  RDS  cluster  in the  public  sub nets. Use AWS Site-to-Site VPN with a customer gateway in the company's office.

B��Create a VPC and two  private  sub nets. Create t he  RDS  cluster  in the  private  sub ne ts.  Use AWS Site-to-Site VPN with a customer gateway  in the company's office.

C��Create a VPC and two  private sub nets. Create t he  RDS  cluster  in the  private sub ne ts.  Use  RDS  security groups to allow the company's office IP  ranges to access the clu ster.
D��Create a VPC and two  public sub nets. Create t he  RDS  cluster  in the  public subnet s. Create a cluster user for each developer.  Use  RDS  security groups to allow the  use rs to access the cluster.
�� ��B

���� �� Correct Answer:  B  Detailed  Explanation: The  most  secure solution is to  place th e  RDS cluster  in  private subnets and  use AWS  Site-to-Site VPN.  Here��s why:  -  Privat e sub nets ensure t he  RDS  cluster isn��t directly exposed to the  public internet,  red uci ng attack surface. - Site-to-Site VPN creates an encrypted tunnel  between the compa ny��s office (via a customer gateway) and the VPC, allowing secure access to t he  RDS	cluster in the  private sub nets.  - Other options are  less  secure:  - A/D:  Placing  RDS  in public sub nets (even with VPN) risks exposure if misconfigured. - C: Allowing office I Ps via security groups would  require  public  RDS endpoints, exposing  it to the  interne t.  Reference  Links:  -  [AWS  Site-to-Site VPN](https://docs.aws.amazon.com/vpn/latest/s2 svpn/VPC_VPN.html) -  [Amazon  RDS  in VPC](https://docs.aws.amazon.com/Amazon rds /latest/User guide/USER_VPC.html)


70.A solutions architect is creating an application that will handle batch  processing of	large amounts of data. The  input data will  be  he ld  in Amazon S3 and the output da ta will be stored in a different S3 bucket.  For  processing, the application will transfer the data over the  network  between  multiple Amazon  EC2  instances. What should the solutions architect do to  reduce the overall data transfer costs?
A�� Place all the  EC2  instances  in an Auto  Scaling group.

B�� Place all the  EC2  instances  in the  same AWS  Region.

C�� Place all the  EC2  instances  in the same Availability Zone.

D�� Place all the  EC2  instances in  private subnets  in  multiple Availability Zones.

�� ��C

���� ��Correct Answer C.  Place all the  EC2  instances  in the  same Availability Zone.  Det ailed  Explanation To  reduce data transfer costs  in AWS, you  need to  minimize cross-n etwork traffic.  Here��s why:  - Option C  (same Availability Zone): When  EC2  instances a re in the same AZ, data transferred  between them incurs  no charges.  S3-to-EC2 data transfers in the same AWS  Region are also free  (regardless of AZ).  However,  if  EC2  in stances are  in different AZs, data transferred  between them  is  billed at  inter-AZ data  transfer  rates, which can add significant costs for large  batch  processing.  - Option A  (Auto Scaling group): Auto Scaling manages instance counts  but doesn��t directly  red uce costs  unless configured with  placement groups  (not  mentioned  here).  - Option  B (same AWS  Region): While  S3 and  EC2  in the same  Region avoid  cross-Region costs, EC2 instances in different AZs within the  Region still  incur  inter-AZ fees.  - Option  D   (private subnets in multiple AZs): This setup increases inter-AZ data transfer costs,  ma king it worse for cost optimization.  By  keeping all  EC2  instances  in the same AZ, you eliminate inter-AZ data transfer fees while still allowing free S3-to-EC2 transfers (s inc e S3 and  EC2 are  in the  same  Region).  Reference  Links  -  [AWS  Data  Transfer  Pricing] (https://aws.amazon.com/ec2/pricing/on-demand/#Data_Transfer) -  [Amazon  S3  Pricing]
(https://aws.amazon.com/s3/pricing/)



71.A company hosts a  multi-tier web application that  uses an Amazon Aurora  MySQL DB cluster for storage. The application tier is hosted on Amazon  EC2 instances. The company's IT security guidelines mandate that the database credentials be encrypted and  rotated every  14 days. What should a  solutions architect do to  meet this  require ment with the  LEAST operational effort?

A��Create a  new AWS  Key  Management  Service  (AWS  KMS)  encryption  key.  Use  AWS Secrets  Manager to create a  new secret that  uses the  KMS  key with the  appropriate credentials. Associate the secret with the Aurora  DB cluster. Configure a custom  rotati on  period of  14 days.

B��Create two  parameters  in AWS Systems  Manager  Parameter  Store:  one for the  user name as a string  parameter and one that uses the Secure string type for the  passwo
rd. Select AWS  Key  Management  Service  (AWS  KMS)  encryption for the  password  par ameter, and load these  parameters in the application tier. Implement an AWS  Lambda	function that  rotates the  password every  14 days.
C��Store a file that contains the credentials in an AWS  Key  Management  Service  (AWS KMS) encrypted Amazon  Elastic  File  System (Amazon  EFS) file system.  Mount the  EFS	file system in all  EC2  instances of t he application tier.  Restrict the access to the file on the file system so that the application can  read the file and that only super  users can  modify the file. Implement an AWS  Lambda function that  rotates the  key  in Aur ora every  14 days and writes  new credentials  into the file.
D��Store a file that contains the credentials in an AWS  Key  Management  Service  (AWS KMS) encrypted Amazon S3 bucket that the application uses to load the credentials. Download the file to the application regularly to ensure that the correct credentials  are used. Implement an AWS  Lambda function that  rotates the Aurora credentials eve ry  14 days and uploads these credentials to the file  in the S3  bucket.
�� ��A

���� �� Correct Answer A  Detailed  Explanation The best solution is A  because AWS Sec rets  Manager  provides  built  -  in features for automatic credential  rotation with  minim al operational effort.  Here's why:  1. Automatic  Rotation:  Secrets  Manager  can automat ically rotate database credentials every  14 days without  requiring custom code. This e limina tes t he  need to write and  maintain a  Lambda  function (as  in options  B, C, and D). 2.  Encryption:  Secrets  Manager  integrates with AWS  KMS to  encrypt secrets  by de fault, ensuring compliance with the encryption  requirement. 3. Integration with Aurora: Secrets  Manager can directly associate secrets with Aurora  DB clusters, simplifying cr e dential  management for the application tier. 4. Operational Simplicity:  Unlike other o ptions that  require  manual file storage (EFS/S3),  mounting file systems, or custom  La mbda  logic, Secrets  Manager  handles encryption,  rotation, and access control out  - of	- the  -  box. Options  B,  C, and  D  involve  extra steps  like writing  Lambda functions,
managing file systems, or  manually updating S3 files, which increase operational com plexity and maintenance.  Reference  Link  -  [AWS  Secrets  Manager](https://aws.amazon. com/secrets-manager/)


72.A streaming media company is rebuilding its infrastructure to accommodate increa sing demand for video content that users consume daily. The company needs to  proc ess terabyte-sized videos to  block some content in the videos. Video  processing can t ake up to 20  minutes. The company  needs a solution that will scale with demand an
d  remain cost-effective. Which solution will meet these requirements?

A��Use AWS  Lambda functions to  process videos.  Store video  metadata  in Amazon  Dy namoDB. Store video content in Amazon S3 Intelligent-Tiering.

B��Use Amazon  Elastic Container  Service  (Amazon  ECS) and AWS  Fargate  to  implemen t micro services to  process videos. Store video  metadata  in Amazon Aurora. Store vide o content in Amazon S3 Intelligent-Tiering.
C��Use Amazon  EC2  instances  in an Auto Scaling group  behind an Application  Load  B a lancer (ALB) to  process videos. Store video content  in Amazon  S3 Standard.  Use Am azon Simple Queue Service (Amazon SQS) for queuing and to decouple processing ta sks.
D��Deploy a containerized video  processing application on Amazon  Elastic  Ku bernet es Service (Amazon  EKS)  on Amazon  EC2.  Store video  metadata  in Amazon  RDS  in  a sin gle Availability Zone. Store video content in Amazon S3 Glacier  Deep Archive.
�� ��B



���� ��Correct Answer  B  Detailed  Explanation The  best  solution  is  B  because  it  balance s scalability, cost  - effectiveness, and technical  requirements for  processing  large vide o files.  Here��s why:  1. Amazon  ECS and AWS  Fargate:  These services allow the comp any to run containerized  micro services without  managing servers.  Fargate  scales auto matically based on demand, ensuring  resources are  used efficiently. Video  processing  (which can take  up to 20  minutes)  benefits from  Fargate��s  ability to  handle  long  -  r unning tasks, unlike AWS  Lambda  (option A), which  has a  15  -  minute  execution  limit.
 2. Amazon Aurora: A  managed  relational database that scales seamlessly and offers high availability for storing video metadata. This ensures reliability and performance f or frequent read/write operations. 3. Amazon S3 Intelligent - Tiering: Automatically op timizes storage costs  by  moving video content  between access tiers  (frequent,  inf requ

ent, archive)  based on usage  patterns. This  is  more cost  - effective than S3 Standard (option C) or Glacier  Deep Archive  (option  D), which are either too expensive for freq uent access or too slow for active content. Option C (EC2  + ALB)  requires  manual  sc aling and infrastructure  management, increasing operational overhead. Option  D��s  us e of S3 Glacier  Deep Archive  is  unsuitable for daily video access due to  high  retrieval	latency.  Reference  Links  - AWS  Fargate:  [AWS  Fargate](https://aws.amazon.com/fargat e/) - Amazon Aurora:  [Amazon Aurora](https://aws.amazon.com/rds/aurora/)  - Amazon S3 Intelligent - Tiering:  [S3 Intelligent - Tiering](https://aws.amazon.com/s3/storage  - classes/intelligent - tiering/)


73.A company runs an on-premises application on a  Ku bernet es cluster. The company		recently added  millions of new customers. The company's existing on-premises infras tructure is unable to  handle the  large  number of new customers. The company  needs	to migrate the on-premises application to the AWS Cloud. The company will migrate	to an Amazon  Elastic  Ku bernet es  Service  (Amazon  EKS)  cluster. The company does  n ot want to  manage the  underlying compute infrastructure for the  new architecture on AWS. Which solution will meet these requirements with the  LEAST operational overh ead?

A�� Use a self-managed node to supply compute capacity.  Deploy the application to t he  new  EKS  cluster.

B�� Use  managed  node groups to supply compute capacity.  Deploy the  application to the  new  EKS  cluster.

C��Use AWS  Fargate to  supply compute capacity. Create a  Fargate  profile.  Use  the  Far gate  profile to deploy the application.
D��Use  managed  node groups with  Karp enter to supply compute  capacity.  Deploy the application to the  new  EKS cluster.
�� ��C



���� ��Correct Answer C  Explanation The company wants to avoid  managing the under lying compute infrastructure (like servers or virtual  machines) for their Amazon  EKS cl uster. AWS  Fargate  (option C)  is a  serverless compute engine that automatically provi sions and scales the underlying infrastructure for  Ku bernet es  pods. With  Fargate, you don��t need to  manage  node groups,  EC2  instances,  or scaling  policies��AWS  handles all of this. - Option A (self-managed  nodes):  Requires  manual  setup and  management of EC2 instances, which adds operational overhead. - Option  B  (managed node grou ps): AWS  manages the  EC2  instances,  but you still  have to  configure and  maintain  no de groups (e.g., scaling,  updates).  - Option  D  (managed  node groups  +  Karp enter):  K arp enter automates  node  provisioning,  but you��re still  responsible for  node-related c on figurations. - Option C (Fargate):  Eliminates  node  management entirely. You  only d efine  how your  pods should  run, and AWS  handles the  rest.  Reference  Links  -  [AWS   Fargate for Amazon  EKS](https://docs.aws.amazon.com/eks/latest/user guide/fargate.html)
-  [Managed  Node Groups vs.  Fargate](https://aws.amazon.com/eks/features/fargate/)



74.A company is  launching a  new application that  requires a structured database to s tore user  profiles, application settings, and transactional data. The database  must  be s

calable with application traffic and  must offer  backups. Which solution will  meet thes e  requirements  MOST cost-effectively?
A��Deploy a self-managed database on Amazon  EC2  instances  by  using open source s oftware. Use Spot Instances for cost optimization. Configure automated  backups to A mazon S3.

B��Use Amazon  RDS.  Use  on-demand capacity  mode for the  database with General  Pu rpose SSD storage. Configure automatic  backups with a  retention  period of 7 days.

C�� Use Amazon Aurora Serverless for the database.  Use serverless capacity scaling. Co nfigure automated  backups to Amazon S3.
D�� Deploy a self-managed  NoSQL database on Amazon  EC2  instances.  Use  Reserved  I n stances for cost optimization. Configure automated backups directly to Amazon S3 G lacier  Flexible  Retrieval.
�� ��C



���� ��Correct Answer: C  Explanation: The  best choice is Amazon Aurora Serverless (Op tion C)  because it  perfectly  balances scalability, cost-effectiveness, and  managed  back ups.  Here's why:  1. Serverless Auto-Scaling: Aurora Serverless automatically adjusts dat abase capacity  based on traffic. This  means you don��t  pay for  unused  resources  (ide al for unpredictable workloads),  unlike  manually scaling  EC2  instances  (Options A/D) o r fixed-size  RDS  instances  (Option  B).  2.  Fully  Managed  Backups: Aurora  automatically backs up data to Amazon S3 and retains backups for the duration of your retention period. This is simpler and more reliable than  manually configuring backups (Option A) or using Glacier (Option  D, which  is slower and  less suited for frequent  backups).

3. Cost Optimization: With Aurora Serverless, you only  pay for the database  resources you actually use. This is more cost-effective than  running always-on  EC2 instances (e ven with Spot/Reserved Instances in A/D) or  RDS on-demand  (Option  B) during  low-t raffic periods. 4. Structured  Database: Aurora  is a  relational database  (supports  SQL), which fits the  requirement for structured data storage. Option  D��s  NoSQL  database  i s unsuitable  here. Why Other Options Are Worse: - A/D: Self-managed databases on EC2 require you to  handle scaling,  backups, and  maintenance,  increasing operational c osts and complexity.  -  B:  RDS  on-demand  is easier than  EC2  but  still  requires  manual	scaling and incurs costs for idle  resources. Aurora  Serverless is cheaper and  more sc alable.  Reference  Links:  -  [Amazon Aurora  Serverless](https://aws.amazon.com/rds/auror a/serverless/) -  [AWS  Database  Services](https://aws.amazon.com/products/databases/)


75.A company runs its  legacy web application on AWS. The web application server  ru ns on an Amazon  EC2  instance  in the  public subnet of a VPC. The web application s erver collects  images from customers and stores the image files in a  locally attached Amazon  Elastic  Block  Store  (Amazon  EBS) volume. The  image files are  uploaded  every night to an Amazon S3 bucket for backup. A solutions architect discovers that the i mage files are  being  uploaded to Amazon S3 through the  public endpoint. The sol uti ons architect  needs to ensure that traffic to Amazon S3 does  not use the  public end point. Which solution will  meet these requirements?

A��Create a gateway VPC endpoint for the S3 bucket that  has the  necessary  per miss i ons for the VPC. Configure the subnet  route table to use the gateway VPC endpoint.

B�� Move the S3  bucket inside the VPC. Configure the subnet  route table to access th e S3 bucket through  private IP addresses.

C��Create an Amazon S3 access  point for the Amazon  EC2  instance  inside the VPConf igure the web application to upload by using the Amazon S3 access  point.
D��Configure an AWS  Direct Connect connection  between the VPC that  has the Amaz on  EC2  instance and Amazon S3 to  provide a dedicated  network  path.
�� ��A



���� �� Correct Answer: A  Detailed  Explanation: The correct solution is to create a Gate way VPC  Endpoint for Amazon S3 and configure the subnet  route table to  use  it.  Her e��s why: 1. Gateway VPC  Endpoints allow  EC2  instances  in a VPC to communicate wi th S3 without using the public internet. Instead, traffic stays within the AWS  private  n etwork. 2. S3 Gateway  Endpoints are free and act  like  a virtual  router  in your VPC.  By adding a route to the subnet��s  route table that directs S3 traffic (via the endpoint), the  EC2  instance will automatically use the  private AWS  network  instead of the  public endpoint. 3. Other options are incorrect:  -  B: S3  buckets cannot  be  moved into a VP C��they are  regional services. Access  is controlled via endpoints or  policies,  not  physi cal  placement.  - C:  S3 Access  Points  simplify  permissions  but  don��t change  network  r outing (traffic still goes through  public internet unless a VPC endpoint is used).  -  D: Direct Connect is for connecting on -  premises  networks to AWS,  not  needed  here  (b oth  EC2 and  S3 are already  in AWS).  Reference  Link:  [AWS  VPC  Endpoints for  S3](htt
ps://docs.aws.amazon.com/vpc/latest/private link/vpc - endpoints - s3.html)



76.A company is creating a  prototype of an ecommerce website on AWS. The website	consists of an Application  Load  Balancer, an Auto  Scaling group of Amazon  EC2  inst ances for web servers, and an Amazon  RDS for  MySQL  DB  instance that  runs  with th

e Single-AZ configuration. The website is slow to respond during searches of the  pro duct catalog. The  product catalog  is a group of tables in the  MySQL database that th e company does  not update frequently. A solutions architect  has determined that the CPU utilization on the  DB  instance is high when product catalog searches occur. Wh at should the solutions architect  recommend to improve the  performance of the webs ite during searches of the product catalog?
A��Migrate the  product catalog to an Amazon  Redshift  database.  Use the COPY  comm and to  load the  product catalog tables.

B��Implement an Amazon  Elastic ache for  Redis  cluster to cache the  product catalog. Use  lazy  loading to  populate the cache.

C��Add an additional scaling policy to the Auto Scaling group to  launch additional  EC 2 instances when database  response is slow.
D��Turn on the  Multi-AZ configuration for the  DB  instance.  Configure the  EC2  instanc es to throttle the product catalog queries that are sent to the database.
�� ��B



���� ��Correct Answer:  B. Implement an Amazon  Elastic ache for  Redis cluster to  cache the product catalog.  Use  lazy  loading to  populate the  cache.  Detailed  Explanation: Th e  problem is  high CPU  usage on the  MySQL database during  product catalog searche s. Since the  product catalog  is  rarely  updated,  repeatedly querying the database for t he same data  is inefficient. Option  B  solves this  by  introducing a caching  layer  (Elasti Cache for  Redis). Caching stores frequently accessed data  in  memory,  reducing direct database queries.  Lazy  loading  means the cache  is  populated  only when data  is  requ

ested,  minimizing initial  load and ensuring only  popular  items are cached. This  reduce s database  load,  lowers CPU  usage, and speeds up  responses. Why other options are	incorrect: A: Amazon  Redshift  is for analytics,  not  real-time OLTP queries  like ecomm erce searches.  Migrating is  unnecessary complexity. C: Adding  EC2  instances won��t fi x high database CPU usage��it  might even worsen the  problem  by sending  more que ries.  D:  Multi-AZ  improves  database availability (failover),  not  performance. Throttling  queries would slow down the website.  Reference  Link:  [Amazon  Elastic ache  Use  Cases] (https://aws.amazon.com/elastic ache/use-cases/)


77.A company currently stores  5 TB of data  in on-premises  block storage systems. Th e company's current storage solution provides limited space for additional data. The c ompany runs applications on  premises that  must  be able to  retrieve frequently access ed data with  low  latency. The company  requires a cloud-based storage solution. Whic h solution will meet these requirements with the  MOST operational efficiency?

A�� Use Amazon S3  File Gateway. Integrate S3  File Gateway with the on-premises appl ications to store and directly retrieve files  by  using the SMB file system.

B��Use an AWS Storage Gateway Volume Gateway with cached volumes as  i SCSI targe
ts.

C�� Use an AWS Storage Gateway Volume Gateway with stored volumes as i SCSI targe ts.
D�� Use an AWS Storage Gateway Tape Gateway. Integrate Tape Gateway with the on- premises applications to store virtual tapes in Amazon S3.
�� ��B

���� �� Correct Answer:  B  Detailed  Explanation: The company  needs a cloud-based  bloc k storage solution that integrates with their on-premises applications while  minimizing	local storage usage.  Here's why Volume Gateway with cached volumes  (Option  B)  is  the best choice: 1.  Block Storage Compatibility:  - The existing on-premises application s use  block storage, so Volume Gateway's i SCSI interface  maintains compatibility with out requiring application changes. 2.  Local Caching for  Low  Latency:  -  Cached volume s store frequently accessed data  locally (low  latency) while  keeping the full dataset  in Amazon S3. This reduces local storage  requirements (ideal since the company has li mited on-premises space).  3. Cost �� Scalability:  - Only  metadata and  hot data are  st ored on-premises,  minimizing  local storage costs. The 5 TB dataset scales seamlessly i n the cloud. Why Other Options  Fail:  - Option A  (S3  File  Gateway):  Designed for file  storage (SMB/NFS),  not block storage.  - Option C (Stored Volumes):  Requires storing  the entire dataset  locally, which the company can��t support due to  limited space.  - Option  D  (Tape Gateway): Optimized for  backups/archives  (tape-like access),  not  low-l atency data  retrieval.  Reference  Links:  -  [AWS  Storage  Gateway  - Volume  Gateway](htt ps://aws.amazon.com/storage gateway/volume-gateway/) -  [Cached vs.  Stored Volumes] (https://docs.aws.amazon.com/storage gateway/latest/user guide/Storage gateway concepts. html#volume-gateway-concepts)


78.A company operates a food delivery service.  Because of recent growth, the compa ny's order  processing system is experiencing scaling problems during peak traffic  hou rs. The current architecture includes Amazon  EC2  instances in an Auto Scaling group that collect orders from an application. A second group of EC2 instances in an Auto Scaling group fulfills the orders. The order collection process occurs quickly,  but the

order fulfillment  process can take  longer.  Data  must  not  be  lost  because  of  a  scaling event. A solutions architect  must ensure that the order collection process and the or der fulfillment process can  both scale adequately during  peak traffic  hours. Which sol ution will meet these requirements?
A��Use Amazon Cloud watch to  monitor the Cpu utilization  metric for each  instance  in both Auto Scaling groups. Configure each Auto Scaling group's  minimum capacity to meet its  peak workload value.

B��Use Amazon Cloud watch to  monitor the Cpu utilization  metric for each  instance  in both Auto Scaling groups. Configure a Cloud watch alarm to invoke an Amazon Simpl e  Notification Service  (Amazon  SNS) topic to create additional Auto Scaling groups o n demand.

C�� Provision two Amazon Simple Queue Service (Amazon SQS) queues.  Use one SQS queue for order collection.  Use the second SQS queue for order fulfillment. Configure	the  EC2  instances to  poll their  respective queues.  Scale the Auto Scaling groups  bas ed on  notifications that the queues send.
D�� Provision two Amazon Simple Queue Service (Amazon SQS) queues.  Use one SQS queue for order collection.  Use the second SQS queue for order fulfillment. Configure	the  EC2  instances to  poll their  respective queues.  Scale the Auto Scaling groups  bas ed on the  number of messages in each queue.
�� ��D



���� ��Correct Answer:  D  Detailed  Explanation: The  best  solution is to use two Amazon SQS queues to decouple the order collection and fulfillment  processes.  Here's why:  1.

 Decoupling with SQS: - Order collection  EC2  instances can  place orders  into the first SQS queue. This allows the collection process to scale independently because it only needs to  handle quick order submissions. - Order fulfillment  EC2 instances  pull order s from the second SQS queue. Since fulfillment takes  longer, this ensures slower  proc essing doesn��t  block the collection  process. 2. Auto Scaling  Based  on Queue  Depth:
- Auto Scaling groups for  both  EC2 groups can  scale  based on the  number of  messa ges in their respective queues (e.g.,  using the `Approximate number of Messages` Cloud Watch  metric).  -  For example: If the fulfillment queue  has  100  pending  orders, Auto S caling adds  more  EC2  instances to  process them faster. If the queue empties,  it scale s down to save costs. 3.  No  Data  Loss:  -  SQS  retains  messages  until  they��re  success f ully processed.  Even  if an  EC2  instance  is terminated during scaling,  unprocessed  mes sages  remain in the queue and are  picked  up  by other instances after the visibility ti meout expires. Why Other Options  Fail:  - A:  Scaling  based on CPU usage doesn��t dir ectly track order  backlog. Setting  minimum capacity to  peak  is costly and inflexible.  - B:  Dynamically creating new Auto Scaling groups via SNS is overly complex and erro r -  prone.  - C:  Notifications are vague; scaling  must directly track  queue depth to ens ure responsiveness.  Reference  Links:  -  [AWS Auto  Scaling with  SQS](https://docs.aws.a mazon.com/auto scaling/ec2/user guide/as - using - sqs  - queue.html) -  [SQS for  Decou pling Components](https://aws.amazon.com/sqs/)


79.An online gaming company is transitioning user data storage to Amazon  DynamoD B to support the company's growing user  base. The current architecture includes  Dyn amoDB tables that contain user profiles, achievements, and in-game transactions. The company needs to design a  robust, continuously available, and  resilient  Dynamo db ar

chitecture to  maintain a seamless gaming experience for users. Which solution will  m eet these  requirements  MOST cost-effectively?
A��Create  Dynamo db tables  in a single AWS  Region.  Use  on-demand capacity  mode. Use global tables to  replicate data across  multiple  Regions.

B��Use  Dynamo db Accelerator  (DAX) to  cache frequently accessed data.  Deploy tables in a single AWS  Region and enable auto scaling. Configure Cross-Region  Replication manually to additional  Regions.

C��Create  Dynamo db tables  in  multiple AWS  Regions.  Use  on-demand  capacity  mode. Use  Dynamo db Streams for Cross-Region  Replication  between  Regions.
D��Use  Dynamo db global tables for automatic  multi-Region  replication.  Deploy tables in  multiple AWS  Regions.  Use  provisioned  capacity  mode.  Enable auto  scaling.
�� ��D



���� ��Correct Answer  D  Explanation The  best solution  is  D  because  it  balances  high  a vail ability,  resilience, and cost efficiency.  Here��s why:  -  Dynamo db Global Tables auto matically replicate data across  multiple AWS  Regions, ensuring continuous availability  even if one  Region fails. This eliminates  manual  replication efforts  (unlike Option  B or C). -  Multi-Region deployment ensures low-latency access for global users and disast er  recovery. -  Provisioned capacity  mode with auto scaling optimizes costs  by automa tically adjusting capacity  based on workload demand. On-demand  mode (Options A/C)	is simpler  but  more expensive for  predictable traffic, which gaming  platforms often  h ave. Options A and C use on-demand mode (costly for steady growth) or manual  rep lication (error-prone). Option  B  uses  manual cross-region  replication and  DAX  (unnece

ssary unless  read  latency is critical).  Reference  Links  -  [Dynamo db  Global Tables](https: //docs.aws.amazon.com/amazon dynamo db/latest/developer guide/Global tables.html) -
[Auto Scaling for Dynamo db](https://docs.aws.amazon.com/amazon dynamo db/latest/de velo per guide/Auto scaling.html)


80.A company runs its  media  rendering application on  premises. The company wants  to  reduce storage costs and  has  moved all data to Amazon S3. The on-premises  rend ering application needs  low-latency access to storage. The company  needs to design a storage solution for the application. The storage solution must  maintain the desired application performance. Which storage solution will meet these requirements in the MOST cost-effective way?
A��Use  Mount point for Amazon S3 to access the data  in Amazon  S3 for the on-premi ses application.

B��Configure an Amazon S3  File Gateway to  provide storage for the on-premises appl ication.

C��Copy the data from Amazon S3 to Amazon  FSx for Windows  File  Server. Configure an Amazon  FSx  File Gateway to  provide  storage for the on-premises application.
D��Configure an on-premises file server.  Use the Amazon  S3 API to connect to S3 sto rage. Configure the application to access the storage from the on-premises file serve
r.

�� ��B

���� �� Correct Answer:  B  Detailed  Explanation: The correct answer  is  B  because  using an Amazon S3  File Gateway allows the on-premises application to access data stored  in Amazon S3 with  low  latency while  keeping costs  low.  Here's why:  1. Amazon  S3  Fil e Gateway acts as a  bridge between the on-premises application and Amazon S3. It  presents S3  buckets as a  local file share  (via  NFS  or  SMB), so the application can  inte ract with files as if they were stored on-premises. 2. Caching for  Low  Latency: The  Fil e Gateway caches frequently accessed data  locally,  reducing  latency for the  rendering application. This ensures performance remains acceptable even though the primary st orage is in S3 (which is object storage and not inherently  low-latency).  3. Cost-Effecti veness: Storing data  in S3 is cheaper than using  block or file storage solutions  like A mazon  FSx. Options C and  D  involve  additional costs  (e.g.,  FSx  is  expensive, and  main tai ning an on-premises file server adds overhead).  Mount point for S3  (Option A)  is de signed for cloud-based applications (e.g.,  EC2) and doesn��t address on-premises  laten cy issues as effectively. 4. Simplified  Management: The S3  File  Gateway  requires  mini mal setup compared to  maintaining a  hybrid file server  (Option  D) or syncing data to	FSx (Option C).  Reference  Link:  [Amazon  S3  File  Gateway](https://aws.amazon.com/sto rage gateway/file/s3/)


81.A company hosts its enterprise  resource  planning (ERP)  system  in the  us-east-1  Re gion. The system  runs on Amazon  EC2  instances.  Customers use a  public API that  is hosted on the  EC2 instances to exchange  information with the  ERP system. Internatio nal customers  report s low API  response times from their data centers. Which solution will improve response times for the international customers  MOST cost-effectively?

A��Create an AWS  Direct Connect connection that  has a  public virtual  interface  (VIF) t
o provide connectivity from each customer's data center to us-east-1.  Route customer API  requests  by  using a  Direct  Connect gateway to the  ERP  system API.
B��Set up an Amazon Cloud Front distribution in front of the API. Configure the Cachi ng optimized managed cache  policy to  provide  improved cache efficiency.
C��Set up AWS Global Accelerator. Configure  listeners for the  necessary  ports. Configu re endpoint groups for the appropriate  Regions to distribute traffic. Create an endpoi nt in the group for the API.
D�� Use AWS Site-to-Site VPN to establish dedicated VPN tunnels between  Regions an d customer networks.  Route traffic to the API over the VPN connections.
�� ��C



���� �� Correct Answer: C. Set up AWS Global Accelerator. Configure  listeners for the n ec essary  ports. Configure endpoint groups for the appropriate  Regions to distribute tr affic. Create an endpoint in the group for the API.  Detailed  Explanation: AWS  Global Accelerator improves global application performance  by  routing traffic through AWS's high-speed global network infrastructure.  Unlike CDNs (like Cloud Front in Option  B)  that cache static content, Global Accelerator optimizes the  network  path for dynamic API traffic. It automatically directs  user requests to the  nearest AWS edge  location, th en uses AWS's dedicated backbone  network to connect to the  EC2  instances  in  us-ea st-1. This  reduces  latency caused  by  long-distance  internet  hops,  making  it  ideal for  r eal-time APIs. Other options are  less effective:  - Option A  (Direct Connect)  requires e very customer to set  up physical connections, which is impractical and expensive for

global customers. - Option  B  (Cloud Front) works  best for cacheable content,  not dyna mic API calls. - Option  D  (Site-to-Site VPN)  still  relies on  public internet  paths outsid e AWS's network, offering no  latency improvement.  Reference  Links:  [AWS  Global Acc el erator](https://aws.amazon.com/global-accelerator/)  [Global Accelerator vs. Cloud front] (https://docs.aws.amazon.com/white papers/latest/aws-vpc-connectivity-options/global-ac celera tor-and-amazon-cloud front.html)

82.A company tracks customer satisfaction by  using surveys that the company hosts  on its website. The surveys sometimes  reach thousands of customers every  hour. Surv ey  results are currently sent in email  messages to the company so company employe es can  manually  review  results and assess customer sentiment. The company wants to automate the customer survey process. Survey results must be available for the  previ ous 12  months. Which solution will  meet these  requirements  in the  MOST scala ble w ay?

A��Send the survey  results data to an Amazon API Gateway endpoint that  is connect e
d to an Amazon Simple Queue Service (Amazon SQS) queue. Create an AWS  Lambda function to  poll the SQS queue, call Amazon Comprehend for sentiment analysis, an
d save the results to an Amazon  Dynamo db table. Set the TTL for all  records to  365 days in the future.

B��Send the survey  results data to an API that  is  running  on an Amazon  EC2  instance. Configure the API to store the survey  results as a  new  record  in an Amazon  Dynam
oDB table, call Amazon Comprehend for sentiment analysis, and save the  results in a second  Dynamo db table. Set the TTL for all  records to  365 days  in the future.

C��Write the survey  results data to an Amazon S3  bucket.  Use  S3  Event  Notifications to invoke an AWS  Lambda function to  read the  data and call Amazon  Re k ognition fo r sentiment analysis. Store the sentiment analysis  results  in a second S3  bucket.  Use S3  lifecycle policies on each  bucket to expire objects after  365 days.
D��Send the survey  results data to an Amazon API Gateway endpoint that  is connect e d to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the SQS queu e to invoke an AWS  Lambda function that calls Amazon  Lex for  sentiment analysis an d saves the results to an Amazon  Dynamo db table.  Set the TTL for all  records to  365 days in the future.
�� ��A



���� �� Correct Answer: A  Detailed  Explanation: The best solution  here is A  because it  uses a fully serverless and scalable architecture.  Here's why:  1. API Gateway  handles  h igh traffic (thousands of surveys/hour) without scaling  limits. 2. SQS acts as a  buffer t o decouple the survey submission from processing, preventing data  loss during spikes.
 3.  Lambda automatically scales to  process  messages from SQS.  Lambda  now  support s SQS-triggered invocation (no manual polling  needed). 4. Amazon Comprehend is th e correct service for text-based sentiment analysis (Re k ognition in C is for images/vid eo;  Lex  in  D  is for chat bots).  5.  Dynamo db  scales  seamlessly for storage, and TTL aut o matically deletes  records after 365 days. Other options fail  because:  -  B  uses  EC2  (n ot serverless,  harder to  scale).  - C  uses  Re k ognition  (wrong tool) and  S3  (better for  b atch  processing,  less  real-time).  -  D  uses  Lex  (wrong  tool for sentiment analysis).  Refe rence link:  [Amazon Comprehend](https://aws.amazon.com/comprehend/)  [AWS  Serverl ess](https://aws.amazon.com/serverless/)

83.A  medical company wants to  perform transformations on a  large amount of clinica l trial data that comes from several customers. The company must extract the data fr om a relational database that contains the customer data. Then the company will tra nsform the data  by  using a series of complex  rules. The company will  load the data t o Amazon S3 when the transformations are complete. All data  must  be encrypted wh ere it is  processed  before the company stores the data  in Amazon  S3. All data  must be encrypted  by using customer-specific  keys. Which solution will  meet these  require ments with the  LEAST amount of operational effort?
A��Create one AWS Glue job for each customer. Attach a security configuration to ea ch job that uses server-side encryption with Amazon S3 managed  keys (SSE-S3) to en crypt the data.

B��Create one Amazon  EMR cluster for each customer. Attach a security configuration to each cluster that uses client-side encryption with a custom client-side  root  key  (CS E-Custom) to encrypt the data.

C��Create one AWS Glue job for each customer. Attach a security configuration to ea ch job that uses client-side encryption with AWS  KMS  managed  keys  (CSE-KMS) to e ncrypt the data.
D��Create one Amazon  EMR cluster for each customer. Attach a security configuration	to each cluster that uses server-side encryption with AWS  KMS  keys  (SSE-KMS) to e ncrypt the data.
�� ��C

���� ��Correct Answer: C  Detailed  Explanation: The question  requires encrypting data d uring  processing  using customer-specific  keys  before  storing  it in Amazon S3.  Here��s why Option C  is the  best choice:  1. AWS Glue  is serverless,  minimizing operational eff ort compared to  managing Amazon  EMR clusters  (Options  B/D).  2. Client-Side  Encrypt ion with  KMS  (CSE-KMS) ensures data  is encrypted  before  it  leaves the  processing en vironment (AWS Glue),  meeting the encrypted where processed  requirement.  Each cus tomer��s data can  use a unique  KMS  key  (customer-specific).  3.  Server-Side  Encryption	(SSE) in Options A/D encrypts data after it  reaches S3, which doesn��t satisfy the  req u irement to encrypt during  processing. 4. CSE-Custom (Option  B)  requires  managing c ustom keys  manually, increasing operational effort, while  KMS simplifies  key  managem ent. Why  not other options?  - A: SSE-S3 uses S3-managed  keys,  not customer-specific		keys. -  B/D:  EMR  clusters add operational overhead, and SSE encrypts data too  late  (at  rest in S3).  Reference:  [AWS  Glue  Security Configurations](https://docs.aws.amazon. com/glue/latest/dg/encryption-security-configuration.html)  [Client-Side vs Server-Side  E n crypt ion](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Using encryption.htm l)


84.A company hosts a website analytics application on a single Amazon  EC2 On-Dem and Instance. The analytics application is  highly  resilient and is designed to  run  in sta teless  mode. The company notices that the application is showing signs of perform an ce degradation during  busy times and  is  presenting  5xx errors. The company  needs t o make the application scale seamlessly. Which solution will meet these  requirements MOST cost-effectively?

A��Create an Amazon  Machine Image  (AMI) of the web application.  Use the AMI to  l aunch a second  EC2 On-Demand Instance.  Use an Application  Load  Balancer to  distri bute the  load across the two  EC2  instances.
B��Create an Amazon  Machine Image  (AMI) of the web application.  Use the AMI to  l aunch a second  EC2 On-Demand Instance.  Use Amazon  Route  53 weighted  routing to distribute the load across the two  EC2 instances.
C��Create an AWS  Lambda function to  stop the  EC2  instance  and change the instance	type. Create an Amazon Cloud watch alarm to invoke the  Lambda function when CP U  utilization is  more than 75%.
D��Create an Amazon  Machine Image  (AMI)  of the web application. Apply the AMI to a  launch template. Create an Auto Scaling group that  includes the  launch template. C on figure the launch template to use a Spot  Fleet. Attach an Application  Load  Balance r to the Auto Scaling group.
�� ��D



���� �� Correct Answer:  D  Detailed  Explanation: The  best  solution is  D  because  it  uses Auto Scaling with Spot  Fleet and an Application  Load  Balancer  (ALB), which  is  cost-eff ective and ensures seamless scalability.  Here��s why: - Auto Scaling automatically adju sts the number of EC2 instances  based on traffic,  preventing  performance  issues duri ng busy times. - Spot  Fleet  uses Spot Instances, which are  up to 90% cheaper than
On-Demand Instances,  reducing costs significantly. - Application  Load  Balancer  (ALB)  distributes traffic evenly across instances, eliminating  5xx errors caused  by overloaded	servers.  - Since the application is stateless,  it can easily  run on  multiple instances wit

hout  relying on a single server. Other options are  less  effective:  - A and  B  use  On-D emand Instances  (expensive) and  lack automated scaling.  - C stops the  instance to  re size it, causing downtime and failing to scale  horizontally.  -  D combines automation, cost savings, and  reliability.  Reference  Link:  [Auto  Scaling](https://aws.amazon.com/aut o scaling/)  [Spot  Fleet](https://aws.amazon.com/ec2/spot/)  [Application  Load  Balancer](h ttps://aws.amazon.com/elastic load balancing/)


85.A company runs an environment where data is stored in an Amazon S3 bucket. Th e objects are accessed frequently throughout the day. The company has strict da ta e n crypt ion requirements for data that  is stored in the S3  bucket. The company current l y uses AWS  Key  Management  Service  (AWS  KMS) for encryption. The company wants	to optimize costs associated with encrypting S3 objects without making additional ca lls to AWS  KMS. Which solution will  meet these  requirements?

A�� Use server-side encryption with Amazon S3 managed  keys (SSE-S3).

B��Use an S3  Bucket  Key for  server-side encryption with AWS  KMS  keys  (SSE-KMS)  on the new objects.

C�� Use client-side encryption with AWS  KMS customer  managed  keys.

D�� Use server-side encryption with customer-provided keys  (SSE-C) stored in AWS  KM S.
�� ��B



���� ��Correct Answer:  B  Detailed  Explanation: The  best  solution  is  B.  Use  an  S3  Bucke t  Key for server-side encryption with AWS  KMS  keys  (SSE-KMS)  on the  new objects.

Here's why: - S3  Bucket  Key  acts as a  middle  layer  between AWS  KMS  and your  S3  objects. Instead of making a  KMS API call for every object encryption/decryption, the Bucket  Key  itself encrypts  multiple objects. This  reduces the  number of calls to  KMS,  lowering costs  (KMS charges  per API call).  - Compliance  maintained:  Data  remains en crypted with  KMS  keys  (SSE-KMS),  meeting  strict encryption  requirements.  -  No  code changes: Simply enable the  Bucket  Key option  on the S3 bucket, unlike client-side en crypt ion (Option C) which requires application changes. Why other options fail: - A (S SE-S3):  Uses S3-managed  keys,  not  KMS. This violates the  requirement to  use AWS  K MS. - C  (Client-side encryption):  Requires extra  KMS  calls to generate data  keys,  incre asing costs. -  D  (SSE-C):  Uses  customer-provided  keys stored in  KMS,  but  still  require s  passing  keys with every API call, which  is cumbersome and doesn't  reduce  KMS  call s.  Reference  Links:  -  [AWS  S3  Bucket  Key  documentation](https://docs.aws.amazon.com /AmazonS3/latest/user guide/bucket-key.html) -  [AWS  KMS  pricing](https://aws.amazon.c om/kms/pricing/)


86.A company has a three-tier web application that processes orders from customers. The web tier consists of Amazon  EC2 instances  behind an Application  Load  Balancer. The processing tier consists of EC2 instances. The company decoupled the web tier  and  processing tier  by using Amazon Simple Queue Service  (Amazon SQS). The stora ge layer uses Amazon  Dynamo db. At  peak times,  some users  report order  processing delays and halls. The company has noticed that during these delays, the  EC2 instanc es are  running at  100%  CPU usage, and the SQS queue fills  up. The  peak times are v ariable and unpredictable. The company  needs to  improve the  performance of t he ap plication. Which solution will meet these  requirements?

A�� Use scheduled scaling for Amazon  EC2 Auto  Scaling to scale out the  processing ti er instances for the duration of peak usage times.  Use the CPU  Utilization  metric to determine when to scale.

B�� Use Amazon  Elastic ache for  Redis  in front of the  Dynamo db  backend tier.  Use  tar get utilization as a  metric to determine when to scale.

C��Add an Amazon Cloud Front distribution to cache the  responses for the web tier.  U se  HTTP  latency  as a  metric to determine when to scale.
D�� Use an Amazon  EC2 Auto  Scaling target tracking  policy to scale out the  process in g tier instances.  Use the Approximate number of Messages attribute to determine when	to scale.
�� ��D



���� ��Correct Answer:  D  Detailed  Explanation: The  problem occurs  because the  proces sing tier  EC2  instances can't handle the SQS queue workload during unpredictable pe ak times, causing delays. Option  D  uses the Approximate number of Messages  (visible messages in the queue) as the scaling metric. This directly addresses the  root cause: when the queue fills up, Auto Scaling adds  more  EC2  instances to  process  messages  faster, even if CPU  usage is already  high. Other options fail  because:  - A  (Scheduled  Scaling):  Peak times are  unpredictable, so a fixed schedule won't  help.  -  B  (Elastic ach e): While caching can  reduce database  load, the  bottleneck  is the  processing tier,  not Dynamo db. - C (Cloud Front): Caching web tier responses doesn��t solve the process i ng tier/SQS  backlog issue.  Reference  Links:  [AWS Auto  Scaling with  SQS](https://docs. aws.amazon.com/auto scaling/ec2/user guide/as-using-sqs-queue.html)  [Target Tracking S

caling Policies](https://docs.aws.amazon.com/auto scaling/ec2/user guide/as-scaling-target -tracking.html)


87.A company's production environment consists of Amazon  EC2 On-Demand Instance s that  run constantly  between  Monday and  Saturday. The instances  must  run for only
 12 hours on Sunday and cannot tolerate interruptions. The company wants to cost-o ptimize the production environment. Which solution will meet these  requirements  MO ST cost-effectively?

A�� Purchase Scheduled  Reserved Instances for the  EC2  instances that  run for only  12 hours on Sunday.  Purchase  Standard  Reserved Instances for the  EC2  instances that  ru n constantly  between  Monday and  Saturday.

B��Purchase Convertible  Reserved Instances for the  EC2  instances that  run for  only  12 hours on Sunday.  Purchase  Standard  Reserved Instances for the  EC2  instances that  ru n constantly  between  Monday and  Saturday.

C��Use Spot Instances for the  EC2  instances that  run for only  12  hours  on  Sunday.  Pu rchase Standard  Reserved Instances for the  EC2  instances that  run  constantly  between Monday and Saturday.
D�� Use Spot Instances for the  EC2  instances that  run for only  12  hours  on  Sunday.  P urchase Convertible  Reserved Instances for the  EC2  instances that  run  constantly  betw een  Monday and Saturday.
�� ��A

���� ��Correct Answer A.  Purchase Scheduled  Reserved Instances for the  EC2  instances that  run for only  12  hours on Sunday.  Purchase  Standard  Reserved Instances  for the  EC2 instances that  run constantly between  Monday and Saturday.  Detailed  Explanation	-  Monday to  Saturday workload: The instances  run constantly, so Standard  Reserved Instances  (RIs) are  ideal  here.  RIs  offer the  highest  discount (up to  ~72%) for  steady- state,  long-term workloads. This is cheaper than On-Demand or Convertible  RIs.  -  Sun day workload: The instances  run only 12  hours and cannot tolerate  interruptions. Spo t Instances are  risky  (they can  be  interrupted) and aren��t allowed  here.  Scheduled  Re served Instances are designed for  predictable,  recurring time  blocks (like every Sunda y). They  provide cost savings (~5-10% discount vs. On-Demand) while guaranteeing a vail ability during the scheduled window. - Why not other options: -  B/D  (Convertible RIs): Convertible  RIs  offer less discount (~54%) than Standard  RIs  and are  meant for f l exibility (changing instance types). They��re  not optimal for the  Monday-Saturday wor kload. - C/D (Spot Instances): Spot Instances are unsuitable for the Sunday workload due to the no-interruption  requirement. In summary, Standard  RIs  maximize  savings f or the 6-day workload, and Scheduled  RIs  provide guaranteed capacity at  lower  cost  for the  recurring  12-hour Sunday window.  Reference  Links  -  [AWS  Reserved  Instances] (https://aws.amazon.com/ec2/pricing/reserved-instances/) -  [Scheduled  Reserved Instanc es](https://docs.aws.amazon.com/AWSEC2/latest/User guide/ec2-scheduled-instances.htm l)


88.A digital image  processing company wants to  migrate  its on-premises  monolithic a pplication to the AWS Cloud. The company  processes thousands of images and gener ates  large files as  part of the  processing workflow. The company  needs a solution to

manage the growing  number of image processing jobs. The solution  must also  reduc e the manual tasks  in the image  processing workflow. The company does  not want t o manage the underlying infrastructure of the solution. Which solution will meet thes e  requirements with the  LEAST operational overhead?
A��Use Amazon  Elastic Container Service  (Amazon  ECS) with Amazon  EC2  Spot Instanc es to  process the images. Configure Amazon Simple Queue Service (Amazon  SQS) to orchestrate the workflow. Store the  processed files in Amazon  Elastic  File  System  (Am azon  EFS).

B�� Use AWS  Batch jobs to  process the  images.  Use AWS  Step  Functions to  orchestra t e the workflow. Store the processed files in an Amazon S3 bucket.

C��Use AWS  Lambda functions and Amazon  EC2  Spot Instances to  process the  images. Store the  processed files in Amazon  FSx.
D�� Deploy a group of Amazon  EC2  instances to  process the  images.  Use AWS  Step  F unctions to orchestrate the workflow. Store the  processed files in an Amazon  Elastic Block Store (Amazon  EBS) volume.
�� ��B



���� ��Correct Answer:  B  Detailed  Explanation: The  best  solution  is  B  because AWS  Bat ch is a fully managed service that automatically provisions and scales compute  resour ces for  batch processing jobs, eliminating the  need to  manage  infrastructure. AWS St ep  Functions simplifies workflow orchestration,  reducing  manual tasks  by coordinating	the steps of the image  processing  pipeline. Storing files in Amazon S3 is cost-effecti ve, scalable, and fully managed. Why other options are  less  ideal:  - A:  ECS with  EC2

Spot Instances still requires  managing clusters and scaling. Amazon  EFS adds complex ity for shared storage, which  may  not  be  necessary  if  S3 suffices.  - C:  Lambda  has  ru ntime/memory limits for  large  image  processing, and  EC2  Spot Instances  require  man ual  management.  FSx  is overkill  unless  specific file system features are  needed.  -  D:
Managing  EC2 instances and  EBS volumes  introduces operational overhead, contradict i ng the  requirement to avoid  infrastructure  management.  Key AWS  Services: - AWS  Ba tch: Serverless  batch  processing. - AWS Step  Functions: Workflow automation.  - Amaz on S3: Scalable storage for  large files.  Reference  Links:  -  [AWS  Batch](https://aws.amaz on.com/batch/) -  [AWS  Step  Functions](https://aws.amazon.com/step-functions/)  -  [Am azon S3](https://aws.amazon.com/s3/)


89.A company's image-hosting website gives users around the world the ability to up	load, view, and download images from their  mobile devices. The company currently hosts the static website in an Amazon S3  bucket.  Because of the website's growing  p opu larity, the website's  performance has decreased.  Users  have  reported  latency  issue s when they upload and download images. The company must improve the  perform a nce of the website. Which solution will meet these requirements with the  LEAST  impl e mentation effort?
A��Configure an Amazon Cloud Front distribution for the S3 bucket to improve the do wnload  performance.  Enable S3 Transfer Acceleration to  improve the upload  perform a nce.
B��Configure Amazon  EC2  instances of t he  right  sizes in  multiple AWS  Regions.  Migra te the application to the  EC2  instances.  Use an Application  Load  Balancer to  distribut

e the website traffic equally among the  EC2  instances. Configure AWS Global Accelera tor to address global demand with  low  latency.
C��Configure an Amazon Cloud Front distribution that  uses the S3  bucket as an origin to improve the download  performance. Configure the application to  use Cloud Front t o upload images to improve the upload  performance. Create S3  buckets  in  multiple AWS  Regions. Configure  replication  rules for the  buckets to  replicate  users'  data  base d on the users'  location.  Redirect downloads to the  S3  bucket that  is closest to each user's location.
D��Configure AWS Global Accelerator for the S3  bucket to  improve  network  perform a nce. Create an endpoint for the application to use Global Accelerator instead of the S 3  bucket.
�� ��A



���� �� Correct Answer: A  Detailed  Explanation: The company's  main issue is  latency du ring image uploads and downloads.  Here's why Option A  is the  best choice with  mini mal effort:  1.  Download  Performance  (Amazon  Cloud Front):  - Cloud Front  is a Content Delivery  Network (CDN) that caches static content (like images) at edge locations gl obally. When users download images, they retrieve them from the nearest edge locati on instead of the original S3 bucket,  reducing  latency. This  requires  minimal setup: ju st create a Cloud Front distribution  pointing to the S3  bucket. 2.  Upload  Performance  (S3 Transfer Acceleration): - S3 Transfer Acceleration optimizes uploads  by  routing file s through AWS's global network infrastructure (using edge  locations). Instead of uplo ading directly to the S3 bucket,  users upload to the  nearest edge  location, which the

n efficiently transfers the data to S3.  Enabling this only  requires toggling a setting  in the S3  bucket and  using a special endpoint  URL for  uploads. Why other options are  l ess ideal: - Option  B:  Migrating to  EC2  instances  adds  complexity (managing servers, scaling,  load  balancers). This  is overkill for a static website and  requires significant co de changes. - Option C:  Using  multiple S3  buckets with  replication and  location-base d  routing adds operational overhead (managing  replication  rules,  redirect  logic). Cloud Front alone handles geographic optimization for downloads. - Option  D: AWS Global Accelerator does  not support S3 endpoints. This option is invalid.  Reference  Links:  -
[Amazon Cloud front](https://aws.amazon.com/cloud front/) -  [S3 Transfer Acceleration] (https://aws.amazon.com/s3/transfer-acceleration/)


90.A company runs an application in a  private subnet  behind an Application  Load  Bal ancer (ALB)  in a VPC. The VPC  has a  NAT gateway and an  internet gateway. The appl ication calls the Amazon S3 API to store objects. According to the company's security policy, traffic from the application must not travel across the internet. Which sol utio n will meet these requirements  MOST cost-effectively?
A��Configure an S3 interface endpoint. Create a security group that allows outbound t raffic to Amazon S3.
B��Configure an S3 gateway endpoint.  Update the VPC  route table to  use the endpoi nt.
C��Configure an S3  bucket  policy to allow traffic from the  Elastic IP address that  is as signed to the  NAT gateway.

D��Create a second  NAT gateway  in the same subnet where the  legacy application  is deployed. Update the VPC route table to use the second  NAT gateway.
�� ��B



���� �� Correct Answer:  B  Detailed  Explanation: The application  is  in a  private  subnet a nd needs to access Amazon S3 without internet exposure. A gateway endpoint for S3	is the  best solution  because  it allows private connectivity  between the VPC and S3
without using the internet or a  NAT gateway. Gateway endpoints are free  (only data  transfer costs apply),  making them cost-effective.  Here��s why other options are  incorr ect: - A  (Interface  Endpoint):  S3 supports  interface endpoints,  but they  incur  hourly c osts,  making them  less cost-effective than gateway endpoints.  - C  (NAT Gateway  E last ic IP): Traffic would still flow through the internet via the  NAT gateway, violating the security  policy.  -  D  (Second  NAT  Gateway): Adds  unnecessary  cost  and doesn��t solve the internet traffic issue. A gateway endpoint  updates the VPC  route table to  redirect S3 traffic through AWS��s  private network, ensuring security and cost efficiency.  Refe rence  Links: -  [AWS VPC  Endpoints  for S3](https://docs.aws.amazon.com/vpc/latest/priv atelink/vpce-gateway.html) -  [Gateway vs. Interface  Endpoints](https://aws.amazon.com/ vpc/features/)


91.A company has an application that  runs on an Amazon  Elastic  Ku bernet es  Service  (Amazon  EKS) cluster on Amazon  EC2  instances. The  application  has a  UI that  uses A mazon  Dynamo db and data services that  use Amazon  S3 as  part of the application d eployment. The company  must ensure that the  EKS  Pods for the  UI  can access  only  Amazon  Dynamo db and that the  EKS  Pods for the  data services  can access only Ama

zon S3. The company uses AWS Identity and Access  Management  (IAM). Which sol uti on meals these requirements?
A��Create separate IAM  policies for Amazon S3 and  Dynamo db access with the  requir ed permissions. Attach  both IAM  policies to the  EC2  instance  profile.  Use  role-based  access control (RBAC) to control access to Amazon S3 or  Dynamo db for the  respect iv e  EKS Pods.

B��Create separate IAM  policies for Amazon S3 and  Dynamo db access with the  requir ed permissions. Attach the Amazon S3 IAM  policy directly to the  EKS  Pods for the  da ta services and the  Dynamo db  policy to the  EKS  Pods  for the  UI.

C��Create separate  Ku bernet es  service accounts for the  UI  and data services to assume an IAM  role. Attach the AmazonS3Full access  policy to the data services account and the Amazon dynamo db full access  policy to the  UI service account.
D��Create separate  Ku bernet es  service accounts for the  UI and data services to assum e an IAM  role.  Use IAM  Role  for Service Accounts (IRSA) to  provide access to the  EK S  Pods for the  UI to Amazon  S3 and the  EKS  Pods  for the data services to  DynamoD B.
�� ��C



���� ��Correct Answer: C  Detailed  Explanation: The question asks  how to  restrict  EKS  P ods for the  UI to  Dynamo db and  data service  Pods to  S3  using IAM.  Here's the  brea kdown:  1. Why IRSA  is  Required:  EKS  Pods  need AWS  permissions.  The  modern appro ach is IAM  Roles for Service Accounts  (IRSA), which associates  Ku bernet es  Service Acc ounts with IAM  roles. This  is  more secure than  using the  EC2  instance  profile  (which

grants  permissions to all  Pods  on the  node). 2. Option Analysis:  - Option A:  Uses  EC2		instance  profiles. This is insecure  because all  Pods  on the  node  inherit the  per missio ns, violating the requirement to  restrict access  per  Pod type.  RBAC  controls  Ku bernet e s  resources,  not AWS services. ��  -  Option  B: IAM  policies cannot  be attached directly	to  Pods. ��  -  Option C:  - Correct Core Idea: Create separate  Ku bernet es  Service Acc ounts for  UI/data  Pods and  link them to IAM  roles via  IRSA. - Wording Issue: The op tion says attach  policies to the service accounts, which is technically  inaccurate.  Polici es are attached to IAM  roles,  not  Ku bernet es  Service Accounts.  However, the  intent  al igns wit h I RSA:  1. Create IAM  roles with  policies  (e.g.,  `Amazon dynamo db full access` f or  UI, `AmazonS3Full access` for data). 2.  Link these  roles to  Ku bernet es  Service Accou nts using annotations. 3.  Pods  use these Service Accounts,  inheriting the IAM  role  per missions. - Why It��s the  Best Answer:  Despite the wording flaw, this option  correctly describes using I RSA  (via Service Accounts) to assign granular  permissions. Other opti ons are  invalid. ��  -  Option  D:  Reverses  the  permissions  (UI gets S3, data gets  Dyna moDB), which violates the requirements. ��  3.  Key Take away: IRSA  is the AWS-recom mended  method.  Ku bernet es Service Accounts are associated with IAM  roles  (not  poli cies directly), and  Pods  inherit  permissions via these  roles. Option C captures this  logi c, even with slightly imprecise wording.  Reference:  [AWS  EKS  IAM  Roles for  Service A ccounts (IRSA)](https://docs.aws.amazon.com/eks/latest/user guide/iam-roles-for-service-a ccounts.html)


92.A company needs to give a globally distributed development team secure access t
o the company's AWS  resources in a way that complies with security  policies. The co mpany currently uses an on-premises Active  Directory for  internal authentication. The

company uses AWS Organizations to  manage  multiple AWS accounts that support  mu ltiple projects. The company needs a solution to integrate with the existing infrastruc t ure to  provide centralized  identity  management and access control. Which solution wi ll meet these  requirements with the  LEAST operational overhead?
A��Set up AWS  Directory  Service to create an AWS  managed  Microsoft Active  Director y on AWS.  Establish a trust  relationship with the on-premises Active  Directory.  Use IA
M rotes that are assigned to Active  Directory groups to access AWS  resources within the company's AWS accounts.

B��Create an IAM  user for each developer.  Manually  manage  permissions for each IA
M user  based on each user's  involvement with each  project.  Enforce  multi-factor  auth en tication (MFA) as an additional  layer of security.

C��Use AD Connector in AWS  Directory Service to connect to the on-premises Active Directory. Integrate AD Connector with AWS IAM Identity Center. Configure  per missio ns sets to give each AD group access to specific AWS accounts and  resources.
D�� Use Amazon Cognito to deploy an identity federation solution. Integrate the identi ty federation solution with the on-premises Active  Directory.  Use Amazon Cognito to provide access tokens for developers to access AWS accounts and  resources.
�� ��C



���� �� Correct Answer: C  Detailed  Explanation: The  best solution is C  because  it integr ates the existing on -  premises Active  Directory  (AD) with AWS  using AD Connector a nd AWS IAM Identity Center (formerly AWS Single Sign - On).  Here��s why:  - AD Con nector acts as a  bridge to the on -  premises AD, allowing developers to  log  in with t

heir existing AD credentials.  No  need to sync or create  a  new directory.  - IAM Identit y Center centralizes access control across multiple AWS accounts (managed via AWS Organizations).  Permissions are assigned to AD groups  (e.g.,  Developers, Admins)  usin g  permission sets, which define what each group can access  in specific AWS accounts.
 -  Least  operational overhead:  No  manual  user  management  (unlike  Option  B) and  no need to  maintain a separate AWS  Managed AD  (unlike Option A). Option  D (Cognit o) is designed for customer - facing apps,  not internal workforce access.  Reference  Li nks: -  [AWS IAM Identity  Center](https://aws.amazon.com/iam/identity - center/) -  [AW S  Directory Service AD Connector](https://docs.aws.amazon.com/directory service/latest/ admin - guide/directory_ad_connector.html)


93.A company is developing an application in the AWS Cloud. The application's  HTTP API contains critical information that is published in Amazon API Gateway. The critica l information must  be accessible from only a  limited set of trusted IP addresses that belong to the company's internal  network. Which solution will  meet these  require men ts?

A��Set up an API Gateway  private  integration to  restrict  access to a  predefined set of IP addresses.

B��Create a  resource  policy for the API that denies access to any IP address that is  n ot specifically allowed.

C��Directly deploy the API  in a  private  subnet. Create a  network ACL.  Set  up  rules to allow the traffic from specific IP addresses.

D��Modify the security group that is attached to API Gateway to allow inbound traffic from only the trusted IP addresses.
�� ��B



���� �� Correct Answer:  B  Detailed  Explanation: To  restrict access to an Amazon API Ga teway API so it's only accessible from specific trusted IP addresses, the correct appro ach  is to create a  resource  policy for the API. A  resource  policy  lets you define condi t ions (like IP  ranges) for incoming  requests. You can explicitly deny all  requests excep t those coming from allowed IP addresses using the `aws:Source ip` condition key. Wh y other options are incorrect:  - A API Gateway  private  integration  refers to connect in g to  backend services  in a VPC  (e.g.,  EC2),  but  it  doesn't control access to the API  its elf. - C API Gateway  is a  managed service  and cannot  be deployed  in a  private  sub n et.  Network ACLs apply to  subnets in your VPC,  not API Gateway.  -  D API  Gateway d oesn��t use security groups; security groups control traffic for  resources like  EC2  insta nces,  not API Gateway.  Key  Concept:  Resource  policies  in API Gateway act  like a firew all for your API.  By  specifying allowed IP  ranges  in a  policy, you ensure only trusted  sources can access the API, similar to a VIP  list for a club.  Reference  Links:  -  [Controll ing access to API Gateway with resource policies](https://docs.aws.amazon.com/apigate way/latest/developer guide/api gateway-resource-policies.html) -  [Using aws:Source ip  in policies](https://docs.aws.amazon.com/IAM/latest/User guide/reference_policies_condition -keys.html#condition-keys-source ip)


 1.An application development team is designing a  micro service that will convert  larg e images to smaller, compressed images. When a user uploads an image through the

 web interface, the micro service should store the image in an Amazon S3 bucket,  pro cess and compress the image with an AWS  Lambda function, and store the  image  in  its compressed form in a different S3  bucket. A solutions architect  needs to design a 	solution that uses durable, stateless components to process the images automatically. Which combination of actions will meet these requirements? (Choose two.)
A��Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the S3  b ucket to send a  notification to the SQS queue when an  image is uploaded to the S3 bucket.
B��Configure the  Lambda function to  use the Amazon Simple Queue Service  (Amazon SQS) queue as the invocation source. When the SQS  message is successfully  process e d, delete the message in the queue.
C��Configure the  Lambda function to  monitor the  S3  bucket for  new  uploads. When a n uploaded image is detected, write the file  name to a text file  in  memory and  use t he text file to keep track of the images that were  processed.
D��Launch an Amazon  EC2  instance to  monitor an Amazon  Simple Queue  Service  (Am azon SQS) queue. When items are added to the queue,  log the file  name  in a text fil e on the  EC2  instance and  invoke the  Lambda function.
E��Configure an Amazon  Event bridge  (Amazon Cloud watch  Events)  event to  monitor t he S3  bucket. When an  image is  uploaded, send an alert to an Amazon ample  Notifi cation Service (Amazon SNS) topic with the application owner's email address for furt her processing.
�� ��AB

���� ��Correct Answer: A,  B  Detailed  Explanation: A.  Using Amazon  SQS  as a  buffer  be tween S3 and  Lambda ensures durability. When an  image  is  uploaded to S3,  it sends a notification to SQS which acts as a persistent message queue. This guarantees no  image upload event is  lost, even if  Lambda temporarily can't  process  requests.  B.  Con figuring  Lambda with SQS as an event source creates a fully  managed, serverless solu tion.  Lambda automatically  polls the queue,  processes  messages, and only deletes the m after successful compression.  Failed  processing attempts automatically  retry,  maintai ning  reliability without  manual intervention. Why others are wrong:  - C:  Using in-mem ory text files violates statelessness and durability  requirements.  Lambda's ephemeral
memory can't track  processed files  reliably.  -  D:  EC2  instances  add stateful componen ts (text file storage) and  manual  management, contradicting serverless/stateless  requir ements. -  E:  SNS email alerts  introduce  human  intervention instead of automatic  proc essing, breaking the automation  requirement.  Reference  Links:  -  [AWS  S3  Event  Notifi cations](https://docs.aws.amazon.com/AmazonS3/latest/user guide/Notification how to.ht ml) -  [Lambda with  SQS  Event Source](https://docs.aws.amazon.com/lambda/latest/dg/ with-sqs.html)


2.A company has an Amazon S3 bucket that contains critical data. The company must protect the data from accidental deletion. Which combination of steps should a solu t ions architect take to  meet these  requirements?  (Choose two.)

A�� Enable versioning on the S3 bucket.

B�� Enable  MFA  Delete  on the  S3  bucket.

C��Create a  bucket  policy on the  S3  bucket.

D�� Enable default encryption on the S3  bucket.

E��Create a  lifecycle  policy for the objects  in the S3  bucket.

�� ��AB



���� �� Correct Answer A.  Enable versioning on the S3  bucket.  B.  Enable  MFA  Delete  o n the S3 bucket.  Detailed  Explanation To  protect  S3 data from  accidental deletion: 1. Versioning (A)  keeps  multiple versions of an object.  Even  if someone  deletes an objec t, you can  restore a  previous version. This  prevents  permanent data  loss.  2.  MFA  Dele te (B) adds an extra  layer of security. To delete an object or disable versioning, the u ser  must  provide a  Multi  -  Factor Authentication  (MFA)  code from  a  physical  device  (l ike a smartphone app). This  makes accidental or unauthorized deletions  much  harder. Why not other options? - C (Bucket  Policy):  Policies control access  but don��t  directl y  prevent deletions.  -  D  (Encryption):  Protects  data  privacy,  not  deletion.  -  E  (Lifecycle Policy):  Manages object expiration (e.g., deleting old files), which increases deletion ri sk.  Reference  Links  -  [S3 Versioning](https://docs.aws.amazon.com/AmazonS3/latest/use rguide/Versioning.html) -  [MFA  Delete](https://docs.aws.amazon.com/AmazonS3/latest/u ser guide/Multifactor authentication delete.html)


3.A company has a data  ingestion workflow that consists of the following: ? An Amaz on Simple  Notification Service (Amazon SNS) topic for  notifications about  new data d e liveries  ? An AWS  Lambda function to  process the data and  record  metadata The co mpany observes that the ingestion workflow fails occasionally  because of network con nec tivity issues. When such a failure occurs, the  Lambda function does  not ingest the corresponding data unless the company manually reruns the job. Which combination

 of actions should a solutions architect take to ensure that the  Lambda function  inge sts all data  in the future? (Choose two.)
A�� Deploy the  Lambda function in  multiple Availability Zones.

B��Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe  it to the SNS topic.

C��Increase the CPU and  memory that are allocated to the  Lambda function.

D��Increase  provisioned throughput for the  Lambda function.

E��Modify the  Lambda function to  read f rom an Amazon  Simple Queue Service  (Amaz on SQS) queue.
�� ��BE



���� �� Correct Answer:  B,  E  Detailed  Explanation: When an  SNS topic directly triggers a  Lambda function, there's a  risk  of message  loss  if the  Lambda fails  (e.g.,  due to  net work issues).  By adding an  SQS queue  (B)  between SNS and  Lambda,  messages  are d urably stored. Then modifying the  Lambda to  read f rom  SQS  (E) enables automatic  re tries through SQS's visibility timeout mechanism. If  Lambda fails to  process a  messag e, SQS will automatically make  it visible again for  retry, eliminating  manual interventi o n. Other options are  incorrect  because: A)  Lambda already  runs across AZs automatic ally C) CPU/memory doesn't  resolve  network issues  D)  Provisioned  concurrency  handle s startup  latency, not  message durability  Reference  Links:  https://docs.aws.amazon.com /lambda/latest/dg/with-sqs.html https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-s ub scriber.html

4.A company is developing an application that  provides order shipping statistics for  r e trieval  by a  REST API. The company wants to extract the shipping statistics, organize	the data  into an easy-to-read  HTML format, and  send the  report to several email ad dresses at the same time every  morning. Which combination of steps should a sol uti ons architect take to  meet these  requirements?  (Choose two.)
A��Configure the application to send the data to Amazon  Kinesis  Data  Firehose.

B�� Use Amazon Simple  Email Service  (Amazon  SES) to format the data and to send t he  report  by email.

C��Create an Amazon  Event bridge  (Amazon Cloud watch  Events)  scheduled event that invokes an AWS Glue job to query the application's API for the data.
D��Create an Amazon  Event bridge  (Amazon Cloud watch  Events)  scheduled event that invokes an AWS  Lambda function to query the application's API for the data.
E��Store the application data in Amazon S3. Create an Amazon Simple  Notification Se rvice (Amazon SNS) topic as an S3 event destination to send the report  by email.
�� ��BD



���� �� Correct Answer:  D and  B  Detailed  Explanation: To  meet the  requirements,  the s ol ution  needs to:  1.  Fetch  data from the  REST API  on a  schedule ��  Use Amazon  Eve nt bridge (Cloud watch  Events) to trigger an AWS  Lambda function  (Option  D).  - Why Lambda?  Lambda  is  serverless, cost  - effective, and  ideal for short -  running tasks  lik e querying an API,  processing data, and formatting  it into  HTML. AWS Glue  (Option C) is  better suited for  large  - scale  ETL jobs,  not  simple API  calls. 2. Send the  HTML

report via email ��  Use Amazon SES  (Option  B).  - Why  SES? SES specializes  in  bulk e mail sending, supports  HTML content, and allows sending to  multiple  recipients. Whil e SNS (Option  E) can send emails,  it  requires subscribers to opt  -  in to a topic, whic h is  less flexible than SES for this  use case. Why  not other options? - A (Kinesis  Data	Firehose):  Designed for streaming data,  not  scheduled  batch  processing.  -  C  (AWS G l ue): Overkill for simple API calls and  HTML formatting.  -  E  (SNS  +  S3):  S3  storage  is unnecessary  here, and SNS  is  less efficient for direct email delivery compared to SES. Reference  Links:  -  [AWS  Lambda](https://aws.amazon.com/lambda/)  -  [Amazon  SES](htt ps://aws.amazon.com/ses/) -  [Amazon  Event bridge](https://aws.amazon.com/event bridge /)

5.A company recently launched  Linux-based application  instances on Amazon  EC2  in a private subnet and launched a  Linux-based  bastion  host on an Amazon  EC2  instance	in a  public subnet of a VPC. A solutions architect  needs to connect from the on-pre mises  network, through the company's internet connection, to the bastion  host, and t o the application servers. The solutions architect  must  make sure that the security gr oups of all the  EC2  instances will allow that access. Which combination of steps shou ld the solutions architect take to  meet these  requirements? (Choose two.)
A�� Replace the current security group of the  bastion  host with one that only allows  i n bound access from the application instances.

B�� Replace the current security group of the bastion  host with one that only allows i n bound access from the internal IP  range for the company.

C�� Replace the current security group of the bastion  host with one that only allows i n bound access from the external IP  range for the company.
D��Replace the current security group of the application instances with one that allow s in bound SSH access from only the  private IP address of the  bastion  host.
E��Replace the current security group of the application instances with one that allows inbound SSH access from only the  public IP address of the  bastion  host.
�� ��CD



���� �� Correct Answer C,  D  Detailed  Explanation To  securely connect from the on -  pr emises network to the application servers via the  bastion  host:  1. Option C: The  basti on  host��s security group should allow  inbound SSH access only from the company��s external/public IP  range. This ensures that only authorized users (via the company��s internet connection) can  reach the  bastion  host,  blocking  unauthorized  internet traffic.
 2. Option  D: The application  instances�� security group should allow in bound SSH ac cess only from the  bastion  host��s  private IP address.  Since the  bastion  host  and appl ication instances are in the same VPC, traffic  between them  uses private IPs.  Restrict i ng access to the  bastion��s  private IP ensures only the trusted  bastion  can  reach the applications,  not the entire  public  internet. Why  not other options? - A/B: The  bastion host needs inbound access from the company��s external IP (not internal IPs or appl ication instances). -  E:  Using the  bastion��s  public IP for  application access is un reli abl e  (public IPs can change, and VPC traffic uses  private IPs).  Reference  Links  -  [AWS  Se curity Groups](https://docs.aws.amazon.com/vpc/latest/user guide/VPC_Security groups.ht

ml) -  [Bastion  Hosts  Best  Practices](https://aws.amazon.com/architecture/security  -  iden tity - compliance/bastion -  hosts/)


6.A solutions architect is designing a two-tier web application. The application consist s of a  public-facing web tier  hosted on Amazon  EC2  in  public  sub nets. The database tier consists of Microsoft SQL Server  running on Amazon  EC2  in a  private  subnet. Sec urity is a  high  priority for the company.  How  should security groups be configured in	this situation? (Choose two.)
A��Configure the security group for the web tier to allow inbound traffic on  port 443 from 0.0.0.0/0.

B��Configure the security group for the web tier to allow outbound traffic on port 44
3 from 0.0.0.0/0.

C��Configure the security group for the database tier to allow inbound traffic on  port 1433 from the security group for the web tier.
D��Configure the security group for the database tier to allow outbound traffic on po rts 443 and  1433 to the security group for the web tier.
E��Configure the security group for the database tier to allow inbound traffic on port s 443 and  1433 from the security group for the web tier.
�� ��AC



���� ��Correct Answers: A, C  Detailed  Explanation:  1. Option A: The web tier (front end) must accept inbound  HTTPS traffic  (port 443) from the internet to serve users. Allow

ing `0.0.0.0/0` (any IP) on  port 443 ensures  users can access the website securely via HTTPS. This is  necessary for  public-facing web servers. 2. Option C: The database tier	(backend) should only accept inbound traffic from the web tier.  By specifying the we b tier��s security group as the source for  port  1433  (MS SQL Server��s default  port), you ensure only the web servers can communicate with the database. This follows th e  principle of least  privilege and avoids exposing the database to the  public internet. Why other options are incorrect: -  B: While web servers often need outbound intern et access  (e.g., for updates), this isn��t explicitly  required  here. Security groups allow a ll outbound traffic  by default unless  restricted, so explicitly allowing it  is unnecessary unless stricter  rules are  needed.  -  D/E: The  database  doesn��t  need  inbound  HTTPS  (p ort 443) or outbound access to the web tier.  Databases typically only accept connect i ons (inbound) from trusted sources (the web tier) on their specific  port (1433). Outbo und rules for databases are  rarely  needed  unless initiating external connections, which	isn��t  mentioned  here.  Reference  Links:  -  [AWS  Security  Groups](https://docs.aws.amaz on.com/vpc/latest/user guide/VPC_Security groups.html) -  [Microsoft  SQL Server on Ama zon  EC2](https://docs.aws.amazon.com/AWSEC2/latest/Windows guide/EC2Win_Sql serve r.html)


7.A company hosts its web applications in the AWS Cloud. The company configures  E lastic  Load  Balancers to  use  certificates that are  imported  into AWS Certificate  Manag er (ACM). The company's security team  must  be  notified 30 days  before the exp i ratio n of each certificate. What should a solutions architect  recommend to  meet this  requi rement?

A��Add a  rule  in ACM to  publish a custom  message to an Amazon Simple  Notifica tio n Service (Amazon SNS) topic every day,  beginning 30 days  before any certificate will expire.
B��Create an AWS Config rule that checks for certificates that will expire within 30 da ys. Configure Amazon  Event bridge  (Amazon Cloud watch  Events) to  invoke a custom a lert  by way of Amazon Simple  Notification Service  (Amazon SNS) when AWS Config  r eports a noncompliant resource.
C��Use AWS Trusted Advisor to check for certificates that will expire within 30 days. C reate an Amazon Cloud watch alarm that  is  based on Trusted Advisor  metrics for che ck status changes. Configure the alarm to send a custom alert  by way of Amazon Si mple  Notification Service  (Amazon SNS).
D��Create an Amazon  Event bridge  (Amazon Cloud watch  Events)  rule  to detect any cer tificate s that will expire within 30 days. Configure the rule to invoke an AWS  Lambda	function. Configure the  Lambda function to send a custom alert  by way of Amazon Simple  Notification Service (Amazon SNS).
�� ��BD



���� �� Correct Answer  D  Detailed  Explanation The correct approach  is to  use Amazon Event bridge (Cloud watch  Events) to schedule a  periodic check  (e.g., daily) for certifica te expiration. This triggers an AWS  Lambda function that  programmatically checks all certificates in AWS Certificate  Manager  (ACM). If a certificate  is found to expire within 30 days, the  Lambda function sends an alert via Amazon SNS to  notify the security  team. Why this works: - ACM does not automatically  monitor expiration dates for im

ported certificates (only ACM-issued certificates trigger  built-in notifications).  - A  Lam bda function allows custom  logic to calculate the 30-day window, ensuring  precise al erts. -  Event bridge ensures  regular checks without  manual intervention. Why other op t ions are  less ideal:  - A: ACM��s  built-in  notifications only apply to ACM-issued certifi cates and cannot  be customized for a 30-day alert.  -  B: AWS Config  requires custom rules and continuous  monitoring, which is overly complex for this use case.  - C: Trust ed Advisor��s certificate check is  limited to certificates actively  in use (e.g., on  ELBs) a nd requires a  Business/Enterprise support  plan.  Reference  Link  [AWS  Certificate  Mana ger  FAQs](https://aws.amazon.com/certificate-manager/faqs/)


8.A survey company has gathered data for several years from areas  in the  United Sta tes. The company  hosts the data  in an Amazon  S3  bucket that  is  3 TB  in size and gr owing. The company has started to share the data with a  European  marketing firm th at  has S3  buckets. The company wants to ensure that  its data transfer costs  remain a s  low as  possible. Which solution will  meet these  requirements?

A��Configure the  Requester  Pays feature  on the company's S3  bucket.

B��Configure S3 Cross-Region  Replication from the company's S3  bucket to one of th e  marketing firm's  S3  buckets.

C��Configure cross-account access for the marketing firm so that the  marketing firm  h as access to the company's S3 bucket.
D��Configure the company's S3  bucket to  use S3 Intelligent-Tiering. Sync the S3  buck et to one of the marketing firm's S3  buckets.
�� ��AB

���� �� Correct Answer: A. Configure the  Requester  Pays feature on the company's S3 bucket.  Detailed  Explanation: To  minimize data transfer  costs for the survey company, enabling  Requester  Pays  (Option A)  is the  best solution.  Here��s why:  -  Requester  Pa ys shifts all costs  (request fees  +  data transfer fees) to the  marketing firm when they access the data. - Without this, the survey company would pay for request fees (e.g., S3 GET  requests) even if the  marketing firm  pays for cross-region data transfer.  For  l arge datasets,  request fees can add  up quickly. - Option  B  (Cross-Region  Replication) would force the survey company to pay for cross-region replication costs, increasing 	their expenses. - Option C (cross-account access) doesn��t shift request fees to the marketing firm,  leaving the survey company with some costs. - Option  D  (Intelligent-T iering  +  sync) focuses on storage costs,  not transfer costs, and syncing would still  inc ur transfer fees for the survey company.  Reference  Links:  -  [Requester  Pays  in  S3](htt ps://docs.aws.amazon.com/AmazonS3/latest/user guide/Requester pays buckets.html) -  [S3 Pricing](https://aws.amazon.com/s3/pricing/)


9.A company is storing sensitive user information in an Amazon S3 bucket. The comp any wants to  provide secure access to this  bucket from the application tier  running o n Amazon  EC2  instances inside a VPC. Which combination of steps should a solutions architect take to accomplish this? (Choose two.)
A��Configure a VPC gateway endpoint for Amazon S3 within the VPC.

B��Create a  bucket  policy to  make the objects  in the  S3  bucket  public.

C��Create a  bucket  policy that  limits access to only the application tier  running  in the VPC.

D��Create an IAM  user with an S3 access  policy and copy the IAM credentials to the EC2 instance.
E��Create a  NAT  instance  and  have the  EC2  instances  use  the  NAT  instance  to access the S3  bucket.
�� ��AC



���� �� Correct Answer A, C  Detailed  Explanation To securely connect  EC2 instances in a VPC to an S3  bucket:  1. Option A  (VPC  Gateway  Endpoint for  S3) A VPC gateway e ndpoint creates a  private, secure connection between the VPC and Amazon S3. Traffic	stays within the AWS  network  (no  public internet exposure), improving security and avoiding data transfer costs. 2. Option C (Bucket  Policy  Restricting VPC Access) A  buc ket  policy can  limit access to the  S3  bucket only to  requests originating from the VP C. This is done by specifying the VPC endpoint ID or the VPC��s IP  range  in the  poli cy, ensuring  no external access  is allowed. Why Other Options Are Incorrect:  -  B:  Mak ing objects  public is  insecure and violates the  requirement.  -  D:  Hardcoding IAM  cred entials on  EC2  instances  is  risky. Instead,  use IAM  roles  (temporary credentials) attach ed to  EC2.  -  E:  NAT  instances  are  for outbound  internet access,  not  needed  here. A  gateway endpoint is simpler and  more secure.  Reference  Links  -  [VPC  Endpoints  for  S 3](https://docs.aws.amazon.com/AmazonS3/latest/user guide/example-bucket-policies-vpc -endpoint.html) -  [S3  Bucket  Policies](https://docs.aws.amazon.com/AmazonS3/latest/us erguide/using-iam-policies.html)


10.A company wants to  migrate an on-premises data center to AWS. The data center hosts an SFTP server that stores its data on an  NFS-based file  system. The server  ho

lds 200 GB of data that  needs to  be transferred. The  server  must  be  hosted on an A mazon  EC2  instance that  uses an Amazon  Elastic  File  System  (Amazon  EFS)  file syste m. Which combination of steps should a solutions architect take to automate this tas k? (Choose two.)
A�� Launch the  EC2  instance into the same Availability Zone as the  EFS file  system.

B��Install an AWS  Data sync agent  in the on-premises data center.

C��Create a secondary Amazon  Elastic  Block  Store  (Amazon  EBS) volume  on the  EC2  i nstance for the data.
D��Manually use an operating system copy command to  push the data to the  EC2  ins tance.
E�� Use AWS  Data sync to create a suitable  location configuration for the on-premises SFTP server.
�� ��BE



���� �� Correct Answer  B. Install an AWS  Data sync agent in the on-premises data cent er.  E.  Use AWS  Data sync to  create a suitable  location configuration for the on-premis es SFTP server.  Detailed  Explanation To  automate the  migration of 200 GB of data fr om an on-premises SFTP server  (using  NFS) to an Amazon  EC2  instance with Amazon EFS:  1. Option  B  is  required because AWS  Data sync  needs an agent deployed  in the on-premises environment to securely connect and transfer data to AWS. 2. Option  E	is necessary to configure the on-premises SFTP server as a source  location in  DataSy nc, enabling automated data transfer to  EFS  (the destination  location). Why other opti

ons are  incorrect:  - A:  EFS  is  a  regional  service,  so the  EC2  instance  doesn��t  need to be in the same Availability Zone.  - C: The  requirement specifies  EFS,  not  EBS.  -  D:  M anual copying defeats the goal of automation.  Reference  Links  -  [AWS  Data sync  Docu mentation](https://docs.aws.amazon.com/data sync/latest/user guide/how-it-works.html) -	[EFS  Multi-AZ Availability](https://docs.aws.amazon.com/efs/latest/ug/how-it-works.htm l)


11.A solutions architect  must design a  highly available infrastructure for a website. Th e website is  powered  by Windows web servers that  run on Amazon  EC2  instances. Th e solutions architect  must implement a solution that can  mitigate a  large-scale  DDoS attack that originates from thousands of IP addresses.  Downtime  is  not acceptable for	the website. Which actions should the solutions architect take to  protect the website	from such an attack?  (Choose two.)
A�� Use AWS Shield Advanced to stop t he  DDoS attack.

B��Configure Amazon Guard Duty to automatically  block the attackers.

C��Configure the website to use Amazon Cloud Front for  both static and dynamic cont ent.
D��Use an AWS  Lambda function to automatically add attacker IP addresses to VPC  n etwork ACLs.
E��Use  EC2  Spot Instances in an Auto Scaling group with a target tracking scaling  pol icy that is set to 80% CPU  utilization.

���� �� Correct Answer: A, C  Detailed  Explanation: A) AWS Shield Advanced is specific al ly designed to  protect against  DDoS attacks. It  provides advanced detection, automati c  mitigations, and 24/7 support from the AWS Shield  Response Team.  For  mission-crit ical applications where downtime is unacceptable, Shield Advanced is the right choice	to  mitigate  large-scale  DDoS  attacks. C) Amazon Cloud Front is a content delivery  net work (CDN) that distributes traffic across AWS��s global edge network.  By  routing  bot h static and dynamic content through Cloud Front, the origin servers  (EC2 instances) a re shielded from direct exposure to the internet. Cloud Front also integrates with AWS Shield Advanced, enhancing  DDoS  protection by absorbing and scattering attack traff ic. Why other options are incorrect:  -  B) Guard Duty  detects threats  but does  not auto matically  block attackers  in  real-time during a  DDoS  attack.  -  D)  Lambda  +  VPC  NAC Ls is impractical  because  NACLs  have  a  low  rule  limit  (max  20  rules)  and cannot scale	to  block thousands of IPs.  -  E)  Spot Instances  + Auto  Scaling  helps with  cost optimi zation and scaling  but does  nothing to stop  DDoS traffic.  Reference  Links:  [AWS  Shiel d Advanced](https://aws.amazon.com/shield/)  [Amazon Cloud front  DDoS  Protection](ht tps://aws.amazon.com/cloud front/ddos-protection/)


12.A  bicycle sharing company is developing a  multi-tier architecture to track the  locat ion of its  bicycles during peak operating  hours. The company wants to  use these dat a  points in its existing analytics  platform. A solutions architect  must determine the  m ost viable multi-tier option to support this architecture. The data  points  must  be acce ssible from the  REST API. Which action  meets these  requirements for storing and  retri evi ng  location data?

A�� Use Amazon Athena with Amazon S3.

B�� Use Amazon API Gateway with AWS  Lambda.

C�� Use Amazon Quick sight with Amazon  Redshift.

D�� Use Amazon API Gateway with Amazon  Kinesis  Data Analytics.

�� ��BD



���� �� Correct Answer  D.  Use Amazon API Gateway with Amazon  Kinesis  Data Analytic s.  Detailed  Explanation The  bicycle  sharing company  needs to track  real-time  location data during peak  hours and  make this data accessible via a  REST API for analytics.
Here��s why Option  D  is the  best  choice:  1.  Real-Time  Data  Handling  -  Amazon  Kines is  Data Analytics  is designed for  real-time  processing of streaming data  (e.g., GPS  up dates from  bikes). It can  ingest, analyze, and transform  high-volume data  in  real time. This ensures the system handles peak-hour traffic efficiently. 2.  REST API Integration - Amazon API Gateway acts as the front door for the  REST API. It can directly  ingest data from  bike devices (via  HTTP  requests)  and  route  it to  Kinesis  Data  Analytics for processing.  For  retrieval, API Gateway can also expose endpoints to fetch  processed d ata (e.g., stored in S3 or  Dynamo db via  Lambda).  3. Analytics  Compatibility  -  Kinesis Data Analytics  processes the stream and can output  results to storage  (like Amazon S 3 or a database). The existing analytics  platform can then access this data.  For examp le, processed data  in S3 can  be queried  using Amazon Athena, or  real-time  results ca n  be stored in  Dynamo db for  low-latency API access. Why Other Options Are  Less  Su itable - A (Athena  +  S3): Athena/S3  is for  batch  analytics,  not  real-time  streaming.  -  B (API Gateway  +  Lambda):  Lambda  alone  isn��t  optimized for  high-throughput,  real-t ime streams.  - C  (Quick sight  +  Redshift):  Redshift  is  for warehousing  structured data,

not real-time ingestion.  Reference  Links  -  [Amazon  Kinesis  Data  Analytics](https://aws. amazon.com/kinesis/data-analytics/) -  [Amazon API Gateway](https://aws.amazon.com/a pi-gateway/)


13.A social media company allows users to upload images to its website. The website	runs on Amazon  EC2  instances.  During  upload  requests, the website  resizes t he  imag es to a standard size and stores the resized images in Amazon S3. Users are experien c ing slow upload  requests to the website. The company  needs to  reduce coupling wit hin t he application and improve website performance. A solutions architect  must desi gn the  most operationally efficient  process for image  uploads. Which combination of actions should the solutions architect take to  meet these  requirements?  (Choose two.)

A��Configure the application to upload images to S3 Glacier.

B��Configure the web server to upload the original images to Amazon S3.

C��Configure the application to upload images directly from each user's browser to A mazon S3 through the  use of a  pre signed  URL
D��Configure S3  Event  Notifications to  invoke an AWS  Lambda  function when an  i ma ge is uploaded.  Use the function to  resize the  image.
E��Create an Amazon  Event bridge  (Amazon Cloud watch  Events)  rule that  invokes  an A WS  Lambda function on a schedule to  resize  uploaded  images.

���� �� Correct Answer C,  D  Detailed  Explanation To  reduce  coupling and  improve  perf ormance, the solutions architect should: - C.  Use  pre signed  URLs for direct  browser-to -S3 uploads: This  bypasses the  EC2  instances entirely, allowing  users to  upload  image s directly to S3. This  reduces  load on the  EC2 servers and speeds  up  uploads since t here��s  no  intermediate  step.  -  D.  Use  S3  Event  Notifications  with  Lambda  for  resizing: When an image is uploaded to S3, S3 triggers a  Lambda function automatically via  event  notifications.  Lambda  resizes the  image asynchronously, decoupling the  resizing process from the upload flow. This makes the system more scalable and efficient. W hy  not other options:  - A: S3 Glacier is for archival storage,  not frequent  uploads.  -  B: Uploading via  EC2 still ties  resizing to the web server, causing bottlenecks.  -  E: Sche du led resizing via  Event bridge would delay  processing and  is  less efficient than event- driven (S3 notifications).  Reference  Links  -  [S3  Pre signed  URLs](https://docs.aws.amazo n.com/AmazonS3/latest/user guide/Share object pre signed url.html) -  [S3  Event  Notifica ti ons with  Lambda](https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html)


14.A company uses a  popular content  management system  (CMS) for its corporate w ebsite.  However, the  required patching and  maintenance are  burdensome. The compa ny is  redesigning its website and wants anew solution. The website will be updated f our times a year and does  not  need to  have any dynamic content available. The solu tion must  provide high scalability and enhanced security. Which combination of chan ges will meet these  requirements with the  LEAST operational overhead?  (Choose two.)

A��Configure Amazon Cloud front in front of the website to use  HTTPS functionality.

B��Deploy an AWS WAF web ACL in front of the website to provide  HTTPS functional ity.

C��Create and deploy an AWS  Lambda function to  manage and  serve the website con tent.
D��Create the  new website and an Amazon S3  bucket.  Deploy the website  on the S3 bucket with static website hosting enabled.
E��Create the  new website.  Deploy the website  by  using  an Auto Scaling group of Am azon  EC2  instances  behind an Application  Load  Balancer.
�� ��AD



���� ��Correct Answer: A and  D  Detailed  Explanation: The  best  solution  here  is to  host	the static website on Amazon S3 (Option  D) and  use Amazon Cloud Front  (Option A). Here's why: 1. Amazon S3 Static Website  Hosting (Option  D)  -  Perfect for websites  with  no dynamic content (as specified in the  requirement). -  Requires zero server  ma nagement (no patching/maintenance),  reducing operational overhead. - Automatically  scales to  handle traffic spikes  (high scalability).  - Costs  less than  running  EC2  instance s or  Lambda functions for static content. 2. Amazon Cloud Front  (Option A)  - Acts as a CDN  (Content  Delivery  Network) to cache content globally,  improving  performance.
-  Enforces  HTTPS  by  default  (enhanced security)  using AWS  -  managed SSL/TLS certifi cates.  -  Provides  DDoS  protection and  integrates with AWS WAF  (though AWS WAF  i sn't strictly  needed  here since the question only asks for  HTTPS functionality). Why ot her options are wrong: - Option  B: AWS WAF doesn't  provide  HTTPS functionality.  HT TPS  is  handled  by Cloud front/Load  Balancer.  - Option  C:  Lambda  is  overkill for a stat ic website updated only 4 times/year. S3 is simpler and cheaper. - Option  E:  EC2  +  A uto Scaling  requires ongoing  maintenance (patching, scaling policies), which violates t

he  "least  operational overhead"  requirement. In summary: S3 for static  hosting  +  C lou dFront for  HTTPS/CDN  is the  most serverless,  low  -  maintenance, and  secure solution. Reference  Links: -  [Amazon  S3 Static Website  Hosting](https://docs.aws.amazon.com/A mazonS3/latest/user guide/Website hosting.html) -  [Cloud front  HTTPS](https://docs.aws.a mazon.com/Amazon cloud front/latest/Developer guide/using-https.html)


15.A company runs its two-tier ecommerce website on AWS. The web tier consists of a load balancer that sends traffic to Amazon  EC2 instances. The database tier  uses a n Amazon  RDS  DB  instance.  The  EC2  instances and t he  RDS  DB  instance  should  not be exposed to the  public internet. The  EC2  instances  require  internet access to compl ete payment processing of orders through a third-party web service. The application must be highly available. Which combination of configuration options will meet these	requirements? (Choose two.)

A��Use an Auto Scaling group to  launch the  EC2  instances  in  private  sub nets.  Deploy an  RDS  Multi-AZ  DB  instance  in  private  sub nets.

B��Configure a VPC with two  private subnets and two  NAT gateways across two Avail ability Zones.  Deploy an Application  Load  Balancer  in the  private  sub nets.

C�� Use an Auto Scaling group to  launch the  EC2  instances  in  public  sub nets across t wo Availability Zones.  Deploy an  RDS  Multi-AZ  DB  instance  in  private  sub nets.
D��Configure a VPC with one public subnet, one  private subnet, and two  NAT gatewa ys across two Availability Zones.  Deploy an Application  Load  Balancer  in the  public  su b net.  D. Configure a VPC with two  public sub nets, two  private  sub nets, and two  NAT

 gateways across two Availability Zones.  Deploy an Application  Load  Balancer  in the public sub nets.
�� ��AD



���� �� Correct Answer A,  D  Detailed  Explanation The  requirements are:  1.  EC2 ��  RDS  in  private sub nets  (no  public exposure).  2.  EC2  needs  internet access  (for  payment  pr o cessing). 3.  High availability  (Multi  - AZ setup). Option A  - Auto Scaling  in  private s ub nets ensures  EC2  instances are  not  public.  -  RDS  Multi  -  AZ  in  private  sub nets  pro vides  high availability and security. Option  D  - VPC with two  public sub nets  (for ALB and  NAT  gateways) and two  private  sub nets  (for  EC2/RDS).  - Two  NAT  gateways  (one per AZ) allow  EC2 in private subnets to access the internet (for payments) without p ublic IPs.  - ALB  in  public sub nets  routes traffic to  EC2  in  private  sub nets. Why  other  options fail:  -  B: ALB  in  private  subnets  can��t  receive  public traffic.  -  C:  EC2  in  public	sub nets violates the  "no  public  exposure"  rule.  Reference  Links  -  [AWS  NAT  Gateway] (https://docs.aws.amazon.com/vpc/latest/user guide/vpc - nat - gateway.html) -  [High A vail ability for Amazon  RDS](https://docs.aws.amazon.com/Amazon rds/latest/User guide/ Concepts.MultiAZ.html)


16.A company is  running a  multi-tier web application on  premises. The web applicatio n is containerized and runs on a  number of  Linux  hosts connected to a  Postgresql  database that contains user  records. The operational overhead of maintaining the infr a structure and capacity  planning  is  limiting the company's growth. A solutions archite ct must improve the application's infrastructure. Which combination of actions should	the solutions architect take to accomplish this? (Choose two.)

A�� Migrate the  Postgresql database to Amazon Aurora.

B�� Migrate the web application to  be  hosted on Amazon  EC2  instances.

C��Set up an Amazon Cloud Front distribution for the web application content.

D��Set up Amazon  Elastic ache  between the web application and the  Postgresql  data base.
E�� Migrate the web application to  be  hosted on AWS  Fargate with Amazon  Elastic  Co ntainer Service (Amazon  ECS).
�� ��AE



���� �� Correct Answer: AE  Detailed  Explanation: The company wants to  reduce operati onal overhead and eliminate  manual capacity  planning. - A  (Migrate  Postgresql to A mazon Aurora): Amazon Aurora is a fully managed  relational database service compati ble with  Postgresql. It automates tasks  like  backups,  patching, and scaling,  reducing  the maintenance  burden compared to self-managed  Postgresql. This directly address es the database  management  pain  point.  -  E  (Migrate to AWS  Fargate with Amazon  ECS): AWS  Fargate  is a  serverless compute engine for containers.  By  using  Fargate wit h Amazon  ECS, the company avoids  managing  EC2  instances  (no  need to  handle  serv er  provisioning, scaling, or OS  maintenance). Containers  run on  Fargate with automati c  resource allocation, solving capacity  planning challenges. Why  not other options:  -  B (EC2):  Requires  manual server  management, which  doesn��t  reduce operational over head. - C (Cloud Front): Improves content delivery  but doesn��t address infrastructure management. -  D  (Elastic ache): Adds a caching  layer for  performance  but  doesn��t sol

ve the core issue of maintaining servers/databases.  Reference  Links:  [AWS  Aurora](http s://aws.amazon.com/rds/aurora/)  [AWS  Fargate](https://aws.amazon.com/fargate/)


17.A company is  migrating its on-premises  Postgresql database to Amazon Aurora  P ostg re sql. The on-premises database  must  remain online and accessible during the
migration. The Aurora database  must  remain synchronized with the on-premises data base. Which combination of actions must a solutions architect take to  meet these  req ui rements? (Choose two.)

A��Create an ongoing  replication task.

B��Create a database  backup of the on-premises database.

C��Create an AWS  Database  Migration  Service  (AWS  DMS)  replication  server.

D��Convert the database schema  by using the AWS Schema Conversion Tool (AWS SC T).
E��Create an Amazon  Event bridge  (Amazon Cloud watch  Events)  rule to  monitor the  d atabase synchronization.
�� ��AC



���� �� Correct Answer: A, C  Detailed  Explanation: To  migrate an on-premises  Postgres QL database to Amazon Aurora  Postgresql while  keeping the source database online and synchronized, AWS  Database  Migration  Service (AWS  DMS)  is the  right tool.  Her e��s why: 1. Option A  (Create an ongoing  replication task): AWS  DMS  uses  ongoing  r e plication tasks to continuously  replicate changes from the source (on-premises  Postg

reSQL) to the target  (Aurora). This ensures synchronization during and after the initial data  migration. 2. Option C (Create a  DMS  replication server): A  replication  instance  in  DMS acts as the  middleman that  handles data transfer  between the  source and tar get. Without this, the  migration can��t  proceed. Why other options are  incorrect:  -  Op tion  B:  Backups  are  not  required  here  because  DMS  handles  live  replication.  -  Option D: Schema conversion isn��t needed since Aurora  Postgresql  is compatible with  Post greSQL. - Option  E:  DMS  has  built-in  monitoring;  Event bridge  isn��t  mandatory  for  sy n chronization.  Reference  Links:  -  [AWS  DMS  Documentation](https://docs.aws.amazon.c om/dms/latest/user guide/Welcome.html) -  [AWS  DMS  Continuous  Replication](https://d ocs.aws.amazon.com/dms/latest/user guide/CHAP_Task.CDC.html)


18.A solutions architect  needs to  help a company optimize the cost of running an ap plication on AWS. The application will use Amazon  EC2  instances, AWS  Fargate, and  AWS  Lambda for compute within the architecture. The  EC2  instances will  run the data	ingestion layer of the application.  EC2  usage will  be sporadic and  unpredictable. Wor kloads that  run on  EC2  instances can  be  interrupted at any time. The application fron t end will run on  Fargate, and  Lambda will  serve the API  layer. The front-end  utiliza ti on and API  layer  utilization will  be  predictable over the course of the  next year. Whic h combination of purchasing options will provide the  MOST cost-effective solution for hosting this application? (Choose two.)

A�� Use Spot Instances for the data  ingestion  layer

B�� Use On-Demand Instances for the data ingestion  layer

C�� Purchase a  1-year Compute  Savings  Plan for the front end and API  layer.

D�� Purchase  1-year All  Upfront  Reserved  instances for the  data  ingestion  layer.

E�� Purchase a  1-year  EC2  instance  Savings  Plan  for the front end and API  layer.

�� ��AC



���� �� Correct Answer: A.  Use Spot Instances for the data  ingestion layer C.  Purchase a  1-year Compute Savings  Plan for the front end and API  layer  Explanation:  1.  Spot I n stances (Option A):  - The data ingestion  layer uses  EC2 with sporadic,  unpredictable workloads that can  be  interrupted. - Spot Instances are 90% cheaper than On-Deman d  but can  be terminated  by AWS with short  notice. This fits  perfectly for fault-toleran t, interruptible workloads  like the data  ingestion  layer.  - Why  not On-Demand  (B)? O n-Demand is  more expensive and unnecessary  here since the workload can tolerate in terr up tions. 2. Compute Savings  Plan  (Option C):  - The front end (Fargate) and API  la yer (Lambda)  have  predictable  usage over  1 year.  - A  Compute Savings  Plan gives a discount (up to 66%) for committing to a consistent amount of compute usage (mea sured in  $/hour) across  Fargate,  Lambda,  and  EC2. It��s  flexible and covers  multiple se rvices.  - Why  not  EC2  Savings  Plan  (E)?  EC2  Savings  Plans  only  cover  EC2  usage,  but the front end (Fargate) and API  layer  (Lambda) are  not  EC2. A  Compute Savings  Plan	covers all three services. Avoid These  Mistakes:  -  Reserved Instances  (D):  Reserved  In stances  require  predicting  EC2  usage for  1 year,  but the  data  ingestion  layer  is  "spora dic and unpredictable"��this would waste  money. - On-Demand (B):  More  expensive t han Spot for interruptible workloads.  Reference  Links:  -  [AWS  Spot Instances](https://a ws.amazon.com/ec2/spot/) -  [AWS  Savings  Plans](https://aws.amazon.com/savings plans /)

19.A company wants to  migrate  its on-premises data center to AWS. According to th e company's compliance  requirements, the company can use only the ap-northeast-3 Region. Company administrators are  not permitted to connect VPCs to the internet. Which solutions will meet these  requirements? (Choose two.)

A�� Use AWS Control Tower to implement data  residency guardrails to deny  internet a ccess and deny access to all AWS  Regions except ap-northeast-3.

B�� Use  rules  in AWS WAF to  prevent  internet access.  Deny  access to all AWS  Regions except ap-northeast-3 in the AWS account settings.

C��Use AWS Organizations to configure service control policies (SCPS) that  prevent VP Cs from gaining internet access.  Deny access to all AWS  Regions  except ap-northeast -3.
D��Create an outbound  rule for the  network ACL  in each VPC to deny all traffic from 0.0.0.0/0. Create an IAM  policy for each  user to  prevent the  use of any AWS  Region other than ap-northeast-3.
E�� Use AWS Config to activate  managed  rules to detect and alert for  internet gatewa ys and to detect and alert for new  resources deployed outside of ap-northeast-3.
�� ��AC



���� ��Correct Answer A, C  Detailed  Explanation Option A  (Correct): AWS Control Towe r��s data  residency guardrails can enforce two critical  rules:  1.  Deny  internet  access: T his prevents VPCs from having internet gateways or other  paths to the  public interne t. 2.  Region  restriction: It  ensures resources are only deployed in the `ap-northeast-3`

Region. Control Tower is designed for large-scale compliance and governance, making	it a  robust solution for these  requirements. Option C  (Correct): AWS Organizations S ervice Control  Policies  (SCPs) can:  1.  Block  actions  like creating internet gateways  (to prevent VPCs from connecting to the internet). 2.  Use the `aws:Requested Region` con dition to restrict all operations to `ap-northeast-3`. SCPs apply to entire AWS account s, ensuring  no user or service can  bypass these  rules, even with admin  permissions.
Why Other Options Are Incorrect:  -  B: AWS WAF only filters web traffic  (e.g., for ALB/ Cloud Front),  not VPC  internet access.  Region  restrictions  in  account settings are  not g ranular or reliable. -  D:  Network ACLs  are  per-VPC  and  manual  (error-prone). IAM  pol icies for  Regions are  less effective than  SCPs.  -  E: AWS Config only detects violations	(alerts),  but doesn��t enforce compliance.  Reference  Links  -  [AWS  Control Tower  Data Residency Guardrails](https://docs.aws.amazon.com/control tower/latest/user guide/data-
residency-guardrails.html) -  [AWS Organizations SCPs for  Region  Restrictions](https://d ocs.aws.amazon.com/organizations/latest/user guide/orgs_manage_policies_scps_examples. html#example-scp-deny-region)


20.A company produces  batch data that comes from different databases. The compan y also  produces  live stream data from  network sensors and application APIs. The com pany needs to consolidate all the data  into one place for business analytics. The com pany needs to  process the incoming data and then stage the data  in different Amazo n S3  buckets. Teams will  later  run one-time queries and  import the data  into  a  busin ess intelligence tool to show key  performance  indicators (KPIs). Which combination of	steps will meet these  requirements with the  LEAST operational overhead?  (Choose tw o.)

A��Use Amazon Athena for one-time queries.  Use Amazon Quick sight to create dashb oards for  KPIs.

B�� Use Amazon  Kinesis  Data Analytics for  one-time queries.  Use Amazon Quick sight t
o create dashboards for  KPIs.

C��Create custom AWS  Lambda functions to  move the  individual  records from the dat abases to an Amazon  Redshift cluster.
D�� Use an AWS Glue extract, transform, and  load (ETL) job to convert the data  into J SON format.  Load the data  into  multiple Amazon Open search Service  (Amazon  Elastic search Service) clusters.
E��Use  blueprints  in AWS  Lake  Formation  to  identify the  data that can  be  ingested  int
o a data  lake.  Use AWS Glue to crawl the source, extract the data, and  load the data into Amazon S3 in Apache  Parquet format.
�� ��AE



���� �� Correct Answer: A and  E  Detailed  Explanation: The company needs to consolida te  batch data  (from databases) and streaming data  (from sensors/APIs)  into Amazon  S3 for analytics. The goal is to minimize operational overhead, so fully  managed servi ces are ideal. - Option A  is correct  because Amazon Athena allows  running one-time SQL queries directly on data stored in S3 (no infrastructure setup). Amazon Quick sigh t integrates seamlessly with Athena to visualize  KPIs,  both  being serverless  (low  opera tional effort). - Option  E  is correct  because AWS  Lake  Formation  automates data  inge stion into a centralized data  lake  (S3). AWS Glue crawlers auto-detect schemas, and G lue  ETL jobs efficiently convert data  into  Parquet  (a columnar format optimized for At

hena queries). This  minimizes  manual coding and infrastructure  management. Other o ptions are suboptimal: -  B:  Kinesis  Data  Analytics is for  real-time stream  processing,  n ot one-time queries. - C: Custom  Lambda functions and  Redshift  require  manual codi ng and cluster  management (high overhead).  -  D: Open search  is  better for search/an alytics on semi-structured  logs,  not  batch analytics. JSON is  less efficient than  Parquet	for querying.  Reference  Links:  -  [AWS  Lake  Formation](https://aws.amazon.com/lake-fo rmation/) -  [AWS Glue](https://aws.amazon.com/glue/)  -  [Amazon Athena](https://aws.a mazon.com/athena/) -  [Amazon Quick sight](https://aws.amazon.com/quick sight/)


21.A company stores data  in an Amazon Aurora  Postgresql  DB  cluster. The company must store all the data for 5 years and  must delete all the data after  5 years. The c ompany also must indefinitely keep audit  logs of actions that are  performed within t he database. Currently, the company has automated  backups configured for Aurora.
Which combination of steps should a solutions architect take to  meet these  require m ents? (Choose two.)
A��Take a  manual snapshot of the  DB cluster.

B��Create a  lifecycle  policy for the automated backups.

C��Configure automated  backup  retention for  5 years.

D��Configure an Amazon Cloud watch  Logs export for the  DB  cluster.

E�� Use AWS  Backup to take the  backups  and to  keep the  backups for  5 years.

�� ��DE

���� �� Correct Answer:  DE  Detailed  Explanation:  D. Configure an Amazon Cloud watch Logs export for the  DB cluster Audit  logs  must  be  kept  indefinitely. Aurora  Postgres q L can  publish  logs  (like audit  logs) to  Cloud watch  Logs. Once  exported, you can set Cloud watch  Logs  retention  policies to  "Never  Expire"  or  store them indefinitely in Am azon S3/Glacier using additional tools. This ensures audit logs  remain accessible  beyo nd the  5  - year data  retention  period.  E.  Use  AWS  Backup to take  the  backups and t o  keep the  backups for  5 years Aurora automated  backups  have a  maximum  re tent io n  period of 35 days. To  retain  backups for  5 years,  use AWS  Backup.  It allows  life cycl e  policies to automatically delete  backups after  5 years while maintaining compliance. AWS  Backup also automates backup  management, avoiding  manual snapshots  (Optio n A).  --- Why other options are  incorrect:  - A:  Manual  snapshots  require ongoing  ma nagement and don't automatically enforce the  5  - year deletion  rule. -  B: Aurora auto mated  backups cannot  retain data for  5 years  (max  35 days).  - C: Aurora automated backups don't support  5  - year  retention.  Reference  Links:  -  [AWS  Backup  Lifecycle  P olicies](https://docs.aws.amazon.com/aws - backup/latest/dev guide/lifecycle.html) -  [Exp orting Aurora  Postgresql  Logs to Cloud watch](https://docs.aws.amazon.com/AmazonR DS/latest/Aurora user guide/USER_Log access.html#USER_Log access.Procedural.Upload to cl oud watch)


22.A company is  running a  publicly accessible serverless application that  uses Amazon API Gateway and AWS  Lambda. The application��s traffic  recently spiked due to f rau du lent requests from botnets. Which steps should a solutions architect take to block  requests from unauthorized users? (Choose two.)

A��Create a  usage  plan with an API  key that  is shared with genuine  users only.

B��Integrate  logic within the  Lambda function to  ignore the  requests from fraudulent IP addresses.
C��Implement an AWS WAF  rule to target  malicious  requests and trigger actions to fil ter them out.
D��Convert the existing  public API to a  private API.  Update the  DNS  records  to  redire ct users to the  new API endpoint.
E��Create an IAM  role for each  user  attempting to access the API. A user will assume the role when making the API call.
�� ��AC



���� �� Correct Answer A, C  Detailed  Explanation A. Create a usage  plan with an API  k ey shared with genuine users only API Gateway usage  plans allow you to enforce  req uest  rate  limits and quotas for API consumers.  By  requiring  an API  key  (distributed o n ly to authorized users), you can  block unauthenticated traffic from  botnets. While AP I  keys alone are  not a  robust  security  mechanism, they act as a  simple first  layer of  defense against automated attacks when combined with other security  measures. C. I m plement AWS WAF  rules to filter  malicious  requests AWS WAF (Web Application  Fir ewall) is designed to  block  malicious  requests  by  inspecting  HTTP/HTTPS traffic. You  can create  rules to  block IP addresses, detect  SQL  injection, cross  - site scripting (XS S), or abnormal traffic  patterns (e.g., sudden spikes from botnets). Integrating AWS W AF with API Gateway  is the  most scalable and  maintainable way to filter out fraud ule nt  requests. Why Other Options Are Incorrect  -  B  (Lambda  -  based IP filtering):  Manu ally handling IP filtering in  Lambda  is  inefficient, error  -  prone, and  not  scalable for  h

igh traffic. AWS WAF  is  purpose  -  built for this task.  -  D  (Private API):  Converting a  p u blic API to a  private API would  block all  public access, which  contradicts the  require ment for the application to  remain  publicly accessible.  -  E  (IAM  roles  per  user):  IAM  r o les are impractical for public APIs, as they require AWS credentials for every user, w hich is unrealistic for external users.  Reference  Links  -  [AWS  API  Gateway  Usage  Plans] (https://docs.aws.amazon.com/api gateway/latest/developer guide/api - gateway - api -
usage - plans.html) -  [AWS WAF Integration with API Gateway](https://docs.aws.amazo n.com/api gateway/latest/developer guide/api gateway - control - access  - aws - waf.htm l)


23.A company is designing a cloud communications  platform that is driven  by APIs. T he application is  hosted on Amazon  EC2  instances  behind a  Network  Load  Balancer   (NLB). The company uses Amazon API Gateway to  provide external users with access to the application through APIs. The company wants to  protect the platform against web exploits  like SQL  injection and also wants to detect and mitigate  large, sophisti c ated  DDoS attacks. Which combination of solutions  provides the  MOST  protection?  (C hoose two.)

A�� Use AWS WAF to  protect the  NLB.

B�� Use AWS Shield Advanced with the  NLB.

C�� Use AWS WAF to protect Amazon API Gateway.

D�� Use Amazon Guard Duty with AWS Shield Standard

E�� Use AWS Shield Standard with Amazon API Gateway.

�� ��BC



���� �� Correct Answer:  B, C  Detailed  Explanation: To  protect  against SQL  injection (we b exploits),  use AWS WAF on API Gateway  (Option C). API Gateway  is the entry  point	for external users, so applying WAF  here inspects  incoming API  requests for  malic iou s  patterns.  For  sophisticated  DDoS  protection,  use AWS  Shield Advanced with the  NL B (Option  B). Shield Advanced  provides enhanced  DDoS  mitigation for  network-layer  attacks targeting the  NLB, while API Gateway already  includes Shield Standard (autom atic) for  basic  DDoS  protection.  - Why  not A?  NLB  doesn��t  support AWS WAF; WAF only works with ALB, API Gateway, or Cloud Front. - Why not  E?  Shield Standard  is al r eady  included for free with API Gateway,  but  it��s insufficient for  large/complex  DDoS attacks  mentioned  in the question.  - Why not  D? Guard Duty detects threats  but  does n��t directly  mitigate  DDoS  attacks.  Reference  Links:  [AWS  WAF  integrations](https://do cs.aws.amazon.com/waf/latest/developer guide/aws-waf-actions.html)  [AWS Shield Advan ced protections](https://docs.aws.amazon.com/shield/latest/developer guide/ddos-overvie w.html)


24.A company is developing an ecommerce application that will consist of a  load-bala nced front end, a container-based application, and a relational database. A solutions  architect  needs to create a  highly available solution that operates with as  little  manua
l intervention as  possible. Which solutions meet these requirements? (Choose two.)

A��Create an Amazon  RDS  DB  instance  in  Multi-AZ  mode.

B��Create an Amazon  RDS  DB  instance  and one or  more  replicas  in another Availabil i ty Zone.

C��Create an Amazon  EC2  instance-based  Docker  cluster to  handle the dynamic applic ation  load.
D��Create an Amazon  Elastic Container Service  (Amazon  ECS)  cluster with a  Fargate  la unch type to  handle the dynamic application load.
E��Create an Amazon  Elastic Container Service  (Amazon  ECS) cluster with an Amazon EC2 launch type to  handle the dynamic application  load.
�� ��AD



���� �� Correct Answer A and  D  Detailed  Explanation A. Create an Amazon  RDS  DB  ins tance in  Multi-AZ  mode:  -  Multi-AZ  automatically creates a standby  replica  in another Availability Zone (AZ) for high availability. - If the primary database fails,  RDS autom a tically fails over to the standby  replica with zero  manual  intervention.  - This ensures the relational database  layer  is  highly available and self-healing, aligning with the  req u irement for  minimal  manual effort.  D.  Create an Amazon  ECS  c luster with a  Fargate  l aunch type: -  Fargate  is a serverless compute engine that automatically  manages infra structure  (no  EC2  instances to  provision  or scale).  -  ECS with  Fargate  handles  scaling, load balancing, and AZ distribution automatically. - This provides  high availability for  the container-based application while minimizing operational overhead, as AWS  handl es server  management. Why other options are  incorrect:  -  B:  RDS  read  replicas  don't guarantee automatic failover (they're for  read scaling,  not  high availability).  - C:  EC2-b ased  Docker clusters  require  manual  scaling/management  of  EC2 instances.  -  E:  ECS  E C2 launch type  requires  manual cluster  management  (unlike  Fargate's fully  managed a

pproach).  Reference  Links  -  RDS  Multi-AZ:  https://aws.amazon.com/rds/features/multi-a z/ -  ECS  Fargate:  https://aws.amazon.com/fargate/


25.A company needs to store contract documents. A contract  lasts for  5 years.  During	the 5-year  period, the company must ensure that the documents cannot be over writ ten or deleted. The company  needs to encrypt the documents at  rest and  rotate the encryption keys automatically every year. Which combination of steps should a solutio ns architect take to meet these requirements with the  LEAST operational overhead?
(Choose two.)

A��Store the documents in Amazon S3.  Use S3 Object  Lock  in governance  mode.

B��Store the documents in Amazon S3.  Use S3 Object  Lock  in compliance  mode.

C��Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Con figure  key  rotation.
D��Use server-side encryption with AWS  Key  Management  Service  (AWS  KMS)  custome r  managed  keys. Configure  key  rotation.
E��Use server-side encryption with AWS  Key  Management  Service  (AWS  KMS)  custome r  provided (imported)  keys. Configure  key  rotation.
�� ��BD



���� �� Correct Answer:  B,  D  Detailed  Explanation: To  meet the  requirements with the  l east operational overhead:  1. S3 Object  Lock  in  compliance  mode (B) ensures docume nts cannot  be deleted or overwritten during the  5-year  retention  period,  even  by  root

 AWS accounts. Governance mode (A) allows  privileged users to  bypass  restrictions, w hich doesn't fully meet the requirement. 2. AWS  KMS customer-managed  keys with  k ey  rotation (D) automatically rotates encryption  keys yearly. SSE-S3 (C)  uses AWS-man aged  keys,  but  key  rotation  isn't  user-configurable. Customer-provided  keys  (E)  requir e  manual  key  management,  increasing operational effort. Why  not other options?  - A	(Governance  mode):  Lacks  strict enforcement against deletions.  - C  (SSE-S3):  No  user -controlled key  rotation.  -  E  (Imported  keys):  Manual  key  management  adds  overhead. KMS  + compliance  mode  provides automated, secure, and fully  managed compliance with minimal effort.  Reference  Links:  -  [S3 Object  Lock](https://docs.aws.amazon.com/ AmazonS3/latest/user guide/object-lock.html) -  [AWS  KMS  Key  Rotation](https://docs.aw s.amazon.com/kms/latest/developer guide/rotate-keys.html)


26.A  hospital wants to create digital copies for its  large collection of historical written	records. The  hospital will continue to add  hundreds of  new documents each day. Th e  hospital��s data team will scan the documents and will upload the documents to th e AWS Cloud. A solutions architect  must implement a solution to analyze the docum ents, extract the  medical  information, and store the documents so that an application can  run SQL queries on the data. The solution must maximize scalability and operati onal efficiency. Which combination of steps should the solutions architect take to  me et these requirements? (Choose two.)

A��Write the document information to an Amazon  EC2  instance that  runs a  MySQL  da tabase.

B��Write the document information to an Amazon S3 bucket.  Use Amazon Athena to query the data.

C��Create an Auto Scaling group of Amazon  EC2  instances to  run a custom applicatio n that  processes the scanned files and extracts the  medical information.
D��Create an AWS  Lambda function that  runs when  new  documents are  uploaded. Us e Amazon  Re k ognition to convert the documents to  raw text.  Use Amazon Transcribe Medical to detect and extract relevant medical information from the text.
E��Create an AWS  Lambda function that  runs when  new documents are  uploaded.  Us e Amazon T extract to convert the documents to raw text.  Use Amazon Comprehend Medical to detect and extract  relevant  medical information from the text.
�� ��BE



���� ��Correct Answer  B,  E  Detailed  Explanation  The correct combination  is  B and  E.  H ere��s why:  1. Option  B  (Amazon  S3  + Amazon  Athena) - Amazon S3 is ideal for stor ing large volumes of unstructured data (like scanned documents)  because  it is highly scalable, durable, and cost-effective. - Amazon Athena allows  running SQL queries dir ectly on data stored in S3 without  managing servers. This aligns with the  requirement	for operational efficiency and SQL query support. 2. Option  E  (Lambda  +  T extract  + Comprehend  Medical)  - AWS  Lambda automatically triggers when  new documents are uploaded to S3, ensuring real-time processing without managing servers (maximizing		scalability). - Amazon T extract  is designed to extract text and data from scanned do cuments (better suited than  Re k ognition, which focuses on  image/video analysis). - A mazon Comprehend  Medical specializes in detecting  medical  information  (e.g., diagno ses,  medications) from text,  making  it the  right tool for this use case  (unlike Trans crib e  Medical, which  is for speech-to-text  in audio). Why  not other options? - A  (EC2  +

MySQL):  Requires  managing  servers,  lacks  scalability, and  is  less efficient than S3  + At hena. - C (EC2 Auto Scaling): Overly complex compared to serverless solutions like  La mbda. -  D  (Re k ognition  + Transcribe  Medical):  Re k ognition  is  not  optimized for  docu ment text extraction, and Transcribe  Medical  is for audio,  not text analysis.  Reference Links -  [Amazon  S3](https://aws.amazon.com/s3/)  -  [Amazon Athena](https://aws.amazo n.com/athena/) -  [AWS  Lambda](https://aws.amazon.com/lambda/)  -  [Amazon  T extract] (https://aws.amazon.com/t extract/) -  [Amazon Comprehend  Medical](https://aws.amazo n.com/comprehend/medical/)


27.A company has a  Microsoft .NET application that  runs on an on-premises Windows Server. The application stores data by using an Oracle  Database Standard  Edition ser ver. The company is  planning a  migration to AWS and wants to  minimize developme nt changes while  moving the application. The AWS application environment should  be highly available. Which combination of actions should the company take to meet the se  requirements? (Choose two.)
A�� Refactor the application as serverless with AWS  Lambda functions  running .NET Co

re.

B�� Rehost the application  in AWS  Elastic  Beanstalk with the  .NET  platform  in  a  Multi- AZ deployment.
C��Re platform the application to  run on Amazon  EC2 with the Amazon  Linux Amazon Machine Image (AMI).
D�� Use AWS  Database  Migration  Service  (AWS  DMS) to  migrate  from the Oracle data base to Amazon  Dynamo db  in a  Multi-AZ  deployment.

E��Use AWS  Database  Migration  Service  (AWS  DMS)  to  migrate from the  Oracle datab ase to Oracle on Amazon  RDS  in a  Multi-AZ  deployment.
�� ��BE



���� �� Correct Answer:  B,  E  Detailed  Explanation: The  company aims to  migrate  its .NE T application and Oracle database to AWS with minimal development changes and  hi gh availability. - Option  B:  Re hosting the  app  using AWS  Elastic  Beanstalk with the  .N ET  platform in a  Multi-AZ  deployment is ideal. -  Elastic  Beanstalk  supports Windows  Server, allowing a  "lift-and-shift"  (rehost) approach with  no code changes.  -  Multi-AZ ensures high availability by automatically distributing instances across  multiple Availab ility Zones. - Option  E:  Migrating the  Oracle database to Oracle on Amazon  RDS  usin g AWS  DMS  in a  Multi-AZ  deployment  maintains compatibility.  -  RDS  Oracle  retains t he same database engine, avoiding application rewrites. -  Multi-AZ  RDS  provides  auto mated failover and  high availability. Why not other options: - A  (Lambda/.NET Core): Requires refactoring to serverless and .NET Core, which involves code changes. - C (E C2/Amazon  Linux): Switching from Windows to  Linux  may  require app/config changes.
 -  D  (Dynamo db):  Moving from  SQL  (Oracle) to  NoSQL  (Dynamo db) forces app  redes ign.  Reference  Links:  -  [AWS  Elastic  Beanstalk  for  .NET](https://aws.amazon.com/elastic beanstalk/) -  [Amazon  RDS for  Oracle  Multi-AZ](https://aws.amazon.com/rds/oracle/)  - [AWS  DMS](https://aws.amazon.com/dms/)


28.A company has a web server  running on an Amazon  EC2  instance  in a  public sub net with an  Elastic IP address. The default security group  is assigned to the  EC2  insta nce. The default  network ACL  has  been  modified to  block all traffic. A  solutions archit

ect  needs to  make the web server accessible from everywhere on  port 443. Which co m bination of steps will accomplish this task? (Choose two.)
A��Create a security group with a rule to allow TCP  port 443 from source 0.0.0.0/0.

B��Create a security group with a  rule to allow TCP  port 443 to destination 0.0.0.0/0. C�� Update the  network ACL to allow TCP  port 443 from source 0.0.0.0/0.

D�� Update the network ACL to allow inbound/outbound TCP  port 443 from source 0.
0.0.0/0 and to destination 0.0.0.0/0.

E��Update the  network ACL to allow  inbound TCP  port 443 from source 0.0.0.0/0 and outbound TCP port 32768-65535 to destination 0.0.0.0/0.

�� ��AE



���� �� Correct Answer: A,  E  Detailed  Explanation: To  make the web server accessible o n  port 443:  1. Security Group  (A):  - Security groups act as stateful firewalls  (they auto matically allow return traffic). - Option A adds an inbound  rule to allow TCP  port 443	from all IP addresses  (0.0.0.0/0), which is  required for clients to connect to the web server. 2.  Network ACL  (E):  -  Network ACLs  are  stateless, so  both  inbound and outbo und rules must  be explicitly configured. - Option  E allows  inbound traffic on  port 443		(for client  requests) and outbound traffic on ephemeral  ports 32768  - 65535 (to sen d  responses  back to clients). This matches  how TCP connections work: clients  use  ran dom ports (ephemeral  ports) to  receive  responses. Why other options are  incorrect:  - B: Security group rules specify source, not destination. - C:  Blocked outbound  respon ses (ACLs are stateless).  -  D: Allowing outbound  port 443  is  unnecessary  here  (server

responds via ephemeral  ports).  Reference  Links:  -  [AWS  Security  Groups vs.  NACLs](htt ps://docs.aws.amazon.com/vpc/latest/user guide/VPC_Security.html) -  [Ephemeral  Ports]  (https://docs.aws.amazon.com/vpc/latest/user guide/vpc-network-acls.html#nacl-ephemer al-ports)


29.A company has deployed a Java Spring  Boot application as a  pod that  runs  on A mazon  Elastic  Ku bernet es  Service  (Amazon  EKS)  in  private  sub nets. The application  ne eds to write data to an Amazon  Dynamo db table. A solutions architect  must ensure t hat the application can interact with the  Dynamo db table without exposing traffic to the internet. Which combination of steps should the solutions architect take to accom plish this goal? (Choose two.)

A��Attach an IAM  role that  has sufficient  privileges to the  EKS  pod.

B��Attach an IAM  user that  has sufficient  privileges to the  EKS  pod.

C��Allow outbound connectivity to the  Dynamo db table through the private sub nets�� network ACLs.
D��Create a VPC endpoint for  Dynamo db.

E�� Embed the access  keys  in the Java  Spring  Boot  code.

�� ��AD



���� ��Correct Answer: A,  D  Detailed  Explanation: A. Attach an IAM  role to the  EKS  po d: In AWS, applications  running  in  EKS  pods  need  permissions to  access AWS services	like  Dynamo db. Instead of using  hardcoded access  keys  (which  is  insecure) or IAM  u

sers (not ideal for dynamic workloads), you attach an IAM  role to the  pod. This  role grants the  necessary  permissions securely via AWS Identity and Access  Management   (IAM). The  pod temporarily assumes the  role to  interact with  Dynamo db without exp osing credentials.  D. Create a VPC endpoint for  Dynamo db:  Since the  EKS  pods  are i n  private sub nets (no  internet access), a VPC endpoint for  Dynamo db allows  private c on nec tivity  between the VPC and  Dynamo db. Traffic stays within the AWS  network, a voiding the public internet. This is a  "gateway" endpoint that  routes  Dynamo db  requ ests directly through AWS��s internal network, ensuring security and avoiding  NAT gat eways or internet gateways. Why  not other options?  -  B  (IAM  user): IAM  users  are fo r  human/static access,  not  pods.  Roles are  safer for dynamic workloads.  - C  (Network ACLs):  Dynamo db uses AWS - controlled IPs that change frequently.  Managing ACLs	is impractical. VPC endpoints are the proper solution. -  E  (Access  keys):  Hardcoding  keys in code is insecure and violates AWS best  practices.  Roles are temporary  and  m anaged  by AWS.  Reference  Links:  -  [IAM  Roles  for  Ku bernet es  Pods](https://docs.aws.a mazon.com/eks/latest/user guide/iam  -  roles - for  - service  - accounts.html)  -  [VPC  En dpoints for  Dynamo db](https://docs.aws.amazon.com/amazon dynamo db/latest/develope rguide/vpc - endpoints  - dynamo db.html)


30.A company recently  migrated its web application to AWS  by  re hosting the applicat ion on Amazon  EC2  instances  in a single AWS  Region. The company wants to  redesig n its application architecture to  be  highly available and fault tolerant. Traffic  must  rea ch all running  EC2  instances randomly. Which combination of steps should the compa ny take to  meet these  requirements?  (Choose two.)

A��Create an Amazon  Route  53 failover  routing  policy.

B��Create an Amazon  Route  53 weighted  routing  policy.

C��Create an Amazon  Route  53  multivalue answer  routing  policy.

D��Launch three  EC2  instances: two  instances  in one Availability Zone and one instanc e in another Availability Zone.
E�� Launch four  EC2  instances: two  instances in one Availability Zone and two instance s in another Availability Zone.
�� ��CE



���� ��Correct Answer: C,  E  Detailed  Explanation: To achieve  high availability and fault tolerance for the  EC2-based web application, the company  needs to distribute instanc es across  multiple Availability Zones (AZs) and use a  routing  policy that ensures traffi c  reaches all  healthy instances.  - Option C  (Route  53  Multivalue Answer  Routing  Polic y) allows  Route  53 to  return  multiple  healthy  EC2  instance  IP addresses  in  response t
o  DNS queries. Clients connect to these IPs  randomly, distributing traffic across  instan ces. It also  performs  health checks to avoid  routing traffic to  unhealthy  instances.  - O ption  E  (4  EC2  instances:  2  per AZ) ensures fault tolerance  by spreading  instances eve n ly across two AZs. If one AZ fails, the other AZ still  has two  instances to  handle traf fic. This avoids the  risk of uneven distribution  (e.g., 2  instances  in one AZ and  1  in  a nother, as in Option  D), which could overload the AZ with fewer  instances during fail ures. Why  not other options? - A (Failover  Policy):  Designed for  primary/backup  setup s,  not  random traffic distribution. -  B  (Weighted  Policy):  Requires  manual weight  assig nment, which isn��t  needed  here. -  D  (Uneven AZ  distribution):  Risks overloading the AZ with only  1  instance during failures.  Reference  Links:  -  [Route  53  Multivalue  Answ

er  Routing](https://docs.aws.amazon.com/Route53/latest/Developer guide/routing-policy- multivalue.html) -  [AWS  Fault-Tolerant Architecture](https://aws.amazon.com/architectur e/reliability/)


31.A company collects data from thousands of remote devices  by using a  RESTful we b services application that  runs on an Amazon  EC2  instance. The  EC2  instance  receive s the  raw data, transforms the  raw data, and stores all the data  in an Amazon S3  bu cket. The  number of remote devices will increase  into the millions soon. The compan y  needs a  highly scalable solution that  minimizes operational overhead. Which combin ation of steps should a solutions architect take to  meet these  requirements?  (Choose two.)

A�� Use AWS Glue to  process the  raw data  in Amazon S3.

B�� Use Amazon  Route  53 to  route traffic to  different  EC2  instances.

C��Add  more  EC2  instances to accommodate the  increasing amount of incoming data.

D��Send the  raw data to Amazon Simple Queue Service  (Amazon SQS).  Use  EC2  insta nces to  process the data.
E��Use Amazon API Gateway to send the  raw data to an Amazon  Kinesis  data stream. Configure Amazon  Kinesis  Data  Firehose to  use the data stream as a source to deliv er the data to Amazon S3.
�� ��AE

���� ��Correct Answer  E, A  Detailed  Explanation The  best solution  here  is to use Amaz on API Gateway and Amazon  Kinesis  Data  Streams  (option  E).  Here��s  why:  1. Amazon API Gateway scales automatically to handle millions of API requests without needing	to manage servers. This  replaces the  EC2-based  RESTful service,  eliminating operation al overhead. 2. Amazon  Kinesis  Data  Streams  ingests  high-volume  real-time data  (e.g.,		from millions of devices). 3. Amazon  Kinesis  Data  Firehose  (configured with  Kinesis  a s the source) automatically  batches the  raw data and delivers  it to Amazon S3.  For d ata transformation (previously done by  EC2),  use AWS Glue  (option A):  - AWS Glue is a serverless  ETL  (Extract, Transform,  Load) service. It can  process the  raw data stored			in S3 after ingestion,  replacing the  EC2-based transformation step. This  is  more scala ble and fully managed. Why other options are  incorrect:  - C  (Add  EC2  instances):  Ma nual scaling of  EC2 is operationally heavy and  not cost-effective for unpredictable wo rkloads.  -  D  (SQS  +  EC2):  While  SQS decouples data  ingestion and  processing,  EC2  st ill  requires scaling and  management.  -  B  (Route  53):  Route  53  is  for  DNS  routing,  not data  processing.  References -  [Amazon API Gateway](https://aws.amazon.com/api-gate way/) -  [Amazon  Kinesis  Data  Streams](https://aws.amazon.com/kinesis/data-streams/)  -				[AWS Glue](https://aws.amazon.com/glue/)


32.A solutions architect  has created a  new AWS account and  must secure AWS accou nt  root user access. Which combination of actions will accomplish this? (Choose two.)

A�� Ensure the  root user uses a strong  password.

B�� Enable  multi-factor authentication to the  root  user.

C��Store root user access  keys  in an encrypted Amazon  S3  bucket.

D��Add the  root user to a group containing administrative  permissions.

E��Apply the  required  permissions to the  root  user with an  inline  policy document.

�� ��AB



���� �� Correct Answer: A,  B  Detailed  Explanation: A.  Ensure the  root  user  uses a  stron g  password The  root  user  has full access to all AWS services and  resources  in the ac count.  Using a strong, complex  password  is the first and  most  basic step to  prevent unauthorized access. A weak  password could  be easily guessed or cracked, compro mi sing the entire account.  B.  Enable  multi-factor authentication  (MFA) to the  root  user MFA adds an extra  layer of security  by  requiring a  second authentication  method  (e. g., a code from a  hardware device or a  mobile app)  in addition to the  password.  Eve n if someone steals the  root  user's password, they won��t  be able to access the acco unt without the  MFA device. AWS strongly  recommends enabling  MFA for the  root  u ser. Why other options are  incorrect:  - C. Store  root  user access  keys  in an  encrypted Amazon S3  bucket AWS explicitly advises against creating access  keys for the  root u ser unless absolutely  necessary. Access  keys grant  programmatic access, which  is  risky	for the  root  user.  Even  if  stored  securely, this  is  not a  best  practice.  -  D.  Add t he  ro ot user to a group IAM groups and  policies apply only to IAM  users,  not the  root  us er. The  root  user cannot  be added to a group. -  E. Apply  inline  policies to the  root  user The  root  user  has  unrestricted  permissions  by default. You cannot attach IAM  po licies  (inline or  managed) to the  root  user.  Reference  Links:  -  [AWS  Root  User  Best  Pr actices](https://docs.aws.amazon.com/IAM/latest/User guide/best-practices.html#lock-awa y-credentials) -  [AWS  MFA for  Root  User](https://docs.aws.amazon.com/IAM/latest/User Guide/id_credentials_mfa_enable_virtual.html#enable-virt-mfa-for-root)

33.A company has deployed a database in Amazon  RDS for  MySQL.  Due  to  increased	transactions, the database support team is  reporting slow  reads against the  DB  insta nce and  recommends adding a  read  replica. Which combination of actions should a  s ol utions architect take  before  implementing this change? (Choose two.)

A�� Enable  bin log  replication on t he  RDS  primary  node.

B��Choose a failover  priority for the source  DB  instance.

C��Allow  long-running transactions to complete on the source  DB  instance.

D��Create a global table and specify the AWS  Regions where the table will  be availab le.
E�� Enable automatic  backups on the source instance  by setting the  backup  retention period to a value other than 0.
�� ��CE



���� ��Correct Answer: C,  E  Detailed  Explanation:  - C. Allow  long-running transactions t
o complete on the source  DB  instance  Before  creating a  read  replica,  it��s  important t
o let long-running transactions finish. This ensures data consistency during replication.	If active transactions are ongoing, the  read  replica  might start with  incomplete or in consistent data,  leading to errors or  replication  lag.  -  E.  Enable  automatic  backups  on	the source instance by setting the  backup  retention  period to a value other than 0  Read  replicas  in  RDS  rely  on  backups to  initialize the  replica  data. If automatic  back u ps are disabled (retention  period  =  0),  binary  logging  (bin log)  is  also disabled, which  is required for  replication.  Enabling  backups  ensures  bin log  is active and allows the  re

plica to sync  properly. Why other options are  incorrect:  - A:  Bin log  is automatically e nabled when  backups are turned on (via option  E),  so  no separate action  is  needed.
-  B:  Failover  priority  relates  to  Multi-AZ  deployments,  not  read  replicas.  -  D:  Global ta bles are a  Dynamo db feature,  irrelevant to  RDS  MySQL.  Reference:  [Amazon  RDS  Rea d  Replicas  Documentation](https://docs.aws.amazon.com/Amazon rds/latest/User guide/ USER_Read repl.html)


34.A solutions architect is implementing a document  review application using an Ama zon S3 bucket for storage. The solution must  prevent accidental deletion of t he docu ments and ensure that all versions of the documents are available.  Users  must  be abl e to download,  modify, and upload documents. Which combination of actions should be taken to meet these requirements? (Choose two.)
A�� Enable a  read-only  bucket ACL.

B�� Enable versioning on the bucket.

C��Attach an IAM  policy to the  bucket.

D�� Enable  MFA  Delete  on the  bucket.

E�� Encrypt the  bucket  using AWS  KMS.

�� ��BD



���� ��Correct Answer:  B,  D  Detailed  Explanation:  -  B.  Enable versioning  on the  bucket: Versioning ensures that every time a document is modified or deleted, S3 automatic ally saves  multiple versions of the object. This  means even if a  user accidentally de let

es a document, the previous versions remain accessible. Without versioning, over writ in g or deleting a file would  permanently  remove  it.  -  D.  Enable  MFA  Delete  on  the  buc ket:  MFA  (Multi-Factor Authentication)  Delete adds an extra  layer of  protection. It  req ui res users to  provide two forms of authentication  (e.g., a  password  +  a temporary  c ode f rom a  physical device)  before  permanently deleting a document or disabling ver sioning. This prevents accidental or unauthorized deletions. Why  not the other option s? - A: A read-only ACL would  block users from uploading/modifying documents, viol ating the requirement. - C: IAM  policies control  user permissions,  but the question fo cuses on protecting data  (not access control). Also, IAM  policies are attached to  users /roles,  not  buckets.  -  E:  Encryption  protects  data  at  rest  but  doesn��t  prevent deletion s or  preserve versions.  Reference  Links:  -  [Amazon  S3 Versioning](https://docs.aws.ama zon.com/AmazonS3/latest/user guide/Versioning.html) -  [MFA  Delete for  S3](https://doc s.aws.amazon.com/AmazonS3/latest/user guide/Multifactor authentication delete.html)


35.A company recently announced the deployment of its  retail website to a global au dience. The website  runs on  multiple Amazon  EC2  instances  behind  an  Elastic  Load  B a lancer. The instances  run in an Auto Scaling group across  multiple Availability Zones. The company wants to provide its customers with different versions of content base
d on the devices that the customers use to access the website. Which combination of actions should a solutions architect take to  meet these  requirements?  (Choose two.)

A��Configure Amazon Cloud Front to cache  multiple versions of the content.

B��Configure a  host  header  in a  Network  Load  Balancer  to  forward traffic to different instances.

C��Configure a  Lambda@Edge function to send specific objects to  users  based on the User-Agent  header.
D��Configure AWS Global Accelerator.  Forward  requests to a  Network  Load  Balancer (NLB). Configure the  NLB to set  up  host-based  routing to different  EC2  instances.
E��Configure AWS Global Accelerator.  Forward  requests  to a  Network  Load  Balancer  (N LB). Configure the  NLB to  set  up  path-based  routing to  different  EC2  instances.
�� ��AC



���� �� Correct Answer: A, C  Detailed  Explanation: A. Configure Amazon Cloud Front to cache  multiple versions of the content. Cloud Front  is a Content  Delivery  Network  (CD N) that caches content at edge locations globally.  By caching different versions of co ntent (e.g.,  mobile vs. desktop), users  receive the appropriate version faster,  reducing latency. This works well for static content or frequently accessed dynamic content. C. Configure a  Lambda@Edge function to send specific objects to  users  based on the  U ser-Agent  header.  Lambda@Edge allows you to  run  code at Cloud Front edge  location s.  By  inspecting the `User-Agent`  header  in the  request  (which  identifies the device ty pe, e.g., mobile, desktop), the  Lambda function can dynamically  route  requests to the	correct version of the content  (e.g.,  redirecting  mobile users to `/mobile/`  paths). Thi s ensures device-specific content delivery without  requiring changes to the backend  E C2 instances. Why not the other options? -  B:  Network  Load  Balancer  (NLB)  operates at the transport  layer  (TCP/UDP) and does not support  host-based  routing  (this  is a f eature of Application  Load  Balancer/ALB).  -  D/E: AWS  Global Accelerator optimizes tra ffic  routing to  improve  performance  but does  not  handle content versioning.  NLB  can

not perform  host-based or  path-based  routing at the application  layer.  -  B/D/E all  rel y on  lower-layer services (NLB) that  lack the ability to  inspect  HTTP  headers  like  `User -Agent`, which is critical for device detection.  Reference  Links:  -  [Amazon  Cloud Front] (https://aws.amazon.com/cloud front/) -  [Lambda@Edge](https://aws.amazon.com/lambd a/edge/)


36.A company is  building an application that consists of several  micro services. The co mpany has decided to use container technologies to deploy its software on AWS. The	company needs a solution that  minimizes the amount of ongoing effort for  mainten ance and scaling. The company cannot manage additional infrastructure. Which combi nation of actions should a solutions architect take to  meet these  requirements?  (Choo se two.)

A�� Deploy an Amazon  Elastic Container Service  (Amazon  ECS)  cluster.

B�� Deploy the  Ku bernet es control  plane on Amazon  EC2  instances that  span  multiple Availability Zones.

C��Deploy an Amazon  Elastic Container  Service  (Amazon  ECS)  service with an Amazon EC2  launch type. Specify a desired task  number  level of greater than or equal to 2.
D�� Deploy an Amazon  Elastic Container Service  (Amazon  ECS)  service with a  Fargate  l aunch type. Specify a desired task  number  level of greater than or equal to 2.
E��Deploy  Ku bernet es worker  nodes on Amazon  EC2  instances that  span  multiple Avai lability Zones. Create a deployment that specifies two or  more  replicas for each  micro service.

�� ��AD



���� �� Correct Answer A.  Deploy an Amazon  Elastic Container Service (Amazon  ECS) cl uster.  D.  Deploy  an Amazon  Elastic  Container Service (Amazon  ECS)  service with a  Far gate  launch type. Specify a desired task  number  level of greater than  or equal to 2. Detailed  Explanation The company wants to  minimize  maintenance and scaling efforts while avoiding infrastructure  management.  Here��s why A and  D are correct:  1. Amaz on  ECS  (A):  ECS  is AWS��s fully  managed container orchestration  service. It simplifies deploying and scaling containers without  requiring  manual infrastructure setup. 2.  Far gate  Launch Type  (D): AWS  Fargate  is a  serverless compute engine for containers. It  r emoves the  need to  manage  EC2  instances,  auto-scales tasks, and ensures  high availa bility  by spreading tasks across Availability Zones. Specifying  ��2 tasks  guarantees  red undancy. Why other options are wrong: -  B ��  E:  Require  managing  Ku bernet es  contr ol  planes or  EC2 worker  nodes, which adds  infrastructure  overhead.  - C: The  EC2  laun ch type forces the company to  manage  EC2  instances, scaling, and  maintenance.  Refe rences: -  [AWS  Fargate](https://aws.amazon.com/fargate/)  -  [Amazon  ECS](https://aws.a mazon.com/ecs/)


37.A company has a  multi-tier application deployed on several Amazon  EC2  instances	in an Auto Scaling group. An Amazon  RDS for Oracle  instance  is the application�� s data  layer that  uses Oracle-specific  PL/SQL functions. Traffic to the application has  be en steadily increasing. This is causing the  EC2  instances to  become overloaded and th e  RDS  instance to  run  out of storage. The Auto Scaling group does  not  have any sca ling  metrics and defines the  minimum  healthy instance count only. The company  pred

icts that traffic will continue to  increase at a steady  but  unpredictable  rate  before  lev eling off. What should a solutions architect do to ensure the system can automatic all y scale for the increased traffic? (Choose two.)
A��Configure storage Auto Scaling on t he  RDS for Oracle  instance.

B�� Migrate the database to Amazon Aurora to use Auto Scaling storage.

C��Configure an alarm on t he  RDS for Oracle  instance for  low free storage space.

D��Configure the Auto Scaling group to use the average CPU as the scaling  metric.

E��Configure the Auto Scaling group to use the average free memory as the scaling metric.
�� ��AD



���� ��Correct Answer A,  D  Detailed  Explanation  For  the  EC2  instances: Answer  D  is  co rrect  because configuring the Auto Scaling group to  use average CPU utilization as th e scaling metric allows automatic scaling based on workload demand. When CPU usa ge increases due to traffic, Auto Scaling adds  instances; when  usage drops, it  remove s instances. This ensures the  EC2  layer  scales dynamically.  For t he  RDS  storage: Answ er A is correct  because  RDS for Oracle supports  Storage Auto Scaling. This automatic ally increases storage capacity when it  runs low,  preventing outages caused by full st orage.  Migrating to Aurora  (B) isn��t  necessary  here, as Oracle-specific functions woul d  break compatibility with Aurora (which uses  MySQL/Postgresql). Configuring an a la rm (C) only alerts about  low storage  but doesn��t fix  it automatically.  Memory-based scaling (E)  is  less  reliable than CPU for general scaling, as  memory  usage  doesn��t al

ways correlate with traffic  load.  Reference  Links  -  [Amazon  EC2  Auto  Scaling Scaling  Plans](https://docs.aws.amazon.com/auto scaling/ec2/user guide/as-scaling-target-tracking. html) -  [RDS  Storage Auto Scaling](https://docs.aws.amazon.com/Amazon rds/latest/Us erGuide/USER_PIOPS.Storage types.html#USER_PIOPS.Auto scaling)


38.A company wants to create an application to store employee data in a  hierarchical	structured  relationship. The company needs a  minimum-latency  response to  high-traf fic queries for the employee data and  must  protect any sensitive data. The company also  needs to  receive  monthly email  messages  if any financial  information  is  present i n the employee data. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

A�� Use Amazon  Redshift to  store the employee data  in  hierarchies.  Unload the data t
o Amazon S3 every  month.

B��Use Amazon  Dynamo db to  store the employee data  in  hierarchies.  Export the data to Amazon S3 every  month.
C��Configure Amazon  Macie for the AWS account. Integrate  Macie with Amazon  Event Bridge to send monthly events to AWS  Lambda.
D��Use Amazon Athena to analyze the employee data  in Amazon S3. Integrate Athena with Amazon Quick sight to publish analysis dashboards and share the dashboards wi th  users.
E��Configure Amazon  Macie for the AWS account. Integrate  Macie with Amazon  Event Bridge to send monthly  notifications through an Amazon Simple  Notification Service   (Amazon SNS) subscription.

�� ��BE



���� �� Correct Answer:  B,  E  Detailed  Explanation:  1.  B:  Use  Amazon  Dynamo db to  stor e the employee data  in  hierarchies.  Export the  data to Amazon S3 every  month.  -  Dy namoDB is a  NoSQL database designed for  low  -  latency,  high  - traffic applications. I t supports  hierarchical data structures (like  nested JSON objects) and scales seamless l y for high -  performance queries.  Exporting  data  monthly to S3 is cost  - effective for archival or analysis. - Why not A?  Redshift is a data warehouse optimized for analyti cs,  not  low  -  latency  hierarchical data  storage. It��s  overkill for simple queries and  has higher latency than  Dynamo db. 2.  E: Configure Amazon  Macie for the AWS account.	Integrate  Macie with Amazon  Event bridge to  send  monthly  notifications through Am azon SNS. - Amazon  Macie automatically discovers and classifies sensitive data  (e.g., f inancia l  info) in S3.  By  integrating with  Event bridge,  Macie  can trigger  monthly scans. SNS then sends email alerts if sensitive data is found. - Why not C? While  Lambda  can  process events, SNS  is simpler for sending emails directly.  Lambda would  require extra code to  handle email delivery,  making  E the  more  straightforward solution.  Refe rence  Links: -  [Amazon  Dynamo db  Use  Cases](https://aws.amazon.com/dynamo db/use - cases/) -  [Amazon  Macie Integration with  SNS](https://docs.aws.amazon.com/macie/la test/user/notifications - sns.html)

39.A  media company uses Amazon Cloud Front for its  publicly available streaming vide
o content. The company wants to secure the video content that is  hosted in Amazon S3  by controlling who  has access.  Some of the company��s  users are  using a custom HTTP client that does  not support cookies. Some of the company��s users are  unabl

e to change the hardcoded  URLs that they are  using for access. Which services or  m ethods will meet these  requirements with the  LEAST  impact to the  users?  (Choose tw o.)
A��Signed cookies

B��Signed  URLs

C��AWS AppSync

D��JSON Web Token (JWT)

E��AWS Secrets  Manager

�� ��AB



���� ��Correct Answer: A. Signed cookies  B. Signed  URLs  Detailed  Explanation: The  pro blem requires two  methods to secure S3-hosted video content via Cloud Front while a c commo dating two user constraints:  1.  Users with custom  HTTP  clients that don��t su pport cookies  need a cookie-free solution. 2.  Users with  hardcoded  URLs cannot  modi fy their existing  URLs.  Here��s  how the  solutions work:  - Signed  URLs  (B) are time-lim ited URLs with embedded authentication parameters (e.g., expiration time, signature). They are  ideal for the custom  HTTP clients that can��t  handle cookies  because the au then tication is directly in the  URL.  Users with  hardcoded  URLs  can  be given  pre-gener ated signed URLs that work without  requiring  URL changes until they expire. - Signed Cookies (A) allow access to  multiple files (e.g., an entire video  library)  by validating t he user via cookies. This suits users with  hardcoded  URLs  because the original  URL  re mains unchanged��authentication is  handled  by cookies added to their requests.  How

ever, this only works for clients that support cookies.  By combining  both  methods:  - Users with cookie support use signed cookies (no  URL changes  needed).  -  Users with out cookie support use signed  URLs  (authentication via  URL  parameters). This dual ap proach minimizes disruption, as each group uses a  method compatible with their con straints.  Reference  Links:  -  [Cloud front  Signed  URLs](https://docs.aws.amazon.com/Ama z on cloud front/latest/Developer guide/private-content-signed-urls.html) -  [Cloud Front Si gned Cookies](https://docs.aws.amazon.com/Amazon cloud front/latest/Developer guide/p rivate-content-signed-cookies.html)


40.A company is  preparing a  new data  platform that will  ingest  real-time streaming d ata from  multiple sources. The company  needs to transform the data  before writing t he data to Amazon S3. The company  needs the ability to use SQL to query the trans formed data. Which solutions will meet these requirements? (Choose two.)

A�� Use Amazon  Kinesis  Data  Streams to  stream the data.  Use Amazon  Kinesis  Data A na lytics to transform the data.  Use Amazon  Kinesis  Data  Firehose  to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

B��Use Amazon  Managed Streaming for Apache  Kafka  (Amazon  MSK)  to stream the d ata.  Use AWS Glue to transform the data and to write the data to Amazon  S3.  Use A mazon Athena to query the transformed data from Amazon S3.

C��Use AWS  Database  Migration  Service  (AWS  DMS) to  ingest the data.  Use Amazon EMR to transform the data and to write the data to Amazon S3. Use Amazon Athena	to query the transformed data from Amazon S3.

D��Use Amazon  Managed  Streaming for Apache  Kafka  (Amazon  MSK)  to stream the d ata.  Use Amazon  Kinesis  Data Analytics to transform the  data and to write the data t o Amazon S3. Use the Amazon  RDS query editor to query the transformed data from Amazon S3.
E��Use Amazon  Kinesis  Data  Streams to  stream the data.  Use AWS Glue to transform the data.  Use Amazon  Kinesis  Data  Firehose  to write the data to Amazon  S3.  Use the Amazon  RDS query editor to query the transformed data from Amazon S3.
�� ��AB



���� �� Correct Answer: A,  B  Detailed  Explanation: Option A:  -  Kinesis  Data  Streams  ing ests  real-time data.  -  Kinesis  Data Analytics  uses  SQL to transform  streaming data.  -  Kinesis  Data  Firehose writes the transformed data to  S3.  - Athena queries S3 data wit h SQL. This fully meets the  requirements with fully  managed, serverless services. Opti on  B:  - Amazon  MSK streams  data  using Apache  Kafka.  - AWS  Glue transforms data  (using  ETL jobs or serverless Spark) and writes  it to S3. - Athena queries S3 data. Thi s  leverages  MSK for streaming and Glue for flexible transformations, suitable for  Kafk a-based workflows. Why Other Options  Fail:  - C: AWS  DMS  is designed for database migration,  not real-time streaming.  EMR  requires  manual setup for transformations,  m a king it less efficient than serverless options. -  D:  Kinesis  Data  Analytics doesn��t dire ctly  integrate with  MSK.  RDS  Query  Editor  can��t query  S3 data.  -  E:  RDS  Query  Edito r can��t query S3; Athena is  required  here.  Reference  Links:  [AWS  Kinesis](https://aws. amazon.com/kinesis/)  [AWS Glue](https://aws.amazon.com/glue/)  [Amazon Athena](http s://aws.amazon.com/athena/)

41.A company has  multiple AWS accounts that  use consolidated  billing. The company	runs several active  high  performance Amazon  RDS for Oracle On-Demand  DB  instanc es for 90 days. The company��s finance team  has access to AWS Trusted Advisor in t he consolidated  billing account and all other AWS accounts. The finance team  needs  to use the appropriate AWS account to access the Trusted Advisor check recommend ations for  RDS. The finance team  must  review the appropriate Trusted Advisor check  to reduce  RDS costs. Which combination of steps should the finance team take to  m eet these  requirements? (Choose two.)
A�� Use the Trusted Advisor  recommendations from the account where the  RDS  instan ces are  running.

B��Use the Trusted Advisor  recommendations from the consolidated  billing account to see all  RDS  instance checks at the same time.

C��Review the Trusted Advisor check for Amazon  RDS  Reserved Instance  Optimization.

D�� Review the Trusted Advisor check for Amazon  RDS Idle  DB Instances.

E�� Review the Trusted Advisor check for Amazon  Redshift  Reserved  Node  Optimiza tio

n.

�� ��BD



���� ��Correct Answer  B.  Use the Trusted Advisor  recommendations from the consolida ted  billing account to see all  RDS  instance  checks at the same time.  D.  Review the Tr usted Advisor check for Amazon  RDS Idle  DB Instances.  Detailed  Explanation  -  Option B: The consolidated billing account in AWS allows organizations to aggregate billing

and access consolidated Trusted Advisor checks across all  linked accounts.  For cost o pti mization checks  like  RDS, the consolidated  billing account  provides a  unified view, enabling the finance team to  review all  RDS  instances across  multiple  accounts si mult a neously. This avoids the  need to  log  into each  individual account.  - Option  D: The A mazon  RDS Idle  DB Instances  check identifies  RDS  instances with  no connections over	the  past 7 days.  Even  if  instances are  labeled active,  some  might  be  underutilized or accidentally left running (e.g., dev/test environments). Terminating or stopping these  idle instances directly  reduces costs.  - Why  not Option C? The Amazon  RDS  Reserved	Instance Optimization check recommends  purchasing  Reserved Instances  (RIs) for  lon g - term workloads.  However, this check  is only available  in the  payer  (consolidated  billing) account. While  RIs could save costs, the question emphasizes  reviewing checks	to  reduce  RDS costs  now. Idle  instances  provide  immediate  savings, whereas  RIs  req u ire upfront commitments. Since the question doesn��t  mention  long  - term cost  plan ning, Option  D  is  more  relevant.  Reference  Links  -  [Trusted  Advisor  Checks for Consol idated  Billing](https://docs.aws.amazon.com/aws support/latest/user/trusted - advisor  - c heck -  reference.html)  -  [Amazon  RDS  Idle  DB Instances  Check](https://aws.amazon.co m/premium support/technology/trusted - advisor/best  - practice - checklist/)


42.A company recently  migrated its entire IT environment to the AWS Cloud. The co mpany discovers that users are  provisioning oversized Amazon  EC2  instances and  mo difying security group rules without using the appropriate change control process. A  solutions architect  must devise a strategy to track and audit these inventory and conf i guration changes. Which actions should the solutions architect take to  meet these  re quirements? (Choose two.)

A�� Enable AWS Cloud trail and use it for auditing.

B�� Use data  lifecycle  policies for the Amazon  EC2  instances.

C�� Enable AWS Trusted Advisor and  reference the security dashboard.

D�� Enable AWS Config and create  rules for auditing and compliance  purposes.

E�� Restore  previous  resource configurations with an AWS Cloud Formation template.

�� ��AD



���� ��Correct Answer A.  Enable AWS Cloud trail and use it for auditing.  D.  Enable AW S Config and create  rules for auditing and compliance  purposes.  Detailed  Explanation To track and audit inventory/configuration changes in AWS: 1. AWS Cloud trail logs  all API activity  (e.g., who  launched an  EC2  instance or  modified a security group). Thi s answers who did what and when? for auditing. 2. AWS Config continuously records	resource configurations (e.g.,  EC2  instance types, security group  rules) and checks co m pliance with  predefined  rules. This answers what changed, and  is it compliant? Why other options are incorrect:  -  B (Data  lifecycle  policies):  Manages data  storage durati on, unrelated to tracking changes. - C (Trusted Advisor):  Focuses on cost/security opti mization, not change tracking. -  E  (Cloud Formation  restore):  Helps  recover  configura ti ons but doesn��t track changes.  Reference  Links  -  [AWS  Cloud trail](https://aws.amazon. com/cloud trail/) -  [AWS Config](https://aws.amazon.com/config/)


43.An image  hosting company uploads its  large assets to Amazon  S3 Standard  bucket
s. The company uses  multipart  upload  in  parallel  by  using S3 APIs and overwrites  if t he same object is uploaded again.  For the first  30 days after  upload, the objects will

be accessed frequently. The objects will  be used  less frequently after  30 days,  but the access  patterns for each object will be inconsistent. The company must optimize its S3 storage costs while  maintaining  high availability and  resiliency of stored assets. W hich combination of actions should a solutions architect  recommend to  meet these re quirements? (Choose two.)
A�� Move assets to S3 Intelligent-Tiering after 30 days.

B��Configure an S3  Lifecycle  policy to clean  up  incomplete  multipart  uploads. C��Configure an S3  Lifecycle  policy to  clean  up expired object delete  markers.

D�� Move assets to S3 Standard-Infrequent Access  (S3 Standard-IA) after 30 days.

E�� Move assets to S3 One Zone-Infrequent Access  (S3 One Zone-IA) after 30 days.

�� ��AB



���� �� Correct Answer A.  Move assets to S3 Intelligent-Tiering after 30 days.  B. Config ure an S3  Lifecycle  policy to  clean up  incomplete  multipart  uploads.  Detailed  Explan at ion The company needs to optimize costs while ensuring  high availability and  resi lien cy.  Here��s why these choices work:  1. Option A  - S3 Intelligent-Tiering automatically moves objects  between  Frequent and Infrequent Access tiers  based on  usage  patterns.
 - After 30 days, access  becomes  inconsistent (sometimes frequent, sometimes  not). I n telligent-Tiering adapts to these changes without manual intervention, saving costs c ompared to  keeping all objects  in Standard.  - It  maintains  high availability  (unlike  S3 One Zone-IA, which stores data  in only one Availability Zone). 2. Option  B  -  Multipart uploads can  leave incomplete upload parts if the upload fails. These parts stay  in S3

 and incur storage costs.  - A  lifecycle  policy to clean  up  incomplete  uploads after a f ew days  removes these  unused  parts,  reducing  unnecessary costs. Why  not other opti ons? - Option  D  (S3  Standard-IA):  Requires objects to  be  accessed  predictably  less f re quently. Since access  patterns are  inconsistent, Intelligent-Tiering (A)  is  better.  - Optio n  E  (S3 One Zone-IA): Sacrifices  resiliency  (data stored  in one AZ), violating the  high- availability requirement. - Option C (Delete  Markers):  Only  relevant  if the  bucket  uses versioning, which isn��t  mentioned in the scenario.  Reference  Links  -  [S3 Intelligent-Ti ering](https://aws.amazon.com/s3/storage-classes/intelligent-tiering/) -  [S3  Lifecycle for Multipart Uploads](https://docs.aws.amazon.com/AmazonS3/latest/user guide/mpu over vie w.html)

44.A company uses a  100 GB Amazon  RDS for  Microsoft  SQL  Server  Single-AZ  DB  ins tance in the us-east-1  Region to store customer transactions. The company  needs  hig h availability and automatic  recovery for the  DB  instance. The company  must also  run	reports on t he  RDS database  several times a year. The  report  process causes transac tions to take  longer than  usual to  post to the customers�� accounts. The company  ne eds a solution that will improve the performance of the  report  process. Which combi nation of steps will meet these  requirements? (Choose two.)

A�� Modify the  DB  instance from a Single-AZ  DB  instance to  a  Multi-AZ deployment.

B��Take a snapshot of the current  DB  instance.  Restore the  snapshot to a  new  RDS  d eployment in another Availability Zone.

C��Create a  read  replica of the  DB  instance  in a  different Availability Zone.  Point all  r equests for  reports to the  read  replica.

D�� Migrate the database to  RDS Custom.

E�� Use  RDS  Proxy to  limit  reporting  requests  to the  maintenance window.

�� ��AC



���� �� Correct Answer A.  Modify the  DB  instance from a Single-AZ  DB  instance to  a Multi-AZ deployment. C. Create a  read  replica of the  DB  instance  in a different Availa bility Zone.  Point all  requests for  reports to the  read  replica.  Detailed  Explanation  1.  High Availability �� Automatic  Recovery  (Answer A)  -  Multi-AZ  Deployment  ensures  hi gh availability  by automatically creating a standby  replica of the database  in a differe nt Availability Zone. If the  primary database fails,  RDS automatically switches to the st andby,  minimizing downtime. This  meets the  requirement for automatic  recovery. - Si ngle-AZ deployments  lack  redundancy, so  upgrading to  Multi-AZ  (A)  is  necessary.  2. I m proving  Report  Performance  (Answer C)  -  Read  Replica  creates  a copy of the datab ase that  handles  read-only queries  (like  reports).  By  redirecting  report  traffic to the  re plica (C), the  primary database  (used for transactions) avoids slowdowns caused  by  re porting workloads. -  Read  replicas are  ideal for offloading  read-heavy tasks. Options  l ike snapshots  (B) or  RDS  Custom  (D) don��t address  real-time  reporting  needs.  RDS  P roxy (E)  manages connection  pooling  but doesn��t separate  read/write traffic. Why  No t Other Options? -  B:  Restoring  snapshots creates  a static copy (no  live  updates),  so i t��s  useless for  real-time  reports.  -  D:  RDS  Custom  is for specialized  OS/database cust o mization, which isn��t  needed  here.  -  E:  RDS  Proxy  doesn��t  offload  read traffic;  it o nly optimizes connections.  Reference  Links  -  [Multi-AZ  Deployments](https://docs.aws.a mazon.com/Amazon rds/latest/User guide/Concepts.MultiAZ.html) -  [Read  Replicas for

Reporting](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_Read repl.ht ml)

45.A gaming company is  moving its  public scoreboard from a data center to the AW S Cloud. The company uses Amazon  EC2 Windows Server  instances  behind an Applica tion  Load  Balancer to  host  its dynamic application. The company  needs a  highly  avail able storage solution for the application. The application consists of static files and d ynamic server-side code. Which combination of steps should a solutions architect take	to meet these  requirements?  (Choose two.)

A��Store the static files on Amazon S3. Use Amazon Cloud Front to cache objects at t he edge.

B��Store the static files on Amazon S3.  Use Amazon  Elastic ache to  cache objects at t he edge.

C��Store the server-side code on Amazon  Elastic  File  System  (Amazon  EFS).  Mount  the EFS volume on each  EC2  instance to  share the files.
D��Store the server-side code on Amazon  FSx for Windows  File  Server.  Mount the  FSx for Windows  File Server volume on each  EC2  instance to share the files.
E��Store the server-side code on a General  Purpose  SSD (gp2) Amazon  Elastic  Block  S tore (Amazon  EBS) volume.  Mount the  EBS volume  on each  EC2  instance to  share the	files.

���� ��Correct Answer: A. Store the static files on Amazon S3.  Use Amazon Cloud Front	to cache objects at the edge.  D.  Store the server-side code on Amazon  FSx for Win dows  File Server.  Mount the  FSx for Windows  File  Server volume  on each  EC2  instanc e to share the files.  Detailed  Explanation:  1.  For  Static  Files  (Option  A):  - Amazon  S3  i s ideal for storing static files (images, CSS, Javascript)  because it offers 99.99999999  9% durability, scalability, and  low-cost storage.  - Amazon Cloud Front (a Content  Deliv ery  Network) caches static files at edge  locations globally,  reducing  latency for  users  and offloading traffic from the  EC2  instances.  - Why  not Option  B?  Elastic ache  is  desi gned for in-memory caching (e.g.,  Redis/Memcached) to speed  up databases or APIs, not for edge caching  like Cloud Front. 2.  For  Server-Side Code  (Option  D):  - The  EC2  i n stances run Windows Server, which requires a shared file system compatible with Wi ndows (SMB  protocol).  - Amazon  FSx for Windows  File  Server  is a fully  managed Win dows-compatible file system with  built-in  high availability (HA) and automatic failover.
 - Why not Option C? Amazon  EFS  uses the  Linux  NFS  protocol  and  is  incompatible with Windows. - Why not Option  E?  EBS volumes  are tied to a single  EC2  instance.
While you can share  EBS volumes across  instances  using third-party tools,  it��s  not  na tively  HA or scalable  like  FSx.  Reference  Links:  -  [Amazon  S3](https://aws.amazon.com/ s3/) -  [Amazon Cloud front](https://aws.amazon.com/cloud front/)  -  [Amazon  FSx for Wi ndows File Server](https://aws.amazon.com/fsx/windows/)



46.A company uses a  payment  processing system that  requires  messages for a  particu lar  payment ID to  be  received  in the same order that they were sent. Otherwise, the payments might  be  processed  incorrectly. Which actions should a solutions architect t a ke to  meet this  requirement?  (Choose two.)

A��Write the messages to an Amazon  Dynamo db table with the  payment ID as the  p artit ion  key.

B��Write the  messages to an Amazon  Kinesis data  stream with the  payment ID as the partition  key.

C��Write the messages to an Amazon  Elastic ache for  Memcached  cluster with the  pay ment ID as the  key.
D��Write the  messages to an Amazon  Simple Queue Service  (Amazon SQS) queue. Set the message attribute to  use the  payment ID.
E��Write the  messages to an Amazon Simple Queue Service  (Amazon SQS)  FI FO  queu
e. Set the  message group to  use the  payment ID.

�� ��BE



���� �� Correct Answer:  B. Write the  messages to an Amazon  Kinesis data stream with the payment ID as the  partition  key.  E. Write the  messages to  an Amazon Simple Qu eue Service (Amazon SQS)  FI FO  queue. Set the  message group to  use the  payment I D.  Explanation:  For the  payment  system to  process  messages  in the exact order they were sent  per  payment ID, the solution  must ensure strict ordering within groups  (e. g.,  per  payment ID).  1.  Option  B  (Kinesis  Data  Stream):  -  Kinesis  organizes  data  into  s hards. When you use a  partition  key (like the  payment ID),  messages with the same  key go to the same shard. - Within a s hard,  messages are stored  in order, so consu mers can  process them sequentially for each  payment ID. 2. Option  E  (SQS  FIFO  Que ue): -  FI FO  queues guarantee exactly-once  processing and strict ordering.  -  By  setting	the message group ID to the payment ID, all messages for the same  payment ID ar

e grouped and delivered in the order they were sent. Why other options fail: - A (Dy namoDB):  Dynamo db doesn��t enforce  message ordering;  it��s a database,  not a que ue/stream. - C (Elastic ache):  Memcached  is a cache,  not designed for ordered  messag e  processing.  -  D  (SQS  Standard Queue): Standard queues don��t guarantee order.  Re ference  Links:  -  [Kinesis  Data  Streams Ordering](https://docs.aws.amazon.com/streams/l atest/dev/key-concepts.html#partition-key) -  [SQS  FIFO  Queues and  Message Grouping] (https://docs.aws.amazon.com/Aws simple queue service/latest/Sqs developer guide/using -message groupid-property.html)


47.A  hospital is designing a  new application that gathers symptoms from patients. Th e  hospital  has decided to  use Amazon Simple Queue Service  (Amazon SQS) and Ama zon Simple  Notification Service  (Amazon SNS) in the architecture. A solutions architec t is  reviewing the infrastructure design.  Data  must  be  encrypted at  rest and  in transit. Only authorized personnel of the hospital should be able to access the data. Which combination of steps should the solutions architect take to meet these  requirements?	(Choose two.)

A��Turn on server-side encryption on the SQS components.  Update the default  key  po licy to  restrict  key  usage to  a set of authorized  principals.

B��Turn on server-side encryption on the SNS components  by  using an AWS  Key  Man agement Service  (AWS  KMS) customer  managed  key. Apply a  key  policy to  restrict  ke y usage to a set of authorized  principals.

C��Turn on encryption on the SNS components.  Update the default  key  policy to  rest rict  key  usage to a set of authorized  principals.  Set a condition in the topic  policy to allow only encrypted connections over TLS.
D��Turn on server-side encryption on the SQS components  by  using an AWS  Key  Ma nagement Service (AWS  KMS)  customer managed  key. Apply a  key  policy to  restrict  k ey usage to a set of authorized  principals. Set a condition in the queue  policy to allo w only encrypted connections over TLS.
E��Turn on server-side encryption on the SQS components  by  using an AWS  Key  Man agement Service (AWS  KMS) customer  managed  key. Apply an IAM  policy to  restrict  key usage to a set of authorized  principals.  Set a condition in the queue  policy to all ow only encrypted connections over TLS.
�� ��BD



���� �� Correct Answer  B,  D  Detailed  Explanation To  meet the  requirements  of encrypt i ng data at  rest and  in transit while  restricting access to authorized  personnel:  1. Opti on  B  (SNS  +  KMS  +  Key  Policy)  -  SNS  supports  server  -  side  encryption  (SSE)  using AWS  KMS.  By  using  a customer -  managed  key  (not the default AWS  -  managed  ke y), the  hospital gains full control over encryption  keys.  - The  key  policy  in  KMS  is  the primary way to restrict which AWS principals (users/roles) can use/access the encrypt ion  key. This ensures only authorized  personnel can decrypt SNS data. 2. Option  D  (S QS  +  KMS  +  Key  Policy  +  TLS)  -  SQS  also  supports  SSE  with  KMS.  Using  a  customer	-  managed  key allows granular control over encryption.  - The  key  policy  restricts  key usage to authorized principals. - The queue policy condition (aws:Securetransport) e

nforces  HTTPS  (TLS encryption) for all API calls to  SQS, ensuring data  is encrypted in	transit. Why other options are incorrect:  - A/C:  Use the default  key  policy, which gra nts  broad access to AWS services. This violates the requirement to restrict access stric t ly to authorized  personnel.  -  E:  Uses  IAM  policies  instead of  KMS  key  policies.  IAM  p olicies alone cannot enforce encryption or  restrict  KMS  key  usage  effectively.  - C: SNS already encrypts in - transit data via  HTTPS  by default, so explicitly enforcing TLS  is redundant.  Key Concepts for  Beginners:  -  KMS  Customer  -  Managed  Keys:  Create you r own encryption  keys  in AWS  KMS  instead  of using AWS  -  managed  keys for stricter access control. -  Key  Policy: A  KMS  resource  policy that defines who can  use/manag e the encryption  key. This  is  more secure than  relying on IAM  policies alone.  - TLS  E nforcement:  Use  HTTPS  (TLS) for all API  calls to AWS services to encrypt data  in tran sit.  Reference  Links  -  [SQS  Encryption](https://docs.aws.amazon.com/Aws simple queues ervice/latest/Sqs developer guide/sqs-server-side-encryption.html) -  [SNS  Encryption](htt ps://docs.aws.amazon.com/sns/latest/dg/sns-server-side-encryption.html) -  [KMS  Key  Po licies](https://docs.aws.amazon.com/kms/latest/developer guide/key-policies.html)


48.A company needs to create an Amazon  Elastic  Ku bernet es  Service  (Amazon  EKS)  cl uster to  host a digital  media streaming application. The  EKS  cluster will use a  manag ed node group that  is  backed  by Amazon  Elastic  Block  Store  (Amazon  EBS) volumes  f or storage. The company  must encrypt all data at  rest  by  using a  customer  managed	key that is stored in AWS  Key  Management  Service  (AWS  KMS). Which  combination of actions will meet this requirement with the  LEAST operational overhead? (Choose t wo.)

A��Use a  Ku bernet es  plugin that  uses the  customer  managed  key to  perform  data enc ryption.

B��After creation of the  EKS cluster,  locate the  EBS volumes.  Enable  encryption  by  usi ng the customer managed  key.

C��Enable  EBS  encryption  by default  in the AWS  Region where the  EKS  cluster will  be created. Select the customer managed  key as the default  key.
D��Create the  EKS cluster. Create an IAM  role t hat  has  a  policy that  grants  permission to the customer  managed key. Associate the  role with the  EKS cluster.
E��Store the customer managed  key as a  Ku bernet es  secret  in the  EKS  cluster.  Use th e customer  managed  key to encrypt the  EBS volumes.
�� ��CD



���� �� Correct Answer: C,  D  Detailed  Explanation: To meet the requirement of encrypt i ng all data at  rest  using a customer-managed AWS  KMS  key with  minimal  operation a l overhead:  1. Option C  (Enable  EBS encryption  by default  in the AWS  Region...)  ensur es that all new  EBS volumes created  in the  region  (including those for the  EKS worke r  nodes) are automatically encrypted using the specified customer-managed  KMS  key. This eliminates the need to manually configure encryption for each volume. 2. Optio n  D  (Create an IAM  role...) ensures that the  EKS worker  nodes  (EC2  instances)  have  p er mission to use the customer-managed  KMS  key for encryption/decryption. Without  this  role, the  nodes would  lack access to the  key, causing  EBS volume  mounting to f ail. Other options are  less optimal: - A ��  E  require  manual  Ku bernet es-level  config ur ations, adding complexity. -  B  is  impractical  because  EBS volumes cannot  be encrypt e

d after creation; they  must  be encrypted at  launch.  By  combining C  (default encrypt io n) and  D  (automatic  key access), the solution achieves encryption with  minimal  manu al steps.  Reference  Links:  -  [Amazon  EBS  Encryption](https://docs.aws.amazon.com/AW SEC2/latest/User guide/Ebs encryption.html) -  [AWS  KMS  and  EKS Integration](https://do cs.aws.amazon.com/eks/latest/user guide/encryption.html)


49.A new employee  has joined a company as a deployment engineer. The deploy men t engineer will be using AWS Cloud Formation templates to create  multiple AWS  reso urces. A solutions architect wants the deployment engineer to  perform job activities while following the principle of least  privilege. Which combination of actions should t he solutions architect take to accomplish this goal? (Choose two.)

A��Have the deployment engineer use AWS account  root user credentials for  perform i ng AWS Cloud Formation stack operations.

B��Create a  new IAM  user for the deployment engineer and add the IAM  user to a  g roup t hat  has the  Power users IAM  policy attached.

C��Create a  new IAM  user for the deployment engineer and add the IAM  user to a  g roup t hat  has the Administrator access IAM  policy attached.
D��Create a  new IAM  user for the deployment engineer and add the IAM  user to  a g roup that  has an IAM  policy that allows AWS Cloud Formation actions only.
E��Create an IAM  role for the deployment engineer to explicitly define the  permission s specific to the AWS Cloud Formation stack and  launch stacks  using that IAM  role.

���� ��Correct Answer  D and  E  Detailed  Explanation The  principle  of  least  privilege  me ans granting only the  permissions  required to  perform specific tasks.  Here��s w hy  opti ons  D and  E  are correct:  - Option  D:  Creating an IAM  policy that  allows only AWS Cl oud Formation actions ensures the deployment engineer can  perform Cloud Formation  tasks (like creating/updating stacks)  but can��t access  unrelated services  (e.g.,  EC2,  S3). This limits their permissions to the minimum  needed. - Option  E:  Using an IAM  role	to launch stacks allows Cloud Formation to temporarily assume  permissions only whe n creating  resources.  For  example, if a stack  needs to create an  S3  bucket, the  role
(not the user)  has S3  permissions. This way, the engineer��s  user account doesn��t  ha ve direct access to S3,  reducing  risk. Why other options are wrong:  - A:  Root  user  ha s unlimited access �� violates least  privilege.  -  B:  Power users  can  manage  most  servic es (except IAM/billing) �� overly  broad. - C: Administrator access grants full access �� violates  least  privilege.  Reference  Links  -  [AWS  IAM  Best  Practices](https://docs.aws.am azon.com/IAM/latest/User guide/best-practices.html) -  [AWS Cloud Formation  Service  Ro les](https://docs.aws.amazon.com/Aws cloud formation/latest/User guide/using-iam-servic erole.html)

50.A company hosts a three-tier ecommerce application on a fleet of Amazon  EC2  ins tances. The instances  run  in an Auto Scaling group  behind an Application  Load  Balan cer (ALB). All ecommerce data  is stored  in an Amazon  RDS for  MariaDB  Multi-AZ  DB instance. The company wants to optimize customer session  management during trans actions. The application must store session data durably. Which solutions will  meet th ese  requirements? (Choose two.)

A��Turn on the sticky sessions feature (session affinity) on the ALB.

B�� Use an Amazon  Dynamo db table to  store customer session information.

C�� Deploy an Amazon Cognito  user pool to  manage user session  information.

D��Deploy an Amazon  Elastic ache for  Redis  cluster to store customer session  in format ion.
E��Use AWS Systems  Manager Application  Manager  in the application to  manage  user session information.
�� ��AD



���� ��Correct Answer  B,  D  Detailed  Explanation To optimize customer session  manage ment and ensure durable storage of session data, the solutions  must address two  key			requirements:  1.  Durability:  Session data  must  persist even  if instances are terminate d or fail. 2. Centralized Storage: Session data should be accessible across all instances	to support scalability and fault tolerance. Option A (Sticky Sessions on ALB)  - Sticky sessions (session affinity)  route  user requests to the same  EC2  instance.  - Issue: If the		instance fails or scales in, session data stored locally on the instance is  lost. This do es  not meet the durability  requirement.  - Why it��s incorrect: While sticky sessions si mplify session  handling, they do  not durably store data. Option  B  (Dynamo db)  -  Dyn amoDB is a fully managed, serverless  NoSQL database with  built  -  in durability  (data  is replicated across  multiple AZs).  - Why  it��s correct:  Storing session data  in  Dynamo DB ensures durability and allows any  EC2  instance to access the data, enabling statel ess application scaling. Option  D  (Elastic ache for  Redis)  -  Elastic ache for  Redis  provid es an in -  memory data store with optional  persistence  (via snapshots or AOF).  - Wh y it��s correct:  Redis offers  low  -  latency access to  session data. If configured with  pe

rs istence or multi  - AZ  replication,  it ensures durability and  high availability. Why the Original Answer (A,  D)  is Incorrect  -  Sticky  sessions  (A) do  not solve the durability  pr oblem unless  paired with external storage  (which  is  not  mentioned in the question).  -	Elastic ache (D) alone is sufficient for durability (if configured properly) and scalability, making sticky sessions unnecessary.  Reference  Links  -  [AWS  Dynamo db  Durability](ht tps://aws.amazon.com/dynamo db/) -  [Elastic ache for  Redis  Persistence](https://docs.aws. amazon.com/Amazon elastic ache/latest/red - ug/Red is persistence.html)


51.A solutions architect is designing the architecture for a software demonstration env i ronment. The environment will run on Amazon  EC2  instances  in an Auto Scaling gro up behind an Application  Load  Balancer  (ALB). The system will experience significant  i n creases in traffic during working  hours  but is  not  required to operate on weekends. Which combination of actions should the solutions architect take to ensure that the s ystem can scale to  meet demand? (Choose two.)

A�� Use AWS Auto Scaling to adjust the ALB capacity  based on  request  rate.

B�� Use AWS Auto Scaling to scale the capacity of the VPC internet gateway.

C��Launch the  EC2  instances  in  multiple AWS  Regions to distribute the  load  across  Re gions.
D�� Use a target tracking scaling policy to scale the Auto Scaling group  based on inst ance CPU utilization.
E��Use scheduled scaling to change the Auto Scaling group  minimum,  maximum, and desired capacity to zero for weekends.  Revert to the default values at the start of the week.

�� ��DE



���� ��Correct Answer:  D,  E  Detailed  Explanation:  D.  Use  a  target tracking  scaling  polic y to scale the Auto Scaling group based on instance CPU utilization. - Why? Target tr acking scaling automatically adjusts the number of EC2 instances in the Auto Scaling group based on a  predefined  metric  like CPU utilization.  During working  hours, when	traffic increases, CPU usage will  likely spike. This  policy will add  more  instances to  h andle t he  load, ensuring  performance stays  stable.  E.  Use  scheduled scaling to change	the Auto Scaling group capacity to zero for weekends. - Why? The system isn��t  req ui red on weekends. Scheduled scaling lets you predefine when to set the Auto Scalin g group��s  minimum,  maximum, and desired capacity to zero (weekends) and  revert t o normal values on weekdays. This saves costs and aligns with the  requirement to sh ut down on weekends. Why  Not Other Options?  - A: ALB scales automatically��you d on��t  manually adjust  its capacity.  -  B: Internet  gateways are  not scalable components;	they��re fixed. - C:  Multi-Region adds  unnecessary complexity since the question doe sn��t  mention global traffic or  redundancy  needs.  References:  -  [AWS  Auto Scaling Tar get Tracking](https://docs.aws.amazon.com/auto scaling/ec2/user guide/target-tracking.ht ml) -  [AWS  Scheduled Scaling](https://docs.aws.amazon.com/auto scaling/ec2/user guide/ schedule_time.html)


52.A solutions architect is designing a two-tiered architecture that includes a  public s ub net and a database subnet. The web servers  in the  public subnet  must  be open to	the internet on  port 443. The Amazon  RDS for  MySQL  DB  instance  in  the database subnet must  be accessible only to the web servers on  port 3306. Which combination

of steps should the solutions architect take to  meet these  requirements?  (Choose tw o.)
A��Create a  network ACL for the public subnet. Add a  rule to deny outbound traffic t
o 0.0.0.0/0 on  port 3306.

B��Create a security group for the  DB  instance. Add a  rule to allow traffic from the  p ublic subnet CIDR  block on  port  3306.
C��Create a security group for the web servers  in the  public subnet. Add a  rule to all ow traffic from 0.0.0.0/0 on port 443.
D��Create a security group for the  DB  instance. Add a  rule to allow traffic from the w eb servers�� security group on  port 3306.
E��Create a security group for the  DB  instance. Add a  rule to deny all traffic except t raffic from the web servers�� security group on  port 3306.
�� ��CD



���� ��Correct Answer: C and  D  Detailed  Explanation:  C. Create a  security group for th e web servers  in the  public subnet. Add a  rule to allow traffic from 0.0.0.0/0 on  port 443. - Why? The web servers  need to accept internet traffic on port 443 (HTTPS). Se curity groups are stateful, so this inbound  rule allows anyone on the  internet to conn ect to the web servers via  HTTPS. The 0.0.0.0/0 CIDR  block  represents all IPv4 addres ses.  D. Create  a security group for the  DB  instance. Add  a  rule to allow traffic from t he web servers�� security group on  port 3306.  - Why? Instead of using the  public sub net��s IP  range  (option  B),  referencing the web  servers�� security group directly is  mo

re secure and scalable. This ensures only instances associated with the web servers' s
ecurity group can access the database on port 3306 (MySQL), even if the web servers��	IP addresses change. Why  not  B  or  E?  -  B  (Public  subnet  CIDR): This  is  less secure. I     f another  resource (e.g., a  misconfigured  EC2  instance)  is  launched  in the  public sub n
et, it could access the database even  if it��s  not a web server.  -  E  (Deny  all  except tr affic): Security groups default to deny all inbound traffic. You only  need to allow spec ific traffic (like option  D).  Explicit  deny  rules  are  unnecessary  here.  Reference  Links:  -  [AWS Security Groups](https://docs.aws.amazon.com/vpc/latest/user guide/VPC_Security g roups.html) -  [Best  Practices for  Security Groups](https://docs.aws.amazon.com/vpc/late st/user guide/security-group-rules.html)


53.A  rapidly growing global ecommerce company is  hosting its web application on A WS. The web application includes static content and dynamic content. The website st ores online transaction  processing  (OLTP) data  in an Amazon  RDS database The webs ite��s users are experiencing slow page  loads. Which combination of actions should a solutions architect take to  resolve this issue? (Choose two.)

A��Configure an Amazon  Redshift cluster.

B��Set up an Amazon Cloud Front distribution.

C�� Host the dynamic web content  in Amazon S3.

D��Create a  read  replica for t he  RDS  DB  instance.

E��Configure a  Multi-AZ deployment for t he  RDS  DB  instance.

�� ��BD

���� �� Correct Answer:  B,  D  Detailed  Explanation:  B.  Set  up  an Amazon Cloud Front dis tribution Cloud Front is a Content  Delivery  Network  (CDN) that  caches static content  (l ike images, CSS, JS files) at AWS edge locations globally. This  reduces  latency for use rs worldwide  by serving content from the  nearest edge  location, speeding  up  page  lo ads for static elements. Since the ecommerce site  has global users, this directly addre sses slow static content delivery.  D. Create  a  read  replica for t he  RDS  DB  instance  A  read  replica  is a copy of the  primary  RDS  database that  handles  read-only queries. If the OLTP database is overloaded with read  requests (common in ecommerce), offload ing read traffic to a  replica  reduces the  load on the  primary  DB,  improving  response  times for dynamic content (e.g.,  product  listings, user data). This directly tackles slow database-driven  page components.  --- Why  not other options:  - A (Redshift):  Redshift	is for analytics,  not OLTP  performance. Irrelevant  here.  - C  (S3 for dynamic content): S3 hosts static files only.  Dynamic content  requires server-side  processing (e.g., APIs, databases), so this is invalid. -  E (Multi-AZ):  Multi-AZ  improves database availability   (failover),  not  performance. It doesn��t  reduce  latency or offload traffic.  Reference  Link s: -  [Amazon Cloud front Overview](https://aws.amazon.com/cloud front/)  -  [RDS  Read  R eplicas](https://docs.aws.amazon.com/Amazon rds/latest/User guide/USER_Read repl.htm  l)


54.A company uses AWS Organizations with all features enabled and  runs multiple A mazon  EC2 workloads  in t he ap-southeast-2  Region. The company  has a service  contr ol  policy (SCP) that  prevents any  resources from  being created  in any  other  Region. A security  policy  requires the company to encrypt all data at  rest. An audit discovers that employees  have created Amazon  Elastic  Block  Store  (Amazon  EBS) volumes for  E

C2 instances without encrypting the volumes. The company wants any  new  EC2 instan ces that any IAM  user or  root  user  launches  in ap-southeast-2 to  use encrypted  EBS volumes. The company wants a solution that will have  minimal effect on employees
who create  EBS volumes. Which combination of steps will  meet these  requirements? (Choose two.)
A��In the Amazon  EC2 console, select the  EBS encryption  account attribute and define a default encryption  key.

B��Create an IAM  permission  boundary. Attach the  permission  boundary to t he  root  o rganization al unit (OU).  Define the  boundary to  deny the ec2:Create volume action wh en the ec2:Encrypted condition equals false.

C��Create an SCP. Attach the SCP to the  root organizational unit  (OU).  Define the  SCP to deny the ec2:Create volume action whenthe ec2:Encrypted condition equals false.
D��Update the IAM  policies for each account to deny the ec2:Create volume action wh
en the ec2:Encrypted condition equals false.

E��In the Organizations  management account, specify the  Default  EBS volume  encrypt i on setting.
�� ��CE



���� ��Correct Answer: C,  E  Explanation: The correct combination  is C  (Create an SCP) and  E  (Set default  EBS  encryption  in Organizations).  - Option C: An SCP  (Service Cont rol  Policy) attached to the  root OU will enforce encryption for all  EBS volumes across all accounts in the organization. It denies the `ec2:Create volume` action if the vol um

e isn��t encrypted (`ec2:Encrypted` condition). This ensures  no  unencrypted volumes ca n  be created, even if users try to  bypass encryption.  - Option  E:  Enabling  the  Default EBS volume encryption in AWS Organizations automatically encrypts all new EBS vol umes in  member accounts using a default  key. This  minimizes  user  impact  because e mployees don��t  need to  manually enable encryption��it  happens  by default. Together,	these steps ensure encryption is enforced organization-wide (via SCP) while simplify in g compliance (via default encryption). Options  like IAM  policies  (D) would  require  per -account updates, and permission  boundaries  (B) are  more complex. Option A only w orks  per-account,  not organization-wide.  Reference  Links:  -  [AWS  Default  EBS  Encrypt i on](https://docs.aws.amazon.com/AWSEC2/latest/User guide/Ebs encryption.html) -  [Usin g SCPs to  Require  EBS  Encryption](https://docs.aws.amazon.com/organizations/latest/us erguide/orgs_manage_policies_scps_examples.html#scp-ebs-encryption)


55.A solutions architect wants to use the following JSON text as an identity-based  po













licy to grant specific  permissions:                                                                   Which  IAM  pri ncipals can the solutions architect attach this  policy to? (Choose two.)
A�� Role

B��Group

C��Organization

D��Amazon  Elastic Container  Service (Amazon  ECS)  resource

E��Amazon  EC2  resource

�� ��AB



���� ��Correct Answer A.  Role  B. Group  Explanation This JSON  policy  is an  identity-bas ed policy, which  means it  must  be attached to an IAM  identity  (not a  resource). IAM identities include: -  Roles (A): Temporary  permissions for AWS  services  (e.g.,  EC2  insta nces,  Lambda).  - Groups  (B): Collections of IAM  users to  simplify permissions  manage ment. Why  not the other options? - C. Organization:  Uses Service Control  Policies  (SC Ps),  not  identity-based  policies.  -  D.  ECS  Resource  /  E.  EC2  Resource:  Resources  use  r esource-based policies (e.g., S3 bucket  policies) or assume  roles. You don��t attach IA M policies directly to  resources  like  ECS/EC2.  Reference  Links  -  [AWS  IAM  Identity-Bas ed Policies](https://docs.aws.amazon.com/IAM/latest/User guide/access_policies_identity-v s-resource.html) -  [AWS IAM  Roles vs.  Groups](https://docs.aws.amazon.com/IAM/latest /User guide/id_roles_compare-resource-policies.html)


56.A  manufacturing company has  machine sensors that upload .csv files to an Amazo n S3  bucket. These .csv files  must  be  converted  into  images and  must  be  made  avail able as soon as  possible for the automatic generation of graphical reports. The image s  become  irrelevant after  1  month,  but the  .csv files  must  be  kept to train  machine  l earning (ML)  models twice a year. The  ML trainings and audits are  planned weeks  in

advance. Which combination of steps will meet these requirements  MOST cost-effect iv ely? (Choose two.)
A�� Launch an Amazon  EC2 Spot Instance that downloads the .csv files every  hour, ge nerates the image files, and uploads the images to the S3  bucket.
B��Design an AWS  Lambda function that  converts the .csv files  into images and stores	the images in the S3  bucket. Invoke the  Lambda function when a .csv file  is uploade d.
C��Create S3  Lifecycle  rules for  .csv files and  image files in the S3  bucket. Transition t he .csv files from S3 Standard to S3 Glacier  1 day after they are  uploaded.  Expire the	image files after 30 days.
D��Create S3  Lifecycle  rules for  .csv files and  image files  in the S3  bucket. Transition t he .csv files from S3 Standard to S3 One Zone-Infrequent Access  (S3 One Zone-IA)  1 day after they are uploaded. Expire the image files after 30 days.
E��Create S3  Lifecycle  rules for .csv files and  image files  in the S3  bucket. Transition t he .csv files from S3 Standard to S3 Standard-Infrequent Access  (S3 Standard-IA)  1 d ay after they are  uploaded.  Keep the  image files  in  Reduced  Redundancy  Storage  (RR S).
�� ��BC



���� ��Correct Answer:  B, C  Detailed  Explanation:  B  uses AWS  Lambda, which  is  serverl ess and cost-effective. It triggers immediately when a .csv file is uploaded to S3, ensu ring images are generated as soon as  possible.  Lambda charges only for execution ti

me, avoiding idle costs  (unlike  EC2  in option A).  C applies S3  Lifecycle  rules to transit ion .csv files to S3 Glacier  (ultra-low-cost storage for archival) after  1 day, which  is id eal since  ML training  happens twice a year with advance  planning  (Glacier  retrieval  de lays are acceptable). Images are deleted after 30 days, aligning with the requirement. Why not  D/E?  -  D  uses  S3 One Zone-IA, which  is cheaper than Standard-IA  but store s data  in only one Availability Zone (riskier). Glacier is even cheaper and sufficient for planned, infrequent access.  -  E  uses S3 Standard-IA  (higher cost than Glacier) and m entions  RRS  (deprecated). Why  not A?  EC2  Spot Instances are  cheaper  but  require  ma nual scaling and introduce delays (hourly  processing vs.  real-time  Lambda).  Reference Links: -  [AWS  Lambda](https://aws.amazon.com/lambda/)  -  [S3  Lifecycle  Policies](https:/ /docs.aws.amazon.com/AmazonS3/latest/user guide/object-lifecycle-mgmt.html) -  [S3  Sto rage Classes](https://aws.amazon.com/s3/storage-classes/)


57.A company used an Amazon  RDS for  MySQL  DB  instance  during application testin
g.  Before terminating the  DB  instance  at the end of the test cycle, a solutions archite ct created two  backups. The solutions architect created the first  backup  by  using the mysqldump utility to create a database dump. The solutions architect created the sec ond backup  by enabling the final  DB snapshot option  on  RDS termination. The comp any is now  planning for a  new test  cycle and wants to create a  new  DB  instance fro
m the  most  recent  backup. The company  has chosen a  MySQL-compatible edition of Amazon Aurora to  host the  DB  instance. Which  solutions will create the  new  DB  insta nce? (Choose two.)

A��Import t he  RDS snapshot directly  into Aurora.

B�� Upload t he  RDS  snapshot to Amazon S3. Then import t he  RDS  snapshot  into Auro

ra.

C��Upload the database dump to Amazon S3. Then import the database dump into A

urora.

D��Use AWS  Database  Migration  Service  (AWS  DMS)  to  import t he  RDS  snapshot  into Aurora.
E��Upload the database dump to Amazon S3. Then use AWS  Database  Migration  Serv ice (AWS  DMS) to  import the database dump  into Aurora.
�� ��AC



���� ��Correct Answer: A. Import t he  RDS snapshot directly into Aurora. C.  Upload the database dump to Amazon S3. Then import the database dump into Aurora.  Detailed Explanation: Option A is correct because Amazon Aurora (MySQL-compatible) allows direct restoration of RDS for  MySQL snapshots. AWS automatically converts the  MySQ L snapshot into an Aurora-compatible format during the restore  process. Option C is correct  because  mysqldump creates a  logical  backup  (SQL file). To  use  it with Aurora:
 1.  Upload the  SQL file to an Amazon S3  bucket. 2.  Use Aurora��s  LOAD  DATA  FROM S3 command or the mysql client to execute the SQL file, restoring the data into Aur ora. Why other options are  incorrect:  -  B:  RDS  snapshots are  managed  by AWS  and c annot be  manually  uploaded to S3. -  D: AWS  DMS  migrates  data  between  running  d
at a bases,  not static snapshots.  -  E:  DMS  isn��t  designed to  process SQL dump files;  it�� s used for  live database  replication.  Reference  Links:  -  [Migrating  RDS  MySQL  to  Auro	 ra using snapshots](https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/A

ur or a mysql.Migrating.Rds mysql.Import.html) -  [Importing data  into Aurora from S3] (https://docs.aws.amazon.com/Amazon rds/latest/Aurora user guide/Aurora mysql.Integra ting.Load FromS3.html)


58.A company has a three-tier web application that is in a single server. The compan y wants to  migrate the application to the AWS Cloud. The company also wants the a pplication to align with the AWS Well-Architected  Framework and to  be consistent wi th AWS recommended best  practices for security, scalability, and  resiliency. Which co m bination of solutions will meet these requirements? (Choose three.)

A��Create a VPC across two Availability Zones with the application's existing architect ure.  Host the application with existing architecture on an Amazon  EC2  instance  in a  p rivate subnet in each Availability Zone with  EC2 Auto  Scaling groups. Secure the  EC2  instance with security groups and  network access control  lists  (network ACLs).

B��Set up security groups and  network access control  lists  (network ACLs) to control access to the database  layer. Set up a single Amazon  RDS  database  in a  private  sub n
et.

C��Create a VPC across two Availability Zones.  Refactor the application to  host the we b tier, application tier, and database tier.  Host each tier on  its own  private subnet wit h Auto Scaling groups for the web tier and application tier.
D�� Use a single Amazon  RDS database. Allow database access only from the applicati on tier security group.
E�� Use  Elastic  Load  Balancers  in  front of the web tier. Control access  by  using  securit y groups containing references to each  layer's security groups.

F�� Use an Amazon  RDS database  Multi-AZ  cluster  deployment  in  private sub nets. Allo w database access only from application tier security groups.
�� ��CEF



���� ��Correct Answer C,  E,  F  Detailed  Explanation  C: A three-tier  architecture should s eparate the web, application, and database tiers into isolated private sub nets. This im proves security  by limiting direct access to each  layer. Auto Scaling for the web and  application tiers ensures scalability and  resiliency.  E:  Elastic  Load  Balancers  (ELB)  distri bute traffic across instances in the web tier, improving availability. Security groups sh ould reference each  layer��s security groups  (e.g., the web tier only allows traffic from	the  ELB��s security group), enforcing strict access control.  F:  Multi-AZ  RDS  deploy me nts  provide high availability (automatic failover) and durability.  Restricting database ac cess to the application tier��s security group follows the  principle of least  privilege, en hancing security. Why  not other options?  - A:  Keeping the existing  monolithic archite cture on  EC2  (without separating tiers) violates the  best  practice of isolating  layers.  - B/D: A si ngle  RDS  instance  lacks  Multi-AZ  redundancy,  making  it  less  resilient.  -  B:  Us ing network ACLs (stateless) alone is insufficient; security groups (stateful) are  better f or granular control.  Reference  Links  -  [AWS Well-Architected  Framework](https://aws.a mazon.com/architecture/well-architected/) -  [Multi-AZ  RDS](https://docs.aws.amazon.co m/Amazon rds/latest/User guide/Concepts.MultiAZ.html) -  [Elastic  Load  Balancing](https: //aws.amazon.com/elastic load balancing/)


59.A company is  migrating its applications and databases to the AWS Cloud. The co mpany will use Amazon  Elastic Container Service  (Amazon  ECS), AWS  Direct  Connect,

and Amazon  RDS. Which activities will  be  managed  by the company's operational tea m? (Choose three.)
A��Management of the Amazon  RDS  infrastructure  layer, operating system, and  platfor

ms

B��Creation of an Amazon  RDS  DB  instance and configuring the scheduled  maintenan ce window
C��Configuration of additional software components on Amazon  ECS for  monitoring,  p atch  management,  log  management, and  host intrusion detection
D��Installation of patches for all minor and  major database versions for Amazon  RDS

E�� Ensure the  physical security of the Amazon  RDS  infrastructure  in the  data center

F�� Encryption of the data that  moves in transit through  Direct Connect

�� ��BCF



���� ��Correct Answer:  B, C,  F  Detailed  Explanation:  -  B)  Creation  of Amazon  RDS  DB  i nstance and  maintenance window configuration: While AWS  manages  RDS  infrastruc tu re, customers control database setup (instance type, storage) and maintenance windo ws (when updates occur). - C) Configuring additional software on  ECS: AWS  manages	the  ECS service,  but customers are  responsible for  installing  monitoring tools, securit y agents, and  logging software on their container  instances.  -  F)  Encrypting  data  in tr ansit via  Direct Connect:  Direct  Connect  provides a  private connection  but doesn't en crypt data  by default. Customers  must  implement encryption  (e.g., VPN/IPsec) for dat a in transit.  Key AWS  responsibility  split:  - AWS  handles  physical  security  (E),  RDS  infr

a structure/OS (A), and  major  DB  patches  (D).  -  Customers  manage application-layer se curity, data encryption, and service configurations.  Reference  Links:  -  [AWS  Shared  Re sponsibility  Model](https://aws.amazon.com/compliance/shared-responsibility-model/) -  [Amazon  RDS  FAQs](https://aws.amazon.com/rds/faqs/)  -  [Direct  Connect Security](http s://docs.aws.amazon.com/directconnect/latest/User guide/security.html)


60.A company uses AWS Organizations. The company wants to operate some of its A WS accounts with different budgets. The company wants to  receive alerts and automa tically prevent provisioning of additional  resources on AWS accounts when the allocat ed budget threshold is  met during a specific  period. Which combination of solutions will meet these  requirements? (Choose three.)

A��Use AWS  Budgets to  create a  budget.  Set the  budget amount  under the Cost and Usage  Reports section of the  required AWS accounts.

B��Use AWS  Budgets to  create a  budget. Set the  budget amount  under the  Billing da sh boards of t he required AWS accounts.

C��Create an IAM user for AWS  Budgets to  run  budget actions with the  required  per missions.
D��Create an IAM  role for AWS  Budgets to  run  budget  actions with the  required  per missions.
E��Add an alert to  notify the company when each account  meets its  budget thresh ol
d. Add a  budget action that selects the IAM  identity created with the appropriate co nfig  rule to  prevent  provisioning of additional  resources.

F��Add an alert to  notify the company when each account  meets  its  budget threshold. Add a budget action that selects the IAM identity created with the appropriate servi ce control policy (SCP) to  prevent  provisioning of additional  resources.
�� ��BDF



���� ��Correct Answer:  B,  D,  F  Detailed  Explanation:  1.  Option  B �C  AWS  Budgets  are cr eated and managed via the  Billing  Dashboard  (not  Cost and  Usage  Reports). This  is
where you set  budget amounts, thresholds, and alerts. Cost and Usage  Reports are fo r detailed cost analysis,  not  budget configuration. 2. Option  D �C AWS  Budgets  require s an IAM  role  (not a  user) to execute automated actions  like  blocking  resource  provis ioning.  Roles are  preferred for AWS service-to-service  interactions  because they  provi de temporary credentials and better security. 3. Option  F �C When the  budget thresh ol d is  met, AWS  Budgets triggers a  budget  action  linked to a Service Control  Policy  (S CP). SCPs are  part of AWS Organizations and can enforce account-wide  restrictions (e. g.,  blocking  new  resource creation). Config  Rules  (mentioned  in Option  E)  are for co m pliance auditing,  not immediate enforcement.  How It Works Together:  - Set  budgets	in the  Billing  Dashboard  (B).  - AWS  Budgets  uses  an IAM  role  (D) to  execute actions.
 - When the budget is exceeded, an SCP (F) automatically restricts  resource  provision i ng across the account(s).  Reference  Links:  -  [AWS  Budgets  Setup](https://docs.aws.ama zon.com/aws account billing/latest/aboutv2/budgets-managing-costs.html) -  [IAM  Roles f or AWS  Budgets](https://docs.aws.amazon.com/aws account billing/latest/aboutv2/budget s-permissions.html) -  [SCPs  in AWS Organizations](https://docs.aws.amazon.com/organi zations/latest/user guide/orgs_manage_policies_scps.html)

61.A solutions architect is designing a  RESTAPI  in Amazon API  Gateway for a cash  pa y back service. The application  requires  1 GB of memory and 2 GB of storage for its c om putation resources. The application will require that the data  is in a  relational form at. Which additional combination ofAWS services will meet these  requirements with th e  LEAST administrative effort?  (Choose two.)

A��Amazon  EC2

B��AWS  Lambda

C��Amazon  RDS

D��Amazon  Dynamo db

E��Amazon  Elastic  Ku bernet es  Services  (Amazon  EKS)

�� ��BC



���� �� Correct Answer:  B, C (AWS  Lambda and Amazon  RDS)  Detailed  Explanation:  1. AWS  Lambda  (B):  - Why  it fits:  Lambda  is a  serverless compute service that automati cally manages infrastructure, scaling, and patching. It supports  up to  10 GB of  memor y (1 GB  requirement  is easily  met) and  10 GB of ephemeral `/tmp` storage  (sufficient for the 2 GB temporary storage  need). Since the cash  payback  service  likely  involves  event-driven, short-lived computations (e.g.,  processing payments or calculating rewar ds),  Lambda aligns  perfectly with the  least administrative effort  requirement.  -  No  ser ver  management: You only deploy code; AWS handles servers, scaling, and availability.
 2. Amazon  RDS  (C):  - Why  it fits: The application  requires  relational data, and Amaz on  RDS  is a fully  managed  relational database  service  (e.g.,  MySQL,  Postgresql).  It au

tomates backups,  patching, and scaling,  minimizing administrative overhead compared	to self-managed databases on  EC2 or  EKS.  -  Storage:  RDS  allows you to configure st orage (starting at 20 GB, auto-scaling) far exceeding the 2 GB  requirement, with  no
manual storage  management  needed. Why other options are  less  ideal:  - A.  EC2:  Req ui res manual provisioning, scaling, and  maintenance of servers (more administrative w ork).  -  D.  Dynamo db: A  NoSQL  database,  not  relational,  so  it  doesn��t  meet  the data format requirement. -  E.  EKS:  Ku bernet es  involves  managing  clusters,  nodes,  and cont ainers, which adds complexity compared to serverless  Lambda. Workflow:  - API Gatew ay triggers  Lambda functions for  business  logic  (e.g.,  processing cash back  requests).  -	Lambda connects to  RDS for  relational data  storage  (e.g.,  user accounts, transaction records).  Reference  Links:  -  [AWS  Lambda](https://aws.amazon.com/lambda/)  -  [Amazo n  RDS](https://aws.amazon.com/rds/)


62.A company wants to  move from  many standalone AWS accounts to a consolidated, multi-account architecture. The company plans to create  many new AWS accounts fo r different business  units. The company needs to authenticate access to these AWS a ccounts by using a centralized corporate directory service. Which combination of actio ns should a solutions architect  recommend to meet these requirements? (Choose tw  o.)

A��Create a  new organization  in AWS Organizations with all features turned on. Creat e the new AWS accounts in the organization.

B��Set up an Amazon Cognito identity  pool. Configure AWS IAM Identity Center (AWS Single Sign-On) to accept Amazon Cognito authentication.

C��Configure a service control policy (SCP) to  manage the AWS accounts. Add AWS I AM Identity Center (AWS Single Sign-On) to AWS  Directory Service.
D��Create a  new organization in AWS Organizations. Configure the organization's auth en tication mechanism to use AWS  Directory  Service directly.
E��Set up AWS IAM Identity Center (AWS Single Sign-On)  in the organization. Configu re IAM Identity Center, and integrate it with the company's corporate directory servic e.
�� ��AE



���� �� Correct Answer: A,  E  Detailed  Explanation:  - Option A: Creating a  new AWS Or ganizations wit h all features enabled is essential for consolidating multiple AWS accou nts into a centralized structure. AWS Organizations allows you to  manage  billing, secu rity  policies  (via SCPs), and account creation in a unified way.  Enabling all features en sures access to advanced capabilities  like Service Control  Policies  (SCPs), which are cri tical for enforcing security guardrails across accounts.  - Option  E: AWS IAM Identity C enter (AWS Single Sign-On) is the recommended service for centralized authentication across  multiple AWS accounts. It integrates seamlessly with corporate directory servic es (e.g.,  Microsoft Active  Directory,  Okta) to  provide single sign-on  (SSO) access. This setup allows users to  log in using their existing corporate credentials, eliminating the need for separate IAM users  in each account. Why Other Options Are Incorrect: - Opt ion  B: Amazon Cognito is designed for app  user authentication (e.g.,  mobile/web app s),  not for  managing AWS account access. It doesn��t align with the  requirement for centralized corporate directory integration. - Option C: While SCPs are important for s

ecurity, this option mixes unrelated components. Adding IAM Identity Center to AWS Directory Service is  redundant, as IAM Identity Center already supports direct integrat i on with corporate directories. - Option  D: AWS Organizations  itself doesn��t  handle a u then tication. You  must  use IAM Identity Center (or a third -  party IdP) to connect A WS Organizations to a corporate directory.  References:  -  [AWS  Organizations Overvie w](https://aws.amazon.com/organizations/) -  [AWS IAM Identity Center  Documentation] (https://docs.aws.amazon.com/single sign on/latest/user guide/what - is.html)


63.A company wants to use artificial intelligence (AI) to determine the quality of its c ustomer service calls. The company currently manages calls in four different  language s, including  English. The company will offer  new  languages  in the future. The compan y does not  have the  resources to  regularly  maintain  machine  learning  (ML)  models. T he company needs to create written sentiment analysis reports from the customer ser vice call  recordings. The customer service call  recording text  must  be translated  into  English. Which combination of steps will meet these  requirements? (Choose three.)

A�� Use Amazon Comprehend to translate the audio recordings into  English.

B�� Use Amazon  Lex to create the written  sentiment analysis  reports. C�� Use Amazon  Polly to convert the audio  recordings  into text.

D�� Use Amazon Transcribe to convert the audio recordings in any language into text.

E�� Use Amazon Translate to translate text in any  language to  English.

F�� Use Amazon Comprehend to create the sentiment analysis reports.

�� ��DEF

���� ��Correct Answer:  D,  E,  F  Detailed  Explanation:  1.  Amazon Transcribe  (D) converts audio recordings to text. It supports  multiple  languages and automatically  handles sp eech-to-text conversion without requiring  ML  model  maintenance. 2. Amazon Translat e (E) translates text from any supported language to  English. This  meets the  require m ent to translate  non-English call transcripts  into  English and  supports future  language additions. 3. Amazon Comprehend (F)  performs sentiment analysis on text. It generat es  ready-to-use sentiment  reports without  needing custom  ML  models or ongoing  ma in tenance. Why  not other options:  - A: Comprehend doesn't translate audio  (only anal yzes text).  -  B:  Lex  is  for chat bots,  not  sentiment  reports.  - C:  Polly  converts text to  s peech (opposite of what's  needed).  Reference  Links:  - Amazon Transcribe:  https://aws. amazon.com/transcribe/ - Amazon Translate: https://aws.amazon.com/translate/ - Amaz on Comprehend: https://aws.amazon.com/comprehend/


64.A company uses on-premises servers to  host  its applications. The company is  runni ng out of storage capacity. The applications use  both  block storage and  NFS  storage. The company needs a high-performing solution that supports  local caching without r e-architecting its existing applications. Which combination of actions should a solution s architect take to  meet these  requirements?  (Choose two.)

A�� Mount Amazon S3 as a file system to the on-premises servers.

B�� Deploy an AWS Storage Gateway file gateway to  replace  NFS  storage.

C�� Deploy AWS Snowball  Edge to  provision  NFS  mounts to  on-premises  servers.

D�� Deploy an AWS Storage Gateway volume gateway to  replace the  block storage.

E�� Deploy Amazon  Elastic  File  System  (Amazon  EFS) volumes and  mount them to  on- premises servers.
�� ��BD



���� �� Correct Answer:  B,  D  Detailed  Explanation: The  company  needs to extend  both block storage and  NFS  storage with  minimal changes.  Here's why  B  and  D are  correc t:  1. Option  B  (File Gateway for  NFS):  -  Storage  Gateway's  File  Gateway  provides  NFS/ SMB access to Amazon S3. - It  maintains  local cache for frequently accessed files, ens uring high performance. -  Existing  NFS  applications work without  modification. 2. Opti on  D  (Volume Gateway for  block storage):  - Volume Gateway  provides  i SCSI  block  sto rage interface. - Cached volumes  keep frequently used data  locally while storing full  dataset in AWS. -  Maintains  block storage compatibility without app changes. Why  n ot others: - A: S3 isn't suitable for direct file/block storage  needs  (object storage  ��  fi le system).  - C:  Snowball  Edge  is for data transfer,  not  ongoing  hybrid storage.  -  E:  E FS  requires  network connection to AWS,  no  local caching support.  Reference  Links:  -  [AWS Storage Gateway  File Gateway](https://aws.amazon.com/storage gateway/file/) -
[AWS Storage Gateway Volume Gateway](https://aws.amazon.com/storage gateway/volu me/)

65.A company has  multiple Windows file servers on  premises. The company wants to migrate and consolidate its files into an Amazon  FSx for Windows  File  Server file syst em.  File  permissions  must  be  preserved to  ensure that access  rights do  not  change. Which solutions will meet these  requirements? (Choose two.)

A��Deploy AWS  Data sync agents on  premises.  Schedule  Data sync tasks to transfer the data to the  FSx for Windows  File  Server file system.
B��Copy the shares on each file server  into Amazon  S3  buckets  by  using the AWS  CLI. Schedule AWS  Data sync tasks to transfer the data to the  FSx for Windows  File Serve r file system.
C�� Remove the drives from each file server. Ship the drives to AWS for import into A mazon S3. Schedule AWS  Data sync tasks to transfer the data to the  FSx for Windows File Server file system.
D��Order an AWS Snow cone device. Connect the device to the on-premises  network. Launch AWS  Data sync agents on the device. Schedule  Data sync tasks to transfer the data to the  FSx for Windows  File  Server file system.
E��Order an AWS Snowball  Edge Storage Optimized device. Connect the device to the on-premises network. Copy data to the device by using the AWS CLI. Ship the devic e  back to AWS for import into Amazon S3. Schedule AWS  Data sync tasks to transfer 	the data to the  FSx for Windows  File Server file system.
�� ��AD



���� �� Correct Answer: A,  D  Detailed  Explanation: A.  Deploy AWS  Data sync  agents on- premises. Schedule  Data sync tasks to transfer the data to  FSx for Windows  File  Serve r file system.  - Why Correct: AWS  Data sync  is specifically designed to  migrate data w hile preserving  metadata  like  NTFS  permissions, timestamps,  and file ownership.  By  de ploying  Data sync agents on-premises, it directly transfers data from Windows file serv ers to  FSx for Windows, ensuring  permissions  remain  intact. This avoids intermediate

storage (like S3) that  might strip  metadata.  D.  Order an AWS  Snow cone device. Conn ect it to the on-premises  network.  Launch  Data sync  agents on the device. Schedule t asks to transfer data to  FSx.  - Why Correct: Snow cone  (or Snow  Family devices)  can  act as a  portable  Data sync agent. When connected to the on-premises  network,  Data Sync running on Snow cone securely transfers data directly to  FSx for Windows,  preser ving  permissions. This is ideal for  limited on-premises  bandwidth or  large datasets. W hy Other Options  Fail:  -  B/C/E: These  involve copying data to Amazon  S3 first.  S3 do es  not support  NTFS  permissions or Windows file  metadata. Transferring via  S3 would	strip  permissions, violating the  requirement to  preserve access  rights.  - C/E:  Physical l y shipping drives to AWS (via Snowball or S3) adds  unnecessary steps and  risks  losin g  metadata.  Data sync  should  be  used directly  between source and  FSx for Windows. Reference  Links:  -  [AWS  Data sync  for Windows  File  Server  Migration](https://docs.aws. amazon.com/data sync/latest/user guide/using-service.html) -  [Using AWS Snow cone wit h  Data sync](https://docs.aws.amazon.com/snow cone/latest/ug/data sync.html)


66.A company runs a website that uses a content  management system  (CMS) on Ama zon  EC2. The CMS  runs on a single  EC2  instance and  uses an Amazon Aurora  MySQL Multi-AZ  DB instance for the data tier. Website images are stored on an Amazon  Ela stic  Block Store  (Amazon  EBS) volume that  is  mounted  inside the  EC2  instance. Which	combination of actions should a solutions architect take to improve the performance and resilience of the website? (Choose two.)

A�� Move the website images into an Amazon S3  bucket that  is  mounted on every  EC
2 instance

B��Share the website images  by using an  NFS  share from the  primary  EC2  instance. Mount this share on the other  EC2  instances.
C��Move the website images onto an Amazon  Elastic  File  System  (Amazon  EFS) file sy stem that is  mounted on every  EC2  instance.
D��Create an Amazon  Machine Image  (AMI) from the existing  EC2  instance.  Use the A MI to  provision  new  instances  behind an Application  Load  Balancer  as  part of an Aut
o Scaling group. Configure the Auto Scaling group to  maintain a  minimum of two  ins tances. Configure an accelerator in AWS Global Accelerator for the website
E��Create an Amazon  Machine Image  (AMI) from the existing  EC2  instance.  Use the A
MI to  provision  new  instances  behind an Application  Load  Balancer  as  part of an Aut
o Scaling group. Configure the Auto Scaling group to  maintain a  minimum of two  ins tances. Configure an Amazon Cloud Front distribution for the website.
�� ��CE



���� ��Correct Answers: C and  E  Detailed  Explanation:  C.  Move the website  images to Amazon  EFS and  mount  it on all  EC2  instances  - Why? The  current setup  uses an  EB S volume, which is only attached to one  EC2  instance. If the  instance fails, other insta nces (if added  later) can��t access the  images. Amazon  EFS  provides a  shared file syst em accessible  by  multiple  EC2  instances across Availability Zones,  improving resilience and enabling horizontal scaling.  E.  Use Auto Scaling, an Application  Load  Balancer, a nd Amazon Cloud Front - Why?  1. Auto Scaling  +  Load  Balancer:  Running  a  single  EC 2 instance  is a single  point of failure. Auto Scaling ensures at  least 2  instances are al ways  running (resilience), and the  Load  Balancer distributes traffic  (performance). 2. Cl

oud Front: Stores cached copies of static content  (like  images) at edge  locations,  red u c ing latency for users globally (performance). This is  better than Global Accelerator (o ption  D), which optimizes  network  routing  but doesn��t cache content. Why  not other options? - A/B: S3 isn��t a file system (A), and  NFS f rom a single instance  (B) create s a  bottleneck.  -  D:  Global Accelerator doesn��t cache content  like Cloud Front  (E).  Ref erence  Links:  -  [Amazon  EFS](https://aws.amazon.com/efs/)  -  [Auto  Scaling  Groups](htt ps://docs.aws.amazon.com/auto scaling/ec2/user guide/Auto scaling group.html) -  [CloudFr ont vs. Global Accelerator](https://aws.amazon.com/compare/cloud front-vs-global-accele rator/)

67.A company has  migrated  multiple  Microsoft Windows Server workloads to Amazon	EC2 instances that  run  in the  us-west-1  Region. The  company  manually  backs  up the workloads to create an image as needed. In the event of a natural disaster in the us -west-1  Region, the company wants to  recover workloads quickly  in the us-west-2  Re gion. The company wants  no  more than 24  hours of data  loss on the  EC2  instances. The company also wants to automate any  backups of t he  EC2  instances. Which sol uti ons will meet these requirements with the  LEAST administrative effort?  (Choose two.)
A��Create an Amazon  EC2-backed Amazon  Machine Image  (AMI)  lifecycle  policy to  cr eate a  backup  based on tags. Schedule the  backup to  run twice daily. Copy the  imag e on demand.

B��Create an Amazon  EC2-backed Amazon  Machine Image  (AMI)  lifecycle  policy to  cr eate a  backup  based on tags. Schedule the  backup to  run twice daily. Configure the copy to the us-west-2  Region.

C��Create  backup vaults  in  us-west-1 and in us-west-2  by  using AWS  Backup.  Create a  backup  plan for the  EC2  instances  based  on tag values. Create an AWS  Lambda fu nction to  run as a scheduled job to copy the  backup data to us-west-2.
D��Create a  backup vault  by  using AWS  Backup.  Use  AWS  Backup  to  create a  backup plan for the  EC2  instances  based on tag values.  Define the  destination for the copy a s us-west-2. Specify the  backup schedule to  run twice daily.
E��Create a  backup vault  by  using AWS  Backup.  Use  AWS  Backup to  create a  backup plan for the  EC2  instances  based on tag values.  Specify the  backup schedule to  run t w ice daily. Copy on demand to us-west-2.
�� ��BD



���� ��Correct Answer:  B,  D  Explanation: Option  B  uses Amazon  Data  Lifecycle  Manage r (DLM) to automate AMI backups twice daily.  By configuring cross-region copying to us-west-2, backups are automatically replicated to the disaster recovery (DR)  region. This ensures automated  backups and  meets the 24-hour data  loss  limit  (since  backup s are done twice daily). Option  D  uses AWS  Backup to automate  backups  and cross-r egion replication. AWS  Backup allows defining a  backup  plan with a schedule  (twice  daily) and automatically copies backups to us-west-2. This fully automates the process without needing manual steps or custom code (unlike Option C and  E).  Both options eliminate  manual effort, automate  backups, and ensure  backups are stored in the  DR	region (us-west-2) for quick  recovery during a disaster. Why not other options? - A/ C/E  require  manual copying  (A/E) or custom  Lambda code  (C),  increasing admin istrati ve effort. - AWS  Backup  (D) and  DLM  (B)  handle  cross-region  replication  natively,  min

imizing manual work.  Reference  Links:  -  [AWS  Backup  Cross-Region  Copy](https://docs. aws.amazon.com/aws-backup/latest/dev guide/cross-region-backup.html) -  [Amazon  Dat a  Lifecycle  Manager](https://docs.aws.amazon.com/AWSEC2/latest/User guide/snapshot-li fecycle.html)


68.A company is migrating an on-premises application to AWS. The company wants t
o use Amazon  Redshift as a solution. Which  use cases are  suitable for Amazon  Redsh ift in this scenario? (Choose three.)
A��Supporting data APIs to access data with traditional, containerized, and event-drive n applications

B��Supporting client-side and server-side encryption

C��Building analytics workloads during specified  hours and when the application is  not active
D��Caching data to  reduce the  pressure on the  backend database

E��Scaling globally to support  petabytes of data and tens of millions of requests  per minute

F��Creating a secondary  replica of the cluster  by  using the AWS  Management Consol

e

�� ��BCE



���� �� Correct Answer C,  B,  E  Detailed  Explanation  1.  C.  Building  analytics workloads d uring specified hours and when the application is not active Amazon  Redshift  is ideal

 for batch  processing and analytics workloads. Companies often schedule  heavy data  transformations, reporting, or  ETL  (Extract, Transform,  Load) jobs during off-peak  hour s to optimize costs and  performance.  Redshift allows you to  pause or  resize clusters when inactive, saving costs while  handling large-scale analytics during scheduled time s. 2.  B.  Supporting client-side and server-side encryption Security is critical during clo ud migration.  Redshift  natively supports encryption for data  at  rest  (via AWS  Key  Ma nagement Service) and in transit (using SSL). This ensures sensitive on-premises data  remains secure after  migration, aligning with compliance requirements. 3.  E. Scaling gl obally to support  petabytes of data and tens of millions of requests  per  minute  Reds hift is designed for  petabyte-scale data warehousing. With  RA3  nodes,  it separates  st orage and compute, allowing independent scaling. While it��s  not ��globally scaled�� l ike a CDN, it  handles  massive datasets and  high concurrency for analytics,  making  it  suitable for large enterprises migrating  heavy workloads. Why Other Options Are Inco rrect - A.  Redshift  is  not optimized for direct data API access; services  like Amazon A PI Gateway or AppSync are  better for this.  -  D.  Caching  is  handled  by  services  like A mazon  Elastic ache or  DAX  (for  Dynamo db),  not  Redshift.  -  F.  Redshift  does  not  offer a ��secondary  replica�� feature via the console; cross-region backups  require  manual s nap shots.  Reference  Links  -  [Amazon  Redshift  Use  Cases](https://aws.amazon.com/reds hift/use-cases/) -  [Redshift Security  Features](https://docs.aws.amazon.com/redshift/lates t/mgmt/security.html) -  [Redshift  Scaling with  RA3](https://aws.amazon.com/redshift/fea tures/ra3/)

69.A company runs container applications  by using Amazon  Elastic  Ku bernet es  Service (Amazon  EKS). The company's workload  is  not consistent throughout the day. The co

mpany wants Amazon  EKS to scale  in and out according to the workload. Which com bination of steps will meet these  requirements with the  LEAST operational overhead? (Choose two.)
A�� Use an AWS  Lambda function to  resize the  EKS  cluster.

B�� Use the  Ku bernet es  Metrics  Server to activate  horizontal  pod auto scaling.

C��Use the  Ku bernet es Cluster Auto scaler to  manage the  number  of nodes  in the cl us ter.
D�� Use Amazon API Gateway and connect it to Amazon  EKS.

E�� Use AWS App  Mesh to observe  network activity.

�� ��BC



���� �� Correct Answer  B, C  Detailed  Explanation To automatically scale containerized a pplications on Amazon  EKS with  minimal operational overhead:  1. Option  B  (Ku bernet es  Metrics Server for  Horizontal  Pod Auto scaling):  - The  Horizontal  Pod  Auto scaler (H PA) automatically adjusts the  number of  pod replicas in a deployment  based on CPU/ memory usage or custom  metrics.  - The  Metrics  Server collects  resource  metrics  (e.g., CPU usage) from pods, enabling  HPA to  make scaling decisions. -  Example: If CPU  u sage spikes due to  high traffic,  HPA  adds  more  pods to  handle the  load.  2.  Option C	(Ku bernet es Cluster Auto scaler for  Node Scaling):  - The Cluster Auto scaler automatic ally adjusts the  number of worker  nodes  in the  EKS cluster.  - When  pods can��t  be  s chedu led due to  insufficient  resources, it adds  nodes. When  nodes are  underutilized,  i t  removes them.  -  Example: If  HPA  scales  pods  beyond available  node capacity, the C

luster Auto scaler adds  nodes to accommodate the  new  pods. Why  not other options?
 - Option A  (Lambda):  Manually  resizing the cluster via  Lambda  adds operational com plexity compared to the automated Cluster Auto scaler. - Option  D  (API Gateway):  Unr elated to scaling; it��s for managing APIs.  - Option  E  (App  Mesh):  Focuses  on  networ k o b serv ability,  not scaling.  By combining  HPA  (pod-level  scaling) and Cluster Autosca ler (node-level scaling), the system scales efficiently without manual intervention.  Refe rence  Links -  [Horizontal  Pod Auto scaler](https://docs.aws.amazon.com/eks/latest/userg uide/horizontal-pod-auto scaler.html) -  [Cluster Auto scaler on  EKS](https://docs.aws.ama zon.com/eks/latest/user guide/auto scaling.html)


70.A company has a workload in an AWS  Region. Customers connect to and access t he workload by  using an Amazon API Gateway  REST API. The  company uses Amazon Route 53 as its  DNS  provider. The company wants to  provide  individual and secure URLs for all customers. Which combination of steps will meet these  requirements with	the  MOST operational efficiency? (Choose three.)

A�� Register the  required domain in a  registrar. Create a wildcard custom domain  nam e  in a  Route  53  hosted zone and  record  in the zone that  points  to the API Gateway endpoint.

B��Request a wildcard certificate that  matches the domains in AWS Certificate  Manage r  (ACM)  in a different  Region.

C��Create  hosted zones for each customer as  required  in  Route  53.  Create zone  recor ds that  point to the API Gateway endpoint.

D��Request a wildcard certificate that  matches the custom domain  name in AWS Certi ficate  Manager  (ACM)  in the same  Region.
E��Create  multiple API endpoints for each customer in API Gateway.

F��Create a custom domain  name in API Gateway for the  REST API. Import the  certifi cate from AWS Certificate  Manager  (ACM).

�� ��ADF



���� �� Correct Answer A,  D,  F  Detailed  Explanation  1.  Option A  - Why correct: A wildc ard custom domain (e.g., `*.api.example.com`) in  Route  53 allows creating subdomains	(e.g., `customer1.api.example.com`, `customer2.api.example.com`) dynamically for all cu stomers without manual  DNS configuration for each customer. This ensures operation al efficiency.  -  Key  Point:  Route  53  manages  DNS  routing,  and  the wildcard  record  (`* `)  points all subdomains to the same API Gateway endpoint. 2. Option  D  - Why corre ct: A wildcard SSL/TLS certificate (e.g., `*.api.example.com`) from AWS Certificate  Mana ger (ACM) in the same AWS  Region as the API Gateway  is  required. API  Gateway can only use certificates issued in the same  Region.  - Why  not  B: ACM certificates  must be in the same  Region as the API  Gateway.  Using a different  Region  (Option  B) woul d fail. 3. Option  F  - Why correct: Creating a custom domain  name  in API Gateway  lin ks the domain (e.g., `api.example.com`) to the  REST API and associates the ACM wildc ard certificate. This enables  HTTPS and custom  URLs for customers. Why other option s are incorrect:  - Option C: Creating a  hosted zone for each customer is operationally	inefficient (manual setup for every customer). - Option  E: Creating  multiple API endp oi nts  per customer is unnecessary and scales  poorly. A single API with custom domai

ns is  more efficient. Summary:  -  Use a wildcard  DNS  record  (A)  +  wildcard ACM  certi ficate in the same  Region  (D)  +  API Gateway custom domain (F) to securely  route all	customer subdomains to the same API endpoint with minimal management.  Referenc e  Links  -  [API  Gateway Custom  Domain  Names](https://docs.aws.amazon.com/api gate w ay/latest/developer guide/how-to-custom-domains.html) -  [ACM Certificate  Regions](htt ps://docs.aws.amazon.com/acm/latest/user guide/acm-regions.html) -  [Route  53 Wildcar d  Records](https://docs.aws.amazon.com/Route53/latest/Developer guide/Domain name f ormat.html#domain-name-format-asterisk)


71.A company wants to  build a web application on AWS. Client access  requests to th e website are  not  predictable and can  be  idle for a  long time. Only customers who  h ave  paid a subscription fee can  have the ability to  sign in and  use the web applicatio
n. Which combination of steps will meet these  requirements  MOST cost-effectively?  (C hoose three.)

A��Create an AWS  Lambda function to  retrieve  user  information from Amazon  Dynam
oDB. Create an Amazon API Gateway endpoint to accept  RESTful APIs.  Send the API calls to the  Lambda function.
B��Create an Amazon  Elastic Container  Service  (Amazon  ECS) service  behind an Applic ation  Load  Balancer to  retrieve  user  information from Amazon  RDS.  Create an Amazo n API Gateway endpoint to accept  RESTful APIs. Send the API calls to the  Lambda fu nction.
C��Create an Amazon Cognito user  pool to authenticate users.

D��Create an Amazon Cognito identity  pool to authenticate  users.

E��Use AWS Amplify to serve the front end web content with  HTML, CSS, and JS.  Use an integrated Amazon Cloud Front configuration.

F��Use Amazon S3 static web  hosting with  PHP,  CSS, and JS.  Use Amazon  Cloud Front to serve the front end web content.

�� ��ACE



���� ��Correct Answer: ACE  Detailed  Explanation:  1. Option A  (Lambda  + API  Gateway +  Dynamo db):  - AWS  Lambda  is  serverless, so you only pay when the function  runs. This is  perfect for unpredictable traffic with idle  periods.  - API Gateway  handles  RESTf ul API  requests and  integrates seamlessly with  Lambda.  -  Dynamo db  (NoSQL)  scales a u tomatically and charges  based on  usage, avoiding costs for idle time. - Why it��s co st  - effective:  No fixed  infrastructure costs;  pay  -  per  -  use  pricing.  2.  Option C  (Cogn
ito  User  Pool):  - Amazon Cognito  User  Pools  manage  user  authentication  (sign  -  up/s ign - in) and store  user profiles.  - It verifies subscription status  by checking user  pool membership (e.g.,  paid users are  registered  in the  pool).  - Why it��s cost - effective: Cognito charges  based on  monthly active users  (MAUs), which is affordable for most applications. 3. Option  E  (Amplify  +  Cloud Front):  - AWS Amplify  hosts static front end files (HTML/CSS/JS) and automatically configures Cloud Front (a CDN) for fast, global c ontent delivery. - Cloud Front caches content at edge locations,  reducing  latency and  server  load.  - Why  it��s  cost  - effective: Amplify and Cloud Front  use  pay  -  as  - you  - go pricing, with  no charges when there��s  no traffic. Why Other Options Are Wrong:
- Option  B  (ECS  +  ALB  +  RDS):  ECS  and  RDS  require  always  -  running  servers,  which are expensive for unpredictable/idle workloads. - Option  D (Cognito Identity  Pool): Id entity  Pools grant temporary AWS credentials (e.g., for accessing S3),  but the questio

n focuses on user authentication (User  Pools).  -  Option  F  (S3  +  PHP):  S3  cannot  run server  - side code  like  PHP;  it  only  hosts  static files.  Reference  Links:  -  [AWS  Lambda Pricing](https://aws.amazon.com/lambda/pricing/) -  [Amazon Cognito  Pricing](https://a ws.amazon.com/cognito/pricing/) -  [AWS Amplify  Hosting](https://aws.amazon.com/am plify/hosting/)


72.A company runs Amazon  EC2  instances  in  multiple AWS accounts that are  individu ally bled. The company recently purchased a Savings  Pian.  Because  of changes in the	company��s  business  requirements, the company  has decommissioned a  large  numbe r of EC2  instances. The company wants to  use its Savings  Plan discounts on  its other AWS accounts. Which combination of steps will meet these requirements? (Choose t wo.)

A��From the AWS Account  Management Console of the  management account, turn on discount sharing from the billing  preferences section.

B�� From the AWS Account  Management Console of the account that  purchased the e xisting Savings  Plan, turn on discount sharing from the  billing  preferences section. Inc lude all accounts.

C��From the AWS Organizations  management account, use AWS  Resource Access  Man ager (AWS  RAM) to  share the Savings  Plan with other accounts.
D��Create an organization  in AWS Organizations in a  new  payer account. Invite the ot her AWS accounts to join the organization from the management account.

E��Create an organization in AWS Organizations in the existing AWS account with the existing  EC2  instances and Savings  Plan. Invite the other AWS accounts to join the or ganization from the  management account.
�� ��ADE



���� ��Correct Answer A.  From the AWS Account  Management Console of the  manage ment account, turn on discount sharing from the  billing  preferences section.  E. Create an organization in AWS Organizations in the existing AWS account with the existing EC2 instances and Savings  Plan. Invite the other AWS accounts to join the organizati o n from the management account.  Detailed  Explanation To  share Savings  Plan discount s across  multiple AWS accounts:  1. Organize accounts  under AWS Organizations (Opti on  E):  - The company  must first create an AWS Organization  in the existing account where the Savings  Plan was  purchased. This  makes  it the  management  account of the organization. - Other AWS accounts are then invited to join this organization. Only a ccounts in the same organization can share Savings  Plan discounts. 2.  Enable discount	sharing (Option A):  - The  management account  must enable Savings  Plan discount s haring in the AWS  Billing  preferences. This setting allows the Savings  Plan discounts t
o apply to all member accounts in the organization. Why other options are incorrect:
 - Option  B:  Discount sharing  is  controlled by the  management account,  not the acco unt that  purchased the  Savings  Plan.  - Option C: AWS  RAM  is  used to  share  resource s (e.g., sub nets),  not  Savings  Plans.  -  Option  D:  Creating a  new  payer/organization  in a separate account would  not allow sharing with the existing Savings  Plan, which  is ti ed to the original account.  Reference  Links  -  [AWS  Savings  Plans:  Sharing  Discounts]  (https://docs.aws.amazon.com/savings plans/latest/user guide/share-discounts.html) -  [AW

S Organizations and Consolidated  Billing](https://docs.aws.amazon.com/organizations/la test/user guide/orgs_introduction.html)


73.A company is using AWS  Key  Management  Service  (AWS  KMS)  keys to  encrypt A WS  Lambda environment variables. A solutions architect  needs to ensure that the  req ui red permissions are in place to decrypt and use the environment variables. Which s teps  must the solutions architect take to implement the correct  permissions? (Choose	two.)

A��Add AWS  KMS  permissions  in the  Lambda  resource  policy.

B��Add AWS  KMS  permissions  in the  Lambda  execution  role. C��Add AWS  KMS  permissions  in the  Lambda function  policy.

D��Allow the  Lambda execution  role  in the AWS  KMS  key  policy.

E��Allow the  Lambda  resource  policy  in the AWS  KMS  key  policy.

�� ��BD



���� ��Correct Answer:  B,  D  Detailed  Explanation: To  allow a  Lambda function to decry pt environment variables using AWS  KMS, you  need to configure  permissions  in two places:  1.  Lambda  Execution  Role  (Option  B):  The  Lambda function's  execution  role  m ust  have  permissions  (like `kms:Decrypt`) to use the  KMS  key. This  grants the function	the ability to call  KMS APIs  during execution. 2.  KMS  Key  Policy  (Option  D):  The  KM S  key's  policy  must explicitly allow the  Lambda  execution  role to  use the  key.  Even  if	the execution  role  has  permissions, the  KMS  key  policy  must  trust the  role to  grant

access. Why not other options? - A/C:  Lambda  doesn��t  have a  resource  policy  or fun ction policy for execution permissions��these are  managed via the execution  role.  -  E: KMS  key  policies don��t  reference  Lambda  resource  policies.  Permissions  are granted directly to the execution  role,  not the  Lambda function��s  resource  policy. Analogy: Th ink of the execution  role as a worker��s ID  badge  (grants access to tools) and the  K MS  key  policy as a vault��s  security  rule (explicitly  lists who can open  it).  Both are  re quired to access the vault.  Reference  Links:  -  [AWS  KMS  Key  Policies](https://docs.aws. amazon.com/kms/latest/developer guide/key-policies.html) -  [Lambda  Execution  Roles](h ttps://docs.aws.amazon.com/lambda/latest/dg/lambda-permissions.html)


74.A company hosts  multiple applications on AWS for different  product  lines. The app lications use different compute  resources, including Amazon  EC2  instances and Applic ation  Load  Balancers. The applications  run  in different AWS accounts  under the same organization in AWS Organizations across  multiple AWS  Regions. Teams for each  pro duct  line have tagged each compute  resource  in the individual accounts. The compan y wants  more details about the cost for each  product  line from the consolidated  billi ng feature in Organizations. Which combination of steps will meet these  requirements?	(Choose two.)

A��Select a specific AWS generated tag in the AWS  Billing console.

B��Select a specific user-defined tag  in the AWS  Billing console.

C��Select a specific user-defined tag  in the AWS  Resource Groups console.

D��Activate the selected tag from each AWS account.

E��Activate the selected tag from the Organizations management account.

�� ��BE



���� ��Correct Answer:  B,  E  Detailed  Explanation:  To track costs  per  product  line  using tags in AWS Organizations consolidated  billing:  1.  B.  Select a specific  user-defined tag		in the AWS  Billing console  -  User-defined tags  (like `Product line`)  must  be  enabled  i n the AWS  Billing console for cost allocation. AWS-generated tags  (e.g., `aws:ec2:insta nce-type`) are automatic  but won't  help track custom  product  lines. 2.  E. Activate the	selected tag from the Organizations management account - The  management accou nt  must activate the tag once to apply it across all accounts in the organization. This ensures the tag is visible in consolidated billing reports. Activating tags in individual accounts (Option  D)  is  unnecessary and  inefficient. Why  not other options?  - A: AWS- generated tags are  predefined (e.g., for services),  not for custom  product  lines.  -  C:  R esource Groups organize resources,  but don't enable  billing  reports.  -  D: Tags  are acti vated centrally via the  management account,  not  per account.  Reference:  [AWS  Cost  Allocation Tags  Documentation](https://docs.aws.amazon.com/aws account billing/latest/a boutv2/custom-tags.html)


75.A solutions architect needs to ensure that API calls to Amazon  Dynamo db from A mazon  EC2  instances  in a VPC do  not travel across the internet. Which combination of steps should the solutions architect take to  meet this  requirement? (Choose two.)

A��Create a  route table entry for the endpoint.

B��Create a gateway endpoint for  Dynamo db.

C��Create an interface endpoint for Amazon  EC2.

D��Create an elastic  network  interface for the endpoint  in each of the subnets of t he VPC.
E��Create a security group entry in the endpoint's security group to  provide access.

�� ��AB



���� ��Correct Answer  B. Create a gateway endpoint for  Dynamo db. A. Create a  route table entry for the endpoint.  Detailed  Explanation To  keep traffic  between Amazon  EC 2 instances in a VPC and Amazon  Dynamo db within the AWS  network  (avoiding the public internet), you need a VPC Gateway  Endpoint for  Dynamo db  (Option  B). This  e ndpoint acts  like a  private shortcut to  Dynamo db.  However,  simply creating the endp o int isn��t enough. You  must also  update the VPC  route tables  (Option A) to direct tr affic bound for  Dynamo db to this endpoint. This ensures that AWS automatically  rout es  Dynamo db API calls through the endpoint  instead of the  internet. Why  not the ot her options? - C (Interface  Endpoint for  EC2): Interface  endpoints are for services  like	EC2 API calls (e.g., starting/stopping instances),  not  Dynamo db.  -  D  (Elastic  Network  Interfaces): Gateway endpoints don��t  require  ENIs; this  is only  needed for  interface e nd points. -  E  (Security Groups): Gateway endpoints  use  route tables and endpoint  pol icies, not security groups, to control access.  Reference  Links  -  [AWS  VPC Gateway  End points](https://docs.aws.amazon.com/vpc/latest/private link/vpc-endpoints.html) -  [Dyna moDB VPC  Endpoint Setup](https://docs.aws.amazon.com/amazon dynamo db/latest/deve loper guide/vpc-endpoints-dynamo db.html)


76.A  research company uses on-premises devices to generate data for analysis. The c ompany wants to use the AWS Cloud to analyze the data. The devices generate .csv

files and support writing the data to an SMB file share. Company analysts  must  be a ble to use SQL commands to query the data. The analysts will run queries  periodic all y throughout the day. Which combination of steps will  meet these  requirements  MOS T cost-effectively? (Choose three.)
A�� Deploy an AWS Storage Gateway on premises in Amazon S3  File Gateway  mode.

B�� Deploy an AWS Storage Gateway on  premises in Amazon  FSx  File  Gateway  made.

C��Set up an AWS Glue crawler to create a table  based on the data that is i n Amazo n  S3.
D��Set up an Amazon  EMR cluster with  EMR  File  System  (EMRFS) to  query the data t hat is in Amazon S3.  Provide access to analysts.
E��Set  up an Amazon  Redshift cluster to query the data that  is  in Amazon  S3.  Provide access to analysts.

F��Setup Amazon Athena to query the data that  is in Amazon S3.  Provide access to a nalysts.

�� ��ACF



���� �� Correct Answer: A, C,  F  Detailed  Explanation:  1. A.  Deploy AWS  Storage Gatewa y in S3  File Gateway  mode The  on  -  premises devices  use SMB file shares, so  Storag e Gateway's S3  File Gateway  mode allows them to write CSV files directly to Amazon S3 via an SMB interface. This avoids costly data migration tools and uses S3's  pay - as - you - go storage, which is cost - effective.  FSx  File Gateway  (Option  B)  is  unne cessary  here since the goal  is to store data  in S3,  not  in a  managed Windows/FSx fil

e system. 2. C. Use AWS Glue crawler AWS Glue crawler automatically discovers the s chema of the CSV files in S3 and creates a  metadata table  in the Glue  Data Catalog. This  makes the data query able  by  SQL tools  like Athena. Glue  is  serverless, so there��s no extra cost when it��s not running, aligning with the periodic queries requirement. 3.  F.  Use Amazon Athena Athena  is a serverless  SQL query service that directly  reads data from S3 using the Glue  Data Catalog. Analysts  pay only  per query, with  no  infra structure to  manage. This is far  more cost  - effective than  provisioning an  EMR  c luste r (Option  D) or a  Redshift  cluster  (Option  E), which  charge  hourly even when  idle. At hena is ideal for intermittent, ad  -  hoc  SQL analysis. Why  not other options?  -  B  (FSx Gateway):  Designed for integrating with  FSx for Windows/ONTAP,  not  needed  here. - D (EMR): Overkill for simple SQL queries;  high cost for periodic use. -  E  (Redshift):  E x pensive for intermittent queries;  meant for  large  - scale analytics,  not ad  -  hoc  SQL. Reference  Links:  -  [AWS  Storage Gateway](https://aws.amazon.com/storage gateway/)  -  [AWS Glue](https://aws.amazon.com/glue/) -  [Amazon Athena](https://aws.amazon.com/ athena/)


77.A company wants to use Amazon  Elastic Container  Service (Amazon  ECS) clusters a nd Amazon  RDS  DB  instances to  build  and  run a  payment  processing application. The	company will run the application in its on-premises data center for compliance purp oses. A solutions architect wants to use AWS Outposts as  part of the solution. The s ol utions architect is working with the company's operational team to  build the applic ation. Which activities are the  responsibility of the company's operational team?  (Cho ose three.)

A�� Providing resilient  power and  network connectivity to the Outposts  racks

B��Managing the virtualization hypervisor, storage systems, and the AWS services that run on Outposts
C�� Physical security and access controls of t he data center environment

D��Availability of the Outposts  infrastructure  including the  power supplies, servers, and networking equipment within the Outposts  racks
E�� Physical maintenance of Outposts components

F��Providing extra capacity for Amazon  ECS c lusters to  mitigate  server failures and  ma in tenance events

�� ��ACF



���� ��Correct Answer: A, C,  F  Detailed  Explanation:  1. A.  Providing  resilient  power  and network connectivity to the Outposts  racks - The operational team  must ensure stabl e  power and  network connections  in their on  -  premises data center. AWS  manages t he Outposts  hardware  but  relies on the customer to  provide  proper  infrastructure  sup port. 2. C.  Physical security and access controls of t he data center environment  - The customer owns physical security for their data center (e.g., locks, cameras). AWS han dles security within the Outposts  racks,  but the surrounding environment  is the custo mer's responsibility.  3.  F.  Providing  extra capacity for Amazon  ECS c lusters to  mitigate	server failures  - The operational team  must design the application to  handle failures 		(e.g., scaling  ECS tasks across servers). AWS ensures Outposts  hardware availability,  b ut application - level  redundancy is the customer's job. Why other options are  incorre ct: -  B/D/E: AWS  manages the  hypervisor,  Outposts  hardware  (servers,  power  supplies), and  physical maintenance. These are  part of the AWS -  managed layer. -  D specific a

lly refers to Outposts  infrastructure availability, which AWS guarantees as  part of its s ervice.  Reference:  [AWS  Outposts  Responsibility  Model](https://docs.aws.amazon.com/o utposts/latest/user guide/what-is-outposts.html#shared-responsibility)


78.A company wants to  migrate an on-premises data center to AWS. The data center hosts a storage server that stores data in an  NFS-based file system. The storage serv er  holds 200 GB of data. The company  needs to  migrate the data without interruptio n to existing services.  Multiple  resources  in AWS  must  be  able to access the data  by using the  NFS  protocol. Which combination of steps will  meet these  requirements  M OST cost-effectively? (Choose two.)
A��Create an Amazon  FSx for  Lustre file  system.

B��Create an Amazon  Elastic  File  System  (Amazon  EFS) file  system. C��Create an Amazon S3  bucket to  receive the data.

D��Manually use an operating system copy command to  push the data into the AWS destination.
E��Install an AWS  Data sync agent  in the on-premises data center.  Use a  Data sync tas k between the on-premises  location and AWS.
�� ��BE



���� �� Correct Answer  B. Create an Amazon  Elastic  File System  (Amazon  EFS) file  syste m.  E. Install an AWS  Data sync agent  in the on-premises data center.  Use a  Data sync task  between the on-premises  location and AWS.  Detailed  Explanation  1. Why Amazo

n  EFS  (B)?  - Amazon  EFS  is  a fully  managed  NFS-based file  system designed for AWS.	It allows  multiple AWS  resources (like  EC2  instances,  Lambda,  etc.) to access  shared  data simultaneously via the  NFS  protocol,  matching the  requirement  perfectly.  - It��s  cost-effective for 200 GB of data  because you only  pay for the storage  used, and the re��s  no  upfront cost. 2. Why AWS  Data sync  (E)?  -  Data sync  is  purpose-built  for fast, secure, and automated data  migration from on-premises to AWS. It  uses an agent ins talled on-premises to transfer data over a dedicated  network connection,  minimizing  downtime. - It supports incremental transfers (only copying changes after the initial s ync), ensuring  no interruption to existing services during migration. Why other option s are wrong? - A  (FSx for  Lustre):  Lustre  is  optimized for  high-performance computin g,  not general-purpose  NFS. It��s  overkill and  more  expensive for this use case.  - C  (S 3): S3 is object storage and doesn��t  natively support  NFS. While  Data sync  can send data to S3, you��d still  need to  move  it to  EFS/EFS-like  storage for  NFS access,  addin g complexity. -  D  (Manual copy):  Manually  copying 200 GB via OS commands  is slow, error-prone, and doesn��t guarantee consistency during ongoing operations.  Referenc e  Links  -  [Amazon  EFS](https://aws.amazon.com/efs/)  -  [AWS  Data sync](https://aws.am azon.com/data sync/)


79.A company is creating a  new web application for its subscribers. The application w ill consist of a static single page and a  persistent database  layer. The application will have  millions of users for 4  hours  in the  morning,  but the application will  have only a few thousand users during the rest of t he day. The company's data architects  have	requested the ability to  rapidly evolve their schema. Which solutions will  meet these	requirements and  provide the  MOST scalability?  (Choose two.)

A�� Deploy Amazon  Dynamo db as the database solution.  Provision on-demand capacit y.
B��Deploy Amazon Aurora as the database solution. Choose the serverless  DB engine mode.
C��Deploy Amazon  Dynamo db as the database solution.  Ensure that  Dynamo db  auto scaling is enabled.
D��Deploy the static content into an Amazon S3  bucket.  Provision an Amazon  Cloud Fr ont distribution with the S3 bucket as the origin.
E�� Deploy the web servers for static content across a fleet of Amazon  EC2  instances  i n Auto Scaling groups. Configure the instances to  periodically  refresh the content fro m an Amazon  Elastic  File  System  (Amazon  EFS) volume.
�� ��AD



���� �� Correct Answer A.  Deploy Amazon  Dynamo db as the database solution.  Provisi on on-demand capacity.  D.  Deploy the  static content  into an Amazon S3  bucket.  Prov is ion an Amazon Cloud Front distribution with the S3  bucket as the origin.  Detailed  Ex planation  1.  Dynamo db  (Option A)  - Scalability:  Dynamo db  is a  NoSQL  database that automatically scales to  handle massive traffic spikes (millions of users) during peak h ours. On-demand capacity ensures you pay only for what you use, eliminating the ne ed to  manually adjust capacity.  - Schema  Flexibility: As a  NoSQL  database,  Dynamo db allows rapid schema changes without downtime, meeting the requirement to rapidly evolve their schema. Options C (Dynamo db with auto-scaling) is  less ideal because au to-scaling introduces a s light delay in capacity adjustments, while on-demand (Option

 A) scales instantly. 2. S3  +  Cloud Front  (Option  D)  -  Static  Content  Hosting:  S3  is des igned for storing static files (HTML, CSS, JS) and integrates seamlessly with Cloud Fron t (a CDN). This combination delivers content globally with  low  latency.  - Scalability: Cl oud Front automatically handles traffic spikes  by caching content at edge  locations,  re ducing  load on the origin  (S3). Option  E  (EC2  +  EFS)  is  inefficient  because  EC2  instan ces  require  manual scaling and  EFS  introduces  latency for file storage, while S3+Clou d Front is fully serverless and scales  infinitely. 3. Why  Not Aurora  (Option  B)? Aurora  S erver less is a  relational database, which  requires  rigid schema definitions.  Making sche ma changes (e.g., adding columns) is slower and  more complex compared to  Dynamo DB��s schema-less design.  Reference  Links  -  [Dynamo db  On-Demand](https://aws.amaz on.com/dynamo db/pricing/on-demand/) -  [S3 Static Website with Cloud front](https://d ocs.aws.amazon.com/AmazonS3/latest/user guide/website-hosting-cloud front-walk throug h.html)


80.A solutions architect is designing a  new service  behind Amazon API Gateway. The request patterns for the service will be unpredictable and can change suddenly from
0  requests to over  500  per second. The total size of the data that  needs to  be  persis ted in a  backend database  is currently  less than  1 GB with  unpredictable future grow th.  Data can  be queried  using simple  key-value  requests. Which combination ofAWS s ervices would  meet these  requirements?  (Choose two.)

A��AWS  Fargate

B��AWS  Lambda

C��Amazon  Dynamo db

D��Amazon  EC2 Auto  Scaling

E�� MySQL-compatible Amazon Aurora

�� ��BC



���� �� Correct Answer  B. AWS  Lambda C. Amazon  Dynamo db  Detailed  Explanation  1. AWS  Lambda  (B)  is serverless and automatically scales to  handle sudden traffic spikes	(0 to  500+  requests/sec) without  manual  intervention.  Unlike  EC2/Fargate  (which  hav e cold-start delays),  Lambda  instantly  scales to  match demand,  making  it  ideal for un predictable workloads  behind API Gateway. 2. Amazon  Dynamo db  (C)  is a  NoSQL  dat abase optimized for simple key - value queries. It automatically scales storage and th rough put to  handle unpredictable growth  (even  beyond  1GB) while  maintaining  low  -		latency performance. Aurora (E)  is a  relational database  better suited for complex qu eries, not simple  key  - value  lookups. Why  not others?  - A/Fargate ��  D/EC2 Auto  Sc aling:  Require  pre  -  provisioned capacity or scaling  policies, causing delays during sud den traffic spikes.  -  E/Aurora:  Overkill for  key  - value queries and  lacks  Dynamo db��s seamless scalability for unpredictable workloads.  Reference  Links  -  [AWS  Lambda  Scali ng](https://aws.amazon.com/lambda/features/#Scalability) -  [Amazon  Dynamo db  Use C ases](https://aws.amazon.com/dynamo db/use - cases/)


81.A company has an application workflow that  uses an AWS  Lambda function to do wnload and decrypt files from Amazon S3. These files are encrypted using AWS  Key Management Service (AWS  KMS)  keys. A  solutions architect  needs to design a solutio n that will ensure the required permissions are set correctly. Which combination of ac t ions accomplish this? (Choose two.)

A��Attach the kms:decrypt  permission to the  Lambda function��s  resource  policy

B��Grant the decrypt  permission for the  Lambda IAM  role  in the  KMS  key's  policy

C��Grant the decrypt  permission for the  Lambda  resource  policy  in the  KMS  key's  pol icy.
D��Create a  new IAM  policy with the  kms:decrypt  permission  and  attach the  policy to the  Lambda function.
E��Create a  new IAM  role with the  kms:decrypt  permission  and attach the execution  r ole to the  Lambda function.
�� ��BE



���� ��Correct Answer:  B and  E  Detailed  Explanation: To  allow an AWS  Lambda functio n to decrypt files using AWS  KMS  keys, two  permissions  must  be  set  up  properly:  1. KMS  Key  Policy  (Option  B):  The  KMS  key's  resource-based  policy  must  explicitly  grant	the `kms:Decrypt`  permission to the  Lambda function��s IAM  role.  Even  if the  IAM  ro le  has the  permission, the  KMS  key  itself  must trust the  role  by  allowing this action  i n its  policy. This  is a security  measure to  prevent  unauthorized access.  2. IAM  Role  P er missions (Option  E): The  Lambda function  uses an IAM execution  role during  runtim e. This  role  must  have the `kms:Decrypt`  permission attached via an IAM  policy. Optio n  E correctly states that a  new IAM  role with this  permission  should  be  created and  attached to the  Lambda function. Why other options are  incorrect:  - Option A:  Lamb da functions don��t  have  resource  policies for  KMS  permissions.  Resource  policies  for KMS are attached to the  key  itself.  - Option C:  KMS  key  policies  don��t  reference  La mbda  resource  policies.  Permissions are granted to IAM  roles/users,  not  Lambda funct

ions directly.  - Option  D: IAM  policies  can��t be attached directly to  Lambda  functions. Permissions must be granted to the IAM execution role, not the function itself.  Refer ence  Links:  -  [AWS  KMS  Key  Policies](https://docs.aws.amazon.com/kms/latest/develope rguide/key-policies.html) -  [Lambda  Execution  Roles](https://docs.aws.amazon.com/lamb da/latest/dg/lambda-intro-execution-role.html)


82.An international company has a subdomain for each country that the company op erates in. The subdomains are formatted as example.com, country1.example.com, and country2.example.com. The company's workloads are  behind an Application  Load  Bala ncer. The company wants to encrypt the website data that is in transit. Which combi nation of steps will meet these  requirements? (Choose two.)

A��Use the AWS Certificate  Manager  (ACM) console to  request a  public  certificate for the apex top domain example com and a wildcard certificate for *.example.com.

B��Use the AWS Certificate  Manager  (ACM) console to  request a  private  certificate for the apex top domain example.com and a wildcard certificate for *.example.com.

C�� Use the AWS Certificate  Manager  (ACM) console to  request a  public  and  private c ertificate for the apex top domain example.com.
D��Validate domain ownership  by email address. Switch to  DNS validation  by adding t he  required  DNS  records to the  DNS  provider.
E��Validate domain ownership for the domain  by adding the  required  DNS  records  to the  DNS  provider.
�� ��AE

���� ��Correct Answer A,  E  Detailed  Explanation To encrypt data  in transit for the com pany's subdomains (e.g., `country1.example.com`, `country2.example.com`) behind an A pplication  Load  Balancer  (ALB), the  steps are:  1.  Request  Public  Certificates  (Option A):
 - A public certificate for the apex domain (`example.com`) and a wildcard certificate  (`*.example.com`) is  required. This ensures  HTTPS encryption covers the  root domain (` example.com`) and all subdomains (`*.example.com`).  -  Public certificates are  needed  b ecause the ALB serves  public-facing traffic.  Private certificates (Options  B/C) are for  in ternal  resources,  not  public websites. 2. Validate via  DNS  (Option  E):  -  To  prove dom ain ownership for certificates (especially wildcard `*.example.com`),  DNS validation is  r equi red. This involves adding specific  DNS  records  (CNAME) to the domain��s  DNS  pr ovider. -  Email validation  (Option  D)  is  not  sufficient for wildcard certificates and is  le ss secure.  Reference  Links:  -  [AWS  Certificate  Manager  FAQ](https://aws.amazon.com/c ertificate-manager/faqs/) -  [Requesting a  Public Certificate](https://docs.aws.amazon.co m/acm/latest/user guide/gs-acm-request-public.html) -  [DNS Validation for ACM](https:/ /docs.aws.amazon.com/acm/latest/user guide/dns-validation.html)


83.A company runs a web application on Amazon  EC2  instances  in an Auto Scaling g roup t hat  has a target group. The company designed the application to work with se ssion affinity (sticky sessions) for a  better  user experience. The application  must  be av ailable publicly over the internet as an endpoint. A WAF  must  be applied to the end point for additional security. Session affinity (sticky sessions)  must  be configured on t he endpoint. Which combination of steps will meet these requirements? (Choose two.)

A��Create a  public  Network  Load  Balancer.  Specify the  application target group.

B��Create a Gateway  Load  Balancer.  Specify the application target  group.

C��Create a  public Application  Load  Balancer. Specify the application target  group.

D��Create a second target group. Add  Elastic IP addresses to the  EC2  instances.

E��Create a web ACL in AWS WAF. Associate the web ACL with the endpoint

�� ��CE



���� �� Correct Answer C,  E  Detailed  Explanation To  meet the  requirements of session  affinity (sticky sessions),  public accessibility, and WAF  integration, the correct steps ar e: 1. Option C: Create a  public Application  Load  Balancer  (ALB) and  specify the applic ation target group - The Application  Load  Balancer  (ALB)  operates at the application  l ayer (Layer 7) and supports sticky sessions (session affinity) using cookies. This ensure s user sessions are  routed to the same  EC2  instance, which  is  required for t he  applic ation's design. - A  public ALB  is  necessary to expose the application over the  internet.
 2. Option  E: Create a web ACL  in AWS WAF and associate  it with the endpoint  - A WS WAF  (Web Application  Firewall)  protects web applications by filtering  malicious tr affic. After creating a web ACL, you associate it with the ALB  (the  public endpoint) to apply security rules. Why other options are incorrect: - A (Network  Load  Balancer):  N LB operates at  Layer 4  (transport  layer) and does  not support sticky  sessions, which a re critical  here.  -  B  (Gateway  Load  Balancer):  GWLB  is  designed for deploying third  - party security appliances, not for hosting  public web applications. -  D  (Second target group with  Elastic IPs):  Elastic IPs  on  EC2  instances are  impractical for Auto  Scaling  gr oups (instances scale dynamically,  making  EIPs  hard to  manage). This also  doesn��t ad dress WAF or sticky sessions.  Reference  Links  -  [Application  Load  Balancer  Features](h ttps://docs.aws.amazon.com/elastic load balancing/latest/application/introduction.html) -

[AWS WAF Integration with ALB](https://docs.aws.amazon.com/waf/latest/developer guid e/web - acl  - associating  - aws  -  resource.html)


84.A company uses an on-premises  network-attached storage (NAS) system to provid e file shares to its  high  performance computing (HPC) workloads. The company wants	to migrate its  latency-sensitive  HPC workloads and  its storage to the AWS Cloud. Th e company must  be able to  provide  NFS  and SMB  multi-protocol access from the file		system. Which solution will meet these requirements with the  LEAST  latency?  (Choos e two.)
A�� Deploy compute optimized  EC2  instances into a cluster  placement group.

B�� Deploy compute optimized  EC2  instances  into a  partition  placement group. C��Attach the  EC2  instances to an Amazon  FSx for  Lustre  file system.

D��Attach the  EC2  instances to an Amazon  FSx for  OpenZFS file system.

E��Attach the  EC2  instances to an Amazon  FSx for  NetApp  ONTAP file  system.

�� ��AE



���� �� Correct Answer A.  Deploy compute optimized  EC2 instances into a cluster  place ment group.  E. Attach the  EC2  instances to  an Amazon  FSx for  NetApp  ONTAP file s ystem.  Detailed  Explanation  For  latency-sensitive  HPC workloads:  1.  Cluster  Placement Group (Option A) ensures  EC2  instances are  placed in the same Availability Zone with ultra-low latency and high-bandwidth networking. This is ideal for tightly coupled  HP C workloads requiring  minimal  latency. 2. Amazon  FSx for  NetApp  ONTAP  (Option  E)

supports  both  NFS and  SMB  protocols,  meeting the  multi-protocol  requirement. It is optimized for low-latency, high-throughput workloads and integrates seamlessly with HPC environments. Other options: -  FSx for  Lustre  (C)  is  HPC-optimized  but  only  sup ports  NFS,  not  SMB.  -  FSx  for OpenZFS  (D) supports  NFS  but  not  SMB.  -  Partition  Pl acement Group (B)  prioritizes fault tolerance over  low  latency,  making it  less suitable for latency-sensitive  HPC.  Reference  Links  -  [Amazon  EC2  Placement  Groups](https://d ocs.aws.amazon.com/AWSEC2/latest/User guide/placement-groups.html) -  [Amazon  FSx for NetApp ONTAP](https://aws.amazon.com/fsx/netapp-ontap/)


85.A startup company is  hosting a website for its customers on an Amazon  EC2  insta nce. The website consists of a stateless  Python application and a  MySQL  database. Th e website serves only a small amount of traffic. The company is concerned about the	reliability of the instance and needs to  migrate to a  highly available architecture. Th e company cannot modify the application code. Which combination of actions should a solutions architect take to achieve high availability for the website? (Choose two.)
A�� Provision an internet gateway  in each Availability Zone in use.

B�� Migrate the database to an Amazon  RDS for  MySQL  Multi-AZ  DB  instance.

C�� Migrate the database to Amazon  Dynamo db, and enable  Dynamo db auto scaling.

D��Use AWS  Data sync to  synchronize the database data across  multiple  EC2  instances.

E��Create an Application  Load  Balancer to distribute traffic to an Auto  Scaling group of EC2 instances that are distributed across two Availability Zones.
�� ��BE

���� �� Correct Answer  B,  E  Detailed  Explanation To  achieve  high availability  (HA) for t he website without code changes:  1. Option  B  (Migrate to  RDS  Multi  - AZ)  -  Why: Th e current  MySQL database on a single  EC2  instance  is a single  point of failure. Amaz on  RDS  Multi  - AZ  automatically  provisions a standby database  in a different Availabil ity Zone (AZ). If the primary database fails,  RDS switches to the standby, ensuring dat abase  HA.  -  No  code changes: The application connects to the same database endpoi nt;  RDS  handles the failover transparently.  2. Option  E  (ALB  +  Auto  Scaling) - Why: A	single  EC2  instance is unreliable. An Auto Scaling group spreads  EC2  instances acros s two AZs. If one AZ fails, instances in the other AZ  keep  running. The Application  L
oad  Balancer (ALB) distributes traffic evenly, ensuring the stateless app  remains availa    ble. -  No code changes: The ALB  handles traffic  routing automatically; the app doesn�� t  need modifications. Why other options are wrong: - A: Internet Gateways are VPC -   wide, not per - AZ. Irrelevant to  HA.  - C:  Migrating to  Dynamo db  (NoSQL) would  re   quire code changes to adapt from SQL.  -  D: AWS  Data sync  is for  bulk  data transfers,   not real - time database  replication.  Reference  Links  -  [Amazon  RDS  Multi  -  AZ](http   s://aws.amazon.com/rds/details/multi - az/) -  [Application  Load  Balancer](https://aws.a
mazon.com/elastic load balancing/application -  load -  balancer/) -  [Auto  Scaling Groups] (https://docs.aws.amazon.com/auto scaling/ec2/user guide/auto - scaling - groups.html)


86.A company runs a web application on Amazon  EC2  instances  in an Auto Scaling g roup. The application uses a database that  runs on an Amazon  RDS for  Postgresql
DB instance. The application performs slowly when traffic increases. The database exp e riences a  heavy  read  load during  periods of  high traffic. Which actions should a solu t ions architect take to  resolve these  performance  issues?  (Choose two.)

A��Turn on auto scaling for the  DB  instance.

B��Create a  read  replica for the  DB  instance. Configure the application to  send  read t raffic to the  read  replica.

C��Convert the  DB  instance to a  Multi-AZ  DB  instance  deployment. Configure the app lication to send  read traffic to the standby  DB  instance.
D��Create an Amazon  Elastic ache cluster. Configure the application to cache query  res ults in the  Elastic ache cluster.
E��Configure the Auto Scaling group subnets to ensure that the  EC2  instances are  pr o visioned in the same Availability Zone as the  DB  instance.
�� ��BD



���� �� Correct Answer:  BD  Detailed  Explanation: When a database experiences  heavy  r ead  load during traffic spikes, two effective solutions are:  1.  Read  Replica  (B):  Creatin g a  read  replica offloads  read traffic from the  primary database. The application can direct read queries to the  replica,  reducing  load on the  main  DB  instance.  2.  Elastic ac he (D): Caching frequently accessed query results  in  Elastic ache  (e.g.,  Redis/Memcache d)  reduces  repeated database calls, improving  response times and  lowering database  l oad. Why other options are incorrect:  - A:  RDS  does  not support auto  scaling for a si ngle  DB  instance. Scaling  requires  read  replicas  or  manual  instance  resizing.  - C:  Multi -AZ  RDS  deployments are for  high availability  (HA),  not  performance. The standby  rep lica in  Multi-AZ  is  not  accessible for  read traffic. -  E:  Forcing  EC2  instances  into  the  s ame AZ as the  DB  instance  sacrifices  HA and  provides  minimal  latency  benefits comp ared to  B/D.  Reference  Links:  -  [RDS  Read  Replicas](https://docs.aws.amazon.com/Ama

zonRDS/latest/User guide/USER_Read repl.html) -  [Elastic ache  Use Cases](https://aws.am azon.com/elastic ache/use-cases/)


87.A company is developing an application that will run on a  production Amazon  Elas tic  Ku bernet es  Service  (Amazon  EKS)  cluster. The  EKS  cluster  has  managed  node grou ps that are  provisioned with On-Demand Instances. The company  needs a dedicated  EKS cluster for development work. The company will use the development cluster infr e quently to test the resiliency of the application. The  EKS cluster  must  manage all the nodes. Which solution will meet these requirements  MOST cost-effectively?
A��Create a  managed  node group that contains only Spot Instances.

B��Create two managed  node groups.  Provision one  node group with On-Demand Ins tances.  Provision the second  node group with Spot Instances.

C��Create an Auto Scaling group that  has a  launch configuration that uses Spot Insta nces. Configure the  user data to add the  nodes to the  EKS cluster.
D��Create a  managed  node group that contains only On-Demand Instances.

�� ��AB



���� �� Correct Answer: A  Explanation:  For a development  EKS cluster that is used  inf re quently and  needs to cost-effectively test application  resiliency, using Spot Instances  i n a  managed  node group  (Option A)  is the  best choice.  Here��s why:  -  Spot Instance s are up to 90% cheaper than On-Demand Instances,  making them  ideal for  non-criti cal, intermittent workloads like development/testing. - While Spot Instances can  be  int er rupted, this aligns with testing the application��s  resiliency  (e.g.,  handling  node failur

es). - The  requirement for  EKS to  manage all  nodes  rules  out Option C  (manual Auto Scaling groups). - Option  B (mixing On-Demand and Spot) adds  unnecessary cost fo r a dev environment, as On-Demand Instances are  more expensive and  rarely  needed here. - Option  D  (only On-Demand) is the  least cost-effective. Why  not  B?  Using tw o node groups (On-Demand  +  Spot)  increases costs  because On-Demand Instances ar e  billed continuously, even  if the cluster  is idle.  Since the dev cluster  is  rarely  used,  p ure Spot (Option A)  minimizes costs while fulfilling  resiliency testing.  Reference:  [Ama zon  EKS  Managed  Node Groups](https://docs.aws.amazon.com/eks/latest/user guide/ma naged-node-groups.html)  [Spot Instances for  EKS](https://aws.amazon.com/ec2/spot/)


88.A company wants to  back  up its on-premises virtual  machines  (VMs) to AWS. The company's  backup solution exports on-premises backups to an Amazon S3 bucket as objects. The S3  backups  must  be  retained for  30 days and  must  be automatically del eted after 30 days. Which combination of steps will  meet these  requirements? (Choos e three.)

A��Create an S3  bucket that  has S3 Object  Lock  enabled.

B��Create an S3  bucket that  has object versioning enabled.

C��Configure a default  retention  period of 30 days for the objects.

D��Configure an S3  Lifecycle  policy to  protect the  objects for 30 days.

E��Configure an S3  Lifecycle  policy to expire the objects after  30 days.

F��Configure the backup solution to tag the objects with a 30-day  retention  period

�� ��ACE

���� �� Correct Answer: A, C,  E  Explanation: To  meet the  requirements of retaining  bac kups for 30 days and automatically deleting them afterward:  1. A  (Enable S3 Object  L ock): S3 Object  Lock enforces  retention  periods,  preventing accidental  or intentional d eletion of objects during the retention  period. This ensures  backups stay  intact for 30 days. 2. C (Default 30-day retention): Setting a default retention period applies to all objects in the bucket. This guarantees backups are automatically protected for exactl y 30 days without  manual configuration. 3.  E  (Lifecycle  policy to expire after  30 days): The lifecycle expiration action deletes objects automatically after 30 days. This works after the Object  Lock  retention  period ends, ensuring timely deletion. Why not other options? -  B  (Versioning):  Not  required  here since  backups are single objects,  not ve rsions. -  D  (Lifecycle  protect):  Lifecycle  policies  don��t  protect  objects��retention  is  ha ndled by Object  Lock.  -  F  (Tag-based  retention):  Unnecessary  because  the default  rete ntion (C) applies to all objects, overriding tags.  Reference  Links:  -  [S3  Object  Lock](htt ps://docs.aws.amazon.com/AmazonS3/latest/user guide/object-lock.html) -  [S3  Lifecycle  Policies](https://docs.aws.amazon.com/AmazonS3/latest/user guide/object-lifecycle-mgmt. html)

89.A company is  migrating its  multi-tier on-premises application to AWS. The applicat ion consists of a single-node  MySQL database and a  multi-node web tier. The compa ny must minimize changes to the application during the migration. The company wan ts to improve application resiliency after the migration. Which combination of steps w ill meet these requirements?  (Choose two.)

A�� Migrate the web tier to Amazon  EC2  instances  in an Auto Scaling group behind a n Application  Load  Balancer.

B��Migrate the database to Amazon  EC2  instances  in an Auto  Scaling group  behind a Network  Load  Balancer.
C�� Migrate the database to an Amazon  RDS  Multi-AZ  deployment.

D�� Migrate the web tier to an AWS  Lambda function.

E�� Migrate the database to an Amazon  Dynamo db table.

�� ��AC



���� �� Correct Answer: A.  Migrate the web tier to Amazon  EC2 instances in an Auto S caling group behind an Application  Load  Balancer. C.  Migrate the  database to an Am azon  RDS  Multi-AZ  deployment.  Detailed  Explanation:  -  Option A  improves the web ti er��s  resiliency by distributing traffic across  multiple  EC2  instances  using an Applicatio n  Load  Balancer  (ALB). Auto  Scaling ensures the web tier automatically adjusts capacit y  based on demand and  replaces  unhealthy instances,  requiring  minimal code change s (e.g.,  no  need to  rewrite the entire application).  - Option C  replaces the single-node MySQL database with Amazon  RDS  Multi-AZ, which automatically  maintains a standb y database in a different Availability Zone (AZ). This  provides  high availability and aut omated failover without requiring code changes (only the database connection string needs updating). Why  not other options?  -  B: Auto  Scaling  +  Network  Load  Balancer (NLB) is unsuitable for stateful databases  like  MySQL, as  scaling  horizontally  requires  complex replication and application-level changes. -  D:  Migrating to AWS  Lambda wo uld likely require significant code changes (e.g.,  rewriting the web tier as serverless fu nctions).  -  E:  Dynamo db  is  a  NoSQL database, which would  require  redesigning the  d ata  model and application  logic if the original system  uses  MySQL.  Reference  Links:  -

[Amazon  EC2 Auto  Scaling](https://aws.amazon.com/ec2/auto scaling/) -  [Amazon  RDS Multi-AZ](https://aws.amazon.com/rds/features/multi-az/)


90.A company that  uses AWS  needs a solution to  predict the  resources  needed for  m an ufacturing processes each  month. The solution  must  use historical values that are c ur rently stored in an Amazon S3  bucket. The company has  no  machine  learning  (ML) experience and wants to use a  managed service for the training and predictions. W hi ch combination of steps will meet these  requirements?  (Choose two.)

A�� Deploy an Amazon Sage maker  model. Create a Sage maker endpoint for inference.

B�� Use Amazon Sage maker to train a  model  by  using the  historical data  in the  S3  bu cket.

C��Configure an AWS  Lambda function with a function  URL that  uses Amazon  SageMa ker endpoints to create  predictions  based on the  inputs.
D��Configure an AWS  Lambda function with a function  URL that  uses an Amazon  For ecast  predictor to create a  prediction  based on the  inputs.
E��Train an Amazon  For se cast  predictor  by  using the  historical  data  in the S3  bucket.

�� ��DE



���� �� Correct Answer:  D. Configure an AWS  Lambda function with a function  URL tha t uses an Amazon  Forecast  predictor to  create a  prediction  based on the  inputs.  E. Tr ain an Amazon  Forecast  predictor  by  using the  historical  data  in the  S3  bucket.  Detail ed  Explanation: The company  needs a fully  managed service for forecasting, and they

 have  no  ML  experience.  Here's why  D  and  E  are correct:  1. Amazon  Forecast  (Option	E) is a  managed service specifically designed for time-series forecasting (like predict i ng monthly resource  needs). It automates  model training and  requires  no  ML expert is e. You simply provide  historical data from S3, and  Forecast  handles the  rest.  -  SageM aker  (Option  B)  is  more flexible  but  requires  ML  knowledge  to choose algorithms, tun e  models, etc. 2. AWS  Lambda  +  Forecast  (Option  D)  allows  the  company to trigger  predictions via a simple API  (using  Lambda��s function  URL).  Forecast  manages the  pr e diction logic, so  no  need to deploy or  maintain a Sage maker endpoint  (Option A/C). Why not Sage maker (A/B/C)? Sage maker is a general-purpose  ML service, which  me ans the company would  need  ML expertise to  build, train, and deploy  models. Amazo n  Forecast abstracts these complexities,  making  it  ideal for  non-experts.  Reference  Lin ks:  -  [Amazon  Forecast](https://aws.amazon.com/forecast/)  -  [AWS  Lambda](https://aws. amazon.com/lambda/)

91.A company regularly uploads GB-sized files to Amazon S3. After the company uplo ads the files, the company uses a fleet of Amazon  EC2 Spot Instances to transcode t he file format. The company needs to scale throughput when the company uploads d ata from the on-premises data center to Amazon S3 and when the company downloa ds data from Amazon S3 to the  EC2  instances. Which solutions will  meet these  requir ements? (Choose two.)

A�� Use the S3  bucket access  point  instead  of accessing the S3  bucket directly.

B�� Upload the files into  multiple S3  buckets.

C�� Use S3  multipart  uploads.

D�� Fetch  multiple  byte-ranges of an object  in  parallel.

E��Add a  random  prefix to each object when  uploading the files.

�� ��CD



���� �� Correct Answer C,  D  Detailed  Explanation  For  a company handling GB-sized file s, C. S3  multipart  uploads splits  large files  into  parts for  parallel  uploads,  boosting  up load speed and resilience.  D.  Fetching  byte-ranges allows  EC2  instances to  download different parts of a file simultaneously,  maximizing download throughput. Why not ot hers? - A  (Access  Points):  Simplifies access  management  but  doesn��t improve throug hput. -  B  (Multiple  buckets):  S3 scales automatically; splitting  buckets adds complexity without solving transfer bottlenecks.  -  E (Random  prefixes):  Helps with small object  p erformance (e.g., avoiding partition hotspots), irrelevant for large file transfers.  Referen ce  Links  -  [S3  Multipart  Uploads](https://docs.aws.amazon.com/AmazonS3/latest/usergu ide/mpu overview.html) -  [Byte-Range  Fetches](https://docs.aws.amazon.com/AmazonS3/ latest/user guide/using-ranges.html)


92.A solutions architect is designing a shared storage solution for a web application t hat is deployed across  multiple Availability Zones. The web application  runs on Amaz on  EC2  instances that are  in an Auto Scaling group. The company  plans to  make freq uent changes to the content. The solution must  have strong consistency in returning  the new content as soon as the changes occur. Which solutions  meet these  require m ents? (Choose two.)

A��Use AWS Storage Gateway Volume Gateway Internet Small Computer Systems Inter face (iSCSI)  block storage that is  mounted to the  individual  EC2  instances.

B��Create an Amazon  Elastic  File  System  (Amazon  EFS)  file  system.  Mount  the  EFS file system on the individual  EC2  instances.
C��Create a shared Amazon  Elastic  Block  Store  (Amazon  EBS) volume.  Mount  the  EBS volume on the individual  EC2  instances.
D��Use AWS  Data sync to  perform  continuous synchronization of data  between  EC2  ho sts  in the Auto Scaling group.
E��Create an Amazon S3  bucket to store the web content. Set the  metadata for the C ache-Control header to  no-cache.  Use Amazon Cloud Front to deliver the content.
�� ��BE



���� ��Correct Answer  B,  E  Detailed  Explanation  -  Option  B  (Amazon  EFS):  Amazon  EFS	is a fully managed  network file system  (NFS) that supports  multiple Availability Zone s (AZs). It  provides strong consistency,  meaning any write operation is immediately vi sible to all connected  EC2  instances. This  makes  it  ideal for  shared storage in an Aut o Scaling group where instances  need immediate access to  updated content.  EFS aut o matically scales storage capacity and  handles frequent content changes seamlessly. - Option  E (Amazon S3  +  Cloud Front): Amazon S3 offers strong consistency for read-a fter-write operations.  By setting the `Cache-Control:  no-cache`  header, you  instruct  br owsers and Cloud Front (a content delivery  network) to  bypass caching and fetch the  l atest content directly from S3 on every  request. This ensures users always see the  m ost recent changes. While S3 isn't a traditional file system, this approach works well f or web content delivery via Cloud Front. Why Other Options  Fail:  - A (Storage Gatewa y):  Designed for  hybrid cloud  (on-premises  + AWS),  not  scalable for  dynamic Auto Sc

aling groups.  - C (Shared  EBS):  EBS volumes  can't  be  mounted to  multiple  EC2  instan ces simultaneously (except in  limited cluster scenarios). -  D  (Data sync):  Meant for  peri odic bulk data transfers,  not  real-time synchronization or strong consistency.  Referenc e  Links  -  [Amazon  EFS  Consistency](https://docs.aws.amazon.com/efs/latest/ug/how-it- works.html#consistency) -  [Amazon S3 Consistency  Model](https://aws.amazon.com/s3/ consistency/) -  [Cloud front Cache-Control](https://docs.aws.amazon.com/Amazon cloud f ront/latest/Developer guide/Expiration.html)


93.A gaming company wants to  launch a  new internet-facing application in  multiple
AWS  Regions. The application will  use the TCP and  UDP  protocols for communication. The company needs to  provide high availability and  minimum  latency for global user s. Which combination of actions should a solutions architect take to  meet these  requi rements?  (Choose two.)
A��Create internal  Network  Load  Balancers  in  front of the application in each  Region.

B��Create external Application  Load  Balancers  in front  of the application in each  Regi

on.

C��Create an AWS Global Accelerator accelerator to  route traffic to the  load  balancers in each  Region.
D��Configure Amazon  Route  53 to  use a geolocation  routing  policy to  distribute the t raffic.
E��Configure Amazon Cloud Front to  handle the traffic and  route  requests to t he appli cation in each  Region

�� ��AC



���� �� Correct Answer: C and  D  Detailed  Explanation:  For a  gaming application  requi ri ng high availability,  low  latency, and support for TCP/UDP  protocols across  multiple A WS  Regions, the  best solutions are:  1. C. AWS  Global Accelerator: - Global Accelera to r uses AWS��s global  network  infrastructure with Anycast IP addresses to  route traffic to the  nearest  healthy endpoint  (e.g.,  Network  Load  Balancers).  -  It  optimizes  latency by directing traffic over AWS��s  private  backbone,  bypassing  public  internet congest io n. - Supports TCP/UDP and  provides automatic failover for  high availability. 2.  D. Am azon  Route  53 Geolocation  Routing:  -  Route  53��s  geolocation  routing  policy directs users to the closest AWS  Region  based on their geographic  location.  - This  minimizes		latency by  reducing the distance  between  users and the application. - Works alongsi de Global Accelerator to enhance  redundancy and user-specific  routing. Why not othe r options: - A (Internal  NLB): Internal  NLBs  are  designed for  private VPC traffic,  not  in ternet-facing applications.  -  B  (ALB): ALBs  operate at  Layer  7 (HTTP/HTTPS) and don��t	support TCP/UDP. -  E  (Cloud front): Cloud front is a CDN for  HTTP/HTTPS  content,  n ot TCP/UDP  protocols.  References:  -  [AWS  Global Accelerator](https://aws.amazon.com/ global-accelerator/) -  [Amazon  Route  53  Routing  Policies](https://docs.aws.amazon.com /Route53/latest/Developer guide/routing-policy.html)


94.A company runs a three-tier application in a VPC. The database tier uses an Amaz on  RDS for  MySQL  DB  instance. The  company  plans to  migrate  t he  RDS for  MySQL
DB instance to an Amazon Aurora  Postgresql  DB  cluster. The company  needs a  sol ut

ion that  replicates the data changes that  happen during the  migration to the  new da tabase. Which combination of steps will meet these  requirements? (Choose two.)
A��Use AWS  Database  Migration  Service  (AWS  DMS)  Schema  Conversion to transform the database objects.

B��Use AWS  Database  Migration  Service  (AWS  DMS)  Schema  Conversion to create an Aurora  Postgresql  read  replica  on t he  RDS for  MySQL  DB  instance.

C��Configure an Aurora  MySQL  read  replica for t he  RDS  for  MySQL  DB  instance.

D��Define an AWS  Database  Migration  Service  (AWS  DMS) task  with change data capt ure (CDC) to  migrate the data.
E�� Promote the Aurora  Postgresql  read  replica to  a standalone Aurora  Postgresql  D B cluster when the  replica  lag  is zero.
�� ��AD



���� ��Correct Answer A,  D  Detailed  Explanation To  migrate an  RDS  MySQL  database  t o Aurora  Postgresql while  replicating ongoing data  changes:  1. A:  Use AWS  DMS  Sc hema Conversion - Converts the  MySQL schema  (tables,  indexes, etc.) into  Postgresql -compatible format. This is  necessary  because  MySQL and  Postgresql  have  different s yntax and features. 2.  D:  Define  a  DMS task with  CDC  - AWS  DMS  handles  both  init i al data  migration and ongoing changes. CDC (Change  Data Capture) ensures that any writes to the  MySQL database during migration are replicated to Aurora  Postgresql,	keeping both databases in sync. Why not other options? -  B:  Schema Conversion do esn��t create  read  replicas;  it  only converts schemas.  - C: Aurora  MySQL  replicas  only

work with  MySQL,  not  Postgresql.  -  E:  You don��t ��promote�� a  DMS  replica;  replic ation is  managed  by the  DMS task.  Reference  Links  -  [AWS  DMS  Schema  Conversion] (https://docs.aws.amazon.com/dms/latest/user guide/CHAP_Schema conversion.html) -  [A WS  DMS CDC Tasks](https://docs.aws.amazon.com/dms/latest/user guide/CHAP_Task.CD C.html)

95.A company has an organization in AWS Organizations. The company runs Amazon	EC2 instances across four AWS accounts in the  root organizational  unit (OU). There are three nonproduction accounts and one production account. The company wants t o  prohibit users from  launching  EC2  instances of a certain size  in the  nonproduction accounts. The company  has created a service control  policy  (SCP) to deny access to  l aunch instances that use the prohibited types. Which solutions to deploy the SCP will meet these requirements? (Choose two.)
A��Attach the SCP to t he  root OU for the organization.

B��Attach the SCP to the three nonproduction Organizations  member accounts. C��Attach the SCP to the Organizations management account.

D��Create an OU for the production account. Attach the SCP to the OU.  Move the  pr oduction member account into the new OU.
E��Create an OU for the  required accounts. Attach the SCP to the OU.  Move the  non production member accounts into the  new OU.
�� ��BE

���� �� Correct Answer  B,  E  Detailed  Explanation To  restrict  EC2  instance  types  in  nonp roduction accounts without affecting the  production account: - Option  B: Attaching th e SCP directly to the three  nonproduction accounts ensures the  policy applies only to	those accounts. This is a straightforward way to enforce  restrictions without impact in g other accounts. - Option  E: Creating a dedicated OU for  nonproduction accounts a nd attaching the SCP to that OU allows centralized  management.  Moving the  nonpro duction accounts into this OU ensures the SCP applies to all of them, while the prod uction account (in the root OU)  remains  unaffected. The other options are  incorrect:  - A/C: Attaching to the root OU (A) or management account (C) would impact all acc ounts, including the production account. -  D: Creating an OU for the  production acco unt and applying the SCP there would incorrectly  restrict the  production account  inst ead of the  nonproduction ones.  Reference  Links  -  [AWS  Service  Control  Policies](https: //docs.aws.amazon.com/organizations/latest/user guide/orgs_manage_policies_scps.html)
-  [Organizing Accounts  in OUs](https://docs.aws.amazon.com/organizations/latest/userg uide/orgs_manage_ous.html)


96.A company has an application that delivers on-demand training videos to students around the world. The application also allows authorized content developers to uplo ad videos. The data  is stored  in an Amazon S3  bucket in the us-east-2  Region. The  c ompany has created an S3  bucket  in the eu-west-2  Region and  an S3  bucket  in the ap-southeast-1  Region. The company wants to  replicate the  data to the  new S3  buck ets. The company  needs to  minimize  latency for developers who  upload videos and s tudents who stream videos near eu-west-2 and ap-southeast-1. Which combination of

 steps will meet these  requirements with the  FEWEST changes to the application?  (Ch oose two.)
A��Configure one-way  replication from the  us-east-2 S3  bucket to the eu-west-2 S3  b ucket. Configure one-way  replication from the us-east-2  S3 bucket to t he ap-south eas t-1 S3  bucket.

B��Configure one-way  replication from the  us-east-2  S3  bucket to the eu-west-2 S3  b ucket. Configure one-way  replication from the eu-west-2 S3  bucket to t he ap-southea st-1 S3  bucket.

C��Configure two-way (bidirectional)  replication among the S3  buckets that are  in all t hree  Regions.
D��Create an S3  Multi-Region Access  Point.  Modify  the application to  use the Amazon Resource  Name  (ARN) of the  Multi-Region Access  Point  for video streaming.  Do  not modify the application for video uploads.
E��Create an S3  Multi-Region Access  Point.  Modify the  application to  use the Amazon Resource  Name (ARN) of the  Multi-Region Access  Point for video  streaming and upl oads.
�� ��CE



���� ��Correct Answer C,  E  Detailed  Explanation To  minimize  latency for  users  (develop ers uploading videos and students streaming videos) near the eu-west-2 and ap-sout heast-1  Regions, while ensuring data  is  replicated across  all three S3  buckets  (us-east -2, eu-west-2, ap-southeast-1) with minimal application changes:  1. Option C: Config ur

e  bidirectional  replication among all three S3  buckets  - Why?  -  Bidirectional  replica tio n ensures that any object uploaded to any  bucket (us-east-2, eu-west-2, or ap-southe ast-1) is automatically  replicated to the other two  buckets.  - This  is critical  because d eve lopers  might upload videos to the  nearest  bucket  (e.g., a developer in  Europe  upl oads to eu-west-2), and  bidirectional  replication ensures these new objects are copied	to the other  Regions. Without this, objects  uploaded to eu-west-2 or ap-southeast-1 would  not replicate back to us-east-2 or other  Regions if using one-way  replication  (as  in Options A or  B),  leading to  data  inconsistency. 2. Option  E:  Use  an  S3  Multi-Re gion Access  Point  (MRAP) for  both  uploads  and streaming  - Why?  - An  S3  MRAP  pr ovides a single global endpoint that automatically routes  requests to the nearest S3
bucket  based on the  user��s  location.  -  By  modifying the  application to use the  MRA P��s ARN for  both uploads and streaming, all users  (developers and students) will exp erience  minimal  latency.  For  example: - A developer in  Europe  uploads via the  MRAP, which routes the request to the eu-west-2  bucket. - A student in Asia streams via th e  MRAP, which  routes the  request to t he ap-southeast-1  bucket.  - If the  application  i sn��t  updated to  use the  MRAP for  uploads  (e.g.,  Option  D),  developers would still up load directly to the original us-east-2  bucket,  resulting  in  higher  latency for  non-US  u sers. Why  Not Other Options?  - Options A/B (one-way  replication): These only  replica te objects from the source bucket  (us-east-2) to the destination buckets. Objects uplo aded to eu-west-2 or ap-southeast-1 would  not replicate  back to  us-east-2 or other
Regions, causing data silos. - Option  D  (MRAP for streaming only):  Uploads would stil l go to the original us-east-2  bucket,  increasing  latency for developers outside the  US. Reference  Links -  [Amazon S3  Replication](https://docs.aws.amazon.com/AmazonS3/lat

est/user guide/replication.html) -  [Amazon  S3  Multi-Region Access  Points](https://docs.a ws.amazon.com/AmazonS3/latest/user guide/Multiregion access points.html)


97.A company is deploying an application that  processes streaming data  in  near-real time. The company  plans to  use Amazon  EC2  instances for the workload. The  networ k architecture  must  be configurable to  provide the  lowest  possible  latency  between  n odes. Which combination of network solutions will meet these requirements? (Choose	two.)

A�� Enable and configure enhanced  networking on each  EC2  instance.

B��Group the  EC2  instances  in separate accounts.

C�� Run the  EC2  instances  in a cluster  placement group.

D��Attach  multiple elastic  network  interfaces to each  EC2  instance.

E�� Use Amazon  Elastic  Block  Store  (Amazon  EBS)  optimized  instance types.

�� ��AC



���� ��Correct Answer: A.  Enable and configure enhanced networking on each  EC2 inst ance. C.  Run the  EC2  instances  in  a cluster  placement group.  Detailed  Explanation:  - Option A:  Enhanced  networking  uses advanced  hardware/software  (like  Elastic  Networ k Adapters) to reduce  latency, increase  bandwidth, and  improve  packet  processing sp eed. This directly optimizes communication  between  EC2  instances, which is critical fo r  real  - time streaming.  - Option C: Cluster  placement groups  physically  place  EC2  ins tances in the same Availability Zone and close to each other on AWS hardware. This

minimizes  physical distance  between  nodes, ensuring  ultra  -  low  network  latency��per fect for tightly coupled workloads  like  real  - time data  processing. Why  not the other s? -  B:  Separate accounts add administrative complexity  but don��t  reduce  latency.  - D:  Multiple  network  interfaces  (ENIs)  increase  bandwidth  but don��t  directly  reduce  lat ency. -  E:  EBS  optimization  improves storage  performance,  not  inter -  instance  networ k latency.  Reference  Links:  -  [Enhanced  Networking](https://docs.aws.amazon.com/AWS EC2/latest/User guide/enhanced - networking.html) -  [Cluster  Placement  Groups](https:/ /docs.aws.amazon.com/AWSEC2/latest/User guide/placement - groups.html)


98.A company has a  multi-tier  payment  processing application that  is  based on virtua
l machines (VMs). The communication between the tiers occurs asynchronously throug h a third-party  middleware solution that guarantees exactly-once delivery. The compa ny needs a solution that  requires the  least amount of infrastructure  management. The	solution must guarantee exactly-once delivery for application messaging. Which com bination of actions will meet these  requirements? (Choose two.)

A�� Use AWS  Lambda for the compute  layers  in the architecture.

B�� Use Amazon  EC2  instances for the compute  layers  in the architecture.

C�� Use Amazon Simple  Notification Service (Amazon  SNS) as the  messaging compone nt  between the compute  layers.
D��Use Amazon Simple Queue Service (Amazon  SQS)  FI FO queues as the  messaging c omponent  between the compute  layers.
E��Use containers that are  based on Amazon  Elastic  Ku bernet es  Service  (Amazon  EKS) for the compute  layers in the architecture.

�� ��AD



���� ��Correct Answer: AD  Explanation: The question asks for a solution that  minimizes	infrastructure management and ensures exactly-once message delivery. - A  (AWS  La mbda):  Lambda  is a serverless compute service,  meaning AWS fully  manages the  infra structure. This  reduces operational overhead compared to  managing  EC2  instances  (B) or  EKS clusters (E), which  require provisioning, scaling, and  maintenance.  Lambda aut o matically scales with workload and charges only for execution time. -  D  (Amazon SQ S  FIFO queues): SQS  FIFO  queues guarantee exactly-once  processing and first-in-first- out message delivery. They prevent duplicate  messages using de duplication IDs and e nsure ordered processing within message groups. This  matches the  requirement to  re place the third-party  middleware with a  managed AWS service that  maintains exactly- once guarantees. Combining  Lambda  (serverless compute) with SQS  FIFO  (managed  m ess aging) achieves minimal infrastructure  management while ensuring exactly-once del ivery. Options  B and  E  involve  managing  servers/containers, and  SNS  (C) does  not  nat ively support exactly-once delivery.  Reference  Links:  -  [AWS  Lambda](https://aws.amaz on.com/lambda/) -  [Amazon  SQS  FIFO Queues](https://aws.amazon.com/sqs/features/fif o/)

99.A company built an application with  Docker containers and  needs to  run t he appli cation in the AWS Cloud. The company wants to use a  managed service to  host the application. The solution must scale in and out appropriately according to demand on	the individual container services. The solution also  must  not  result in additional oper

ational overhead or infrastructure to  manage. Which solutions will  meet these require ments? (Choose two.)
A�� Use Amazon  Elastic Container  Service (Amazon  ECS) with AWS  Fargate.

B�� Use Amazon  Elastic  Ku bernet es  Service  (Amazon  EKS) with AWS  Fargate.

C��Provision an Amazon API Gateway API. Connect the API to AWS  Lambda to  run th e containers.
D�� Use Amazon  Elastic Container Service  (Amazon  ECS) with Amazon  EC2 worker  nod

es.

E��Use Amazon  Elastic  Ku bernet es  Service  (Amazon  EKS) with Amazon  EC2  worker  nod

es.

�� ��AB



���� �� Correct Answers A,  B  Detailed  Explanation The  requirements are to  run  Docker containers on AWS using a  managed service with auto-scaling and  minimal operation al overhead.  Here's why the correct answers are A and  B:  - A  (ECS with  Fargate)  and B (EKS with  Fargate):  Both Amazon  ECS  and Amazon  EKS are container orchestration	services. When paired with AWS  Fargate  (a serverless  compute engine), AWS fully  m anages the underlying servers, scaling, and infrastructure. You only define your contai ners and their  resource  needs.  Fargate automatically scales the containers  based on d emand without  requiring you to  manage  EC2  instances. This eliminates operational ta sks  like  patching  servers or scaling worker  nodes.  - Why other options are  incorrect:
- C  (API Gateway  +  Lambda):  Lambda  runs  code  (not  long-running  containers)  and  is

 event-driven. While  Lambda supports container  images,  it��s  not designed for  hosting	scalable containerized applications  like  ECS/EKS.  -  D  and  E  (ECS/EKS with  EC2):  Using		EC2 worker  nodes  requires you to  manage the  EC2  instances  (e.g.,  scaling, security  u pdates), which adds operational overhead.  Reference  Links  -  [AWS  Fargate](https://aws. amazon.com/fargate/) -  [Amazon  ECS](https://aws.amazon.com/ecs/)  -  [Amazon  EKS](ht tps://aws.amazon.com/eks/) Since you want it to  remove  markdown (the  bold format which  here is  represented  by **),  here  is the final  result:  Correct Answers A,  B  Detaile d  Explanation The  requirements  are to  run  Docker containers on AWS  using a  manag ed service with auto - scaling and  minimal operational overhead.  Here's why the corr ect answers are A and  B:  - A  (ECS with  Fargate)  and  B  (EKS with  Fargate):  Both  Amaz on  ECS and Amazon  EKS are container  orchestration services. When paired with AWS		Fargate (a serverless compute engine), AWS fully  manages the underlying servers, sc aling, and infrastructure. You only define your containers and their  resource needs.  Fa rgate automatically scales the containers based on demand without requiring you to manage  EC2  instances. This eliminates operational tasks  like  patching servers or scalin g worker  nodes.  - Why other options are  incorrect:  - C  (API  Gateway  +  Lambda):  La mbda  runs code  (not  long  -  running containers) and  is event - driven. While  Lambda	supports container images, it��s  not designed for  hosting scalable containerized appli cations like  ECS/EKS.  -  D  and  E  (ECS/EKS with  EC2):  Using  EC2  worker  nodes  requires you to  manage the  EC2 instances (e.g., scaling, security updates), which adds operati onal overhead.  Reference  Links  -  [AWS  Fargate](https://aws.amazon.com/fargate/)  -  [A mazon  ECS](https://aws.amazon.com/ecs/) -  [Amazon  EKS](https://aws.amazon.com/eks  /)

100.A company uses AWS Organizations for its  multi-account AWS setup. The security organizational unit (OU) of the company needs to share approved Amazon  Machine Images (AMIs) with the development OU. The AMIs are created  by  using AWS  Key  M anagement Service (AWS  KMS) encrypted snapshots. Which solution will  meet these  r equirements? (Choose two.)

A��Add the development team's OU Amazon  Resource  Name  (ARN) to the  launch  per mission list for the AMIs.

B��Add the Organizations  root Amazon  Resource  Name  (ARN) to  the  launch  per missio n  list for the AMIs.

C�� Update the  key  policy to allow the development team's OU to  use the AWS  KMS keys that are used to decrypt the snapshots.
D��Add the development team��s account Amazon  Resource  Name  (ARN) to the  laun ch  permission  list for the AMIs.
E�� Recreate the AWS  KMS  key. Add  a  key  policy to allow the Organizations  root Ama zon  Resource  Name  (ARN) to  use the AWS  KMS  key.
�� ��AC



���� �� Correct Answer: A, C  Detailed  Explanation: A. Add the development team's OU Amazon  Resource  Name  (ARN) to the  launch  permission  list for the AMIs. To  share A MIs across accounts in AWS Organizations, you can grant  launch  permissions to an e ntire Organizational  Unit  (OU). This allows all accounts within the OU to use the AMI.	Instead of manually adding individual account IDs  (as  in Option  D),  using the  OU A

RN simplifies  management and automatically includes future accounts added to the O U. C.  Update the  key  policy to allow the development team's  OU to  use the AWS  K MS keys that are used to decrypt the snapshots. When an AMI is encrypted with a  K MS key, the target accounts  must  have  permissions to  use the  KMS  key for  decrypt io n.  By  updating the  KMS  key  policy to  allow the entire development OU, all accounts  in that OU gain access to decrypt the snapshots. This avoids  recreating the  key  (Opti on  E) and ensures compliance with the  principle of least  privilege compared to granti ng access to the entire organization  (Option  B).  --- Why  not other options?  -  B/D: A dding the root ARN  (B) or individual account ARNs  (D)  is  less scalable.  Root ARN  gra nts access to all accounts in the organization, which is overly  permissive. Adding indi vidual accounts (D)  requires  manual  updates for each  new account.  -  E:  Recreating  th e  KMS  key  is  unnecessary  if the  existing  key��s  policy  can  be  updated.  Root ARN  acc ess (E)  is also overly  broad.  References:  -  [Sharing AMIs  across accounts  using AWS
Organizations](https://docs.aws.amazon.com/AWSEC2/latest/User guide/share-amis.html)
-  [Allowing access to  KMS  keys for cross-account AMIs](https://docs.aws.amazon.com/k ms/latest/developer guide/key-policy-modifying-external-accounts.html)


101.A company's web application that  is  hosted in the AWS Cloud recently increased in popularity. The web application currently exists on a single Amazon  EC2  instance  i n a single public subnet. The web application  has  not  been able to  meet the deman d of the increased web traffic. The company needs a solution that will provide  high a vail ability and scalability to  meet the increased user demand without  rewriting the we b application. Which combination of steps will meet these  requirements? (Choose tw o.)

A�� Replace the  EC2  instance with a  larger compute optimized instance.

B��Configure Amazon  EC2 Auto Scaling with  multiple Availability Zones  in  private  sub
nets.

C��Configure a  NAT  gateway  in a  public subnet to  handle web  requests.

D�� Replace the  EC2  instance with a  larger  memory  optimized instance.

E��Configure an Application  Load  Balancer  in a  public subnet to distribute web traffic.

�� ��BE



���� ��Correct Answer  B. Configure Amazon  EC2 Auto Scaling with  multiple Availability Zones in  private sub nets.  E.  Configure an Application  Load  Balancer  in  a  public sub ne t to distribute web traffic.  Detailed  Explanation To achieve  high availability and  scala bi lity without  rewriting the web application:  1.  EC2 Auto  Scaling  (Option  B)  - Auto  Scali ng automatically launches or terminates  EC2 instances  based on traffic demands, ensu ring the application scales  up during high traffic and saves costs when demand drops.
 -  Multiple Availability Zones  (AZs) ensure  redundancy. If one AZ fails, the application	remains available in other AZs.  -  Private  sub nets enhance  security by isolating  backe nd instances from direct internet access. 2. Application  Load  Balancer  (Option  E)  - Th e ALB distributes incoming web traffic across  multiple  EC2  instances  (even across AZs). This prevents overloading a single instance and ensures smooth traffic handling. - It  sits in a  public subnet to accept  internet traffic and  routes  requests to  instances  in  p rivate sub nets. Why  Not Other Options?  - A/D (Larger Instances) Scaling vertically (up grading instance size) only temporarily solves the problem. It doesn��t  provide high a vail ability or true scalability. - C (NAT Gateway) A  NAT gateway allows  instances in  pr

ivate subnets to connect to the internet (e.g., for updates),  but  it doesn��t  handle inc oming web traffic.  Reference  Links  -  [EC2 Auto  Scaling](https://aws.amazon.com/ec2/a u to scaling/) -  [Application  Load  Balancer](https://aws.amazon.com/elastic load balancing/ application-load-balancer/)


102.A company is designing a tightly coupled  high  performance computing (HPC) env i ronment in the AWS Cloud. The company needs to include features that will optimiz e the  HPC environment for  networking and storage. Which combination of solutions will meet these  requirements? (Choose two.)

A��Create an accelerator  in AWS Global Accelerator. Configure custom  routing for the accelerator.

B��Create an Amazon  FSx for  Lustre file  system. Configure the file system with  scratch storage.

C��Create an Amazon Cloud Front distribution. Configure the viewer protocol policy to be  HTTP  and  HTTPS.
D�� Launch Amazon  EC2  instances. Attach  an  Elastic  Fabric Adapter  (EFA)  to the  instan

ces.

E��Create an AWS  Elastic  Beanstalk  deployment to  manage the environment.

�� ��BD



���� ��Correct Answer  B. Create an Amazon  FSx for  Lustre file system. Configure the fi le system with scratch storage.  D.  Launch Amazon  EC2  instances. Attach  an  Elastic  Fa

bric Adapter (EFA) to the instances.  Detailed  Explanation  For  HPC  workloads, opti mizi ng storage and  networking is critical:  1. Option  B  (Amazon  FSx  for  Lustre)  -  FSx  for  L ustre is a  high-performance file system designed for  HPC.  -  Scratch storage  is a temp orary, ultra-fast storage tier optimized for short-lived data  (common in  HPC s imul atio ns). - It  provides  low-latency,  parallel access to  data across  multiple compute  nodes.
2. Option  D  (EFA)  -  Elastic  Fabric  Adapter  (EFA)  is  a  network  interface for  EC2  instanc es. - It enables  low-latency,  high-bandwidth communication  between  HPC  instances  (c rucial for tightly coupled tasks  like  MPI applications).  -  Unlike  standard  networking,  EF A bypasses the hypervisor for faster  performance. Why other options are wrong: - A  (Global Accelerator): Optimizes traffic for global users,  not internal  HPC cluster comm unication. - C (Cloud Front): A CDN for caching content globally��irrelevant for  HPC��s internal data transfers.  -  E  (Elastic  Beanstalk):  A  PaaS for web apps,  not  low-level  HPC	infrastructure control.  Reference  Links  -  [Amazon  FSx  for  Lustre](https://aws.amazon.c om/fsx/lustre/) -  [Elastic  Fabric Adapter  (EFA)](https://aws.amazon.com/hpc/efa/)


103.A company runs several Amazon  RDS for Oracle On-Demand  DB  instances that  h ave  high  utilization. T he  RDS  DB  instances  run  in  member  accounts that  are  in an  or ganization in AWS Organizations. The company's finance team  has access to the orga nization's management account and  member accounts. The finance team wants to fin
d ways to optimize costs  by  using AWS Trusted Advisor. Which combination of steps will meet these  requirements? (Choose two.)

A�� Use the Trusted Advisor  recommendations in the  management account.

B��Use the Trusted Advisor  recommendations  in the  member  accounts where t he  RDS DB  instances are  running.

C�� Review the Trusted Advisor checks for Amazon  RDS  Reserved Instance  Optimiza tio

n.

D�� Review the Trusted Advisor checks for Amazon  RDS Idle  DB Instances.

E�� Review the Trusted Advisor checks for compute optimization. Crosscheck the  result s  by using AWS Compute Optimizer.
�� ��AC



���� ��Correct Answer: A, C  Detailed  Explanation: To optimize costs for high-utilization RDS instances across  multiple  member accounts in an organization:  1.  Use Trusted Ad visor in the  management account (A):  - AWS Trusted Advisor��s  Reserved Instance  (RI) Optimization checks are most effective at the management account level when using AWS Organizations. - The management account has visibility into usage across all m ember accounts (via consolidated billing). This allows it to recommend  RI  purchases t hat cover aggregated usage across the organization, maximizing cost savings. -  Mem ber accounts (B) only see their own usage, so  RI  recommendations there would  be fr ag mented and  less optimal. 2.  Review  RDS  Reserved  Instance Optimization checks  (C):
 -  High-utilization On-Demand  RDS  instances are  prime  candidates for  Reserved Insta nces, which offer significant discounts (up to 72%) compared to On-Demand  pricing.
- Trusted Advisor��s  RDS  Reserved Instance  Optimization check identifies this opportu nity by analyzing usage  patterns and suggesting optimal  RI  purchases. Why  not  other
 options:  -  D  (Idle  DB Instances): Irrelevant  here  because  t he  RDS  instances  are descri bed as  having  high  utilization. -  E  (Compute Optimizer): AWS Compute Optimizer foc uses on  EC2  instances,  not  RDS.  -  B  (Member  accounts): While  member accounts can

 run Trusted Advisor,  RI  optimization  requires a  holistic view of usage across all acco unts, which only the  management account provides.  Reference:  [AWS Trusted Advisor Cost Optimization](https://docs.aws.amazon.com/aws support/latest/user/trusted-advisor-
cost-optimization-checks.html)  [AWS  Reserved Instances for  RDS](https://aws.amazon.co m/rds/reserved-instances/)

104.A company's  near-real-time streaming application is  running on AWS. As the data		is ingested, a jo b runs on the data and takes 30  minutes to complete. The workload	frequently experiences  high  latency due to  large amounts of incoming data. A soluti ons architect  needs to design a scalable and serverless solution to enhance  perform a nce. Which combination of steps should the solutions architect take?  (Choose two.)
A�� Use Amazon  Kinesis  Data  Firehose  to  ingest the data.

B�� Use AWS  Lambda with AWS  Step  Functions to  process the  data.

C�� Use AWS  Database  Migration  Service  (AWS  DMS) to  ingest the  data.

D�� Use Amazon  EC2  instances  in an Auto Scaling group to  process the data.

E��Use AWS  Fargate with Amazon  Elastic  Container Service  (Amazon  ECS) to  process t he data.
�� ��AE



���� �� Correct Answer A,  E  Detailed  Explanation A.  Use Amazon  Kinesis  Data  Firehose to ingest the data  Kinesis  Data  Firehose  is  designed for  near  -  real  -  time streaming data  ingestion. It automatically scales to  handle  large volumes of incoming data and

seamlessly integrates with other AWS services (like S3,  Redshift, etc.) for storage or fu rther  processing. Since  it��s fully managed (serverless),  it  removes the  need to  manag e infrastructure, which aligns with the requirement for scalability and serverless archite cture.  E.  Use AWS  Fargate with Amazon  ECS  to  process the  data AWS  Fargate  is a  se rver less compute engine for containers. When paired with Amazon  ECS, it allows you to run containerized applications without managing  EC2  instances. Since the data  pro cessing job takes 30  minutes  (exceeding  Lambda��s  15  -  minute timeout),  Fargate  is  i deal here. It  provides scalability,  handles  long  -  running tasks, and eliminates server
management,  meeting the serverless and performance  requirements. Why  not other o ptions?  -  B.  Lambda  +  Step  Functions:  Lambda  has  a  15  -  minute  execution  limit,  w hi ch cannot  handle the 30  -  minute job.  - C. AWS  DMS:  Designed for  database  migrati on, not  real  - time data streaming.  -  D.  EC2 Auto  Scaling:  Requires  managing  servers, which contradicts the serverless requirement.  Reference  Links  -  [Amazon  Kinesis  Data	Firehose](https://aws.amazon.com/kinesis/data - firehose/) -  [AWS  Fargate](https://aws. amazon.com/fargate/)


105.A company hosts an application on Amazon  EC2  instances that  run  in a single Av a il ability Zone. The application is accessible by using the transport  layer of the Open Systems Interconnection (OSI)  model. The company needs the application architecture	to have  high availability. Which combination of steps will  meet these  requirements
MOST cost-effectively? (Choose two.)

A��Configure  new  EC2  instances  in a different Availability Zone.  Use Amazon  Route  53 to  route traffic to all instances.

B��Configure a  Network  Load  Balancer  in  front of the  EC2  instances.

C��Configure a  Network  Load  Balancer  for TCP traffic to the  instances. Configure an A pplication  Load  Balancer for  HTTP  and  HTTPS traffic to the  instances.
D��Create an Auto Scaling group for the  EC2  instances. Configure the Auto Scaling gr oup to use multiple Availability Zones. Configure the Auto Scaling group to  run appli cation health checks on the instances.
E��Create an Amazon Cloud watch alarm. Configure the alarm to  restart  EC2  instances that transition to a stopped state.
�� ��BD



���� �� Correct Answer  B. Configure a  Network  Load  Balancer  in front  of the  EC2  insta nces.  D. Create an Auto  Scaling group for the  EC2  instances. Configure the Auto Scali ng group to use  multiple Availability Zones. Configure the Auto Scaling group to  run application health checks on the instances.  Detailed  Explanation To achieve  high avai lability cost-effectively:  1. Option  B  (Network  Load  Balancer  -  NLB):  -  The  application uses the OSI transport  layer (Layer 4), which  handles  protocols  like TCP/UDP. A  Netw ork  Load  Balancer  (NLB)  operates at  Layer 4 and  efficiently distributes traffic across  E C2 instances. -  NLB automatically scales and  provides  built-in  redundancy  across  multi ple Availability Zones  (AZs). If one AZ fails,  NLB  reroutes traffic to  healthy  instances  i n other AZs. This ensures continuous availability without manual intervention. 2. Optio n  D  (Auto  Scaling with  Multi-AZ):  - Auto  Scaling automatically  launches  replacement  i n stances if an  EC2  instance fails  (detected via  health checks).  -  Deploying  instances  ac ross  multiple AZs ensures redundancy. If one AZ goes down, Auto Scaling  maintains t he desired number of instances in the  remaining AZs. -  Health checks  remove  unheal

thy instances from service, ensuring only functional instances receive traffic. Why Othe r Options Are  Less Ideal:  - Option A:  Route  53  is  a  DNS  service, which  has  slower fail over than  NLB and  lacks  automatic  health checks for  EC2  instances.  -  Option C:  Using both  NLB and ALB  (Layer 7)  is unnecessary  here since the app uses  Layer 4  (TCP). T his adds complexity and cost. - Option  E:  Restarting  stopped  instances with Cloud wat ch alarms doesn��t address  multi-AZ  redundancy or automatic scaling.  Reference  Links	-  [Network  Load  Balancer](https://aws.amazon.com/elastic load balancing/network-load- balancer/) -  [Auto Scaling Groups](https://docs.aws.amazon.com/auto scaling/ec2/usergu ide/auto-scaling-groups.html)


106.A company runs an ecommerce application on AWS. Amazon  EC2  instances  proce ss  purchases and store the  purchase details in an Amazon Aurora  Postgresql  DB  c lus ter. Customers are experiencing application timeouts during times of peak usage. A s ol utions architect  needs to  re architect the application so that the application can scal e to meet  peak usage demands. Which combination of actions will meet these requir ements  MOST cost-effectively?  (Choose two.)

A��Configure an Auto Scaling group of new  EC2  instances to  retry the  purchases  until the processing is complete.  Update the applications to connect to the  DB  cluster  by using Amazon  RDS  Proxy.

B��Configure the application to use an Amazon  Elastic ache cluster  in front of the Aur ora  Postgresql  DB  cluster.

C��Update the application to send the purchase  requests to an Amazon Simple Queue Service (Amazon SQS) queue. Configure an Auto Scaling group of new  EC2  instances that  read from the SQS queue.
D��Configure an AWS  Lambda function to  retry the ticket  purchases  until the  process i ng is complete.
E��Configure an Amazon AP! Gateway  REST API with a  usage  plan.

�� ��AC



���� �� Correct Answer C, A  Detailed  Explanation The  best solution combines de coupl in g the application and  managing database connections efficiently:  1. Option C:  Using  Amazon SQS to queue  purchase  requests decouples the front end (customer  requests)	from the backend  (order processing). This allows the application to  handle sudden tr affic spikes without timeouts.  EC2  instances with Auto Scaling can dynamically adjust based on the SQS queue length, ensuring cost - effective scaling (you only  pay for w hat you use). 2. Option A:  RDS  Proxy  helps  manage  database  connections.  During  pe ak traffic, too  many direct connections to Aurora  Postgresql can overload the databa se.  RDS  Proxy  pools  and  reuses connections,  reducing strain  on the database and  im proving scalability. The retry in Option A  likely  refers to  processing orders from the  S QS queue until completion,  not  manual  retries  by customers. Why  not other options?
 -  B:  Elastic ache  is for  read  -  heavy workloads  (caching),  not  write  -  heavy  purchase t ransactions.  -  D:  Lambda  isn��t  ideal  for  long  -  running  purchase  processing  (timeout limits,  higher cost for sustained workloads).  -  E: API Gateway  usage  plans  limit  reques

ts, which worsens timeouts during  peaks.  Reference  Links  -  [Amazon  SQS](https://aws. amazon.com/sqs/) -  [Amazon  RDS  Proxy](https://aws.amazon.com/rds/proxy/)


107.A company is  hosting a  high-traffic static website on Amazon S3 with an Amazon Cloud Front distribution that has a default TTL of 0 seconds. The company wants to i m plement caching to improve  performance for the website.  However, the company al so wants to ensure that stale content is  not served for  more than a few  minutes afte r a deployment. Which combination of caching methods should a solutions architect i m plement to  meet these  requirements?  (Choose two.)
A��Set the Cloud Front default TTL to 2 minutes.

B��Set a default TTL of 2  minutes on the S3  bucket.

C��Add a Cache-Control  private directive to the objects  in Amazon S3.

D��Create an AWS  Lambda@Edge function to add an  Expires  header to  HTTP  respons es. Configure the function to  run on viewer  response.
E��Add a Cache-Control  max-age directive of 24  hours to the objects  in Amazon S3. On deployment, create a Cloud Front invalidation to clear any changed files from edg e caches.
�� ��AE



���� ��Correct Answer: A,  E  Detailed  Explanation: To  balance  performance and fresh nes s for a static website on S3/Cloud Front:  1. Option A  (Set Cloud Front  Default TTL to  2 minutes): - The original setup  has a default TTL of 0 seconds, meaning no caching.

Setting this to 2  minutes enables  basic caching for objects without Cache-Control  hea ders, improving  performance while  keeping stale content  limited to 2  minutes. This ac ts as a safety  net for  uncached objects. 2. Option  E  (Cache-Control  max-age =24h  +  I n validations):  - Adding `max-age =86400` (24  hours) allows  long-term caching for  most objects, optimizing performance.  During deployments, Cloud Front Invalidations instan t ly  purge updated files from edge caches. This ensures  modified content is  refreshed  immediately, while unmodified files remain cached. The 24-hour  max-age only applies	to unmodified content, which doesn't impact freshness since invalidations target cha nged files. Why other options are incorrect:  -  B:  S3 doesn't control Cloud Front TTL; t his setting is irrelevant. - C: `Cache-Control: private`  prevents Cloud Front from caching, hurting performance. -  D:  Lambda@Edge adds complexity.  Using Cache-Control  head ers  +  invalidations  is  simpler and  more  cost-effective.  Reference  Links:  -  [Cloud Front  T TL Settings](https://docs.aws.amazon.com/Amazon cloud front/latest/Developer guide/Exp iration.html) -  [Cache-Control  Best  Practices](https://aws.amazon.com/blogs/networking- and-content-delivery/optimizing-http-caching-with-amazon-cloud front/) -  [Invalidations Guide](https://docs.aws.amazon.com/Amazon cloud front/latest/Developer guide/Invalid ati on.html)


108.A solutions architect is designing a three-tier web application. The architecture co nsists of an internet-facing Application  Load  Balancer  (ALB) and a web tier that  is  hos ted on Amazon  EC2  instances  in  private sub nets. The application tier with the busines s  logic  runs on  EC2  instances  in  private  sub nets. The database tier consists of  Micros oft SQL Server that  runs on  EC2  instances  in  private  sub nets.  Security  is a  high  priorit

y for the company. Which combination of security group configurations should the so lutions architect  use? (Choose three.)
A��Configure the security group for the web tier to allow inbound  HTTPS traffic from the security group for the ALB.

B��Configure the security group for the web tier to allow outbound  HTTPS traffic to 0.
0.0.0/0.

C��Configure the security group for the database tier to allow inbound  Microsoft  SQL Server traffic from the security group for the application tier.
D��Configure the security group for the database tier to allow outbound  HTTPS traffic and  Microsoft SQL Server traffic to the security group for the web tier.
E��Configure the security group for the application tier to allow inbound  HTTPS traffic from the security group for the web tier.

F��Configure the security group for the application tier to allow outbound  HTTPS traff ic and  Microsoft SQL Server traffic to the security group for the web tier.

�� ��ACE



���� �� Correct Answer: A, C,  E  Detailed  Explanation:  1. Option A  - The web tier (backe nd servers)  in  private subnets should only accept  HTTPS traffic from the ALB.  By  allo wing inbound  HTTPS traffic from the ALB's security group, this enforces that only the ALB can communicate with the web tier, following the principle of least privilege. 2. Option C - The database tier should allow  Microsoft SQL  Server traffic (port  1433  by default) exclusively from the application tier. Since the application tier processes busin

ess  logic and  needs to query the database, this  rule ensures the database only accep ts connections from authorized application servers. 3. Option  E  - The application tier  should accept inbound  HTTPS traffic from the web tier. The web tier forwards  user  re quests to the application tier for processing, so this rule enables secure communicati on between these layers while restricting access to only the web tier's security group. Why other options are incorrect: -  B: Allowing outbound  HTTPS to 0.0.0.0/0 is overly permissive. While the web tier might need internet access (e.g., for updates), a bette r  practice  is to  use a  NAT  Gateway with  restricted outbound  rules.  -  D/F:  Security  gro ups are stateful, so explicit outbound rules aren't needed for response traffic.  For exa mple, if the database tier  receives a  request from the application tier, the  return traffi c is automatically allowed without needing an outbound  rule targeting the application	tier.  Reference  Links:  -  [Security  group  rules](https://docs.aws.amazon.com/vpc/latest/u ser guide/VPC_Security groups.html) -  [Best  practices for security groups](https://docs.aw s.amazon.com/white papers/latest/best-practices-securing-your-aws-infrastructure/security -group-recommendations.html)


109.A company has  released a  new version of its  production application. The compan y's workload uses Amazon  EC2, AWS  Lambda, AWS  Fargate,  and Amazon  Sage maker. The company wants to cost optimize the workload now that usage is at a steady st ate. The company wants to cover the  most services with the fewest savings  plans. W hich combination of savings  plans will meet these  requirements? (Choose two.)

A�� Purchase an  EC2 Instance  Savings  Plan for Amazon  EC2  and Sage maker.

B�� Purchase a Compute Savings  Plan for Amazon  EC2,  Lambda,  and  Sage maker.

C�� Purchase a Sage maker Savings  Plan.

D�� Purchase a Compute Savings  Plan for  Lambda,  Fargate,  and Amazon  EC2.

E�� Purchase an  EC2 Instance  Savings  Plan for Amazon  EC2  and  Fargate.

�� ��CD



���� ��Correct Answer C.  Purchase a Sage maker Savings  Plan.  D.  Purchase  a Compute Savings  Plan for  Lambda,  Fargate,  and Amazon  EC2.  Detailed  Explanation  To cover th e  most services  (EC2,  Lambda,  Fargate,  Sage maker) with the fewest  Savings  Plans:  1.  Compute Savings  Plan  (Option  D)  applies to  EC2,  Lambda,  and  Fargate. It  offers flexib ility  because it applies to any  EC2  instance type,  Fargate task,  or  Lambda  usage,  rega rdless of size or family. 2. Sage maker Savings  Plan  (Option C)  is  required specifically f or Sage maker, as it��s  not covered  by Compute or  EC2 Instance  Savings  Plans.  Other options fail  because:  - A ��  E:  EC2 Instance  Savings  Plans  only apply to  specific  EC2  i nstance types,  not Sage maker/Fargate.  -  B: Compute  Savings  Plans don��t cover Sage Maker.  Reference  Links  -  [AWS  Compute Savings  Plans](https://aws.amazon.com/saving splans/compute-pricing/) -  [AWS Sage maker Savings  Plans](https://aws.amazon.com/sa v ings plans/sage maker-pricing/)


110.A company uses a  Microsoft SQL  Server database. The company's applications are connected to the database. The company wants to migrate to an Amazon Aurora  Po st gre sql database with minimal changes to the application code. Which combination of steps will meet these requirements? (Choose two.)

A�� Use the AWS Schema Conversion Tool (AWS SCT) to  rewrite the SQL queries  in th e applications.

B�� Enable  Babelfish on Aurora  Postgresql to  run the  SQL  queries from the applicatio

ns.

C��Migrate the database schema and data  by using the AWS Schema Conversion Tool (AWS SCT) and AWS  Database  Migration  Service  (AWS  DMS).
D�� Use Amazon  RDS  Proxy to  connect the applications to Aurora  Postgresql.

E��Use AWS  Database  Migration  Service  (AWS  DMS)  to  rewrite the  SQL queries  in the applications.
�� ��BC



���� �� Correct Answer  B.  Enable  Babelfish on Aurora  Postgresql to  run the  SQL queri es from the applications. C.  Migrate the database schema and data  by  using the AW S Schema Conversion Tool (AWS SCT) and AWS  Database  Migration  Service  (AWS  D MS).  Detailed  Explanation To  migrate a  Microsoft  SQL  Server database to Amazon Au rora  Postgresql with  minimal application code changes:  1. Option  B  uses  Babelfish fo r Aurora  Postgresql, which acts as a translation  layer. It allows Aurora  Postgresql to understand  Microsoft SQL Server��s T-SQL syntax (queries, stored  procedures, etc.). T his means applications originally written for SQL Server can  run on Aurora  Postgres q L without  rewriting  most SQL code. 2. Option C involves using AWS SCT (Schema Co n version Tool) to automatically convert the database schema (tables, data types) from SQL Server to  Postgresql format. Then, AWS  DMS  (Database  Migration Service)  han dles the actual data transfer from the old database to the new Aurora  Postgresql da

tabase. This combo ensures the schema and data are  migrated accurately. Why other options are wrong: - A ��  E: AWS SCT and  DMS do  not  rewrite application SQL que ries; they focus on schema/data  migration.  -  D:  RDS  Proxy  manages  database  connect ions (e.g.,  pooling, failover)  but doesn��t  help with  SQL Server-to-Postgresql compati bility.  Reference  Links  -  [AWS  Babelfish](https://aws.amazon.com/rds/aurora/babelfish/)
-  [AWS Schema Conversion Tool](https://aws.amazon.com/dms/schema-conversion-tool /) -  [AWS  Database  Migration  Service](https://aws.amazon.com/dms/)


111.A company wants to  use Amazon  Elastic Container  Service (Amazon  ECS) to  run  i ts on-premises application in a  hybrid environment. The application currently  runs on containers on premises. The company  needs a single container solution that can scale	in an on-premises,  hybrid, or cloud environment. The company  must  run  new applic ation containers in the AWS Cloud and  must use a  load  balancer for  HTTP traffic. Wh ich combination of actions will meet these  requirements? (Choose two.)

A��Set up an  ECS  c luster that  uses the AWS  Fargate  launch type  for the cloud applica tion containers.  Use an Amazon  ECS Anywhere external  launch type for the on-premis es application containers.

B��Set up an Application  Load  Balancer for  cloud  ECS  services. C��Set up a  Network  Load  Balancer  for cloud  ECS  services.

D��Set  up an  ECS  c luster that  uses the AWS  Fargate  launch  type.  Use  Fargate  for the cloud application containers and the on-premises application containers.

E��Set up an  ECS c luster that  uses the Amazon  EC2  launch type  for the cloud applica tion containers.  Use Amazon  ECS Anywhere with an AWS  Fargate  launch type for the on-premises application containers.
�� ��AB



���� �� Correct Answer A,  B  Detailed  Explanation The company  needs a  hybrid  setup w here Amazon  ECS  runs  both on-premises and  in the  cloud.  Here��s why A and  B  are correct:  1. Option A:  - AWS  Fargate  (serverless)  is  ideal for cloud  containers  because  i t  removes the  need to  manage  EC2  instances.  -  ECS Anywhere with  external  launch t ype allows  running containers on-premises  by connecting on-premises servers to  ECS clusters.  Fargate cannot  run on-premises, so  using  ECS Anywhere��s  external  launch ty pe (which uses on-premises infrastructure) is correct. 2. Option  B:  - Application  Load Balancer (ALB) is designed for  HTTP/HTTPS traffic. It supports advanced  routing  (like  path-based  routing) and  integrates seamlessly with  ECS services.  -  Network  Load  Bala ncer (NLB)  (Option C) is for  low-latency TCP/UDP traffic,  not  HTTP-specific features.
Why other options are wrong: -  D:  Fargate  cannot  run on-premises;  ECS Anywhere  is	required. -  E:  Fargate  is AWS-only  and cannot  be  used for on-premises containers.  - C:  NLB  is  not optimized for  HTTP traffic.  Reference  Links  -  [ECS Anywhere](https://do cs.aws.amazon.com/Amazon ecs/latest/developer guide/ecs-anywhere.html) -  [ALB vs  NL B](https://aws.amazon.com/elastic load balancing/features/)


112.A company wants to improve the availability and  performance of its  hybrid applic ation. The application consists of a stateful TCP-based workload hosted on Amazon  E C2 instances in different AWS  Regions and a stateless  UDP-based workload  hosted  o

n premises. Which combination of actions should a solutions architect take to improv e availability and  performance? (Choose two.)
A��Create an accelerator using AWS Global Accelerator. Add the load  balancers as en dpoints.

B��Create an Amazon Cloud Front distribution with an origin that uses Amazon  Route
53  latency-based  routing to  route  requests to the  load  balancers.

C��Configure two Application  Load  Balancers  in each  Region. The first will  route to th e  EC2 endpoints, and the second will  route to the on-premises endpoints.
D��Configure a  Network  Load  Balancer  in  each  Region  to address the  EC2 endpoints. Configure a  Network  Load  Balancer  in  each  Region that  routes to the  on-premises en dpoints.
E��Configure a  Network  Load  Balancer  in  each  Region to  address the  EC2  endpoints. Configure an Application  Load  Balancer  in each  Region that  routes to the  on-premise s endpoints.
�� ��AD



���� ��Correct Answer: AD  Detailed  Explanation: 1. Option A: AWS Global Accelerator - Why it's correct: AWS Global Accelerator improves availability and performance by di recting traffic through AWS's global network infrastructure. It  uses static anycast IPs,  r ed uci ng  latency  by  routing  users to the  nearest  healthy endpoint.  Since the applicatio n spans  multiple AWS  Regions and on-premises, Global Accelerator can efficiently  rou te traffic to the closest  EC2  instances  (TCP) and on-premises  UDP workloads via  Netw

ork  Load  Balancers  (NLBs).  -  Key  Point:  Global  Accelerator supports  both TCP and  UD P  protocols,  making  it  ideal for  hybrid  (AWS  +  on-premises)  setups. 2. Option  D:  Net work  Load  Balancers  (NLBs) for  EC2  and On-Premises  - Why  it's correct:  -  NLB for  EC 2 (TCP):  NLBs operate at  Layer 4  (TCP/UDP),  ideal  for stateful TCP workloads. They  ha ndle high throughput and  low  latency, ensuring seamless communication with  EC2  ins tances across  Regions.  -  NLB  for On-Premises (UDP):  NLBs  can  route traffic to on-pre mises servers  by  registering their IP addresses as targets. This is  possible via VPN or AWS  Direct Connect, enabling  hybrid connectivity.  NLBs  natively  support  UDP,  making	them suitable for stateless  UDP workloads.  -  Key  Point:  NLBs  are  protocol-specific  (T CP/UDP) and integrate with Global Accelerator to  route traffic efficiently. Why Other Options Are Incorrect: - Option  B: Cloud front  is a CDN for  HTTP(S) content,  not TCP/ UDP. - Option C/E: Application  Load  Balancers  (ALBs) work at  Layer  7  (HTTP/HTTPS) a nd don��t support  UDP or  raw TCP.  Reference  Links:  -  [AWS  Global Accelerator](https: //aws.amazon.com/global-accelerator/) -  [Network  Load  Balancer](https://aws.amazon.c om/elastic load balancing/network-load-balancer/)


113.A company hosts a  monolithic web application on an Amazon  EC2  instance. Appli cation users  have  recently  reported  poor  performance at specific times. Analysis of A mazon Cloud watch  metrics shows that CPU  utilization is  100% during the  periods of poor performance. The company wants to  resolve this  performance  issue and improve application availability. Which combination of steps will meet these requirements  MO ST cost-effectively? (Choose two.)

A��Use AWS Compute Optimizer to obtain a  recommendation for an instance type to scale vertically.

B��Create an Amazon  Machine Image  (AMI) from the web server.  Reference the AMI  i n a  new  launch template.
C��Create an Auto Scaling group and an Application  Load  Balancer to  scale vertically.

D��Use AWS Compute Optimizer to obtain a  recommendation for an instance type to scale  horizontally.
E��Create an Auto Scaling group and an Application  Load  Balancer to  scale  horizontal ly.
�� ��AE



���� �� Correct Answer A  E  Detailed  Explanation The  key  here  is to address  high CPU usage (100%) and improve availability cost-effectively. - Vertical Scaling (Option A): C ompute Optimizer recommends a  larger instance type. This immediately reduces CPU strain by upgrading the current instance (e.g., from t2.medium to t2.x large). It��s a qu ick,  low-effort fix for the  monolithic app without code changes.  -  Horizontal Scaling
(Option  E): Adding an Auto Scaling group and Application  Load  Balancer  (ALB) allows	the app to  handle traffic spikes  by distributing  load across  multiple  instances. This  i mproves availability (no single point of failure) and scales cost-effectively (add/remove		instances as  needed). Why  not Option  D?  Horizontal  scaling  (Option  E)  requires the app to support  multiple instances (stateless,  load-balanced).  For a  monolithic app, this might require refactoring. Option A provides an immediate, low-complexity fix, while Option  E prepares for future scalability. Cost-Effectiveness: - Vertical scaling (A) solve s the immediate CPU issue with  minimal changes.  -  Horizontal scaling  (E) adds  long-t erm resilience and pays only for what��s  used during spikes.  Reference  Links:  [AWS  C

ompute Optimizer](https://aws.amazon.com/compute-optimizer/)  [Auto Scaling Groups] (https://docs.aws.amazon.com/auto scaling/)  [Application  Load  Balancer](https://aws.ama zon.com/elastic load balancing/)


114.A  healthcare company is developing an AWS  Lambda function that  publishes  noti fications to an encrypted Amazon Simple  Notification Service (Amazon SNS) topic. Th e  notifications contain  protected  health  information  (PHI). The SNS topic uses AWS  K ey  Management Service  (AWS  KMS)  customer  managed  keys for encryption. The com pany must ensure that the application has the  necessary  permissions to  publish  mess ages securely to the SNS topic. Which combination of steps will meet these  require m ents? (Choose three.)

A��Create a  resource  policy for the SNS topic that allows the  Lambda function to  pub lish  messages to the topic.

B�� Use server-side encryption with AWS  KMS  keys  (SSE-KMS) for the  SNS topic  instea
d of customer  managed  keys.

C��Create a  resource  policy for the encryption  key that the  SNS topic  uses that  has t he necessary AWS  KMS  permissions.
D��Specify the  Lambda function's Amazon  Resource  Name  (ARN)  in the  SNS topic's  re source policy.
E��Associate an Amazon API Gateway  HTTP API with the  SNS topic to control access t
o the topic by using API Gateway  resource  policies.

F��Configure a  Lambda execution  role that  has the  necessary IAM  permissions to  use a customer managed  key  in AWS  KMS.
�� ��ACF



���� �� Correct Answer: A, C,  F  Detailed  Explanation:  1. A.  Create a  resource  policy for the SNS topic that allows the  Lambda function to  publish  messages to the topic. The SNS topic's resource policy must explicitly grant the  Lambda function  permission to publish messages. This is done by specifying the  Lambda function's execution  role AR N as a  principal  in the  policy and allowing the `sns:Publish` action. Without this, the  Lambda function cannot interact with the SNS topic. 2. C. Create a  resource  policy fo r the encryption  key that the SNS topic uses that  has the  necessary AWS  KMS  permi ssions. The  KMS customer-managed  key  must allow the  SNS service to  use it for encr yption. The key's  resource  policy should include  permissions  like `kms:Generated a take y` and `kms:Decrypt` for the SNS service  principal (`sns.amazon aws.com`). This ensures SNS can encrypt messages  before storing them. 3.  F. Configure a  Lambda execution role that  has the  necessary IAM  permissions to  use a customer  managed  key  in AWS KMS. The  Lambda function's execution role needs permissions to use the  KMS  key
(e.g., `kms:Generated at a key`, `kms:Encrypt`). This is because, when publishing to an en crypted SNS topic, AWS enforces that the  publisher (Lambda)  has  permissions to  use the  KMS  key for encryption, even though  SNS  handles the actual encryption. Why Ot her Options Are Incorrect:  -  B: The question specifies the  use of customer-managed  k eys, so switching to SSE-KMS (AWS-managed  keys)  is  irrelevant. -  D: While specifying	the  Lambda ARN  in the  SNS  resource  policy  is  required, this  is  implicitly part of con figuring the resource  policy  (Option A). Option A already covers the need to define t

he  principal  (Lambda ARN).  -  E: API Gateway  is  unrelated to the  scenario. The  Lambd a function publishes directly to SNS, so  no API Gateway  integration  is  needed.  Refere nce  Links: -  [SNS Access  Control](https://docs.aws.amazon.com/sns/latest/dg/sns-access -policy-use-cases.html) -  [KMS  Key  Policies  for SNS  Encryption](https://docs.aws.amazo n.com/sns/latest/dg/sns-key-management.html) -  [Lambda  Permissions for  KMS](https:/ /docs.aws.amazon.com/lambda/latest/dg/configuration-encryption.html)


115.A  media company has a  multi-account AWS environment in the us-east-1  Region. The company has an Amazon Simple  Notification Service (Amazon SNS) topic in a  p roduction account that  publishes  performance metrics. The company has an AWS  Lam bda function in an administrator account to  process and analyze  log data. The  Lambd a function that  is in the administrator account  must be  invoked  by  messages from th e SNS topic that is in the  production account when significant metrics are  reported. Which combination of steps will meet these  requirements?  (Choose two.)

A��Create an IAM  resource  policy for the  Lambda function that  allows Amazon SNS t
o invoke the function.

B��Implement an Amazon Simple Queue Service  (Amazon SQS) queue in t he ad ministr ator account to  buffer  messages from the SNS topic that  is in the  production accoun t. Configure the SQS queue to invoke the  Lambda function.
C��Create an IAM  policy for the SNS topic that allows the  Lambda function to subscri be to the topic.

D�� Use an Amazon  Event bridge  rule  in the  production account to capture the  SNS to pic notifications. Configure the  Event bridge  rule to forward  notifications to the  Lambd a function that  is in the administrator account.
E��Store  performance  metrics  in an Amazon S3  bucket in the  production account.  Use Amazon Athena to analyze the metrics from the administrator account.
�� ��AC



���� ��Correct Answer A, C  Detailed  Explanation To allow a  Lambda function in t he ad ministrator account to  be invoked  by an  SNS topic in the  production account across AWS accounts, you need two  key configurations:  1. Option A: The  Lambda function
must  have a  resource-based  policy  (IAM  resource  policy) that  explicitly grants  permiss ion to the SNS topic in the production account to invoke it. Without this  policy, cros s-account access to the  Lambda function is denied. 2. Option C: The SNS topic in the production account needs an IAM  policy that allows the  Lambda function (or its ass ociated role) in the administrator account to subscribe to the topic. This ensures the SNS topic accepts subscription  requests from the  Lambda function  in a different acco unt. Why other options are  incorrect:  - Option  B:  Using  SQS as a  buffer adds  unnece ssary complexity. SNS can directly invoke  Lambda across accounts  if permissions are c on figured  properly.  - Option  D:  Event bridge  is  not  required  here.  SNS  can directly tri gger  Lambda functions across accounts.  - Option  E:  S3 and Athena are  unrelated to  r eal-time invocation of Lambda via SNS.  Reference  Links  -  [AWS  Cross-Account  SNS to	Lambda](https://docs.aws.amazon.com/lambda/latest/dg/access-control-resource-based. html) -  [AWS  SNS Access Control](https://docs.aws.amazon.com/sns/latest/dg/sns-acces s-policy-use-cases.html)

116.A software company needs to upgrade a critical web application. The application currently runs on a single Amazon  EC2  instance that the company  hosts  in a  public  s ub net. The  EC2  instance  runs a  MySQL  database. The  application's  DNS  records  are  p ub lished in an Amazon  Route  53 zone. A solutions architect  must  reconfigure the app lication to  be scalable and  highly available. The solutions architect  must also  reduce MySQL  read  latency. Which combination of solutions will  meet these  requirements?  (C hoose two.)

A�� Launch a second  EC2  instance  in a second AWS  Region.  Use  a  Route  53  failover  r outing  policy to  redirect the traffic to the second  EC2  instance.

B��Create and configure an Auto Scaling group to  launch  private  EC2  instances  in  mul tiple Availability Zones. Add the instances to a target group behind a new Application	Load  Balancer.

C��Migrate the database to an Amazon Aurora  MySQL cluster. Create the  primary  DB instance and  reader  DB  instance  in separate Availability Zones.
D��Create and configure an Auto Scaling group to  launch  private  EC2  instances  in  mu ltiple AWS  Regions. Add the  instances to a target group  behind a  new Application  Lo ad  Balancer.
E�� Migrate the database to an Amazon Aurora  MySQL cluster with cross-Region  read replicas.

���� �� Correct Answer:  BC  Detailed  Explanation: Option  B  uses an Auto  Scaling group to automatically  launch  EC2  instances across  multiple Availability Zones  (AZs),  improvi ng scalability and availability.  Placing these  instances in  private sub nets behind an Ap plication  Load  Balancer  (ALB) ensures traffic  is distributed evenly and enhances securit y. This addresses the  need for a scalable and  highly available application layer. Optio n C  replaces the single  MySQL  instance with an Amazon Aurora  MySQL  cluster. Auror a  provides  built-in  high availability  by  replicating data across  multiple AZs. The  reader DB instance (read  replica)  handles  read  requests,  reducing  latency for  read operation s  by offloading them from the  primary instance. This directly solves the  MySQL  read  l atency issue while ensuring database  resilience. Why  Not Other Options?  - A:  Launchi ng a second  EC2  instance  in another AWS  Region with  Route  53 failover  is  overly co mplex for  high availability within a single  Region.  Multi-AZ  deployments  (as  in  B and C) are simpler and faster for failover. -  D: Auto  Scaling across  multiple  Regions  is  un necessary for typical  high-availability scenarios.  Multi-AZ within one  Region  is suffic ien t and  more cost-effective.  -  E: Cross-Region Aurora  replicas are for disaster  recovery, not reducing  read  latency  (which  is  better addressed  by  multi-AZ  read  replicas  as  in C).  Key Concepts  Simplified:  - Auto Scaling  +  Load  Balancer:  Automatically adds/remo ves  EC2  instances  based on traffic, spreading them across AZs for redundancy.  - Auro ra with  Read  Replicas: A  managed  database  service that  replicates data to a  read-onl y copy in another AZ, speeding up read operations and ensuring  backups.  Reference Links: -  [Auto  Scaling Groups](https://aws.amazon.com/auto scaling/)  -  [Aurora  High Av a il ability](https://aws.amazon.com/rds/aurora/high-availability/)

117.A company runs thousands of AWS  Lambda functions. The company  needs a solu tion to securely store sensitive information that all the  Lambda functions  use. The sol ution must also  manage the automatic  rotation of the sensitive  information. Which co m bination of steps will meet these requirements with the  LEAST operational overhead?	(Choose two.)

A��Create  HTTP  security  headers  by  using  Lambda@Edge to  retrieve  and create sensiti ve information

B��Create a  Lambda  layer that  retrieves sensitive  information C��Store sensitive information in AWS Secrets  Manager

D��Store sensitive information in AWS Systems  Manager  Parameter  Store

E��Create a  Lambda consumer with dedicated throughput to  retrieve sensitive  informa tion and create environmental variables
�� ��BC



���� �� Correct Answer C. Store sensitive information in AWS Secrets  Manager  B. Creat e a  Lambda  layer that  retrieves  sensitive information  Detailed  Explanation  For  securely	storing sensitive information and  managing automatic  rotation, AWS Secrets  Manage r (Option C) is the  best choice. Secrets  Manager  natively supports automatic cre denti al  rotation,  reducing operational overhead compared to Systems  Manager  Parameter  Store (Option  D), which  requires additional setup  (e.g., custom  Lambda functions) for rotation.  Using a  Lambda  layer  (Option  B)  allows all  Lambda functions to  share  reusa ble code for  retrieving secrets from Secrets  Manager. This avoids duplicating code acr

oss thousands of functions,  minimizing  maintenance. Other options are  less  ideal: -  P a rameter Store (Option  D)  lacks  built-in  rotation for  secrets.  -  Lambda@Edge  (Option A) is unrelated to secret  management. -  Lambda  consumer with dedicated throughput	(Option  E) adds unnecessary complexity.  Reference  Links  -  [AWS  Secrets  Manager](ht tps://aws.amazon.com/secrets-manager/) -  [Lambda  Layers](https://docs.aws.amazon.co m/lambda/latest/dg/configuration-layers.html)


118.A company is using an Amazon  Elastic  Ku bernet es  Service  (Amazon  EKS)  cluster. The company  must ensure that  Ku bernet es service accounts  in the  EKS  cluster  have  s ecure and granular access to specific AWS  resources  by  using IAM  roles for service a ccounts (IRSA). Which combination of solutions will meet these requirements? (Choos e two.)

A��Create an IAM  policy that defines the  required  permissions Attach the  policy direct ly to the IAM  role of the  EKS  nodes.

B��Implement network  policies within the  EKS cluster to  prevent  Ku bernet es  service ac counts from accessing specific AWS services.

C�� Modify the  EKS cluster's IAM  role to  include  permissions for  each  Ku bernet es servi ce account.  Ensure a one-to-one  mapping  between IAM  roles and  Ku bernet es  roles.
D��Define an IAM  role that  includes the  necessary  permissions. Annotate the  Ku bernet es service accounts with the Amazon  Resource name  (ARN) of the IAM  role.
E��Set up a trust  relationship  between the IAM  roles for the service accounts and an OpenID Connect (OI DC) identity  provider.

�� ��DE



���� �� Correct Answer  D,  E  Detailed  Explanation To  securely grant  Ku bernet es service  accounts in Amazon  EKS granular access to AWS  resources  using IAM  Roles for  Servi  ce Accounts (IRSA), you  need to:  1. Option  D  -  Define  an IAM  role with the  required permissions and annotate the  Ku bernet es service account with the IAM  role��s ARN. This  links the  Ku bernet es service account to the IAM  role,  allowing  pods using that s ervice account to assume the  role. 2. Option  E  -  Establish  a trust  relationship  betwee n the IAM  role and the  EKS  cluster��s OpenID Connect  (OI DC)  identity  provider. This  ensures only authorized  Ku bernet es service accounts  in the  EKS cluster can assume th e IAM  role, adding a security  layer. Why other options are  incorrect:  - A: Attaching  p olicies directly to  EKS  node  roles  grants  permissions to  all  pods on the  node,  not  spe cific service accounts (not granular). -  B:  Network  policies  control  network traffic,  not  AWS  resource  permissions.  - C:  Modifying the  EKS  cluster��s IAM  role would  grant  pe r missions to the cluster itself,  not  individual service accounts.  How I RSA works:  1.  EKS 	cluster is associated wit h an OI DC  provider. 2. An IAM  role  is created with a trust  p olicy allowing the OI DC  provider to grant the  role to  specific service accounts. 3.  Kub ernetes service accounts are annotated with the IAM  role��s ARN. 4.  Pods  using these accounts accounts automatically receive temporary AWS credentials via the role, ena bling secure, granular access.  References  Links  [AWS  IAM  Roles for Accounts Accounts] (https://docs.aws.amazon.com/eks/latest/user guide/iam-roles-for-service-accounts.html)
[EKS OIDC Identity  Provider](https://docs.aws.amazon.com/eks/latest/user guide/enable-i am-roles-for-service-accounts.html)

119.A company needs to design a  hybrid network architecture. The company's worklo ads are currently stored in the AWS C loud and i n on-premises data centers. The wor kloads  require single-digit latencies to communicate. The company uses an AWS Tran sit Gateway transit gateway to connect multiple VPCs. Which combination of steps wil
l meet these  requirements  MOST cost-effectively?  (Choose two.)

A�� Establish an AWS Site-to-Site VPN connection to each VPC.

B��Associate an AWS  Direct Connect gateway with the transit gateway that  is attache
d to the VPCs.

C�� Establish an AWS Site-to-Site VPN connection to an AWS  Direct Connect gateway.

D��Establish an AWS  Direct Connect connection.  Create a transit virtual  interface  (VIF) to a  Direct Connect gateway.
E��Associate AWS Site-to-Site VPN connections with the transit gateway that is attach ed to the VPCs.
�� ��BD



���� ��Correct Answer:  B,  D  Detailed  Explanation:  To achieve single-digit  latency for  hy b rid networks  between AWS and on-premises data centers, AWS  Direct  Connect is re qui red instead of VPN (which uses  public  internet and introduces  higher  latency).  Her e's why:  1. Option  B associates a  Direct  Connect Gateway with Transit Gateway, allowi ng centralized connectivity  between on-premises (via  Direct  Connect) and all VPCs att ached to Transit Gateway. This avoids  per-VPC configurations,  reducing costs.  2. Optio n  D creates a  Direct  Connect connection with a transit virtual  interface  (VIF), which co

nnects to the  Direct Connect Gateway. Transit VI Fs enable traffic  routing through the Direct Connect Gateway to multiple VPCs via Transit Gateway, eliminating the need fo r  multiple VI Fs/VPNs. Options A/C/E  rely on VPN  (higher  latency) or  inefficient  per-VP C setups.  B+D combine  Direct Connect's  low-latency dedicated connections with Trans it Gateway's  hub-and-spoke architecture for cost efficiency.  Reference  Links:  -  [AWS  D irect Connect  + Transit Gateway](https://docs.aws.amazon.com/white papers/latest/aws-v pc-connectivity-options/transit-gateway.html) -  [Direct Connect Transit VIF](https://docs. aws.amazon.com/directconnect/latest/User guide/create-vif.html)

120.A company is designing a web application with an internet-facing Application  Loa
d  Balancer  (ALB). The company needs the ALB to  receive  HTTPS web traffic from the public internet. The ALB  must send only  HTTPS traffic to the web application servers hosted on the Amazon  EC2  instances on  port 443. The ALB  must  perform a  health ch eck of the web application servers over  HTTPS on  port 8443. Which combination of c on figurations of the security group that is associated with the ALB will meet these re quirements? (Choose three.)

A��Allow  HTTPS inbound traffic from 0.0.0.0/0 for  port 443.

B��Allow all outbound traffic to 0.0.0.0/0 for  port 443.

C��Allow  HTTPS outbound traffic to the web application instances for  port 443.

D��Allow  HTTPS  inbound traffic from the web application instances for  port 443.

E��Allow  HTTPS outbound traffic to the web application instances for the  health check on  port 8443.

F��Allow  HTTPS  inbound traffic from the web application instances for the  health chec k on  port 8443.
�� ��ACE



���� �� Correct Answer A, C,  E  Detailed  Explanation  1. Option A: The ALB  must accept HTTPS traffic from the  public internet. Allowing inbound  HTTPS  (port 443) from `0.0.0. 0/0` ensures the ALB can  receive traffic from any client. 2. Option C: The ALB  needs t o forward  HTTPS traffic to the  EC2  instances on  port 443. An outbound  rule  allowing HTTPS (port 443) to the  EC2 instances ensures this communication. 3. Option  E: The ALB  performs  health checks over  HTTPS on  port 8443. An outbound  rule allowing  H TTPS (port 8443) to the  EC2  instances ensures the ALB can  reach the  health check en dpoint. Why  Not Other Options? -  B/D/F:  Security groups are stateful. If the ALB sen ds outbound traffic (C and  E),  return traffic  is automatically allowed, so  inbound  rules	(D/F) are unnecessary. Option  B  is overly  permissive  (allows outbound traffic to any destination,  not just  EC2  instances).  Reference  Links  -  [AWS  Security  Groups](https://d ocs.aws.amazon.com/vpc/latest/user guide/VPC_Security groups.html) -  [ALB  Health  Chec ks](https://docs.aws.amazon.com/elastic load balancing/latest/application/target-group-hea lth-checks.html)

121.A company hosts an application on AWS. The application gives users the ability t
o upload  photos and store the  photos  in an Amazon S3  bucket. The company wants to use Amazon Cloud Front and a custom domain name to  upload the photo files to the S3  bucket in the eu-west-1  Region. Which  solution will  meet these  requirements?	(Choose two.)

A�� Use AWS Certificate  Manager  (ACM) to create  a  public certificate in the  us-east-1 Region.  Use the certificate  in Cloud Front.

B��Use AWS Certificate  Manager  (ACM) to create a  public certificate  in eu-west-1.  Use the certificate in Cloud Front.

C��Configure Amazon S3 to allow uploads from Cloud Front. Configure S3 Transfer Acc el eration.
D��Configure Amazon S3 to allow uploads from Cloud Front origin access control (OA
C).

E��Configure Amazon S3 to allow uploads from Cloud Front. Configure an Amazon S3 website endpoint.
�� ��AD



���� �� Correct Answer: A,  D  Detailed  Explanation:  - A  (ACM  in  us-east-1) Cloud Front  r equires SSL/TLS certificates to  be created in the us-east-1  (N. Virginia)  region, even  if your S3 bucket or application is in another region like eu-west-1. Certificates created	in other regions (e.g., eu-west-1) won��t work with Cloud Front.  -  D  (S3  +  Cloud Front OAC) To securely allow Cloud Front to upload files to S3, you must use Origin Acces s Control (OAC). OAC  replaces the older Origin Access Identity  (OAI) and ensures that only Cloud Front can access the S3  bucket (blocking direct public access). You config ure S3  bucket  policies to grant access to Cloud Front via OAC. Why  not other options?
 -  B: ACM certificates for Cloud Front  must  be  in  us-east-1.  - C: Transfer Acceleration  is unrelated to Cloud Front  integration  (it��s for faster direct  uploads to S3).  -  E:  S3 we bsite endpoints are for  hosting static websites,  not secure uploads via Cloud Front.  Ref

erence  Links: -  [ACM  +  Cloud front](https://docs.aws.amazon.com/Amazon cloud front/la test/Developer guide/cnames-and-https-requirements.html#https-requirements-aws-regio n) -  [Cloud front OAC](https://docs.aws.amazon.com/Amazon cloud front/latest/Develope rGuide/private-content-restricting-access-to-s3.html#oac-iam-policies)


122.A company recently  launched a  new application for its customers. The application	runs on multiple Amazon  EC2  instances across two Availability Zones.  End  users  use TCP to communicate with the application. The application must  be  highly available an d  must automatically scale as the  number of users  increases. Which combination of st eps will meet these requirements  MOST cost-effectively?  (Choose two.)
A��Add a  Network  Load  Balancer  in  front of the  EC2  instances.

B��Configure an Auto Scaling group for the  EC2  instances.

C��Add an Application  Load  Balancer  in front of the  EC2  instances.

D�� Manually add  more  EC2  instances for the application.

E��Add a Gateway  Load  Balancer  in front  of the  EC2  instances.

�� ��AB



���� ��Correct Answer: A,  B  Detailed  Explanation: To achieve  high availability and auto- scaling for a TCP-based application on  EC2, the  most cost-effective solution  is:  1.  Net work  Load  Balancer  (NLB)  (Option A)  -  NLB  operates  at  Layer 4  (TCP/UDP),  making  it	ideal for non-HTTP applications  like this TCP-based service. - It distributes traffic acr oss  EC2  instances  in  multiple Availability Zones  (AZs), ensuring  high availability.  -  NLB

s are optimized for performance and cost for TCP workloads compared to Application		Load  Balancers  (ALBs), which are  better  suited for  HTTP/HTTPS  (Layer 7). 2. Auto Sca ling Group (Option  B)  - Auto Scaling automatically adjusts the  number of  EC2  instanc es  based on demand  (e.g., CPU usage or  network traffic).  - It  maintains availability  by	replacing failed instances and scaling out during traffic spikes,  reducing costs  by scal ing in during low demand. -  Manual scaling  (Option  D)  is  inefficient  and doesn��t  me et the automatic  requirement. Why  not other options? - Option C  (ALB): ALB  is for  H TTP/HTTPS traffic,  not  raw TCP, so it��s  unnecessary  here.  - Option  E  (Gateway  Load Balancer):  Designed for  routing traffic through third -  party security appliances  (e.g., fi rewalls),  not general  -  purpose  load  balancing.  -  Option  D  (Manual  scaling):  Not auto matic, so  it fails the  requirement.  Reference  Links:  -  [Network  Load  Balancer](https://a ws.amazon.com/elastic load balancing/network-load-balancer/) -  [Auto Scaling Groups](ht tps://docs.aws.amazon.com/auto scaling/ec2/user guide/auto-scaling-groups.html)


123.A company is designing the architecture for a  new  mobile app that  uses the AW S Cloud. The company uses organizational units (OUs) in AWS Organizations to  mana ge its accounts. The company wants to tag Amazon  EC2  instances with data sensitiv it y  by  using values of sensitive and  non sensitive. IAM  identities  must  not  be able to d elete a tag or create  instances without a tag. Which combination of steps will  meet t hese requirements? (Choose two.)

A��In Organizations, create a  new tag  policy that  specifies the data  sensitivity tag  key and the required values.  Enforce the tag values for the  EC2  instances. Attach the tag policy to the appropriate OU.

B��In Organizations, create a  new service control  policy (SCP) that specifies the data s en sitivity tag  key and the  required tag values.  Enforce the tag values for the  EC2  inst ances. Attach the SCP to the appropriate OU.
C��Create a tag  policy to deny  running  instances when a tag  key  is  not  specified. Cre ate another tag  policy that  prevents  identities from deleting tags. Attach the tag  poli cies to the appropriate OU.
D��Create a service control  policy (SCP) to deny creating  instances when a tag  key  is not specified. Create another SCP that  prevents identities from deleting tags. Attach t he SCPs to the appropriate OU.
E��Create an AWS Config  rule to check  if EC2  instances use the data sensitivity tag a nd the specified values. Configure an AWS  Lambda function to delete the  resource  if a  noncompliant resource  is found.
�� ��AD



���� ��Correct Answer A,  D  Explanation The  requirements  have two  parts:  (1) enforce t agging with specific values (sensitive or  non sensitive) on  EC2  instances, and  (2)  preve nt IAM identities from deleting tags or creating untagged instances. - Option A uses a Tag  Policy  (specific to AWS Organizations) to enforce the tag  key `data  sensitivity`  and its allowed values (`sensitive`/`non sensitive`). Tag  Policies are  designed to standar dize tags across accounts in an OU. - Option  D  uses  Service Control  Policies  (SCPs) t o enforce  permissions:  - The first SCP denies `ec2:Run instances` if the  required tag  is missing, ensuring instances cannot  be created without the tag.  - The second SCP den ies `ec2:Delete tags` to  prevent tag deletion. SCPs control  permissions at the OU  level,

 ensuring all accounts in the OU comply. Other options fail  because: -  B/C  mix SCPs    and Tag  Policies incorrectly (SCPs don��t enforce tag values directly; Tag  Policies can�� t  block API actions).  -  E  uses AWS Config for  reactive compliance checks,  not  proacti    ve  prevention.  Reference  Links  -  [Tag  Policies](https://docs.aws.amazon.com/organizati o  ns/latest/user guide/orgs_manage_policies_tag-policies.html) -  [SCPs for Tag  Enforce men   t](https://docs.aws.amazon.com/organizations/latest/user guide/orgs_manage_policies_scp   s_examples.html#example_scp_tags)


124.A company uses AWS Systems  Manager for  routine  management and  patching of Amazon  EC2 instances. The  EC2  instances are in an IP address type target group be hind an Application  Load  Balancer  (ALB).  New  security  protocols  require the  company	to remove  EC2  instances from service during a  patch. When the company attempts t o follow the security protocol during the  next  patch, the company  receives errors dur ing the patching window. Which combination of solutions will resolve the errors? (Ch oose two.)
A��Change the target type of the target group from IP address type to instance type.

B��Continue to  use the existing Systems  Manager document without changes  because it is already optimized to  handle  instances that are  in an IP address type target grou p  behind an ALB.

C��Implement the AWSEC2-Patch Load Balan acer instance Systems  Manager Automation document to  manage the  patching  process.
D��Use Systems  Manager  Maintenance Windows to automatically  remove the  instances from service to  patch the  instances.

E��Configure Systems  Manager State  Manager to  remove the  instances from  service a nd manage the patching schedule. Use ALB  health checks to  re-route traffic.
�� ��CD



���� �� Correct Answer: C,  D  Explanation: The errors occur because the  EC2  instances i n the ALB's IP-based target group are  not  properly  removed during  patching.  Here's why C and  D work:  - C:  Use the AWSEC2-Patch Load Balancer instance Automation doc ument This Systems  Manager Automation document  is specifically designed to  patch  i n stances  behind a  load  balancer. It automatically de registers  instances from the ALB t arget group before  patching and re-registers them afterward. This ensures traffic stop s flowing to the instance during patching, complying with security  protocols.  -  D:  Use Systems  Manager  Maintenance Windows  Maintenance Windows allow you to sched ul e  patching tasks. When combined with the Automation document (C),  it ensures insta nces are  removed from the ALB  before  patching  begins. Without this,  instances  might	still receive traffic during  patching, causing errors. Why  not other options? - A: Switc hing to  instance type target groups avoids IP changes, but it doesn��t solve the core issue of de registering instances during patching. -  B: The default  Systems  Manager d ocument isn��t optimized for ALB  integration, so  it won��t deregister instances.  -  E:  St ate  Manager  manages configurations,  not  patching schedules. ALB  health checks alon e are too slow to reroute traffic during a  patching window.  Reference:  -  [AWSEC2-Pat ch loadbalancer instance Automation  Document](https://docs.aws.amazon.com/systems- manager-automation-run books/latest/user guide/automation-aws-patch-load-balancer-ins tance.html) -  [Systems  Manager  Maintenance Windows](https://docs.aws.amazon.com/s ystems-manager/latest/user guide/systems-manager-maintenance.html)

125.A company runs  multiple workloads on virtual  machines  (VMs)  in an on-premises data center. The company is expanding rapidly. The on-premises data center is  not a ble to scale fast enough to  meet  business  needs. The company wants to  migrate the workloads to AWS. The migration is time sensitive. The company wants to use a  lift- and-shift strategy for non-critical workloads. Which combination of steps will meet th ese  requirements? (Choose three.)
A�� Use the AWS Schema Conversion Tool (AWS SCT) to collect data about the VMs.

B�� Use AWS Application  Migration Service. Install the AWS  Replication Agent on the
VMs.

C��Complete the initial  replication of the VMs.  Launch test  instances to  perform accep tance tests on the VMs.
D��Stop all operations on the VMs.  Launch a cutover  instance.

E�� Use AWS App2Container (A2C) to collect data about the VMs.

F�� Use AWS  Database  Migration  Service  (AWS  DMS)  to  migrate the VMs.

�� ��BCD



���� �� Correct Answer  B, C,  D  Detailed  Explanation  1.  Option  B:  Use AWS Application Migration Service. Install the AWS  Replication Agent on the VMs.  - AWS Application Migration Service (MGN)  is designed for  lift-and-shift  migrations. It  replicates VMs (or physical servers) to AWS with minimal downtime. - Installing the AWS  Replication Ag ent on the VMs allows continuous replication of data from the on-premises environm ent to AWS. This ensures the workloads are  ready for testing and cutover. 2. Option

C: Complete the initial replication of the VMs.  Launch test  instances to  perform accep tance tests.  - After  replication, you  must test the  migrated workloads in AWS to ensu re they function correctly. This is a critical step to validate the  migration  before final cutover. - Testing  minimizes  risks  (e.g., configuration errors) and ensures  business con tinuity. 3. Option  D:  Stop all operations on the VMs.  Launch a cutover  instance.  -  Dur ing the final cutover, you stop operations on the source VMs to ensure data consiste ncy  between the on-premises environment and AWS.  -  Launching a cutover  instance  i n AWS finalizes the migration,  redirecting traffic to the cloud-based workload. Why O ther Options Are Incorrect: - Option A (AWS SCT): The AWS Schema Conversion Tool	converts database schemas (e.g., Oracle to Amazon Aurora). It��s irrelevant for VM m i gration. - Option  E  (AWS App2Container): App2Container  modernizes applications  by container iz ing them. This isn��t  lift-and-shift.  - Option  F  (AWS  DMS): AWS  Database Migration Service  migrates databases,  not entire VMs.  Reference  Links  -  [AWS  Applica tion  Migration Service](https://aws.amazon.com/application-migration-service/) -  [AWS Migration Best  Practices](https://aws.amazon.com/blogs/aws-best-practices/migration-be st-practices/)


126.A company hosts an application in a  private subnet. The company  has already int egrated the application with Amazon Cognito. The company uses an Amazon Cognito user pool to authenticate users. The company needs to  modify the application so th e application can securely store user documents in an Amazon S3 bucket. Which com bination of steps will securely integrate Amazon S3 with the application? (Choose tw o.)

A��Create an Amazon Cognito identity  pool to generate secure Amazon S3 access tok ens for users when they successfully log in.

B��Use the existing Amazon Cognito user  pool to generate Amazon S3 access tokens for users when they successfully log in.

C��Create an Amazon S3 VPC endpoint in the same VPC where the company hosts th e application.
D��Create a  NAT gateway  in the VPC where the company  hosts the application. Assig n a  policy to the  S3  bucket to deny any  request that  is  not  initiated from Amazon C ognito.
E��Attach a  policy to the S3  bucket that allows access only from the  users' IP address

es.

�� ��AC



���� ��Correct Answer: A, C  Explanation: To securely integrate Amazon S3 with an appl ication in a  private subnet using Amazon Cognito:  1. Option A  (Create a Cognito Ide ntity  Pool):  - Amazon Cognito  User  Pools  handle  user authentication  (like  login/passw ord). - Identity  Pools  (now called Identity  Pools with  User  Pools)  generate temporary AWS credentials (e.g., S3 access tokens) for authenticated users. This allows users to i nteract with AWS services  like S3 securely without  hardcoding credentials. 2. Option C	(S3 VPC  Endpoint):  - A VPC endpoint enables  private, secure communication  betwee n the application (in the  private subnet) and S3 without exposing traffic to the  public	internet. This avoids  needing a  NAT  gateway  (which  is costlier and  less secure for S3 access). Why other options are incorrect: -  B:  User  Pools alone cannot generate AWS

 credentials for S3; Identity  Pools are  required.  -  D:  A  NAT  gateway  isn��t  needed  for S3 if a VPC endpoint  is used. Also, S3  bucket  policies cannot validate  requests  initiate d from Cognito directly. -  E:  Restricting  access  by IP addresses  is  impractical  (users�� I Ps change) and insecure compared to IAM  roles  + VPC  endpoints.  References:  -  [Ama zon Cognito Identity  Pools](https://docs.aws.amazon.com/cognito/latest/developer guide /identity-pools.html) -  [AWS  S3 VPC  Endpoints](https://docs.aws.amazon.com/AmazonS 3/latest/user guide/private link-interface-endpoints.html)
�𰸣�C



�𰸣�D



�𰸣�C



�𰸣�C



�𰸣�C



instances.



cluster.



�� ��D



�� ��AC



�� ��CD



�� ��AD



�� ��DE



�� ��BC



